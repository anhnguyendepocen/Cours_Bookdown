
\begin{frame}
\begin{Large}
\begin{center}
Linear Regressions

and Ordinary Least Squares (OLS)
\end{center}
\end{Large}
\end{frame}

\begin{frame}
\begin{scriptsize}

\begin{exmpl}[Infant mortality rate and GDP/capita]
		\begin{figure}
			\caption{Infant mortality rate (deaths per 1000 births) and GDP/capita}
			\includegraphics[width=.95\linewidth]{../../figures/Figure_OLS_infantmortality_all3.pdf}
			\label{fig:infantmortality111}
			
		\begin{tiny}
		Source of the data \href{https://vincentarelbundock.github.io/Rdatasets/doc/car/UN.html}{United Nations (1998) Social indicators}.
		\end{tiny}

		\end{figure}
\end{exmpl}
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}

\begin{exmpl}[Income, Seniority and Education]\label{exmpl:income}

		\begin{figure}
		\caption{Income, Seniority and Education}

			\includegraphics[width=.95\linewidth]{../../figures/Figure_3d_income.pdf}
			
			\label{fig:3dIncome}
		\begin{tiny}
		Source of the data \href{http://www-bcf.usc.edu/~gareth/ISL/data.html}{An Introduction to Statistical Learning, with applications in R},  (Springer, 2013)
		
		with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani.
		\end{tiny}

		\end{figure}
\end{exmpl}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{exmpl}[Advertising and Sales]\label{exmpl:advertising}

		\begin{figure}
		\caption{Advertising and Sales}

			\includegraphics[width=.95\linewidth]{../../figures/Figure_advertising.pdf}
			
			\label{fig:3dIncome}
		\begin{tiny}
		Source of the data \href{http://www-bcf.usc.edu/~gareth/ISL/data.html}{An Introduction to Statistical Learning, with applications in R},  (Springer, 2013)
		
		with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani.
		\end{tiny}

		\end{figure}
\end{exmpl}
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}
\begin{equation}
\boxed{\underbrace{y_i}_{\mbox{\tiny regressand}} = \underbrace{x_{i,1}}_{\begin{array}{c}\mbox{\tiny first}\\ \mbox{\tiny regressor} \end{array}} \times \beta_1 + \dots + \underbrace{x_{i,K}}_{\begin{array}{c}\mbox{\tiny $k^{th}$}\\ \mbox{\tiny regressor} \end{array}} \times \beta_K + \underbrace{\varepsilon_i}_{\mbox{\tiny disturbance}}}\label{eq:context}
\end{equation}
\begin{itemize}
	\item Using the notation $\bv{x}_i = [x_{i,1},\dots,x_{i,K}]'$, we get:
	$$
	\underset{scalar}{y_i} = \bv{x}_i' \underset{(K \times 1)}{\boldsymbol\beta} + \underset{scalar}{\varepsilon_i}.
	$$
	\item $\boldsymbol\beta$ is the vector of unknown parameters.
	\item Regressors are also called independent variables, explicative variables, or covariates.
	\item We have a sample of $n$ observations for $y_i$ and for $\bv{x}_i$.
\end{itemize}
\begin{remark}
\begin{itemize}
	\item Important to distinguish between {\color{blue} population quantities ($\boldsymbol\beta$ and $\varepsilon_i$)} and  {\color{red} their sample estimates (denoted by $\bv{b}$ and $e_i$)}.
	\item In general we will have:
	$$
	y_i = \bv{x}_i' \boldsymbol\beta + \varepsilon_i = \bv{x}_i' \bv{b} +e_i \quad \mbox{but} \quad \boldsymbol\beta \ne \bv{b}  \quad \mbox{and} \quad \varepsilon_i \ne e_i.
	$$
\end{itemize}
\end{remark}

\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}
\begin{alertblock}{Context and Objective}
\begin{itemize}
	\item \textbf{Context}: Eq.\,(\ref{eq:context}).
	\item \textbf{Objective}: Estimate $\boldsymbol\beta$ and test hypotheses based on this vector.
\end{itemize}
\end{alertblock}

\vspace{1cm}

\begin{exampleblock}{Notations}

We will use the notations $\bv{y}$, $\boldsymbol\varepsilon$ and $\bv{X}$, where:
	$$
	\underset{(n \times 1)}{\bv{y}} = \left[ \begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n \end{array}\right], \quad
	\underset{(n \times 1)}{\boldsymbol\varepsilon} = \left[ \begin{array}{c} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{array}\right] \quad \mbox{and} \quad \underset{(n \times K)}{\bv{X}} = \left[ \begin{array}{c} \bv{x}_1' \\ \bv{x}_2' \\ \vdots \\ \bv{x}_n' \end{array}\right].
	$$
\end{exampleblock}


\end{scriptsize}
\end{frame}

\subsection{Assumptions}

\begin{frame}{Assumptions of the classical linear regression model}
\begin{scriptsize}

%\begin{exampleblock}{Linear vs. nonlinear model}
%In our model (Eq.\,\ref{eq:context}), the relationship between $y$ and $x_1$, ..., $x_K$ is linear:
%$$
%y_i = x_{i,1} \beta_1 + \dots + x_{i,K} \beta_K + \varepsilon_i
%$$
%Examples of non-linear relationships:
%$y_i = \beta_1 \times x_{i,1} \times \varepsilon_i$ or $y_i = \beta_1 x_{i,1}^{\beta_2} + \varepsilon_i$. (See also Fig. \ref{fig:infantmortality111} or \ref{fig:infantmortality122})
%\end{exampleblock}

\begin{assum}[Full rank]\label{Assum:fullrank}
There is no exact linear relationship among the independent variables (the $x_{i}$s).
\end{assum}
\begin{exampleblock}{Example of exact linear relationship among the $x_{i}$s}
$x_{i,1} = 3 x_{i,2} - x_{i,5}$.
\end{exampleblock}

\vspace{.3cm}

\begin{assum}[The conditional mean-zero assumption]\label{Assum:exogeneity}
$$
\mathbb{E}(\boldsymbol\varepsilon|\bv{X}) = 0.
$$
%i.e., the independent variables do not allow to predict the $\varepsilon_i$s.
\end{assum}
%\begin{exampleblock}{Counter-example}
%$x_{i} = f_i(\varepsilon_i)$.
%\end{exampleblock}
\begin{exampleblock}{Implications}
Under Assumption \ref{Assum:exogeneity}:
\begin{itemize}
\item[i)] $\mathbb{E}(\varepsilon_{i})=0$;
\item[ii)] the $x_{ij}$s and the $\varepsilon_{i}$s are uncorrelated, i.e. $\forall i,\,j \quad \mathbb{C}orr(x_{ij},\varepsilon_{i})=0$.
\end{itemize}
\end{exampleblock}
\begin{tiny}
\begin{proof} i) By the law of iterated expectations (Prop.\,\ref{prop:lawiteratedexpect}):
$$
\mathbb{E}(\boldsymbol\varepsilon)=\mathbb{E}(\mathbb{E}(\boldsymbol\varepsilon|\bv{X}))=\mathbb{E}(0)=0.
$$
ii) $\mathbb{E}(x_{ij}\varepsilon_i)=\mathbb{E}(\mathbb{E}(x_{ij}\varepsilon_i|\bv{X}))=\mathbb{E}(x_{ij}\underbrace{\mathbb{E}(\varepsilon_i|\bv{X})}_{=0})=0$.\qed
\end{proof}
\end{tiny}
\end{scriptsize}
\end{frame}

%\begin{frame}
%\begin{scriptsize}
%\begin{assum}[Sampling assumption for the $(y_i,\bv{x}_{i})s$]\label{Assum:independence}
%The $(y_i,x_{i,1},\dots,x_{i,K})$, for $i \in \{1,\dots,n\}$s are independently distributed.
%\end{assum}
%
%\begin{exampleblock}{Typical counterexamples}
%\begin{itemize}
%	\item Observation $i$ depends on the realisation of observation $i-1$.
%	\item For some regressor $k$, the $x_{i,k}$s are correlated.
%\end{itemize}
%\end{exampleblock}
%
%\begin{exampleblock}{Assumption \ref{Assum:independence} versus Assumption \ref{Assum:exogeneity}}
%If $\varepsilon_i \sim \mathcal{N}(0,g(\bv{x}_{j}))$ for some $i \ne j$, Assumption \ref{Assum:exogeneity} is verified but not Assumption \ref{Assum:independence}.
%\end{exampleblock}
%
%\begin{exampleblock}{Assumption \ref{Assum:independence}, Assumption \ref{Assum:exogeneity} and exogeneity}
%The explanatory variables are said to be {\color{red}strictly exogenous} if $\mathbb{E}(\varepsilon_i|\bv{x}_1,\dots,\bv{x}_n)=0$. This is the case if  Assumption \ref{Assum:exogeneity} and Assumption \ref{Assum:independence} are satisfied.
%\end{exampleblock}
%\begin{tiny}
%\begin{proof} Under Assumption \ref{Assum:independence}, we have $\mathbb{E}(\varepsilon_i|\bv{x}_1,\dots,\bv{x}_n)=\mathbb{E}(\varepsilon_i|\bv{x}_i)$ (by the law of iterated expectations, Prop.\,\ref{prop:lawiteratedexpect}). Under Assumption \ref{Assum:exogeneity}, $\mathbb{E}(\varepsilon_i|\bv{x}_i)=0$.\qed
%\end{proof}
%\end{tiny}
%\end{scriptsize}
%\end{frame}

\begin{frame}
\begin{scriptsize}

\begin{assum}[Homoskedasticity]\label{Assum:homoskedasticity}
$$
\forall i, \quad \mathbb{V}ar(\varepsilon_i|\bv{X}) = \sigma^2.
$$
\end{assum}

		\begin{figure}
		\caption{Heteroskedasticity (left) and homoskedasticity (right)}
			\includegraphics[width=.95\linewidth]{../../figures/Figure_OLS_heterosk.pdf}
			
					\begin{tiny}
$X_i \sim \mathcal{N}(0,1)$ and $\varepsilon^*_i \sim \mathcal{N}(0,1)$.

Left-hand side (heteroskedasticity): $Y_i = 2 + 2X_i + \left(2\mathds{1}_{\{X_i<0\}}+0.2\mathds{1}_{\{X_i\ge0\}}\right)\varepsilon^*_i$.

Right-hand side (homoskedasticity): $Y_i = 2 + 2X_i + \varepsilon^*_i$.

		\end{tiny}

		\end{figure}
	
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}
		\begin{figure}
		\caption{Salaries versus years since end of PhD}
			\includegraphics[width=.95\linewidth]{../../figures/Figure_OLS_heterosk2.pdf}
			
					\begin{tiny}
		Data from \href{https://vincentarelbundock.github.io/Rdatasets/doc/car/Salaries.html}{Fox J. and Weisberg, S. (2011)}.
		\end{tiny}

		\end{figure}

\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}

\begin{assum}[Non-correlated residuals]\label{Assum:noncorrel_resid}
\begin{center}
$\forall i \ne j, \quad \mathbb{C}ov(\varepsilon_i,\varepsilon_j|\bv{X})=0.$
\end{center}
\end{assum}

\begin{remark}[Implication of \ref{Assum:homoskedasticity} and \ref{Assum:noncorrel_resid}]\label{remark:Sigma}
If \ref{Assum:homoskedasticity} and \ref{Assum:noncorrel_resid} hold, then:
$$
\mathbb{V}ar(\boldsymbol\varepsilon|\bv{X})= \sigma^2 Id,
$$
where $Id$ is the $n \times n$ identity matrix.
\end{remark}

\vspace{.5cm}

\begin{assum}[Normal distribution]\label{Assum:normality}
\begin{center}
$\forall i, \quad \varepsilon_i \sim \mathcal{N}(0,\sigma^2).$
\end{center}
\end{assum}

\begin{exampleblock}{About normality}
As we will see later (Slide \ref{slide:largesplm}), even if this assumption is not satisfied, the normal distribution will show up in asymptotic results (by virtue of the Central Limit Theorem \ref{thm:LindbergLevyCLT}).
\end{exampleblock}
\end{scriptsize}
\end{frame}


\subsection{OLS}


\begin{frame}{The least square coefficient vector}
\begin{scriptsize}
\begin{itemize}
	\item For a given vector of coefficients $\bv{b}$, the sum of square residuals is:
	$$
	f(\bv{b}) =\sum_{i=1}^n \left(y_i - \sum_{j=1}^K x_{i,j} b_j \right)^2 = \sum_{i=1}^n (y_i - \bv{x}_i' \bv{b})^2
	$$
	\item Minimizing the sum of squared residuals amounts to minimizing:
	$$
	f(\bv{b}) = (\bv{y} - \bv{X}\bv{b})'(\bv{y} - \bv{X}\bv{b}).
	$$
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{itemize}
	\item We have (see Def.\,\ref{def:FOD} for the computation of $\frac{\partial f}{\partial \bv{b}}(\bv{b})$):
	$$
	\frac{\partial f}{\partial \bv{b}}(\bv{b}) = - 2 \bv{X}'\bv{y} + 2 \bv{X}'\bv{X}\bv{b}.
	$$
	\item Necessary first-order condition (FOC):
	\begin{equation}\label{eq:OLS_FOC}
	\boxed{\bv{X}'\bv{X}\bv{b} = \bv{X}'\bv{y}.}
	\end{equation}
	\item Under Assumption \ref{Assum:fullrank}, $\bv{X}'\bv{X}$ is invertible. Hence:
	$$
	\boxed{\bv{b} = (\bv{X}'\bv{X})^{-1} \bv{X}'\bv{y}.}
	$$
	\item $\bv{b}$ minimises the sum of squared residuals. ($f$ is a non-negative quadratic function, it admits a minimum.)
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{exmpl}[Income, Seniority and Education (Example \ref{exmpl:income})]
		\begin{figure}
		\caption{Income, Seniority and Education}
			\includegraphics[width=.9\linewidth]{../../figures/Figure_3d_income2.pdf}
			\label{fig:3dIncome}
		\end{figure}
\input{../../Rcode/tables/outfile_income3d.txt}
\end{exmpl}
\end{scriptsize}
\end{frame}




\begin{frame}\label{slide:residmatrix}
\begin{scriptsize}
\begin{itemize}
	\item The estimated residuals are:
	$$
	\bv{e} = \bv{y} - \bv{X} (\bv{X}'\bv{X})^{-1} \bv{X}' \bv{y} = \bv{M} \bv{y}
	$$
	where $\bv{M} := \bv{I} - \bv{X} (\bv{X}'\bv{X})^{-1} \bv{X}'$ is called the {\color{blue} residual maker} matrix.
	\item We have $\bv{M} \bv{X} = \bv{0}$: if one regresses one of the explanatory variables on $\bv{X}$, the residuals are null.
	\item We have $\bv{M}\bv{y}=\bv{M}\boldsymbol\varepsilon$ (because $\bv{y} = \bv{X}\boldsymbol\beta + \boldsymbol\varepsilon$ and $\bv{M} \bv{X} = \bv{0}$).
	\item The fitted values are:
	$$
	\hat{\bv{y}}=\bv{X} (\bv{X}'\bv{X})^{-1} \bv{X}' \bv{y} = \bv{P} \bv{y},
	$$
	where $\bv{P}=\bv{X} (\bv{X}'\bv{X})^{-1} \bv{X}' $ is a {\color{blue}projection matrix}. ($\hat{\bv{y}}$ is the projection of the vector $\bv{y}$ onto the vectorial space spanned by the columns of $\bv{X}$.)
\end{itemize}
\begin{remark}
\begin{itemize}
	\item It can be shown that each column $\tilde{\bv{x}}_k$ of $\bv{X}$ is orthogonal to $\bv{e}$.
	\item[$\Rightarrow$] If intercepts are included in the regression ($x_{i,1} \equiv 1$), the average of the residuals is null.
\end{itemize}
\end{remark}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{block}{Properties of $\bv{M}$ and $\bv{P}$}
\begin{itemize}
	\item $\bv{M}$ is symmetric ($\bv{M} = \bv{M}'$) and {\color{blue}idempotent} ($\bv{M} = \bv{M}^2 = \bv{M}^k$ for $k>0$).
	\item $\bv{P}$ is symmetric and idempotent.
	\item $\bv{P}\bv{X} = \bv{X}$.
	\item $\bv{P} \bv{M} = \bv{M} \bv{P} = 0$.
	\item $\bv{y} = \bv{P}\bv{y} + \bv{M}\bv{y}$ (two orthogonal parts).
\end{itemize}
\end{block}
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}
\begin{prop}[Properties of the OLS estimator]\label{thm:propOLS}
\begin{itemize}
	\item[(i)] Under Assumptions \ref{Assum:fullrank} and \ref{Assum:exogeneity}, the OLS estimator is {\color{blue} linear} and {\color{blue}unbiased}.
	\item[(ii)] Under Assumptions \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}, the conditional covariance matrix of $\bv{b}$ is: $\mathbb{V}ar(\bv{b}|\bv{X}) = \sigma^2 (\bv{X}'\bv{X})^{-1}$.
\end{itemize}
\end{prop}
	\begin{tiny}
	\begin{proof} Under \ref{Assum:fullrank}, $\bv{X}'\bv{X}$ can be inverted. We have:
	$$
	\bv{b} = (\bv{X}'\bv{X})^{-1} \bv{X}'\bv{y} = \boldsymbol\beta + (\bv{X}'\bv{X})^{-1} \bv{X}' \bv{\varepsilon}.
	$$
	\begin{itemize}
	\item[(i)] Let us consider the expectation of the last term, i.e. $\mathbb{E}((\bv{X}'\bv{X})^{-1} \bv{X}' \bv{\varepsilon})$. Using the law of iterated expectations (Prop.\,\ref{prop:lawiteratedexpect}), we obtain:
	$$
	\mathbb{E}((\bv{X}'\bv{X})^{-1} \bv{X}' \bv{\varepsilon}) = \mathbb{E}(\mathbb{E}[(\bv{X}'\bv{X})^{-1} \bv{X}' \bv{\varepsilon}|\bv{X}]) = \mathbb{E}((\bv{X}'\bv{X})^{-1} \bv{X}'{\color{blue}\mathbb{E}[\bv{\varepsilon}|\bv{X}]}).
	$$
	By \ref{Assum:exogeneity}, we have ${\color{blue}\mathbb{E}[\bv{\varepsilon}|\bv{X}]=0}$. Hence $\mathbb{E}((\bv{X}'\bv{X})^{-1} \bv{X}' \bv{\varepsilon}) =0$ and result (i) follows.
	\item[(ii)] $\mathbb{V}ar(\bv{b}|\bv{X}) = (\bv{X}'\bv{X})^{-1} \bv{X}' \mathbb{E}(\boldsymbol\varepsilon\boldsymbol\varepsilon'|\bv{X}) \bv{X} (\bv{X}'\bv{X})^{-1}$.
	By Remark\,\ref{remark:Sigma}, if \ref{Assum:homoskedasticity} and \ref{Assum:noncorrel_resid} hold, then we have $\mathbb{E}(\boldsymbol\varepsilon\boldsymbol\varepsilon'|\bv{X})=\mathbb{V}ar(\boldsymbol\varepsilon|\bv{X})=\sigma^2 Id$. The result follows. \qed
	\end{itemize}
	\end{proof}
	\end{tiny}
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}
\begin{exmpl}[Bivariate case ($K=2$)]\label{exmpl:bivariate}
\begin{scriptsize}
\begin{itemize}
	\item If $K=2$ and if the regression includes a constant ($x_{i,1}=1$), $\bv{X}$ is a $n \time 2$ matrix whose $i^{th}$ row is $[1,x_{i,2}]$. Let us use the notation $w_i = x_{i,2}$:
	\begin{tiny}
	\begin{eqnarray*}
	\bv{X}'\bv{X} &=& 
	\left[\begin{array}{cc}
	n & \sum_i w_i \\
	\sum_i w_i & \sum_i w_i^2
	\end{array}
	\right],\\
	(\bv{X}'\bv{X})^{-1} &=& 
	\frac{1}{n\sum_i w_i^2-(\sum_i w_i)^2}
	\left[\begin{array}{cc}
	\sum_i w_i^2 & -\sum_i w_i \\
	-\sum_i w_i & n
	\end{array}
	\right],\\
	(\bv{X}'\bv{X})^{-1}\bv{X}'\bv{y} &=& 
	\frac{1}{n\sum_i w_i^2-(\sum_i w_i)^2}
	\left[\begin{array}{c}
	\sum_i w_i^2\sum_i y_i -\sum_i w_i \sum_i w_iy_i \\
	-\sum_i w_i \sum_i y_i + n \sum_i w_i y_i
	\end{array}
	\right]\\
	&=& \frac{1}{\frac{1}{n}\sum_i(w_i - \bar{w})^2}
		\left[\begin{array}{c}
	\frac{\bar{y}}{n}\sum_i w_i^2 -\frac{\bar{w}}{n}\sum_i w_iy_i \\
	\frac{1}{n}\sum_i (w_i-\bar{w})(y_i-\bar{y})
	\end{array}
	\right].
	\end{eqnarray*}
	\end{tiny}
	\item It can be seen that the second element of $\bv{b}=(\bv{X}'\bv{X})^{-1}\bv{X}'\bv{y}$ is:
	$$
	b_2 = \frac{\overline{\mathbb{C}ov(W,Y)}}{\overline{\mathbb{V}ar(W)}},
	$$
	where $\overline{\mathbb{C}ov(W,Y)}$ and $\overline{\mathbb{V}ar(W)}$ are sample estimates.
	\item Since there is a constant in the regression, we have $b_1 = \bar{y} - b_2 \bar{w}$.
\end{itemize}
\end{scriptsize}
\end{exmpl}
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}

	\begin{thm}[Gauss-Markov Theorem]\label{thm:GaussMarkov}
	Under Assumptions \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}, for any vector $w$, the minimum-variance linear unbiased estimator of $w' \boldsymbol\beta$ is $w' \bv{b}$, where $\bv{b}$ is the least squares estimator. ({\color{blue}BLUE}: Best Linear Unbiased Estimator.)
	\end{thm}

	\begin{tiny}
	{\bf Proof.}: Consider $\bv{b}^* = C \bv{y}$, another linear unbiased estimator of $\boldsymbol\beta$. Since it is unbiased, we must have $\mathbb{E}(C\bv{y}|\bv{X}) = \mathbb{E}(C\bv{X}\boldsymbol\beta + C\boldsymbol\varepsilon|\bv{X}) = \boldsymbol\beta$. We have $\mathbb{E}(C\boldsymbol\varepsilon|\bv{X})=C\mathbb{E}(\boldsymbol\varepsilon|\bv{X})=0$ (by \ref{Assum:exogeneity}).
Therefore $\bv{b}^*$ is unbiased if $\mathbb{E}(C\bv{X})\boldsymbol\beta=\boldsymbol\beta$. This has to be the case for any	$\boldsymbol\beta$, which implies that we must have $C\bv{X}=\bv{I}$.\\

Let us compute $\mathbb{V}ar(\bv{b^*}|\bv{X})$. For this, we introduce $D = C - (\bv{X}'\bv{X})^{-1}\bv{X}'$, which is such that $D\bv{y}=\bv{b}^*-\bv{b}$. The fact that $C\bv{X}=\bv{I}$ implies that $D\bv{X} = \bv{0}$.

We have $\mathbb{V}ar(\bv{b^*}|\bv{X}) = \mathbb{V}ar(C \bv{y}|\bv{X}) =\mathbb{V}ar(C \boldsymbol\varepsilon|\bv{X}) = \sigma^2CC'$ (by Assumptions \ref{Assum:homoskedasticity} and \ref{Assum:noncorrel_resid}, see Remark\,\ref{remark:Sigma}). Using $C=D+(\bv{X}'\bv{X})^{-1}\bv{X}'$ and exploiting the fact that $D\bv{X} = \bv{0}$ leads to:
$$
\mathbb{V}ar(\bv{b^*}|\bv{X}) =\sigma^2\left[(D+(\bv{X}'\bv{X})^{-1}\bv{X}')(D+(\bv{X}'\bv{X})^{-1}\bv{X}')'\right] = \mathbb{V}ar(\bv{b}|\bv{X}) + \sigma^2 \bv{D}\bv{D}'.
$$
Therefore, we have $\mathbb{V}ar(w'\bv{b^*}|\bv{X})=w'\mathbb{V}ar(\bv{b}|\bv{X})w + \sigma^2 w'\bv{D}\bv{D}'w\ge w'\mathbb{V}ar(\bv{b}|\bv{X})w=\mathbb{V}ar(w'\bv{b}|\bv{X})$.\qed
\end{tiny}
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}
\begin{remark}
Prop.\,\ref{thm:propOLS}, Theorems \ref{thm:GaussMarkov} and \ref{thm:FW} (next slide) hold for any sample size.
\end{remark}

\begin{notation}\label{notation:proj}
Consider the linear least square regression of $\bv{y}$ on $\bv{X}$. We introduce the notations:
\begin{itemize}
	\item $\bv{b}^{\bv{y}/\bv{X}}$: OLS estimates of $\boldsymbol\beta$,
	\item $\bv{M}^{\bv{X}}$: residual-maker matrix of any regression on $\bv{X}$ (see Slide \ref{slide:residmatrix}),
	\item $\bv{P}^{\bv{X}}$: projection matrix of any regression on $\bv{X}$ (see Slide \ref{slide:residmatrix}).
\end{itemize}
\end{notation}
\begin{itemize}
	\item Consider the case where we have two sets of explanatory variables: $\bv{X} = [\bv{X}_1,\bv{X}_2]$.
	\item With obvious notations: $\bv{b}^{\bv{y}/\bv{X}}=[\bv{b}_1',\bv{b}_2']'$.
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{thm}[Frisch-Waugh Theorem]\label{thm:FW}
We have:
$$
\bv{b}_2 = \bv{b}^{\bv{M^{\bv{X}_1}y}/\bv{M^{\bv{X}_1}\bv{X}_2}}.
$$
\end{thm}
\begin{tiny}
\begin{proof}
The minimization of the least squares leads to (these are first-order conditions, see Eq.\,\ref{eq:OLS_FOC}):
$$
\left[ \begin{array}{cc} \bv{X}_1'\bv{X}_1 & \bv{X}_1'\bv{X}_2 \\ \bv{X}_2'\bv{X}_1 & \bv{X}_2'\bv{X}_2\end{array}\right]
\left[ \begin{array}{c} \bv{b}_1 \\ \bv{b}_2\end{array}\right] =
\left[ \begin{array}{c} \bv{X}_1' \bv{y} \\ \bv{X}_2' \bv{y} \end{array}\right].
$$
Use the first-row block of equations to solve for $\bv{b}_1$ first; it comes as a function of $\bv{b}_2$. Then use the second set of equations to solve for $\bv{b}_2$, which leads to:
$$
\bv{b}_2 = [\bv{X}_2'\bv{X}_2 - \bv{X}_2'\bv{X}_1(\bv{X}_1'\bv{X}_1)\bv{X}_1'\bv{X}_2]^{-1}\bv{X}_2'(Id - \bv{X}_1(\bv{X}_1'\bv{X}_1)\bv{X}_1')\bv{y}=[\bv{X}_2' \bv{M}^{\bv{X}_1}\bv{X}_2]^{-1}\bv{X}_2'\bv{M}^{\bv{X}_1}\bv{y}.
$$
Using the fact that $\bv{M}^{\bv{X}_1}$ is idempotent and symmetric leads to the result.\qed
\end{proof}
\end{tiny}
\begin{remark}
This suggests a second way of estimating $\bv{b}_2$:
\begin{itemize}
	\item[1.] Regress $Y$ on $X_1$, regress $X_2$ on $X_1$.
	\item[2.] Regress the former residuals on the latter.	
\end{itemize}
\end{remark}
\end{scriptsize}
\end{frame}


\begin{frame}{}
\begin{scriptsize}
\begin{figure}
	%\includegraphics[width=.8\linewidth, height=7.5cm]{Rcodes/fig_copulas.pdf}
	\includegraphics[width=\linewidth]{../../figures/figure_OLS_parapluie.pdf}
	\caption{\href{https://trends.google.fr/trends/explore?date=all&q=parapluie}{Google searches for ``parapluie''} (red) versus precipitations (black)}
\end{figure}
\input{../../Rcode/tables/outfile_parapluie2.txt}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
% latex table generated in R 3.2.2 by xtable 1.8-0 package
% Thu Feb  4 16:49:07 2016
\input{../../Rcode/tables/outfile_parapluie1.txt}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
% latex table generated in R 3.2.2 by xtable 1.8-0 package
% Thu Feb  4 16:49:07 2016
\input{../../Rcode/tables/outfile_parapluie3.txt}
\input{../../Rcode/tables/outfile_parapluie4.txt}
\end{scriptsize}
\end{frame}


\begin{frame}{Partial regression coefficients}
\begin{scriptsize}
\begin{exampleblock}{Special case: $\bv{x}_2$ is scalar}
\begin{itemize}
	\item When $b_2$ is scalar (and then $\bv{X}_2$ is of dimension $n \times 1$), Theorem\;\ref{thm:FW} leads to:
	$$
	b_2 = \frac{\bv{X}_2'M^{\bv{X}_1}\bv{y}}{\bv{X}_2'M^{\bv{X}_1}\bv{X}_2} \quad \text{(partial regression coefficient)}.
	$$
\end{itemize}
\end{exampleblock}

\end{scriptsize}
\end{frame}

\begin{frame}{Goodness of fit}\label{slide:SSR}
\begin{scriptsize}
\begin{itemize}
	\item Total variation in $y$ = sum of squared deviations:
	$$
	TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2.
	$$
	\item We have:
	$$
	\bv{y} = \bv{X}\bv{b} + \bv{e} = \hat{\bv{y}} + \bv{e}
	$$
	\item In the following, we assume that the regression includes a constant (i.e. for all $i$, $x_{i,1}=1$).
	\item Let us denote by $\bv{M}^0$ the matrix that transforms observations into deviations from sample means. Using that $\bv{M}^0 \bv{e} = \bv{e}$ and that $\bv{X}' \bv{e}=0$, we have:
	\begin{eqnarray*}
	\underbrace{\bv{y}'\bv{M}^0\bv{y}}_{\mbox{Total sum of sq.}} &=& (\bv{X}\bv{b} + \bv{e})' \bv{M}^0 (\bv{X}\bv{b} + \bv{e})\\
	&=& \underbrace{\bv{b}' \bv{X}' \bv{M}^0 \bv{X}\bv{b}}_{\mbox{"Explained" sum of sq.}} + \underbrace{\bv{e}'\bv{e}}_{\mbox{Sum of sq. residuals}}\\
	TSS &=& Expl.SS + SSR.
	\end{eqnarray*}
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{Coefficient of determination $R^2$}
\begin{scriptsize}
\begin{equation}\label{eq:RR2}
\boxed{\mbox{Coefficient of determination} = \frac{Expl.SS}{TSS} = 1 - \frac{SSR}{TSS} = 1 - \frac{\bv{e}'\bv{e}}{\bv{y}'\bv{M}^0\bv{y}}.}
\end{equation}

\begin{remark}
\begin{itemize}
	\item It can be shown [Greene, 2012, Section 3.5] that:
	$$
\mbox{Coefficient of determination} = \frac{[\sum_{i=1}^n(y_i - \bar{y})(\hat{y_i} - \bar{y})]^2}{\sum_{i=1}^n(y_i - \bar{y})^2 \sum_{i=1}^n(\hat{y_i} - \bar{y})^2}.
$$
\item[$\Rightarrow$] $R^2$ is the sample squared correlation between $y$ and the (regression-implied) $y$'s predictions.
\end{itemize}
\end{remark}

\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{figure}
	%\includegraphics[width=.8\linewidth, height=7.5cm]{Rcodes/fig_copulas.pdf}
	\caption{Bivariate regressions with low/high $R^2$}
	\includegraphics[width=\linewidth]{../../figures/figure_OLS_R2.pdf}
\end{figure}
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}
\begin{defn}[Partial correlation coefficient]\label{defn:partialcorrel}
The {\color{blue}partial correlation} between $y$ and $z$, controlling for some variables $\bv{X}$ is the sample correlation between $y^*$ and $z^*$, where the latter two variables are the residuals in regressions of $y$ on $\bv{X}$ and of $z$ on $\bv{X}$, respectively.
\vspace{.5cm}

The correlation is denoted by $r_{yz}^\bv{X}$. By definition, we have:
\begin{equation}\label{eq:pc}
r_{yz}^\bv{X} = \frac{\bv{z^*}'\bv{y^*}}{\sqrt{(\bv{z^*}'\bv{z^*})(\bv{y^*}'\bv{y^*})}}.
\end{equation}
\end{defn}
%\begin{exampleblock}[Example].
%XXX
%\end{exampleblock}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{prop}[Change in SSR (see Slide \ref{slide:SSR}) when a variable is added]\label{thm:chgeR2}
We have:
\begin{equation}\label{eq:uu}
\bv{u}'\bv{u} = \bv{e}'\bv{e} - c^2(\bv{z^*}'\bv{z^*}) \qquad (\le \bv{e}'\bv{e})
\end{equation}
where (i) $\bv{u}$ and $\bv{e}$ are the residuals in the regressions of $\bv{y}$ on $[\bv{X},\bv{z}]$ and of $\bv{y}$ on $\bv{X}$, respectively, (ii) $c$ is the regression coefficient on $\bv{z}$ in the former regression and where $\bv{z}^*$ are the residuals in the regression of $\bv{z}$ on $\bv{X}$.
\end{prop}
\begin{tiny}
{\bf Proof.}: The OLS estimates $[\bv{d}',\bv{c}]'$ in the regression of $\bv{y}$ on $[\bv{X},\bv{z}]$ satisfies (first-order cond., Eq.\,\ref{eq:OLS_FOC})
$$
\left[ \begin{array}{cc} \bv{X}'\bv{X} & \bv{X}'\bv{z} \\ \bv{z}'\bv{X} & \bv{z}'\bv{z}\end{array}\right]
\left[ \begin{array}{c} \bv{d} \\ \bv{c}\end{array}\right] =
\left[ \begin{array}{c} \bv{X}' \bv{y} \\ \bv{z}' \bv{y} \end{array}\right].
$$
Hence, in particular $\bv{d} = \bv{b} - (\bv{X}'\bv{X})^{-1}\bv{X}'\bv{z}\bv{c}$, where $\bv{b}$ is the OLS of $\bv{y}$ on $\bv{X}$. Substituting in $\bv{u} = \bv{y} - \bv{X}\bv{d} - \bv{z}c$, we get $\bv{u} = \bv{e} - \bv{z}^*c$. We therefore have:
\begin{equation}\label{eq:uuu}
\bv{u}'\bv{u} = (\bv{e} - \bv{z}^*c)(\bv{e} - \bv{z}^*c)= \bv{e}'\bv{e} + c^2(\bv{z^*}'\bv{z^*}) - 2 c\bv{z^*}'\bv{e}.
\end{equation}
Now $\bv{z^*}'\bv{e} = \bv{z^*}'(\bv{y} - \bv{X}\bv{b}) = \bv{z^*}'\bv{y}$ because $\bv{z}^*$ are the residuals in an OLS regression on $\bv{X}$. Since $c = (\bv{z^*}'\bv{z^*})^{-1}\bv{z^*}'\bv{y^*}$ (by an application of Theorem \ref{thm:FW}), we have $(\bv{z^*}'\bv{z^*})c = \bv{z^*}'\bv{y^*}$ and, therefore, $\bv{z^*}'\bv{e} = (\bv{z^*}'\bv{z^*})c$. Inserting this in Eq.\,(\ref{eq:uuu}) leads to the results.\qed
\end{tiny}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{prop}[Change in $R^2$ when a variable is added]\label{thm:chge_in_R2}
Denoting by $R_W^2$ the coefficient of determination in the regression of $\bv{y}$ on some variable $\bv{W}$, we have:
$$
R_{\bv{X},\bv{z}}^2 = R_{\bv{X}}^2 + (1-R_{\bv{X}}^2)(r_{yz}^\bv{X})^2,
$$
where $r_{yz}^\bv{X}$ is the coefficient of partial correlation (Def.\,\ref{defn:partialcorrel})
\end{prop}
\begin{tiny}
{\bf Proof.}: Let's use the same notations as in Prop.\,\ref{thm:chgeR2}. Theorem \ref{thm:FW} implies that $c = (\bv{z^*}'\bv{z^*})^{-1}\bv{z^*}'\bv{y^*}$. Using this in Eq.\,(\ref{eq:uu}) gives $\bv{u}'\bv{u} = \bv{e}'\bv{e} - (\bv{z^*}'\bv{y^*})^2/(\bv{z^*}'\bv{z^*})$. Using the definition of the partial correlation (Eq.\,\ref{eq:pc}), we get $\bv{u}'\bv{u} = \bv{e}'\bv{e}\left(1 - (r_{yz}^\bv{X})^2\right)$. The results is obtained by dividing both sides of the previous equation by $\bv{y}'\bv{M}_0\bv{y}$.\qed
\end{tiny}
\vspace{.5cm}
\begin{itemize}
	\item The previous theorem shows that we necessarily increase the $R^2$ if we add variables, {\color{red}even if they are irrelevant}.
	\item The {\color{blue} adjusted $R^2$}, denoted by $\bar{R}^2$, is a fit measure that penalizes large numbers of regressors:
	\begin{equation*}
	\boxed{\bar{R}^2 = 1 - \frac{\bv{e}'\bv{e}/(n-K)}{\bv{y}'\bv{M}^0\bv{y}/(n-1)} = 1 - \frac{n-1}{n-K}(1-R^2).}
	\end{equation*}
\end{itemize}

\end{scriptsize}
\end{frame}




\subsection{Inference}

\begin{frame}{Inference and Prediction}\label{slide:bnormal}
\begin{scriptsize}
\begin{itemize}
	\item Under the normality assumption (Assumption \ref{Assum:normality}), we know the distribution of $\bv{b}$ (conditional on $\bv{X}$).
	\item Indeed, $(\bv{b}|\bv{X}) \equiv (\bv{X}'\bv{X})^{-1} \bv{X}'\bv{y}$ is multivariate Gaussian:
	\begin{equation}\label{eq:distriBcondi}
	\bv{b}|\bv{X} \sim \mathcal{N}(\beta,\sigma^2(\bv{X}'\bv{X})^{-1}).
	\end{equation}
	\item Problem: In practice, we do not know $\sigma^2$ ({\color{blue}population parameter}).
\end{itemize}
\begin{prop}[Conditional expectation of $s^2$]\label{prop:expect_s2}
Under \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}, an unbiased estimate of $\sigma^2$ is given by:
\begin{equation}\label{eq:s2}
s^2 = \frac{\bv{e}'\bv{e}}{n-K}.
\end{equation}
(It is sometimes denoted by $\sigma^2_{OLS}$.)
\end{prop}
	\begin{tiny}
	{\bf Proof.}: $\mathbb{E}(\bv{e}'\bv{e}|\bv{X})=\mathbb{E}(\boldsymbol{\varepsilon}'\bv{M}\boldsymbol{\varepsilon}|\bv{X})=\mathbb{E}(\mbox{Tr}(\boldsymbol{\varepsilon}'\bv{M}\boldsymbol{\varepsilon})|\bv{X}))
	=\mbox{Tr}(\bv{M}\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|\bv{X}))=\sigma^2 \mbox{Tr}(\bv{M})$. (Note that we have $\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|\bv{X})=\sigma^2Id$ by Assumptions \ref{Assum:homoskedasticity} and \ref{Assum:noncorrel_resid}, see Remark\,\ref{remark:Sigma}.) Finally: $\mbox{Tr}(\bv{M})=n-\mbox{Tr}(\bv{X}(\bv{X}'\bv{X})^{-1}\bv{X}')=n-\mbox{Tr}((\bv{X}'\bv{X})^{-1}\bv{X}'\bv{X})=n-\mbox{Tr}(Id_{K\times K})$ (see also Prop.\,\ref{thm:chi2_idempotent}). \qed
	\end{tiny}

\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}

\begin{itemize}
	\item Two results will prove important to perform hypothesis testing:
	\begin{itemize}
		\item[(i)] We know the distribution of $s^2$ (Prop.\,\ref{thm:s2distri}). 
		\item[(ii)] $s^2$ and $\bv{b}$ are independent random variables (Prop.\,\ref{thm:indep_s2_b}). 
	\end{itemize}
\end{itemize}

\begin{prop}[Distribution of $s^2$]\label{thm:s2distri}
Under \ref{Assum:fullrank} to \ref{Assum:normality}, we have: $\dfrac{s^2}{\sigma^2} | \bv{X} \sim \chi^2(n-K)/(n-K)$.
\end{prop}
\begin{tiny}
\begin{proof}

\begin{itemize}
\item We have $\bv{e}'\bv{e}=\boldsymbol\varepsilon'\bv{M}\boldsymbol\varepsilon$. $\bv{M}$ is an idempotent symmetric matrix. Therefore it can be decomposed as $PDP'$ where $D$ is a diagonal matrix and $P$ is an orthogonal matrix. As a result $\bv{e}'\bv{e} = (P'\boldsymbol\varepsilon)'D(P'\boldsymbol\varepsilon)$, i.e. $\bv{e}'\bv{e}$ is a weighted sum of independent squared Gaussian variables (the entries of $P'\boldsymbol\varepsilon$ are independent because they are Gaussian -- under \ref{Assum:normality} -- and uncorrelated). The variance of each of these i.i.d. Gaussian variable is $\sigma^2$.
\item Because $\bv{M}$ is an idempotent symmetric matrix, its eigenvalues are either 0 or 1 (Prop.\,\ref{thm:roots_idempotent}) and its rank equals its trace (Prop.\,\ref{thm:chi2_idempotent}). Further, its trace is equal to $n-K$ (see proof of Eq.\,(\ref{eq:s2})). Therefore $D$ has $n-K$ entries equal to 1 and $K$ equal to 0. 
\item Hence,  $\bv{e}'\bv{e} = (P'\boldsymbol\varepsilon)'D(P'\boldsymbol\varepsilon)$ is a sum of $n-K$ squared independent Gaussian variables of variance $\sigma^2$.
\item Therefore $\frac{\bv{e}'\bv{e}}{\sigma^2} = (n-K)\frac{s^2}{\sigma^2}$ is a sum of $n-k$ squared i.i.d. standard normal  variables.\qed
\end{itemize}
\end{proof}
\end{tiny}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{prop}[Independence of $s^2$ and $\bv{b}$]\label{thm:indep_s2_b}
Under \ref{Assum:fullrank} to \ref{Assum:normality}, $\bv{b}$ and $s^2$ are independent.
\end{prop}
\begin{tiny}
\begin{proof}
We have $\bv{b}=\boldsymbol\beta + [\bv{X}'{\bv{X}}]^{-1}\bv{X}\boldsymbol\varepsilon$ and $s^2 = \boldsymbol\varepsilon' \bv{M} \boldsymbol\varepsilon/(n-K)$. Hence  $\bv{b}$ is an affine combination of $\boldsymbol\varepsilon$ and $s^2$ is a quadratic combination of the same Gaussian shocks. One can write $s^2$ as $s^2 = (\bv{M}\boldsymbol\varepsilon)' \bv{M} \boldsymbol\varepsilon/(n-K)$ and $\bv{b}$ as $\boldsymbol\beta + \bv{T}\boldsymbol\varepsilon$. Since $\bv{T}\bv{M}=0$, $\bv{T}\boldsymbol\varepsilon$ and $\bv{M}\boldsymbol\varepsilon$ are independent (because two uncorrelated Gaussian variables are independent), therefore $\bv{b}$ and $s^2$, which are functions of respective independent variables, are independent.\qed
\end{proof}
\end{tiny}
\end{scriptsize}
\end{frame}


\begin{frame}{}
\begin{scriptsize}
\begin{itemize}
	\item Under \ref{Assum:fullrank} to \ref{Assum:normality}, let us consider $b_k$, the $k^{th}$ entry of $\bv{b}$:
	$$
	b_k | \bv{X} \sim \mathcal{N}(\beta_k,\sigma^2 v_k),
	$$
	where $v_k$ is the k$^{th}$ component of the diagonal of $(\bv{X}'\bv{X})^{-1}$.
	\item Besides, we have (Prop.\,\ref{thm:s2distri}):
	$$
	\frac{(n-K)s^2}{\sigma^2} | \bv{X} \sim \chi ^2 (n-K).
	$$
	\item As a result (using Props. \ref{thm:s2distri} and \ref{thm:indep_s2_b}), we have:
	\begin{equation}\label{eq:resultstudentt}
	\boxed{t_k = \frac{\frac{b_k - \beta_k}{\sqrt{\sigma^2 v_k}}}{\sqrt{\frac{(n-K)s^2}{\sigma^2(n-K)}}} = \frac{b_k - \beta_k}{\sqrt{s^2v_k}} \sim t(n-K),}
	\end{equation}
	where $t(n-K)$ denotes a $t$ distribution with $n-K$ degrees of freedom (Def.\,\ref{def:tStudent}).
	\item Remark: $\frac{b_k - \beta_k}{\sqrt{\sigma^2 v_k}} | \bv{X} \sim \mathcal{N}(0,1)$ and $\frac{(n-K)s^2}{\sigma^2} | \bv{X} \sim \chi ^2 (n-K)$. These two distributions do not depend on $\bv{X}$ $\Rightarrow$ the \textit{marginal} distribution of $t_k$ is also $t$.
\end{itemize}
\end{scriptsize}
\end{frame}


\begin{frame}{}
\begin{scriptsize}
\begin{remark}
\begin{itemize}
\item $s^2 v_k$ is not exactly the conditional variance of $b_k$:

	The variance of $b_k$ conditional on $\bv{X}$ is $\sigma^2 v_k$.
\item However $s^2 v_k$ is an unbiased estimate of $\sigma^2 v_k$ (by Prop.\,\ref{prop:expect_s2}).
\end{itemize}
\end{remark}
\begin{itemize}
	\item The previous result (Eq.\,\ref{eq:resultstudentt}) can be extended to any linear combinations of elements of $\bv{b}$ (Eq.\,\ref{eq:resultstudentt} is for its $k^{th}$ component only).
	\item Let us consider $\boldsymbol\alpha'\bv{b}$, the OLS estimate of $\boldsymbol\alpha'\boldsymbol\beta$.
	\item From Eq.\,(\ref{eq:distriBcondi}), we have:
	$$
	\boldsymbol\alpha'\bv{b} | \bv{X} \sim \mathcal{N}(\boldsymbol\alpha'\boldsymbol\beta,\sigma^2 \boldsymbol\alpha'(\bv{X}'\bv{X})^{-1}\boldsymbol\alpha).
	$$
	\item Therefore:
	$$
	\frac{\boldsymbol\alpha'\bv{b} - \boldsymbol\alpha'\boldsymbol\beta}{\sqrt{\sigma^2 \boldsymbol\alpha'(\bv{X}'\bv{X})^{-1}\boldsymbol\alpha}} | \bv{X} \sim \mathcal{N}(0,1).
	$$
	\item Using the same approach as the one used to derive Eq.\,(\ref{eq:resultstudentt}), one can show that Props.\,\ref{thm:s2distri} and \ref{thm:indep_s2_b} imply that:
	\begin{equation}\label{eq:resultstudentt2}
	\boxed{\frac{\boldsymbol\alpha'\bv{b} - \boldsymbol\alpha'\boldsymbol\beta}{\sqrt{s^2\boldsymbol\alpha'(\bv{X}'\bv{X})^{-1}\boldsymbol\alpha}} \sim t(n-K).}
	\end{equation}

\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{figure}
	\caption{Normal and Student-t distributions}
	%\includegraphics[width=.8\linewidth, height=7.5cm]{Rcodes/fig_copulas.pdf}
	\includegraphics[width=\linewidth]{../../figures/Figure_OLS_StudentvsNorm.pdf}
	\begin{tiny}
	Note: See Def.\,\ref{def:tStudent} of the t distribution. The chart shows that the higher the degree of freedom $\nu$, the closer the distribution of $t(\nu)$ gets to the normal distribution. See also Table \ref{tab:Studenttable}.
	\end{tiny}
\end{figure}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{exampleblock}{Confidence interval of $\beta_k$}
\begin{itemize}
	\item Assume we want to compute a (symmetrical) confidence interval $[I_{d,1-\alpha},I_{u,1-\alpha}]$ that is such that $\mathbb{P}(\beta_k \in [I_{d,1-\alpha},I_{u,1-\alpha}])=1-\alpha$.
	\item In particular, we want to have: $\mathbb{P}(\beta_k < I_{d,1-\alpha})=\frac{\alpha}{2}$.
	\item For this purpose, we make use of $t_k = \frac{b_k - \beta_k}{\sqrt{s^2v_k}} \sim t(n-K)$ (Eq.\,\ref{eq:resultstudentt}).
	\item We have:
	$$
	\mathbb{P}(\beta_k < I_{d,1-\alpha})=\frac{\alpha}{2} \Leftrightarrow
	$$
	\begin{eqnarray*}
	\mathbb{P}\left(\frac{b_k - \beta_k}{\sqrt{s^2v_k}} > \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} \Leftrightarrow \mathbb{P}\left(t_k > \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} \Leftrightarrow&&\\
	1 - \mathbb{P}\left(t_k \le \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} \Leftrightarrow \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}} = \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right),&&
	\end{eqnarray*}
	where $\Phi_{t(n-K)}(\alpha)$ is the c.d.f. of the $t(n-K)$ distribution (Table \ref{tab:Studenttable}).
	\item Doing the same for $I_{u,1-\alpha}$, we obtain:
	\begin{eqnarray*}
	&&[I_{d,1-\alpha},I_{u,1-\alpha}] =\\
	&&\left[b_k - \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\sqrt{s^2v_k},b_k + \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\sqrt{s^2v_k}\right].
	\end{eqnarray*}
\end{itemize}
\end{exampleblock}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{exmpl}[Income, Seniority and Education (Example \ref{exmpl:income})]\label{exmpl:income2}
\input{../../Rcode/tables/outfile_income3d.txt}
		\begin{tiny}
		The data are the same as in Example \ref{exmpl:income}.
		\end{tiny}
\end{exmpl}
\begin{exmpl}[Advertising and sales (Example \ref{exmpl:advertising})]\label{exmpl:advertising2}
\input{../../Rcode/tables/outfile_advertising.txt}
		\begin{tiny}
		The data are the same as in Example \ref{exmpl:advertising}.
		\end{tiny}
\end{exmpl}

\end{scriptsize}
\end{frame}

\begin{frame}\label{parttestcall}
\begin{scriptsize}
% latex table generated in R 3.2.2 by xtable 1.8-0 package
% Thu Feb  4 16:49:07 2016
\begin{itemize}
	\item In Examples \ref{exmpl:income2} and \ref{exmpl:advertising2}, the last two columns give the test statistic and p-values associated to the test (Section \ref{section:tests}) whose null hypothesis is:
	$$
	H_0: \beta_k=0.
	$$ 
	\item The {\color{blue}t-statistics}, that is $b_k/\sqrt{s^2 v_k}$, is the test statistic of the test.
	\item Under $H_0$, the t-statistic is $t(n-K)$ (see Eq.\,\ref{eq:resultstudentt}). Hence, the critical region (see Slide \ref{slide:criticalregion}) for the test of size $\alpha$ is:
	$$
	\left]-\infty,-\Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\right] \cup \left[\Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right),+\infty\right[.
	$$
	\item The {\color{blue}p-value} is defined as the probability that $|Z| > |t|$, where $t$ is the (computed) t statistics and where $Z \sim t(n-K)$. That is, the p-value is given by $2(1 - \Phi_{t(n-K)}(|t_k|))$.
\end{itemize}
\hyperlink{ptestcharts}{\beamergotobutton{Charts on tests (critical region, level, p-value)}}

\end{scriptsize}

\end{frame}


\begin{frame}{Set of linear restrictions}
\begin{scriptsize}
\begin{itemize}
	\item We consider the following model:
	$$
	\bv{y} = \bv{X}\boldsymbol\beta + \boldsymbol\varepsilon, \quad \varepsilon \sim i.i.d. \mathcal{N}(0,\sigma^2).
	$$
	and we want to test for the {\color{blue} joint} validity of a set of restrictions involving the components of $\boldsymbol\beta$ in a linear way.
	\item Set of linear restrictions:
	\begin{equation}\label{eq:restrictions}
	\begin{array}{ccc}
	r_{1,1} \beta_1 + \dots + r_{1,K} \beta_K &=& q_1\\
	\vdots && \vdots\\
	r_{J,1} \beta_1 + \dots + r_{J,K} \beta_K &=& q_J,
	\end{array}
	\end{equation}
	that can be written in matrix form:
	\begin{equation}
	\bv{R}\boldsymbol\beta = \bv{q}.
	\end{equation}
\end{itemize}

\begin{exmpl}[Advertising and sales (Examples \ref{exmpl:advertising} and \ref{exmpl:advertising2})]
Is the effect of advertising on TV (one unit) equal to that of advertising on radio (one unit)?

$\Rightarrow$ $\bv{R}=[0,1,-1,0]$, $\bv{q}=0$.
\end{exmpl}

\end{scriptsize}
\end{frame}


%\begin{frame}
%\begin{scriptsize}
%% latex table generated in R 3.2.2 by xtable 1.8-0 package
%% Thu Feb  4 16:49:07 2016
%\input{../../Rcode/tables/outfile_articles.txt}
%		\begin{tiny}
%\textbf{art}: count of articles produced during last 3 years of Ph.D; \textbf{fem}: factor indicating gender of student, with levels Men and Women; \textbf{mar}: factor indicating marital status of student, with levels Single and Married; \textbf{kid5}: number of children aged 5 or younger; \textbf{phd}: prestige of Ph.D. department; \textbf{ment}: count of articles produced by Ph.D. mentor during last 3 years. (\href{https://vincentarelbundock.github.io/Rdatasets/doc/pscl/bioChemists.html}{Source of the data})
%		\end{tiny}
%		\vspace{.5cm}
%\begin{itemize}
%	\item Is the effect of having a children equal to that of getting married? $\Rightarrow$ $\bv{R}=[0,0,1,-1,0,0]$, $\bv{q}=0$.
%\end{itemize}
%\end{scriptsize}
%\end{frame}


\begin{frame}{}
\begin{scriptsize}
\begin{itemize}
	\item {\color{blue} Discrepancy vector}: $\bv{m} = \bv{R}\bv{b} - \bv{q}$.
	\item Under the null hypothesis:
	\begin{eqnarray*}
	\mathbb{E}(\bv{m}|\bv{X}) &=& \bv{R}\boldsymbol\beta - \bv{q} = 0 \quad \mbox{and} \\
	\mathbb{V}ar(\bv{m}|\bv{X}) &=& \bv{R} \mathbb{V}ar(\bv{b}|\bv{X}) \bv{R}'.
	\end{eqnarray*}
	\item Under \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}, $\mathbb{V}ar(\bv{m}|\bv{X}) = \sigma^2 \bv{R} (\bv{X}'\bv{X})^{-1} \bv{R}'$ (see Prop.\,\ref{thm:propOLS}).
	\item Test:
	\begin{equation}\label{eq:H0Ftest}
	\boxed{H_0: \bv{R}\boldsymbol\beta - \bv{q} = 0$ against $H_1: \bv{R}\boldsymbol\beta - \bv{q} \ne 0.}
	\end{equation}
	\item We could perform a {\color{blue} Wald test}. The test statistic is denoted by $W$.
	
	Under \ref{Assum:fullrank} to \ref{Assum:normality} (we need the normality assumption) and under $H_0$, we have:
	\begin{equation}\label{eq:W1}
	W = \bv{m}'\mathbb{V}ar(\bv{m}|\bv{X})^{-1}\bv{m} \sim \chi^2(J)
	\end{equation}
	(see Prop.\,\ref{prop:waldtypeproduct}).
	\item However, $\sigma^2$ is unknown. Hence we cannot compute $W$.
	
	We can however approximate it be replacing $\sigma^2$ by $s^2$. The distribution of this new statistic is not $\chi^2(J)$ any more;
 it is an {\color{blue} $\mathcal{F}$ distribution}, and the test is called {\color{blue} $F$ test} (see next slide).
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{prop}[Definition and distribution of the $F$ stat.]\label{thm:Ftest1}
Under Assumptions \ref{Assum:fullrank} to \ref{Assum:normality} and if Eq.\,(\ref{eq:H0Ftest}) holds, we have:
	\begin{equation}\label{eq:defFstatistics}
	F = \frac{W}{J}\frac{\sigma^2}{s^2} = \frac{\bv{m}'(\bv{R}(\bv{X}'\bv{X})^{-1}\bv{R}')^{-1}\bv{m}}{s^2J} \sim \mathcal{F}(J,n-K),
	\end{equation}
	where $\mathcal{F}$ is the distribution of the F-statistic (see Def.\,\ref{def:fstatistics}).
\end{prop}
\begin{tiny}
\begin{proof}
According to Eq.\,(\ref{eq:W1}), $W/J \sim \chi^2(J)/J$. Consider the denominator $s^2/\sigma^2$. We have $s^2 = \bv{e}'\bv{e}/(n-K)$ (Eq.\,(\ref{eq:s2})) and $\bv{e}'\bv{e} = \boldsymbol\varepsilon'M\boldsymbol\varepsilon$. By Prop.\,\ref{thm:chi2_idempotent}, the latter term is $\sim \chi^2(n-K)$. Therefore, $F$ is the ratio of a r.v. distributed as $\chi^2(J)/J$ and another distributed as $\chi^2(n-K)/(n-K)$. It remains to verify that these r.v. are independent.
\vspace{.3cm}

Under $H_0$, we have $\bv{m} =  \bv{R}(\bv{b}-\boldsymbol\beta) = \bv{R}(\bv{X}'\bv{X})^{-1}\bv{X}'\boldsymbol\varepsilon$.
Therefore $\bv{m}'(\bv{R}(\bv{X}'\bv{X})^{-1}\bv{R}')^{-1}\bv{m}$ is of the form $\boldsymbol\varepsilon'\bv{T}\boldsymbol\varepsilon$ with $\bv{T}=\bv{D}'\bv{C}\bv{D}$ where $\bv{D}=\bv{R}(\bv{X}'\bv{X})^{-1}\bv{X}'$ and $\bv{C}=(\bv{R}(\bv{X}'\bv{X})^{-1}\bv{R}')^{-1}$.
\vspace{.3cm}

Under \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}, the covariance between $\bv{T}\boldsymbol\varepsilon$ and $\bv{M}\boldsymbol\varepsilon$ is $\sigma^2\bv{T}\bv{M} = \bv{0}$. Therefore, under \ref{Assum:normality}, these variables are Gaussian variables with 0 covariance. Hence they are independent.\qed
\end{proof}
\end{tiny}

\begin{remark}
For large $n-K$, the $\mathcal{F}_{J,n-K}$ distribution converges to $\mathcal{F}_{J,\infty}=\chi^2(J)/J$.
\end{remark}

\end{scriptsize}
\end{frame}

%\begin{frame}{}
%\begin{scriptsize}
%\begin{itemize}
%	\item Special case: $J=1$. In this case:
%	$$
%	F = \frac{(\sum_i r_i b_i - q)^2}{\sum_{j,k} r_j r_k \widehat{\mathbb{C}ov} (b_j,b_k)},
%	$$
%	where $\widehat{\mathbb{C}ov} (\bv{b},\bv{b})=\widehat{\mathbb{V}ar} (\bv{b})=s^2(\bv{X}'\bv{X})^{-1}$.
%	\item Alternative approach: consider $\hat{q} = \sum_i r_i b_i$. Idea: test the distance between $\hat{q}$ and $q$. 
%	$$
%	t = \frac{\hat{q} - q}{se(\hat{q})} \sim t(n-K),
%	$$
%	where $se(\hat{q})$, that we take equal to $\sqrt{\bv{r}' (s^2 (\bv{X}'\bv{X})^{-1}) \bv{r}}$, is an estimate of the standard error of $\hat{q}$ (under $H_0$).
%	\item[$\Rightarrow$] In the case of a single restriction ($J=1$), it can be seen that $t^2 = F$.
%\end{itemize}
%\end{scriptsize}
%\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{prop}[Other expressions of the $F$-statistics]\label{thm:Ftest}
The F-statistic defined by Eq.\,(\ref{eq:defFstatistics}) is also equal to:
\begin{equation}\label{eq:defFstatistics2}
F = \frac{(R^2-R_*^2)/J}{(1-R^2)/(n-K)} =  \frac{(SSR_{restr}-SSR_{unrestr})/J}{SSR_{unrestr}/(n-K)},
\end{equation}
where $R_*^2$ is the coef. of determination (Eq.\,\ref{eq:RR2}) of the ``restricted regression'' {\color{blue}(SSR: sum of squared residuals, see Slide \ref{slide:SSR}.)}

\end{prop}
\begin{tiny}
\begin{proof}
Let's denote by $\bv{e}_*=\bv{y}-\bv{X}\bv{b}_*$ the vector of residuals associated to the "restricted regression" (i.e. $\bv{R}\bv{b}_*=\bv{q}$).
We have $\bv{e}_*=\bv{e} - \bv{X}(\bv{b}_*-\bv{b})$. Using $\bv{e}'\bv{X}=0$, we get $\bv{e}_*'\bv{e}_*=\bv{e}'\bv{e} + (\bv{b}_*-\bv{b})'\bv{X}'\bv{X}(\bv{b}_*-\bv{b}) \ge \bv{e}'\bv{e}$. 

By Prop.\,\ref{thm:constrained_LS}, we know that $\bv{b}_*-\bv{b}=-(\bv{X}'\bv{X})^{-1} \bv{R}'\{\bv{R}(\bv{X}'\bv{X})^{-1}\bv{R}'\}^{-1}(\bv{R}\bv{b} - \bv{q})$. Therefore:
$$
\bv{e}_*'\bv{e}_* - \bv{e}'\bv{e} = (\bv{R}\bv{b} - \bv{q})'[\bv{R}(\bv{X}'\bv{X})^{-1}\bv{R}']^{-1}(\bv{R}\bv{b} - \bv{q}).
$$
This implies that the F statistic defined in Prop.\,\ref{thm:Ftest1} is also equal to:
$$
\frac{(\bv{e}_*'\bv{e}_* - \bv{e}'\bv{e})/J}{\bv{e}'\bv{e}/(n-K)}.\qed
$$
\end{proof}
\end{tiny}
\begin{defn}[F-test of size $\alpha$]\label{defn:Ftest}
The null hypothesis $H_0$ (Eq.\,\ref{eq:H0Ftest}) of the F-test is rejected if $F$ --defined by Eq. (\ref{eq:defFstatistics}) or (\ref{eq:defFstatistics2})-- is higher than $\mathcal{F}_{1-\alpha}(J,n-K)$. [one-sided test: see Slide \ref{ptestcharts_onesided}]
\end{defn}
\end{scriptsize}
\end{frame}

%\begin{frame}{Non-linear restrictions}
%\begin{scriptsize}
%\begin{itemize}
%	\item Null hypothesis: $H_0: c(\boldsymbol\beta) = q$.
%	\item t and F tests are based on the statisics:
%	$$
%	z = \frac{c(\bv{b}) - q}{\sqrt{\widehat{\mathbb{V}ar}\{c(\bv{b})}\}}.
%	$$
%	Under $H_0$, $z$ is approximately $t(n-K)$ and $z^2$ is $\mathcal{F}(1,n-K)$.
%	\item How to approximate $\mathbb{V}ar\{c(\bv{b})\}$? $\Rightarrow$ {\color{blue}Delta method}.
%	\item Taylor series extension:
%	$$
%	c(\bv{b}) \approx c(\boldsymbol\beta) + \frac{\partial c(\boldsymbol\beta)}{\partial \boldsymbol\beta}'(\bv{b} - \boldsymbol\beta).
%	$$
%	\item The result relies on the consistency of the estimate (not unbiasedness since $\mathbb{E}(c(X)) \ne c(\mathbb{E}(X)))$.
%		\item Assuming the approximation holds:
%	\begin{equation}
%	\boxed{\mathbb{V}ar\{c(\bv{b})\} \approx \frac{\partial c}{\partial \boldsymbol\beta}(\bv{b})' \mathbb{V}ar(\bv{b}) \frac{\partial c}{\partial \boldsymbol\beta}(\bv{b}).}
%	\end{equation}
%\end{itemize}
%\end{scriptsize}
%\end{frame}


\begin{frame}{Prediction}
\begin{scriptsize}
\begin{itemize}
	\item We consider the model:
	$$
	y_i = \bv{x}_i' \boldsymbol\beta + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,\sigma^2).
	$$
	\item Prediction exercise: we want to predict $y^*$, which would be associated with a vector of explanatory variables $\bv{x}^*$ (but this ``entity $*$'' is not in the estimation sample).
	\item The Gauss-Markov Theorem (\ref{thm:GaussMarkov}) implies that, under under \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}, $\widehat{y^*} = {\bv{x}^*}'\bv{b}$ is the minimum-variance linear unbiased estimator of $\mathbb{E}(y^*|\bv{x}^*)={\bv{x}^*}'\boldsymbol\beta$, where $\bv{x}^*$ is a specific (deterministic) realization of $\bv{x}$.
	\item Forecast error: $e^* =  y^*- \widehat{y^*}= y^*- \bv{x^*}'\bv{b}= \bv{x^*}'(\boldsymbol\beta - \bv{b}) + \varepsilon^*$.
	\item Prediction variance:
	\begin{equation}\label{eq:predictt}
	\mathbb{V}ar(e^*|\bv{X},\bv{x}=\bv{x}^*) = {\bv{x}^*}' \{ \sigma^2 (\bv{X}'\bv{X})^{-1} \} {\bv{x}^*} + \sigma^2.
	\end{equation}
	\item The prediction variance is estimated by replacing $\sigma^2$ by $s^2$.
	%\item Note that since $\bv{x}^*\boldsymbol\beta$ is not random given $\bv{x}^*$, we have $\mathbb{V}ar(y^*|\bv{X},x=\bv{x}^*)=\mathbb{V}ar(e^*|\bv{X},x=\bv{x}^*)$.
\end{itemize}
\end{scriptsize}
\end{frame}


\begin{specialframe}
\begin{scriptsize}
\begin{itemize}
	\item Let's focus on the specific case where the regression includes a constant term and a single regressor, i.e. each row of $\bv{X}$ is of the form $[1,w_i]$. (This is Example \ref{exmpl:bivariate}.)
	\item In this case, we have $\bv{x}^*=[1,w^*]'$ and, using the expression of $(\bv{X}'\bv{X})^{-1}$ computed in Example \ref{exmpl:bivariate} (Eq.\,\ref{eq:predictt}), we have:
	\begin{equation}\label{eq:variancepred}
	\mathbb{V}ar(\underbrace{y^*- \bv{x^*}'\bv{b}}_{\mbox{forecast error}}|\bv{X},\bv{x}=\bv{x}^*) = \sigma^2 \left[ 1 + \frac{1}{n} + \frac{(w^* - \bar{w})^2}{\sum_i (w_i - \bar{w})^2}  \right].
	\end{equation}
	\item The farther the considered $w^*$ is from the center of observations (i.e. $\bar{w}$), the lower the confidence in the predicted $y^*$.
	\item Conditional on $(\bv{X},\bv{x}=\bv{x}^*)$, the forecast error $e^*=\bv{x^*}'(\boldsymbol\beta - \bv{b}) + \varepsilon^*$ is Gaussian (under \ref{Assum:fullrank} to \ref{Assum:normality}, using in particular Eq.\,\ref{eq:distriBcondi}).
	
	It expectation is 0 and its variance is given by Eq.\,(\ref{eq:variancepred}). Hence
	$$
	y^*|\bv{X},\bv{x}=\bv{x}^* \sim \mathcal{N}(\bv{x^*}'\bv{b},\underbrace{\mathbb{V}ar(e^*|\bv{X},x=\bv{x}^*)}_{\mbox{given by Eq.\,(\ref{eq:variancepred})}}).
	$$
	\item Hence, under \ref{Assum:fullrank} to \ref{Assum:normality}, a 95\% confidence interval for $y^*$ is:
	$$
	\left[\bv{x^*}'\bv{b}-1.96\sqrt{\mathbb{V}ar(e^*|\bv{X},x=\bv{x}^*)},\bv{x^*}'\bv{b}+1.96\sqrt{\mathbb{V}ar(e^*|\bv{X},x=\bv{x}^*)}\right].
	$$
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{figure}
	%\includegraphics[width=.8\linewidth, height=7.5cm]{Rcodes/fig_copulas.pdf}
	\caption{Prediction -- Three cases: low, medium and large dispersion of the $x_i$s}
	\includegraphics[width=1.2\linewidth]{../../figures/figure_prediction_all3.pdf}
\end{figure}
\end{scriptsize}
\end{specialframe}

%\begin{frame}{}
%\begin{scriptsize}
%\begin{figure}
%	%\includegraphics[width=.8\linewidth, height=7.5cm]{Rcodes/fig_copulas.pdf}
%		\caption{Prediction: case with medium dispersion of the $x_i$s}
%	\includegraphics[height=7cm]{../../figures/figure_prediction2.pdf}
%\end{figure}
%\end{scriptsize}
%\end{frame}
%\begin{frame}{}
%\begin{scriptsize}
%\begin{figure}
%	%\includegraphics[width=.8\linewidth, height=7.5cm]{Rcodes/fig_copulas.pdf}
%		\caption{Prediction: case with high dispersion of the $x_i$s}
%
%	\includegraphics[height=7cm]{../../figures/figure_prediction3.pdf}
%\end{figure}
%\end{scriptsize}
%\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{figure}
	%\includegraphics[width=.8\linewidth, height=7.5cm]{Rcodes/fig_copulas.pdf}
		\caption{Seniority and income (data of Example \ref{exmpl:income})}

	\includegraphics[width=.9\linewidth]{../../figures/Figure_predict_income.pdf}
\end{figure}
\end{scriptsize}
\end{frame}






\subsection{Common pitfalls}


\begin{frame}{Potential common pitfalls}
\begin{scriptsize}
\begin{remark}[Multicollinearity]\label{exmpl:multicol}
\begin{itemize}
	\item Model: $y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i$ where all variables are zero-mean and $\mathbb{V}ar(\varepsilon_i)=\sigma^2$.
	\item We have
	\begin{tiny}
	$$
	\bv{X}'\bv{X} = \left[ \begin{array}{cc}
					\sum_i x_{i,1}^2 & \sum_i x_{i,1} x_{i,2} \\
					\sum_i x_{i,1} x_{i,2} & \sum_i x_{i,2}^2
					\end{array}\right]
					\quad \mbox{therefore}
	$$
	\begin{eqnarray*}
	&&(\bv{X}'\bv{X})^{-1} =\\
	&& \frac{1}{\sum_i x_{i,1}^2\sum_i x_{i,2}^2 - (\sum_i x_{i,1} x_{i,2})^2} \left[ \begin{array}{cc}
					\sum_i x_{i,2}^2 & -\sum_i x_{i,1} x_{i,2} \\
					-\sum_i x_{i,1} x_{i,2} & \sum_i x_{i,1}^2
					\end{array}\right].
	\end{eqnarray*}
	\end{tiny}
	\item The inverse of the upper-left parameter of $(\bv{X}'\bv{X})^{-1}$ is:
	\begin{equation}\label{eq_multicollin}
	\sum_i x_{i,1}^2 - \frac{(\sum_i x_{i,1} x_{i,2})^2}{\sum_i x_{i,2}^2} = \sum_i x_{i,1}^2(1 - correl_{1,2}^2),
	\end{equation}
	where $correl_{1,2}$ is the sample correlation between $\bv{x}_{1}$ and $\bv{x}_{2}$.
	\item Hence, the closer to one $correl_{1,2}$, the higher the variance of $b_1$ (recall that the variance of $b_1$ is the upper-left component of $\sigma^2(\bv{X}'\bv{X})^{-1}$).
\end{itemize}
\end{remark}
\end{scriptsize}
\end{frame}

\begin{frame}{Potential common pitfalls}
\begin{scriptsize}
\begin{remark}[Omitted variables]\label{exmpl:omitted}
\begin{itemize}
	\item ``True model'':
	$$
	\bv{y} = \underbrace{\bv{X}_1}_{n \times K_1}\underbrace{\boldsymbol\beta_1}_{K_1 \times 1} + \underbrace{\bv{X}_2}_{n\times K_2}\underbrace{\boldsymbol\beta_2}_{K_2 \times 1} + \boldsymbol\varepsilon
	$$
	\item Then, if one computes $\bv{b}_1$ by regressing $\bv{y}$ on $\bv{X}_1$ only:
	$$
	\bv{b}_1 = (\bv{X}_1'\bv{X}_1)^{-1}\bv{X}_1'\bv{y} = \boldsymbol\beta_1 + (\bv{X}_1'\bv{X}_1)^{-1}\bv{X}_1'\bv{X}_2\boldsymbol\beta_2 + 
	(\bv{X}_1'\bv{X}_1)^{-1}\bv{X}_1'\boldsymbol\varepsilon.
	$$
	\item Omitted-variable formula:
	$$
	\boxed{\mathbb{E}(\bv{b}_1|\bv{X}) = \boldsymbol\beta_1 + \underbrace{(\bv{X}_1'\bv{X}_1)^{-1}(\bv{X}_1'\bv{X}_2)}_{K_1 \times K_2}\boldsymbol\beta_2}
	$$
	(each column of $(\bv{X}_1'\bv{X}_1)^{-1}(\bv{X}_1'\bv{X}_2)$ are the OLS regressors obtained when regressing the columns of $\bv{X}_2$ on $\bv{X}_1$).
\end{itemize}
\end{remark}
\end{scriptsize}
\end{frame}


\begin{frame}{Potential common pitfalls}
\begin{scriptsize}
\begin{exmpl}[Omitted variables \& Effect of education on wages]\label{example:wageeduc}
\begin{itemize}
	\item Consider the ``true model'':
	\begin{equation}
	wage_i = \beta_0 +\beta_1 {\color{blue}edu_i} + \beta_2 {\color{red}ability_i} + \varepsilon_i, \quad \varepsilon_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\varepsilon^2)
	\end{equation}
	\item Further, we assume that the {\color{blue}$edu$} variable is correlated to the {\color{red}$ability$}. Specifically:
	$$
	{\color{blue}edu_i} = \alpha_0 +\alpha_1 {\color{red}ability_i} + \eta_i, \quad \eta_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\eta^2).
	$$
	\item Assume we mistakingly run the regression omitting the {\color{red}$ability$} variable:
	\begin{equation}
	wage_i = \gamma_0 +\gamma_1 {\color{blue}edu_i} + \xi_i.
	\end{equation}
	\item It can be seen that $\xi_i = \varepsilon_i - (\beta_2/\alpha_1) \eta_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\varepsilon^2+(\beta_2/\alpha_1)^2\sigma_\eta^2)$ and that the population regression coefficient is $\gamma_1 = \beta_1 + \beta_2/\alpha_1 \ne \beta_1$.
\end{itemize}
\end{exmpl}
\end{scriptsize}
\end{frame}


\begin{frame}{Potential common pitfalls}
\begin{scriptsize}
\begin{exmpl}[Omitted variables \& Effect of class size on performances]\label{example:STR}
		\begin{figure}
			\caption{California school district data}
			\includegraphics[width=.95\linewidth]{../../figures/Figure_OLS_CaliforniaEduc2.pdf}
			
					\begin{tiny}
		Source of the data \href{https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Caschool.html}{Stock and Watson (2004)}.
		\end{tiny}

		\end{figure}
\end{exmpl}
\end{scriptsize}
\end{frame}

\begin{frame}{Potential common pitfalls}
\addtocounter{exmpl}{-1}
\begin{scriptsize}
\begin{exmpl}[Effect of class size on performances (cont'd)]\label{example:STR}
%\begin{tiny}
\input{../../Rcode/tables/outfile_CaliforniaEduc1.txt}

\vspace{-.5cm}
\input{../../Rcode/tables/outfile_CaliforniaEduc2.txt}
%\input{../../Rcode/tables/outfile_CaliforniaEduc3.txt}

\vspace{-.5cm}
\input{../../Rcode/tables/outfile_CaliforniaEduc4.txt}
%\end{tiny}
\end{exmpl}
\end{scriptsize}
\end{frame}




\begin{frame}{Potential common pitfalls}
\begin{scriptsize}
\begin{remark}[Irrelevant variable]
\begin{itemize}
	\item ``True model'':
	$$
	\bv{y} = \bv{X}_1\boldsymbol\beta_1 + \boldsymbol\varepsilon
	$$
	\item ``Estimated model'':
	$$
	\bv{y} = \bv{X}_1\boldsymbol\beta_1 + \bv{X}_2\boldsymbol\beta_2 + \boldsymbol\varepsilon
	$$
	\item Unbiased estimate. Problem however: the variance of the estimate of $\boldsymbol\beta_1$ resulting from this regression is higher than that stemming from the correct regression (unless the correlation between $\bv{X}_1$ and $\bv{X}_2$ is null, see Eq.\,\ref{eq_multicollin}).
	\item[$\Rightarrow$] The estimator is {\color{blue}inefficient}, i.e. that there exists an alternative consistent estimator whose variance is lower.
	\item The inefficiency problem can have serious consequences when testing hypotheses of type $H_0: \beta_1 = 0$ due to the loss of power, so we might infer that they are no relevant variables when they truly are (Type-II error; False Negative, see Slide \ref{slide:FP_FN}).
\end{itemize}
\end{remark}
\end{scriptsize}
\end{frame}

\subsection{Large sample}

\begin{frame}{Least Squares: Large Sample Properties}\label{slide:largesplm}
\begin{scriptsize}
\begin{itemize}
	\item Even if we relax the normality assumption (Assumption \ref{Assum:normality}), we can approximate the finite-sample behavior of the estimators by using {\color{blue} large-sample} or  {\color{blue} asymptotic properties}.
	\item To begin with, we proceed under Assumptions \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}. (We will see later how to deal with --partial-- relaxations of Assumptions \ref{Assum:homoskedasticity} and \ref{Assum:noncorrel_resid}.)
\end{itemize}

\begin{exampleblock}{Preview of OLS large-sample properties under \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}}
\begin{itemize}
	\item Under \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}, even if the residuals are not normally-distributed, the least square estimators can be {\color{blue}asymptotically normal} and inference can be performed as in small samples when \ref{Assum:fullrank} to \ref{Assum:normality} hold.
	\item This derives from Prop.\,\ref{thm:asymptOLS} (below).
	\item The F-test (Prop.\,\ref{thm:Ftest}) and the t-test described in Slide \ref{parttestcall} can then be performed.
\end{itemize}


\end{exampleblock}

\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{prop}[Asymp. distri. of $\bv{b}$ with independent observations]\label{thm:asymptOLS}
Under Assumptions \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid}, and assuming further that:
\begin{equation}\label{eq:Qasympt}
Q = \mbox{plim}_{n \rightarrow \infty} \frac{\bv{X}'\bv{X}}{n},
\end{equation}
and that the $(\bv{x}_i,\varepsilon_i)$s are independent (across entities $i$), we have:
\begin{equation}\label{eq:convgceOLS}
\sqrt{n}(\bv{b} - \boldsymbol\beta)\overset{d} {\rightarrow} \mathcal{N}\left(0,\sigma^2Q^{-1}\right),
\end{equation}
\end{prop}
\begin{tiny}
\begin{proof}
Since $\bv{b} = \boldsymbol\beta + \left( \frac{\bv{X}'\bv{X}}{n}\right)^{-1}\left(\frac{\bv{X}'\boldsymbol\varepsilon}{n}\right)$, we have: $\sqrt{n}(\bv{b} - \boldsymbol\beta) = \left( \frac{\bv{X}'\bv{X}}{n}\right)^{-1} \left(\frac{1}{\sqrt{n}}\right)\bv{X}'\boldsymbol\varepsilon$. Since $f:A \rightarrow A^{-1}$ is a continuous function (for $A \ne \bv{0}$), $\mbox{plim}_{n \rightarrow \infty} \left(\frac{\bv{X}'\bv{X}}{n}\right)^{-1} = \bv{Q}^{-1}$ (see see Prop.\,\ref{prop:limitingdistri}(ii), continuous mapping theorem). Let us denote by $V_i$ the vector $\bv{x}_i \varepsilon_i$. Because the $(\bv{x}_i,\varepsilon_i)$s are independent, the $V_i$s are independent as well. Their covariance matrix is $\sigma^2\mathbb{E}(\bv{x}_i \bv{x}_i')=\sigma^2Q$. Applying the multivariate central limit theorem (Theorem \ref{thm:MCLT}) on the $V_i$s gives $\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n \bv{x}_i \varepsilon_i\right) = \left(\frac{1}{\sqrt{n}}\right)\bv{X}'\boldsymbol\varepsilon \overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2Q)$. An application of Prop.\;\ref{prop:limitingdistri}(i)  (Slutsky's theorem) then leads to the results.\qed
\end{proof}
\end{tiny}
\begin{itemize}
	\item In practice, $\sigma^2$ is estimated with $\frac{\bv{e}'\bv{e}}{n-K}$ (Eq.\,\ref{eq:s2}) and $\bv{Q}^{-1}$ with $\left(\frac{\bv{X}'\bv{X}}{n}\right)^{-1}$.
	\item Eq.\,(\ref{eq:Qasympt}) and Eq.\,(\ref{eq:convgceOLS}) respectively correspond to convergences in probability (Def.\,\ref{def:convergence_proba}) and in distribution (Def.\,\ref{def:cvgce_distri}) .
\end{itemize}
\end{scriptsize}
\end{frame}


\subsection{IV}

\begin{frame}{Instrumental Variables}
\begin{scriptsize}
\begin{itemize}
	\item Here, we want to relax Assumption \ref{Assum:exogeneity} (conditional mean zero assumption, implying in particular that $\bv{x}_i$ and $\varepsilon_i$ are uncorrelated).
	\item Model:
	\begin{equation}\label{eq:modelIV}
	y_i = \bv{x_i}'\boldsymbol\beta + \varepsilon_i, \quad \mbox{where } \mathbb{E}(\varepsilon_i)=0  \mbox{ and } \bv{x_i}\not\perp \varepsilon_i.
	\end{equation}
\end{itemize}

\begin{defn}[Valid set of instruments]\label{def:instruments}
The $L$-dimensional random variable $\bv{z}_i$ is a {\color{blue}valid set of instruments} if:
\begin{itemize}
	\item[(a)] $\bv{z}_i$ is correlated to $\bv{x}_i$;
	\item[(b)] we have $\mathbb{E}(\boldsymbol\varepsilon|\bv{Z})=0$ and
	\item[(c)] the orthogonal projections of the $\bv{x}_i$s on the $\bv{z}_i$s are not multicollinear.
\end{itemize}
\end{defn}

\begin{exmpl}[A situation where $\mathbb{E}(\varepsilon_i)=0$ and $\bv{x_i}\not\perp \varepsilon_i$]\label{exmpl:bias}
\begin{itemize}
	\item Let us make the assumption $ \bv{x_i}\not\perp \varepsilon_i$ in Model (\ref{eq:modelIV}) more precise:
	$$
		\mathbb{E}(\varepsilon_i)=0 \quad \mbox{and} \quad \mathbb{E}(\varepsilon_i \bv{x_i})=\boldsymbol\gamma
	$$
	by the law of large numbers (Theorem \ref{thm:LLN}) $\mbox{plim}_{n \rightarrow \infty} \bv{X}'\boldsymbol\varepsilon / n = \boldsymbol\gamma$.
	%	\item Notations: $\mbox{plim } \bv{Z}'\bv{Z}/n =: \bv{Q}_{zz}$, $\mbox{plim } \bv{Z}'\bv{X}/n =: \bv{Q}_{zx}$, . 
	\item If $\bv{Q}_{xx} := \mbox{plim } \bv{X}'\bv{X}/n$, the OLS estimator is not consistent because
	$$
	\bv{b} = \boldsymbol\beta + (\bv{X}'\bv{X})^{-1}\bv{X}'\boldsymbol\varepsilon \overset{p}{\rightarrow} \boldsymbol\beta + \bv{Q}_{xx}^{-1}\boldsymbol\gamma \ne \boldsymbol\beta.
	$$
\end{itemize}
\end{exmpl}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{itemize}
	\item If $\bv{z}_i$ is a valid set of instruments, we have:
	$$
	\mbox{plim}\left( \frac{\bv{Z}'\bv{y}}{n} \right) =\mbox{plim}\left( \frac{\bv{Z}'(\bv{X}\boldsymbol\beta + \boldsymbol\varepsilon)}{n} \right) = \mbox{plim}\left( \frac{\bv{Z}'\bv{X}}{n} \right)\boldsymbol\beta
	$$
	Indeed, by the law of large numbers (Theorem \ref{thm:LLN}), $\frac{\bv{Z}'\boldsymbol\varepsilon}{n} \overset{p}{\rightarrow}\mathbb{E}(\bv{z}_i\varepsilon_i)=0$.
	\item If $L = K$, the matrix $\frac{\bv{Z}'\bv{X}}{n}$ is of dimension $K \times K$ and we have:
	$$
	\left[\mbox{plim }\left( \frac{\bv{Z}'\bv{X}}{n} \right)\right]^{-1}\mbox{plim }\left( \frac{\bv{Z}'\bv{y}}{n} \right) = \boldsymbol\beta.
	$$
	\item By continuity of the inverse funct.: $\left[\mbox{plim }\left( \frac{\bv{Z}'\bv{X}}{n} \right)\right]^{-1}=\mbox{plim }\left( \frac{\bv{Z}'\bv{X}}{n} \right)^{-1}$.
	\item The Slutsky Theorem (Prop.\,\ref{prop:limitingdistri})	further implies that
	$$
	\mbox{plim }\left( \frac{\bv{Z}'\bv{X}}{n} \right)^{-1} \mbox{plim }\left( \frac{\bv{Z}'\bv{y}}{n} \right)  = \mbox{plim }\left( \left( \frac{\bv{Z}'\bv{X}}{n} \right)^{-1} \frac{\bv{Z}'\bv{y}}{n} \right).
	$$
	\item Hence $\bv{b}_{iv}$ is consistent if it is defined by:
	$$
	\boxed{\bv{b}_{iv} = (\bv{Z}'\bv{X})^{-1}\bv{Z}'\bv{y}.}
	$$
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}

\begin{prop}[Asymp. distri. of $\bv{b}_{iv}$]\label{thm:IV}
If $\bv{z}_i$ is a $L$-dimensional random variable that constitutes a valid set of instruments (see Def.\,\ref{def:instruments}) and if $L=K$, then the asymptotic distribution of $\bv{b}_{iv}$ is:
$$
\bv{b}_{iv} \overset{d}{\rightarrow} \mathcal{N}\left(\boldsymbol\beta,\frac{\sigma^2}{n}\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\right]^{-1}\right)
$$
where $\mbox{plim } \bv{Z}'\bv{Z}/n =: \bv{Q}_{zz}$, $\mbox{plim } \bv{Z}'\bv{X}/n =: \bv{Q}_{zx}$, $\mbox{plim } \bv{X}'\bv{Z}/n =: \bv{Q}_{xz}$.
\end{prop}
\begin{tiny}
\begin{proof}
The proof is very similar to that of Prop.\,\ref{thm:asymptOLS}, the starting point being that $\bv{b}_{iv} = \boldsymbol\beta + (\bv{Z}'\bv{X})^{-1}\bv{Z}'\boldsymbol\varepsilon$. \qed
\end{proof}
\end{tiny}
\begin{itemize}
	\item When $L=K$, we have:
	$$
	\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\right]^{-1}=Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}
	$$
	\item In practice, to estimate $\mathbb{V}ar(\bv{b}_{iv}) = \frac{\sigma^2}{n}Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}$, we replace $\sigma^2$ by:
	$$
	s_{iv}^2 = \frac{1}{n}\sum_{i=1}^{n} (y_i - \bv{x}_i'\bv{b}_{iv})^2
	$$
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{itemize}
	\item And when $L > K$?
	\item Idea: First regress $\bv{X}$ on the space spanned by $\bv{Z}$ and then regress $\bv{y}$ on the fitted values $\hat{\bv{X}}:=\bv{Z}(\bv{Z}'\bv{Z})^{-1}\bv{Z}'\bv{X}$. That is $\bv{b}_{iv} = (\hat{\bv{X}}'\hat{\bv{X}})^{-1}\hat{\bv{X}}'\bv{y}$:
	\begin{equation}\label{eq:IV}
	\boxed{
	\bv{b}_{iv} = [\bv{X}'\bv{Z}(\bv{Z}'\bv{Z})^{-1}\bv{Z}'\bv{X}]^{-1}\bv{X}'\bv{Z}(\bv{Z}'\bv{Z})^{-1}\bv{Z}'\bv{Y}.
	}
	\end{equation}
	\item In this case, Prop.\,\ref{thm:IV} still holds, with $\bv{b}_{iv}$ given by Eq.\,(\ref{eq:IV}).
	\item $\bv{b}_{iv}$ is also the result of the regression of $\bv{y}$ on $\bv{X^*}$, where the columns of $\bv{X}^*$ are the (othogonal) projections of those of $\bv{X}$ on $\bv{Z}$, i.e. $\bv{X^*} = \bv{P^{Z}X}$ (using the notations introduced in \ref{notation:proj}). Hence the other names of this estimator: {\color{blue}Two-Stage Least Squares} (TSLS).
\end{itemize}
\begin{defn}[Weak instruments]
\begin{itemize}
	\item If the instruments do not properly satisfy Condition (a) in Def.\;\ref{def:instruments} (i.e. if $\bv{x}_i$ and $\bv{z}_i$ are only loosely related), the instruments are said to be {\color{blue} weak}.
	
	\item This problem is for instance discussed in \href{http://scholar.harvard.edu/files/stock/files/testing_for_weak_instruments_in_linear_iv_regression.pdf}{Stock and Yogo (2003)}. See also \href{https://www.pearson.com/us/higher-education/product/Stock-Introduction-to-Econometrics-3rd-Edition/9780138009007.html}{Stock and Watson} pp.\,489-490.
\end{itemize}
\end{defn}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{defn}[Hausman test]\label{Hausman}
\begin{itemize}
	\item The Hausman test can be used to test if IV necessary.
	\item IV techniques are required if  $\mbox{plim}_{n \rightarrow \infty} \bv{X}'\boldsymbol\varepsilon / n \ne 0$.
	\item \href{http://www.jstor.org/stable/1913827?seq=1\#page_scan_tab_contents}{Hausman (1978)} proposes a test of the efficiency of estimators.
	\item Under the null hypothesis two estimators, $\bv{b}_0$ and $\bv{b}_1$, are consistent but $\bv{b}_0$ is (asymptotically) efficient relative to $\bv{b}_1$. Under the alternative hypothesis, $\bv{b}_1$ (IV in the present case) remains consistent but not $\bv{b}_0$ (OLS in the present case).
	\item The test statistic is:
	$$
	H = (\bv{b}_1 - \bv{b}_0)' {\color{black}MPI(\mathbb{V}ar(\bv{b}_1) - \mathbb{V}ar(\bv{b}_0))}(\bv{b}_1 - \bv{b}_0),
	$$
	where $MPI$ is the Moore-Penrose pseudo-inverse (see Def.\,\ref{def:MPI}).
	\item Under the null hypothesis, $H \sim \chi^2(q)$, where $q$ is the rank of $\mathbb{V}ar(\bv{b}_1) - \mathbb{V}ar(\bv{b}_0)$.
\end{itemize}
\end{defn}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{exmpl}[Estimation of price elasticity]
\begin{itemize}
	\item See e.g. \href{http://www.who.int/tobacco/economics/2_2estimatingpriceincomeelasticities.pdf?ua=1}{WHO and estimation of tobacco price elasticity of demand}.
	\item We want to estimate what is the effect on demand of an {\color{blue} exogenous increase} in prices of cigarettes (say).
	\item Model:
	\begin{eqnarray*}
	\underbrace{q^d_t}_{\mbox{log(demand)}} &=& \alpha_0 + \alpha_1 \underbrace{\times p_t}_{\mbox{log(price)}} + \alpha_2 \underbrace{\times w_t}_{\mbox{income}} + \varepsilon_t^d\\
	\underbrace{q^s_t}_{\mbox{log(supply)}} &=& \gamma_0 + \gamma_1 \times p_t + \gamma_2 \underbrace{\times \bv{y}_t}_{\mbox{cost factors}} + \varepsilon_t^s,
	\end{eqnarray*}
	where $\bv{y}_t$, $w_t$, $ \varepsilon_t^s \sim \mathcal{N}(0,\sigma^2_s)$ and $\varepsilon_t^d \sim \mathcal{N}(0,\sigma^2_d)$ are independent.	\item Equilibrium: $q^d_t = q^s_t$. This implies that prices are {\color{blue}endogenous}:
	$$
	p_t = \frac{\alpha_0 + \alpha_2 w_t + \varepsilon_t^d - \gamma_0 - \gamma_2 \bv{y}_t - \varepsilon_t^s}{\gamma_1 - \alpha_1}.
	$$
	\item In particular we have $\mathbb{E}(p_t \varepsilon_t^d) = \frac{\sigma^2_d}{\gamma_1 - \alpha_1} \ne 0$.
	\item [$\Rightarrow$] Regressing by OLS $q_t^d$ on $p_t$ gives biased estimates (see Example\;\ref{exmpl:bias}).
\end{itemize}
\end{exmpl}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\addtocounter{exmpl}{-1}
\begin{exmpl}[Estimation of price elasticity, continued]
		\begin{figure}
			\caption{Demand and Supply}
			\includegraphics[width=1\linewidth]{../../figures/Figure_IV1.pdf}
			
					\begin{tiny}
		%Source of the data \href{https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Caschool.html}{Stock and Watson (2004)}.
		\end{tiny}

		\end{figure}

\end{exmpl}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\addtocounter{exmpl}{-1}
\begin{exmpl}[Estimation of price elasticity, continued]
\begin{itemize}
%	\item What can be done here?
	%\item[$\Rightarrow$] IV with a regression of $p_t$ on $(\bv{y_t},w_t)$ in a first step.
	\item \href{https://rpubs.com/wsundstrom/t_ivreg}{Estimation of the price elasticity of cigarette demand}.
	
	Instrument: real tax on cigarettes arising from the state's general sales tax. Presumption: in states with a larger general sales tax, cigarette prices are higher, but the general tax is not determined by other forces affecting $\varepsilon_t^d$.
\end{itemize}


% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Mon, Jan 30, 2017 - 14:15:54
% Requires LaTeX packages: dcolumn 
\begin{tiny}
\begin{table}[!htbp] \centering 
\caption{IV Regression Results} 
  \label{} 
\begin{tabular}{ccccccccc} 
\hline 
\hline
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & \multicolumn{1}{c}{log(packs)} \\ 
\hline \\[-1.8ex] 
 log(rprice) & $-1.277^{***} $\\ 
  & $(0.263)$ \\ 
  & \\ 
 log(rincome) & $0.280 $\\ 
  &$ (0.239) $\\ 
  & \\ 
 Constant &$ 9.895^{***}$ \\ 
  & $(1.059) $\\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{48} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.429} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.404} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.188 (df = 45)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 
\end{tiny}

\end{exmpl}
\end{scriptsize}
\end{frame}






\begin{frame}{}
\begin{scriptsize}
\begin{exmpl}[Education and Wages]\label{example:wageeducIV}
\begin{itemize}
	\item Potential problem with OLS regression of wages on education: omitted variables (see Example\,\ref{example:wageeduc}).
	\item \href{https://www.nber.org/papers/w4483.pdf}{Card (1993)}'s idea: distance to college used as an instrument.
\end{itemize}

\vspace{-.7cm}
\begin{tiny}
% Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Fri, Feb 14, 2020 - 22:03:25
\begin{table}[!htbp] \centering 
  \caption{Effect of education on wage} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable: wage}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{2}{c}{\textit{OLS}} & \textit{instrumental} \\ 
 & \multicolumn{2}{c}{\textit{}} & \textit{variable} \\ 
\\[-1.8ex] & (1) & (2) & (3)\\ 
\hline \\[-1.8ex] 
 urbanyes & 0.070 & 0.046 & 0.046 \\ 
  & (0.045) & (0.045) & (0.060) \\ 
  genderfemale & $-$0.085$^{**}$ & $-$0.071$^{*}$ & $-$0.071 \\ 
  & (0.037) & (0.037) & (0.050) \\ 
  ethnicityafam & $-$0.556$^{***}$ & $-$0.227$^{***}$ & $-$0.227$^{**}$ \\ 
  & (0.052) & (0.073) & (0.099) \\ 
  ethnicityhispanic & $-$0.544$^{***}$ & $-$0.351$^{***}$ & $-$0.351$^{***}$ \\ 
  & (0.049) & (0.057) & (0.077) \\ 
  unemp & 0.133$^{***}$ & 0.139$^{***}$ & 0.139$^{***}$ \\ 
  & (0.007) & (0.007) & (0.009) \\ 
  education & 0.005 &  & 0.647$^{***}$ \\ 
  & (0.010) &  & (0.136) \\ 
  ed.pred &  & 0.647$^{***}$ &  \\ 
  &  & (0.101) &  \\ 
 \hline \\[-1.8ex] 
Observations & 4,739 & 4,739 & 4,739 \\ 
Adjusted R$^{2}$ & 0.109 & 0.116 & $-$0.614 \\ 
F Statistic (df = 6; 4732) & 97.274$^{***}$ & 104.971$^{***}$ &  \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}
\end{tiny}

\end{exmpl}
\end{scriptsize}
\end{frame}




\subsection{GRM}


\begin{frame}{General Regression Model}
\begin{scriptsize}
\begin{itemize}
	\item We want to relax the assumption according to which the disturbances are uncorrelated with each other (Assumption \ref{Assum:noncorrel_resid}) or the homoskedasticity Assumption \ref{Assum:homoskedasticity}.
	\item We replace the latter two assumptions by the general formulation:
	\begin{eqnarray}
	\mathbb{E}(\boldsymbol\varepsilon \boldsymbol\varepsilon'| \bv{X}) &=& \boldsymbol\Sigma. \label{eq:assumGLS2}
	\end{eqnarray}
	\item Note that Eq.\,(\ref{eq:assumGLS2}) is more general than Assumptions \ref{Assum:homoskedasticity} and \ref{Assum:noncorrel_resid} because the diagonal entries of $\boldsymbol\Sigma$ may be different (not the case under \ref{Assum:homoskedasticity}), and the non-diagonal entries of $\boldsymbol\Sigma$ can be $\ne 0$ (contrary to \ref{Assum:noncorrel_resid}).
\end{itemize}
\begin{defn}[General Regression Model]\label{defn:GRM}
Assumptions \ref{Assum:fullrank} and \ref{Assum:exogeneity}, together with Eq.\,(\ref{eq:assumGLS2}), form the {\color{blue}General Regression Model} (GRM) framework.
\end{defn}
\begin{remark}[A specific case]\label{remark:grm15}
A regression model where Assumptions \ref{Assum:fullrank} to \ref{Assum:noncorrel_resid} hold is a specific case of the {\color{blue}General Regression Model} (GRM) framework.
\end{remark}
\end{scriptsize}
\end{frame}


\begin{frame}{}
\begin{scriptsize}
\begin{itemize}
	\item The GRM context notably allows to model {\color{blue}heteroskedasticity} and {\color{blue}autocorrelation}.
	\item Heteroskedasticity:
	\begin{equation}\label{eq:heteroskedasticity}
	\boldsymbol\Sigma =	\left[	\begin{array}{cccc}
				\sigma_1^2 & 0 & \hdots & 0 \\
				0 & \sigma_2^2 &  & 0 \\
				\vdots && \ddots& \vdots \\
				0 & \hdots & 0 & \sigma_n^2
				\end{array} \right].
	\end{equation}
	\item Autocorrelation:
	\begin{equation}\label{eq:autocorrelation}
	\boldsymbol\Sigma = \sigma^2 \left[	\begin{array}{cccc}
				1 & \rho_{2,1} & \hdots & \rho_{n,1} \\
				\rho_{2,1} & 1 &  & \vdots \\
				\vdots && \ddots& \rho_{n,n-1} \\
				\rho_{n,1} & \rho_{n,2} & \hdots & 1
				\end{array} \right].
	\end{equation}
\end{itemize}
\end{scriptsize}
\end{frame}


\begin{frame}{}
\begin{scriptsize}
\begin{exmpl}[Autocorrelation in the time-series context]\label{exmpl:autocorrelaaa}
\begin{itemize}
	\item Autocorrelation is, in particular, a recurrent problem when time-series data are used (see Section \ref{section:TS}).
	\item In a time-series context, subscript $i$ refers to a date.
	\item Assume for instance that:
	\begin{equation}\label{eq:usual}
	y_i = \bv{x}_i' \boldsymbol\beta + \varepsilon_i
	\end{equation}
	with
	\begin{equation}\label{eq:usual2}
	\varepsilon_i = \rho \varepsilon_{i-1} + v_i, \quad v_i \sim \mathcal{N}(0,\sigma_v^2).
	\end{equation}
	\item In this case, we are in the GRM context, with:
		\begin{equation}\label{eq:Sigma_autocorrel}
	\boldsymbol\Sigma =\frac{ \sigma_v^2}{1 - \rho^2} \left[	\begin{array}{cccc}
				1 & \rho & \hdots & \rho^{n-1} \\
				\rho & 1 &  & \vdots \\
				\vdots && \ddots& \rho \\
				\rho^{n-1} & \rho^{n-2} & \hdots & 1
				\end{array} \right].
	\end{equation}
\end{itemize}
\end{exmpl}
\end{scriptsize}
\end{frame}

%\begin{frame}{}
%\begin{scriptsize}
%\begin{itemize}
%	\item PARTIE sur pp 193-200. HAC
%\end{itemize}
%\end{scriptsize}
%\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{defn}[Generalized Least Squares]\label{exmpl:GLS}
\begin{itemize}
	\item Assume $\boldsymbol\Sigma$ is known (``feasible GLS'').
	\item Because $\boldsymbol\Sigma$ is symmetric positive, it admits a spectral decomposition of the form $\boldsymbol\Sigma = \bv{C} \boldsymbol\Lambda \bv{C}'$, where $\bv{C}$ is an orthogonal matrix (i.e. $\bv{C}\bv{C}'=Id$) and $\boldsymbol\Lambda$ is a diagonal matrix (the diagonal entries are the eigenvalues of $\boldsymbol\Sigma$).
	\item $\boldsymbol\Sigma = (\bv{P}\bv{P}')^{-1}$ with $\bv{P} = \bv{C}\boldsymbol\Lambda^{-1/2}$.
	\item Consider the transformed model:
	$$
	\bv{P}'\bv{y} = \bv{P}'\bv{X}\boldsymbol\beta + \bv{P}'\boldsymbol\varepsilon \quad \mbox{or} \quad \bv{y}^* = \bv{X}^*\boldsymbol\beta + \boldsymbol\varepsilon^*.
	$$
	\item The variance of $\boldsymbol\varepsilon^*$ is $\bv{I}$. In the transformed model, OLS is BLUE (Gauss-Markow Theorem \ref{thm:GaussMarkov}).
	\begin{equation}\label{eq:betaGLS}
	\boxed{\bv{b}_{GLS} = (\bv{X}'\boldsymbol\Sigma^{-1}\bv{X})^{-1}\bv{X}'\boldsymbol\Sigma^{-1}\bv{y}}
	\end{equation}
	\item $\bv{b}_{GLS}$ = {\color{blue}Generalized least squares} estimator of $\boldsymbol\beta$.
	\item We have:
	$$
	\mathbb{V}ar(\bv{b}_{GLS}|\bv{X}) = (\bv{X}'\boldsymbol\Sigma^{-1}\bv{X})^{-1}.
	$$
\end{itemize}
\end{defn}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{itemize}
	\item When $\boldsymbol\Sigma$ is unknown, the GLS estimator is said to be {\color{blue}infeasible}. Some structure is required.
	\item Assume $\boldsymbol\Sigma$ admits a parametric form $\boldsymbol\Sigma(\theta)$.
	\item The estimation becomes "feasible" (FGLS) if one replaces $\boldsymbol\Sigma(\theta)$ by $\boldsymbol\Sigma(\hat\theta)$.
	\item If $\hat\theta$ is a consistent estimator of $\theta$, then the FGLS is asymptotically efficient (see Example \ref{exmpl:autocorrelaa}).
	\item By contrast, when $\boldsymbol\Sigma$ has no obvious structure: the OLS (or IV) is the only estimator available. It remains unbiased, consistent, and asymptotically normally distributed, but not efficient. Standard inference procedures are not appropriate any longer.
\end{itemize}
\begin{exmpl}[Autocorrelation in the time-series context (continued)]\label{exmpl:autocorrelaa}
\begin{itemize}
	\item Consider the case presented in Example \ref{exmpl:autocorrelaaa}.
	\item Because the OLS estimate $\bv{b}$ of $\boldsymbol\beta$ is consistent, the estimates $e_i$s of the $\varepsilon_i$s also are.
	\item Consistent estimators of $\rho$ and $\sigma_v$ are then obtained by regressing  the $e_i$s on  the $e_{i-1}$s.
	\item Using these estimates in Eq.\,(\ref{eq:Sigma_autocorrel}) provides a consistent estimate of $\boldsymbol\Sigma$.
\end{itemize}
\end{exmpl}
\end{scriptsize}
\end{frame}


\begin{frame}{}
\begin{scriptsize}
\begin{prop}[Finite sample properties of $\bv{b}$ in the GRM (Def.\,\ref{defn:GRM})]\label{thm:Asympt_OLS_GRM}%THM 10.1 in Greene
%Regressors and disturbances uncorrelated. $\bv{b}$ still unbiased.
Conditionally on $\bv{X}$, we have:
\begin{equation}\label{eq:xsx}
\mathbb{V}ar(\bv{b}|\bv{X}) = \frac{1}{n}\left(\frac{1}{n}\bv{X}'\bv{X}\right)^{-1}\left(\frac{1}{n}\bv{X}'\boldsymbol\Sigma\bv{X}\right)\left(\frac{1}{n}\bv{X}'\bv{X}\right)^{-1}.
\end{equation}
Under \ref{Assum:normality}, since $\bv{b}$ is linear in $\boldsymbol\varepsilon$,
\begin{equation}
\bv{b}|\bv{X} \sim \mathcal{N}\left(\boldsymbol\beta,\left(\bv{X}'\bv{X}\right)^{-1}\left(\bv{X}'\boldsymbol\Sigma\bv{X}\right)\left(\bv{X}'\bv{X}\right)^{-1}\right).
\end{equation}
\end{prop}
%\begin{proof}
%\begin{tiny}
%XXX. \qed
%\end{tiny}
%\end{proof}
\begin{remark}
The variance of the estimator is not $\sigma^2 (\bv{X}'\bv{X})^{-1}$ any more, so using $s^2 (\bv{X}'\bv{X})^{-1}$ for inference may be misleading.
\end{remark}
\begin{prop}[OLS consistency in the GRM (Def.\,\ref{defn:GRM})]\label{thm:XXX}%THM 10.1 in Greene
If $\mbox{plim }(\bv{X}'\bv{X}/n)$ and $\mbox{plim }(\bv{X}'\boldsymbol\Sigma\bv{X}/n)$ are finite positive definite matrices, then $\mbox{plim }(\bv{b})=\boldsymbol\beta$.
\end{prop}
\begin{tiny}
\begin{proof}
$\mathbb{V}ar(\bv{b})=\mathbb{E}[\mathbb{V}ar(\bv{b}|\bv{X})]+\mathbb{V}ar[\mathbb{E}(\bv{b}|\bv{X})]$. Since $\mathbb{E}(\bv{b}|\bv{X})=\boldsymbol\beta$, $\mathbb{V}ar[\mathbb{E}(\bv{b}|\bv{X})]=0$. Eq.\,(\ref{eq:xsx}) implies that $\mathbb{V}ar(\bv{b}|\bv{X}) \rightarrow 0$. Hence $\bv{b}$ converges in mean square and therefore in probability (Prop.\,\ref{thm:implications_conv}).\qed
\end{proof}
\end{tiny}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}

\begin{prop}[Asymptotic distri. of $\bv{b}$ in the GRM (Def.\,\ref{defn:GRM})]\label{thm:Asympt_GRM}%THM 10.1 in Greene
if $Q_{xx}=\mbox{plim }(\bv{X}'\bv{X}/n)$ and $Q_{x\boldsymbol\Sigma x}=\mbox{plim }(\bv{X}'\boldsymbol\Sigma\bv{X}/n)$ are finite positive definite matrices, then:
$$
\sqrt{n}(\bv{b}-\boldsymbol\beta) \overset{d}{\rightarrow} \mathcal{N}(0,Q_{xx}^{-1}Q_{x\boldsymbol\Sigma x}Q_{xx}^{-1}).
$$
\end{prop}


\begin{prop}[Asymptotic distri. of $\bv{b}_{iv}$ in the GRM]\label{thm:Asympt_IV_GRM}%THM 10.1 in Greene
If regressors and IV variables are ``well-behaved'', then:
$$
\bv{b}_{iv} \overset{a}{\sim} \mathcal{N}(\boldsymbol\beta,\bv{V}_{iv}),
$$
where
$$
\bv{V}_{iv} = \frac{1}{n}(\bv{Q}^*)\mbox{ plim }\left( \frac{1}{n} \bv{Z}'\boldsymbol\Sigma \bv{Z}\right)(\bv{Q}^*)',
$$
with
$$
\bv{Q}^* = [\bv{Q}_{xz}\bv{Q}_{zz}^{-1}\bv{Q}_{zx}]^{-1}\bv{Q}_{xz}\bv{Q}_{zz}^{-1}.
$$
\end{prop}
%\begin{proof}
%\begin{tiny}
%XXX. \qed
%\end{tiny}
%\end{proof}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{itemize}
	\item For practical purposes, one needs to have estimates of $\boldsymbol\Sigma$ in Props. \ref{thm:Asympt_OLS_GRM},  \ref{thm:Asympt_GRM} or \ref{thm:Asympt_IV_GRM}.
	\item Idea: instead of estimating $\boldsymbol\Sigma$ (dimension $n \times n$) directly, one can estimate $\frac{1}{n}\bv{X}'\boldsymbol\Sigma\bv{X}$ (dimension $K \times K$)
	% (or $\frac{1}{n}\bv{Z}'\boldsymbol\Sigma\bv{Z}$ in the IV case) 
	 since this is this expression ($\bv{X}'\boldsymbol\Sigma\bv{X}$) that eventually appears in the formulas (for instance in Eq.\,\ref{eq:xsx}).
	\item We have:
	\begin{equation}\label{eq:General_XSigmaX}
	\frac{1}{n}\bv{X}'\boldsymbol\Sigma\bv{X} =  \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\sigma_{i,j}\bv{x}_i\bv{x}'_j .
	\end{equation}
	\item {\color{blue}Robust estimation of asymptotic covariance matrices} look for estimates of the previous matrix.
	\item Their computation is based on the fact that if $\bv{b}$ is consistent, then the $e_i$s are consistent (pointwise) estimators of the $\varepsilon_i$s.
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}{}
\begin{scriptsize}
\begin{exampleblock}{First case: heteroskedasticity (Eq.\,\ref{eq:heteroskedasticity})}
\begin{itemize}
	\item This is the case of Eq.\,(\ref{eq:heteroskedasticity}).
	\item We then need to estimate $\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}^2\bv{x}_i\bv{x}'_i$.
	\item \href{http://www.jstor.org/stable/1912934}{White (1980)}:  Under general conditions:
	\begin{equation}\label{eq:white}
	\mbox{plim}\left( \frac{1}{n}\sum_{i=1}^{n}\sigma_{i}^2\bv{x}_i\bv{x}'_i \right) = 
	\mbox{plim}\left( \frac{1}{n}\sum_{i=1}^{n}e_{i}^2\bv{x}_i\bv{x}'_i \right).
	\end{equation}
	\item The estimator of $\frac{1}{n}\bv{X}'\boldsymbol\Sigma\bv{X}$ therefore is:
	$$
	\frac{1}{n}\bv{X}'\bv{E}^2\bv{X},
	$$
	where $\bv{E}$ is an $n \times n$ diagonal matrix whose diagonal elements are the estimated residuals $e_i$.
\end{itemize}
\end{exampleblock}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
		\begin{figure}
		\caption{Salaries versus time since PhD obtention}
			\includegraphics[width=.95\linewidth]{../../figures/Figure_OLS_heterosk2.pdf}
			
					\begin{tiny}
		Data from \href{https://vincentarelbundock.github.io/Rdatasets/doc/car/Salaries.html}{Fox J. and Weisberg, S. (2011)}.
		\end{tiny}
		\end{figure}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{itemize}
	\item Let us illustrate the influence of heteroskedasticity using simulations.
	\item We consider the following model:
	$$
	y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2).
	$$
	where the $x_i$s are i.i.d. $t(4)$.
	\item Here is a simulated sample ($n=200$) of this model:
\end{itemize}
		\begin{figure}
		\caption{Simul.: $y_i = x_i + \varepsilon_i, \; \varepsilon_i \sim i.i.d.\mathcal{N}(0,x_i^2)$ and $x_i \sim i.i.d. t(4)$}
			\includegraphics[width=\linewidth]{../../figures/Figure_OLS_GRM_heterosk.pdf}
		\end{figure}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{itemize}
	\item We simulate 1000 samples of the same model with $n=200$.
	\item For each sample, we compute the OLS estimate of $\beta$ (=1).
	\item Using these 1000 estimates of $b$, we construct an approximated {\color{red}(kernel-based) distribution of this OLS estimator} (in red on the figure).
	\item For each of the 1000 OLS estimations, we employ {\color{blue}the standard OLS variance formula ($s^2 (\bv{X}'\bv{X})^{-1}$)} to estimate the variance of $b$. The blue curve is a normal distribution centred on 1 and whose variance is the average of the 1000 previous variance estimates.
\end{itemize}
		\begin{figure}
		\caption{Heteroskedasticity and the inappropriateness of the standard OLS variance formula}
			\includegraphics[width=\linewidth]{../../figures/Figure_OLS_GRM_heterosk2.pdf}
		\end{figure}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{itemize}
	\item The variance of the simulated $b$ is of 0.040 (that is the ``true'' one); the average of the estimated variances based on the standard OLS formula is of 0.005 (``bad'' estimate); the average of the estimated variances based on the White robust covariance matrix is of 0.030 (``better'' estimate).
	\item The standard OLS formula for the variance of $b$ overestimates the precision of this estimator.
	\item For almost 50\% of the simulations, 1 is not included in the 95\% confidence interval of $\beta$ when the computation of the interval is based on the standard OLS formula for the variance of $b$.
	\item When the White robust covariance matrix is used, 1 is not in the 95\% confidence interval of $\beta$ for less than 10\% of the simulations.
\end{itemize}
\end{scriptsize}
\end{frame}


\begin{frame}{}
\begin{scriptsize}
\begin{exampleblock}{Second case: HAC}
\begin{itemize}
	\item This includes the cases of Eqs.\,(\ref{eq:heteroskedasticity}) and (\ref{eq:autocorrelation}).
	\item \href{http://www.jstor.org/stable/1913610}{Newey and West (1987)}: If the correlation between terms $i$ and $j$ gets sufficiently small when $|i-j|$ increases:
	\begin{eqnarray}
	&&\mbox{plim} \left( \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\sigma_{i,j}\bv{x}_i\bv{x}'_j \right) =  \\
	&&\mbox{plim} \left( \frac{1}{n}\sum_{t=1}^{n}e_{t}^2\bv{x}_t\bv{x}'_t +
	\frac{1}{n}\sum_{\ell=1}^{L}\sum_{t=\ell+1}^{n}w_\ell e_{t}e_{t-\ell}(\bv{x}_t\bv{x}'_{t-\ell} + \bv{x}_{t-\ell}\bv{x}'_{t})
	\right) \nonumber \label{eq:NW}
	\end{eqnarray}
	where $w_\ell = 1 - \ell/(L+1)$.
\end{itemize}
\end{exampleblock}
\end{scriptsize}
\end{frame}


\begin{frame}
\begin{scriptsize}
\begin{itemize}
	\item Let us illustrate the influence of autocorrelation using simulations.
	\item We consider the following model:
	\begin{equation}\label{eq:simul11}
	y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2)
	\end{equation}
	where the $x_i$s and the $\varepsilon_i$s are such that:
	\begin{equation}\label{eq:simul22}
	x_i = 0.8 x_{i-1} + u_i \quad and \quad \varepsilon_i = 0.8 \varepsilon_{i-1} + v_i
	\end{equation}
	where the $u_i$s and the $v_i$s are i.i.d. $\mathcal{N}(0,1)$.
	\item Here is a simulated sample ($n=200$) of this model:
\end{itemize}
		\begin{figure}
		\caption{Simulation of Eqs.\,(\ref{eq:simul11}) and (\ref{eq:simul22}) on 200 periods}
			\includegraphics[width=\linewidth]{../../figures/Figure_OLS_GRM_autocorr1.pdf}
		\end{figure}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{itemize}
	\item We simulate 1000 samples of the same model with $n=200$.
	\item For each sample, we compute the OLS estimate of $\beta$ (=1).
	\item Using these 1000 estimates of $b$, we construct an approximated {\color{red}(kernel-based) distribution of this OLS estimator} (in red on the figure).
	\item For each of the 1000 OLS estimations, we employ the {\color{blue}standard OLS variance formula ($s^2 (\bv{X}'\bv{X})^{-1}$)} to estimate the variance of $b$. The blue curve is a normal distribution centred on 1 and whose variance is the average of the 1000 previous variance estimates.
\end{itemize}
		\begin{figure}
		\caption{Auto-correlation and the inappropriateness of the standard OLS variance formula}
			\includegraphics[width=\linewidth]{../../figures/Figure_OLS_GRM_autocorr2.pdf}
		\end{figure}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{itemize}
	\item The variance of the simulated $b$ is of 0.020 (that is the ``true'' one); the average of the estimated variances based on the standard OLS formula is of 0.005 (``bad'' estimate); the average of the estimated variances based on the White robust covariance matrix is of 0.015 (``better'' estimate).
	\item The standard OLS formula for the variance of $b$ overestimates the precision of this estimator.
	\item For about 35\% of the simulations, 1 is not included in the 95\% confidence interval of $\beta$ when the computation of the interval is based on the standard OLS formula for the variance of $b$.
	\item When the Newey-West robust covariance matrix is used, 1 is not in the 95\% confidence interval of $\beta$ for about 13\% of the simulations.
\end{itemize}
\end{scriptsize}
\end{frame}

\begin{frame}
\begin{scriptsize}
\begin{itemize}
	\item For the sake of comparison, let us consider a model with no auto-correlation ($x_i \sim i.i.d. \mathcal{N}(0,2.8)$ and $\varepsilon_i \sim i.i.d. \mathcal{N}(0,2.8)$).
\end{itemize}
		\begin{figure}
			\includegraphics[width=\linewidth]{../../figures/Figure_OLS_GRM_autocorr3.pdf}
		\end{figure}
		\begin{figure}
			\includegraphics[width=\linewidth]{../../figures/Figure_OLS_GRM_autocorr4.pdf}
		\end{figure}
\end{scriptsize}
\end{frame}



\begin{frame}{How to detect autocorrelation in residuals?}
\begin{scriptsize}
\begin{itemize}
	\item Consider the usual regression (say Eq.\,\ref{eq:usual}).
	\item The {\color{blue}Durbin-Watson test} is a typical autocorrelation test. Its test statistic is:
	$$
	DW = \frac{\sum_{i=2}^{n}(e_i - e_{i-1})^2}{\sum_{i=1}^{n}e_i^2}= 2(1 - r) - \underbrace{\frac{e_1^2 + e_n^2}{\sum_{i=1}^{n}e_i^2}}_{\overset{p}{\rightarrow} 0},
	$$
	where $r$ is the slope in the regression of the $e_i$s on the $e_{i-1}$s, i.e.:
	$$
	r = \frac{\sum_{i=2}^{n}e_i e_{i-1}}{\sum_{i=1}^{n-1}e_i^2}.
	$$
	($r$ is a consistent estimator of $\mathbb{C}or(\varepsilon_i,\varepsilon_{i-1})$, i.e. $\rho$ in Eq.\,\ref{eq:usual2}.)
	\item Critical values depend only on T and K: see e.g. \href{http://web.stanford.edu/~clint/bench/dwcrit.htm}{tables}.
	\item The one-sided test for $H_0$: $\rho=0$ against $H_1$: $\rho>0$ is carried out by comparing $DW$ to values $d_L(T, K)$ and $d_U(T, K)$:
	$$
	\left\{
	\begin{array}{ll}
	\mbox{If $DW < d_L$,}&\mbox{ the null hypothesis is rejected;}\\
	\mbox{if $DW > d_U$,}&\mbox{ the hypothesis is not rejected;}\\
	\mbox{If $d_L \le DW \le d_U$,} &\mbox{ no conclusion is drawn.}
	\end{array}
	\right.
	$$
\end{itemize}
\end{scriptsize}
\end{frame}



\begin{frame}{}
\begin{scriptsize}
\begin{center}
\begin{table}
\caption{Properties of the OLS estimator}
\begin{tabular}{l|cccc|}
&  \rotatebox[origin=c]{90}{ Condit. mean-zero} &  \rotatebox[origin=c]{90}{ Homoskedasticity}  & \rotatebox[origin=c]{90}{Uncorrelated residuals} &  \rotatebox[origin=c]{90}{ Normality of disturbances}\\
Under Assumptions \ref{Assum:fullrank}+ &\ref{Assum:exogeneity}  &\ref{Assum:homoskedasticity} &\ref{Assum:noncorrel_resid}&\ref{Assum:normality} \\
\hline
$\bv{b}$ normal in small sample (Slide \ref{slide:bnormal}) & X & X& X&X\\
$\bv{b}$ is BLUE (Thm \ref{thm:GaussMarkov}) & X & X& X &\\
$\bv{b}$ unbiased in small sample (Prop.\,\ref{thm:propOLS}) & X &&&\\
$\bv{b}$ consistent (Prop.\,\ref{thm:XXX})$^*$ & X  & &&\\
$\bv{b}$ $\sim$ normal in large sample (Prop.\,\ref{thm:Asympt_GRM})$^*$ & X &  &&\\
\hline 
\end{tabular}
\end{table}
\begin{center}
\begin{tiny}
$^*$: see however Prop.\,\ref{thm:XXX} and Prop.\,\ref{thm:Asympt_GRM} for additional hypotheses. Specifically $\bv{X}'\bv{X}/n$ and $\bv{X}'\boldsymbol{\Sigma}\bv{X}/n$ must converge in proba. to finite positive definite matrices ($\boldsymbol\Sigma$ is defined in Eq.\,\ref{eq:assumGLS2}).
\end{tiny}
\end{center}
\end{center}
\end{scriptsize}
\end{frame}


%\subsection{Causal interpr.}
%
%\begin{frame}{Regressions and Causal Interpretations}
%\begin{scriptsize}
%\begin{itemize}
%	\item Often, regressions are used to investigate {\color{blue}causal relationships}.
%	\item That is, one wants to have an answer to the question: \textit{"If I raise $X_1$ by 1 unit, what would be the effect on $Y$?"}.
%	\item The causal relationship is fundamentally different from correlation.
%	\item One has to carefully design ones specification in order to get a convincing "causality" answer.
%	\item Context: The true model is:
%	\begin{equation}\label{eq:truemodel}
%	Y_i = \beta_1 \underbrace{X_{1,i}}_{scalar} + \boldsymbol\beta_2' \underbrace{\bv{X}_{2,i}}_{(K-1)\times 1} + \varepsilon_i.
%	\end{equation}
%	\item Under the {\color{blue}conditional mean-zero assumption} \ref{Assum:exogeneity} the OLS provide an unbiased and consistent estimate of $\beta_1$ (Props. \ref{thm:propOLS} for unbiasedness and \ref{thm:XXX} for consistency).
%	\item A weaker (sufficient) assumption is the following:
%\end{itemize}
%\begin{assum}[Conditional mean independence]\label{Assum:CMI}
%$$
%\mathbb{E}(\boldsymbol\varepsilon|\bv{X}_{1}, \bv{X}_{2}) = \mathbb{E}(\boldsymbol\varepsilon|\bv{X}_{2}).
%$$
%\end{assum}
%\end{scriptsize}
%\end{frame}
%
%\begin{frame}
%\begin{scriptsize}
%		\begin{figure}
%			\caption{California school district data}
%			\includegraphics[width=.95\linewidth]{../../figures/Figure_OLS_CaliforniaEduc2.pdf}
%			
%					\begin{tiny}
%		Source of the data \href{https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Caschool.html}{Stock and Watson (2004)}.
%		\end{tiny}
%
%		\end{figure}
%\end{scriptsize}
%\end{frame}
%
%\begin{frame}{}
%\begin{scriptsize}
%%\includegraphics[width=.95\linewidth]{../../figures/Figure_OLS_CaliforniaEduc2.pdf}
%\input{../../Rcode/tables/outfile_CaliforniaEduc1.txt}
%\input{../../Rcode/tables/outfile_CaliforniaEduc2.txt}
%\end{scriptsize}
%\end{frame}
%
%
%\begin{frame}{Regressions and Causal Interpretations}
%\begin{scriptsize}
%\begin{prop}[OLS under the conditional mean independence assum.]
%Under Assumptions \ref{Assum:fullrank} and \ref{Assum:CMI}, and if $\mathbb{E}(\boldsymbol\varepsilon|\bv{X}_{2})$ is linear in $\bv{X}_{2}$, then the OLS estimator of $\beta_1$ is consistent.
%\end{prop}
%\begin{tiny}
%\begin{proof}
%Assume that $\mathbb{E}(\boldsymbol\varepsilon|\bv{X}_{2}) = \gamma_0 +  \bv{X}_{2}\gamma_1$. Denote by $\boldsymbol\nu$ the difference between $\boldsymbol\varepsilon$ and $\mathbb{E}(\boldsymbol\varepsilon|\bv{X}_{1}, \bv{X}_{2})$, i.e. $\boldsymbol\nu = \boldsymbol\varepsilon - \mathbb{E}(\boldsymbol\varepsilon|\bv{X}_{1}, \bv{X}_{2})$ (such that $\mathbb{E}(\boldsymbol\nu|\bv{X}_{1}, \bv{X}_{2})=0$). We have:
%\begin{eqnarray}\label{eq:wqsa}
%\bv{Y} &=& \beta_1 \bv{X}_{1} + \boldsymbol\beta_2' \bv{X}_{2} + \boldsymbol\varepsilon \nonumber\\
%&=& \beta_1 \bv{X}_{1} + \boldsymbol\beta_2' \bv{X}_{2} + \boldsymbol\nu + \mathbb{E}(\boldsymbol\varepsilon|\bv{X}_{1}, \bv{X}_{2})\nonumber\\
%&=& \beta_1 \bv{X}_{1} + \boldsymbol\beta_2' \bv{X}_{2} + \boldsymbol\nu + \gamma_0 + \gamma_1' \bv{X}_{2} \nonumber\\
%&=& \gamma_0 + \beta_1 \bv{X}_{1} + (\boldsymbol\beta_2+\gamma_1)' \bv{X}_{2} + \boldsymbol\nu.
%\end{eqnarray}
%Since $\mathbb{E}(\boldsymbol\nu|\bv{X}_{1}, \bv{X}_{2})=0$, this regression is ``consistent'' with the conditional mean-zero assumption (Assumption \ref{Assum:exogeneity}). Hence, in the last regression, Assumptions \ref{Assum:fullrank} and \ref{Assum:exogeneity} hold for the regression model of Eq.\,(\ref{eq:wqsa}), therefore the OLS estimator of $\beta_1$ is consistent (see Remark\,\ref{remark:grm15} and Prop.\,\ref{thm:XXX}).
%
%\vspace{.2cm}
%Note however that the OLS estimators of the parameters associated to the $\bv{X}_{2,i}$'s are not consistent estimates of $\boldsymbol\beta_2$.\qed
%\end{proof}
%\end{tiny}
%%\begin{remark}
%%$\mathbb{E}(\varepsilon_i|\bv{X}_{2,i})$ is linear in $\bv{X}_{2,i}$ under Assumption \ref{Assum:normality}.
%%\end{remark}
%\end{scriptsize}
%\end{frame}
%
%\begin{frame}{}
%\begin{scriptsize}
%\input{../../Rcode/tables/outfile_CaliforniaEduc1.txt}
%\input{../../Rcode/tables/outfile_CaliforniaEduc2.txt}
%%\input{../../Rcode/tables/outfile_CaliforniaEduc3.txt}
%\input{../../Rcode/tables/outfile_CaliforniaEduc4.txt}
%\end{scriptsize}
%\end{frame}
%
%

# Principal component analysis (PCA) {#PCA}

**Principal component analysis (PCA)** is a classical and easy-to-use statistical method to reduce the dimension of large datasets containing variables that are linearly driven by a relatively small number of factors. This approach is widely used in data analysis and image compression. 

Suppose that we have $T$ observations of a $n$-dimensional random vector $x$, denoted by $x_{1},x_{2},\ldots,x_{T}$. We suppose that each component of $x$ is of mean zero.

Let denote with $X$ the matrix given by $\left[\begin{array}{cccc}
x_{1} & x_{2} & \ldots & x_{T}\end{array}\right]'$. Denote the $j^{th}$ column of $X$ by $X_{j}$.

We want to find the linear combination of the $x_{i}$'s ($x.u$), with $\left\Vert u\right\Vert =1$, with "maximum variance." That is, we want to solve:
\begin{equation}
\underset{u\mbox{ s.t. } \left| u\right| =1}{\arg\max}u'X'Xu.(\#eq:PCA11)
\end{equation}

Since $X'X$ is a positive definite matrix, it admits the following decomposition:
\begin{eqnarray*}
X'X & = & PDP'\\
& = & P\left[\begin{array}{ccc}
\lambda_{1}\\
& \ddots\\
&  & \lambda_{n}
\end{array}\right]P'
\end{eqnarray*}
where $P$ is an orthogonal matrix whose columns are the eigenvectors of $X'X$.

We can order the eigenvalues such that $\lambda_{1}\geq\ldots\geq\lambda_{n}$. Since $X'X$ is positive definite, all these eigenvalues are positive.

Since $P$ is orthogonal, we have $u'X'Xu=u'PDP'u=y'Dy$ where $\left\Vert y\right\Vert =1$. Therefore, we have $y_{i}^{2}\leq 1$ for any $i\leq n$.

As a consequence:
\[
y'Dy=\sum_{i=1}^{n}y_{i}^{2}\lambda_{i}\leq\lambda_{1}\sum_{i=1}^{n}y_{i}^{2}=\lambda_{1}.
\]

It is easily seen that the maximum is reached for $y=\left[1,0,\cdots,0\right]'$. Therefore, the maximum of the optimization program (Eq. \@ref(eq:PCA11))) is obtained for $u=P\left[1,0,\cdots,0\right]'$, i.e. $u$ is the eigenvector of $X'X$ that is associated with its larger eigenvalue (first column of $P$).

Let us denote with $F$ the vector that is given by the matrix product $XP$ (note that its last column is equal to $Xu$). The columns of $F$, denoted by $F_{j}$, are called **factors**. We have:
\[
F'F=P'X'XP=D.
\]
Therefore, in particular, the $F_{j}$ are orthogonal. 

Since $X=FP'$, the $X_{j}$ are linear combinations of the factors. Let us then denote with $\hat{X}_{i,j}$ the part of $X_{i}$ that is explained by factor $F_{j}$, we have:
\begin{eqnarray*}
\hat{X}_{i,j} & = & p_{ij}F_{j}\\
X_{i} & = & \sum_{j}\hat{X}_{i,j}=\sum_{j}p_{ij}F_{j}.
\end{eqnarray*}

Consider the share of variance that is explained --through the $n$ variables ($X_{1},\ldots,X_{n}$)-- by the first factor $F_{1}$:
\begin{eqnarray*}
\frac{\sum_{i}\hat{X}_{i,1}\hat{X}'_{i,1}}{\sum_{i}X_{i}X'_{i}} & = & \frac{\sum_{i}p_{i1}F_{1}F'_{1}p_{i1}}{tr(X'X)}\\
& = & \frac{\sum_{i}p_{i1}^{2}\lambda_{1}}{tr(X'X)}\\
& = & \frac{\lambda_{1}}{\sum_{i}\lambda_{i}}.
\end{eqnarray*}

Intuitively, if the first eigenvalue is large, it means that the first factor embed a large share of the fluctutaions of the $n$ $X_{i}$'s.






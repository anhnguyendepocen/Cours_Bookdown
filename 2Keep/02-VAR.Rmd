
# Vector Auto-Regressive (VAR) models


@Kilian_1998 See [this page](https://rdrr.io/cran/VAR.etp/man/VAR.Boot.html)

Sign restrictions: [package](https://github.com/chrstdanne/VARsignR), @Danne_2015.

@vars
<!-- toBibtex(citation("vars")) -->

@Stargazer


```{r VARBoot, message=FALSE, warning=FALSE}
library(VAR.etp)
library(vars) #standard VAR models
data(dat) # part of VAR.etp package
a <- VAR.Boot(dat,p=2,nb=200,type="const")
b <- VAR(dat,p=2)
rbind(a$coef[1,],(a$coef+a$Bias)[1,],b$varresult$inv$coefficients)
```

### VARs (ans SVARMA) models

VARs are widely used in macroeconomic analysis. While simple and easy to estimate, they make it possible to conveniently capture the dynamics of complex multivariate systems. VAR popularity is notably due to @Sims_1980's influential work. In economics, VAR models are often employed in order to identify *structural* shocks. First, we will present VAR models. Second, we will study its *structural* extension (SVAR models).

:::{.definition #SVAR name="(S)VAR model"}
Let $y_{t}$ denote a $n \times1$ vector of random variables. Process $y_{t}$ follows a $p^{th}$-order VAR if, for all $t$, we have
\begin{eqnarray}
\begin{array}{rllll}
VAR:& y_t &=& c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + \varepsilon_t,& \varepsilon_t:\mbox{ (correlated) innovation}\\
SVAR:& y_t &=& c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + B \eta_t,& \eta_t:\mbox{ ($\bot$) structural shock},
\end{array}(\#eq:yVAR)
\end{eqnarray}
with $\varepsilon_t = B\eta_t$. We assume that $\{\eta_{t}\}$ is a white noise sequence whose components are are mutually and serially independent.
:::

The first line of Eq. \@ref(eq:yVAR) corresponds to the **reduced-form** of the VAR model (**structural form** for the second line).

Eq. \@ref(eq:yVAR) can also be written:
$$
y_{t}=c+\Phi(L)y_{t-1}+\varepsilon_{t},
$$
with $\Phi(L) = \Phi_1 + \Phi_2 L + \dots + \Phi_p L^{p-1}$.

Consequently:
$$
y_{t}\mid y_{t-1},y_{t-2},\ldots,y_{-p+1}\sim \mathcal{N}(c+\Phi_{1}y_{t-1}+\ldots\Phi_{p}y_{t-p},\Omega).
$$

Using @Hamilton_1994's notations, denote with $\Pi$ the matrix $\left[\begin{array}{ccccc}
c & \Phi_{1} & \Phi_{2} & \ldots & \Phi_{p}\end{array}\right]'$ and with $x_{t}$ the vector $\left[\begin{array}{ccccc}
1 & y'_{t-1} & y'_{t-2} & \ldots & y'_{t-p}\end{array}\right]'$, we have:
\begin{equation}
y_{t}= \Pi'x_{t} + \varepsilon_{t}. (\#eq:PIVAR)
\end{equation}
The previous representation is convenient to discuss the estimation of the VAR model, as parameters are gathered in two matrices only: $\Pi$ and $\Omega$.

As was the case for univariate models, VARs can be extended with MA terms in $\eta_t$:

:::{.definition #SVARMA name="(S)VARMA model"}
Let $y_{t}$ denote a $n \times1$ vector of random variables. Process $y_{t}$ follows a VARMA model of order (p,q) if, for all $t$, we have
\begin{eqnarray}
\begin{array}{rllll}
VARMA:& y_t &=& c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + \varepsilon_t + \Theta_1\varepsilon_{t-1} + \dots + \Theta_q ,\\
SVARMA:& y_t &=& c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + B_0 \eta_t+ B_1 \eta_{t-1} + \dots +  B_q \eta_{t-q},
\end{array}(\#eq:yVARMA)
\end{eqnarray}
with $\varepsilon_t = B_0\eta_t$ (and $B_j = \Theta_j B_0$, for $j \ge 0$). We assume that $\{\eta_{t}\}$ is a white noise sequence whose components are are mutually and serially independent.
:::




### IRFs SVARMA


<!-- ```{r NgramIRF, echo=FALSE,fig.cap="Source: Google Ngram. Fraction of books containing the blue and red keywords."} -->
<!-- library(ngramr) -->
<!-- keyw <- c("impulse response function","macroeconomic analysis") -->
<!-- res1 <- ngram(keyw[1],year_start = 1900);res2 <- ngram(keyw[2],year_start = 1900) -->
<!-- plot(res1$Year,res1$Frequency,type="l",lwd=2,xlab="",ylab="",col="blue",las=1) -->
<!-- lines(res2$Year,res2$Frequency,type="l",lwd=2,col="red") -->
<!-- legend("topleft", -->
<!--        keyw,lty=c(1),lwd=c(2), -->
<!--        col=c("blue","red")) -->
<!-- ``` -->

One of the main objectives of macro-econometrics: derivation of IRFs

IRFs = Dynamic effect of structural shocks ($\eta_t$) on endogenous variables ($y_t$)

What are structural shocks? (e.g.

[Bernanke, 1986](https://www.sciencedirect.com/science/article/pii/0167223186900370)
[Ramey, 2016](https://www.nber.org/papers/w21978)

Independent primitive exogenous forces that drive economic variables.

Standard assumptions:

* $\eta_t$ and $y_t$ are $n$-dimensional vectors;
* the components of $\eta_t$ (i.e. $\eta_{1,t},\dots,\eta_{n,t}$) are serially and mutually independent [consistent with {\color{blue}($\ast$)}], zero mean, unit variance;
* $y_t$ is a stationary process driven by $\eta_t$.

IRFs = Conditional expectations:
$$
\boxed{\color{blue}{\Psi_{i,j,h}} = \mathbb{E}(y_{i,t+h}|\color{blue}{\eta_{j,t}=1}) - \mathbb{E}(y_{i,t+h})}
$$
(effect on $y_{i,t+h}$ of a one-unit shock on $\eta_{j,t}$).

If the structural shocks linearly affect $y_t$, then $y_t$ admits the following infinite MA representation (MA($\infty$)):
\begin{equation}
y_t = \mu + \sum_{h=0}^\infty \color{blue}{\Psi_{h}} \eta_{t-h}.(\#eq:InfMA)
\end{equation}

Estimating IRFs amounts to estimating the $\Psi_{h}$'s.

Three main approaches:

* Calibrate and solve a (purely structural) DSGE model at the first order (linearization). The solution takes the form of Eq. \@ref(eq:InfMA).

* Directly estimate the $\Psi_{h}$ based on **projection approaches** (see Section \@ref(LP)).

* Approximate the infinite MA representation by estimating a parsimonious type of model, e.g. **VAR(MA) models** (see Section \@ref(XXX)). Once a (Structural) VARMA representation is obtained, Eq. \@ref(eq:InfMA) is easily deduced.

Eq. \@ref(eq:InfMA) involves an infinite number of parameters, which need to be estimated. MA($\infty$) are often approximated by VARMA processes. VARMA = Linear dynamic model with an auto-regressive (AR) component and a moving average (MA) component:

* AR process -- $y_t$ depends on its own lagged values and one $\eta_t$:
$$
AR(2): \quad y_t = \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + B \eta_t
$$

* MA process -- $y_t$ depends on a finite number of past $\eta_t$'s:
$$
MA(2): \quad y_{t} = B \eta_t+ \Theta_1 B \eta_{t-1}+ \Theta_2 B \eta_{t-2}.
$$

\begin{equation}
\Rightarrow \mbox{VARMA($p$,$q$): } y_t = \underbrace{\Phi_1 y_{t-1} + \dots +\Phi_p y_{t-p}}_{\color{blue}{\mbox{AR component}}} + \underbrace{B \eta_t+ \Theta_1 B \eta_{t-1}+ \dots+ \Theta_q B \eta_{t-q}}_{\color{red}{\mbox{MA component}}}. (\#eq:VARMAstd)
\end{equation}

$\Psi_k$'s (which define IRFs, see Eq. \@ref(eq:InfMA)) are easily deduced from $\Phi_k$'s and $\Theta_k$'s:

Trivial to go from MA(q) to MA($\infty$)
$$
MA(2): \quad y_{t} = \underbrace{C}_{=\Psi_0} \eta_t+ \underbrace{\Theta_1 C}_{=\Psi_1} \eta_{t-1}+ \underbrace{\Theta_2 C}_{=\Psi_2} \eta_{t-2},
$$
and $\Psi_k = 0$ for $k>2$.

AR(p) case: MA($\infty$) representation obtained recursively. Example: AR(2)
\begin{eqnarray*}
y_t &=& \Phi_1 {\color{blue}y_{t-1}} + \Phi_2 y_{t-2} + C \eta_t  \\
&=& \Phi_1 {\color{blue}(\Phi_1 y_{t-2} + \Phi_2 y_{t-3} + C \eta_{t-1})} + \Phi_2 y_{t-2} + C \eta_t  \\
&=& C \eta_t + \Phi_1 C \eta_{t-1} + (\Phi_2 + \Phi_1^2) \color{red}{y_{t-2}} + \Phi_1\Phi_2 y_{t-3}  \\
&=& C \eta_t + \Phi_1 C \eta_{t-1} + (\Phi_2 + \Phi_1^2) \color{red}{(\Phi_1 y_{t-3} + \Phi_2 y_{t-4} + C \eta_{t-2})} + \Phi_1\Phi_2 y_{t-3} \\
&=& \underbrace{C}_{=\Psi_0} \eta_t + \underbrace{\Phi_1 C}_{=\Psi_1} \eta_{t-1} + \underbrace{(\Phi_2 + \Phi_1^2)C}_{=\Psi_2} \eta_{t-2} + f(y_{t-3},y_{t-4}).
\end{eqnarray*}

VARMA($p$,$q$) case: same type of recursive approach to get the $\Psi_k$'s. Very similar to the univariate case: Proposition \@ref(prp:computPsi).

In particular, $B = \Psi_0$ : contemporaneous impact of $\eta_t$ on $y_t$.

Example of IRFs in the context of a VARMA(1,1) model:
\begin{eqnarray}
\quad y_t &=&
\underbrace{\left[\begin{array}{cc}
0.5 & 0.3 \\
-0.4 & 0.7
\end{array}\right]}_{\Phi_1}
y_{t-1} +  
\underbrace{\left[\begin{array}{cc}
1 & 2 \\
-1 & 1
\end{array}\right]}_{B}\eta_t + \underbrace{\left[\begin{array}{cc}
2 & 0 \\
1 & 0.5
\end{array}\right]}_{\Theta_1} \underbrace{\left[\begin{array}{cc}
1 & 2 \\
-1 & 1
\end{array}\right]}_{B}\eta_{t-1}.(\#eq:VARMA111)
\end{eqnarray}


```{r simVAR}
library(AEC)
distri <- list(type=c("gaussian","gaussian"),df=c(4,4))
n <- length(distri$type)
nb.sim <- 30
eps <- simul.distri(distri,nb.sim)
Phi <- array(NaN,c(n,n,1))
Phi[,,1] <- matrix(c(.5,-.4,.3,.7),2,2)
p <- dim(Phi)[3]
Theta <- array(NaN,c(n,n,1))
Theta[,,1] <- -matrix(c(2,1,0,.5),2,2)
q <- dim(Theta)[3]
Mu <- rep(0,n)
C <- matrix(c(1,-1,2,1),2,2)
Model <- list(
  Mu = Mu,
  Phi = Phi,
  Theta = Theta,
  C = C,
  distri = distri
)
Y0 <- rep(0,n)
eta0 <- c(1,0)
res.sim.1 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)
eta0 <- c(0,1)
res.sim.2 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)

par(plt=c(.1,.95,.25,.8))
par(mfrow=c(2,2))
plot(res.sim.1$Y[1,],las=1,
     type="l",lwd=3,xlab="",ylab="",
     main=expression(paste("Response of ",y[1,"*,*",t]," to a one-unit increase in ",eta[1],sep="")))
abline(h=0,col="grey",lty=3)
plot(res.sim.2$Y[1,],las=1,
     type="l",lwd=3,xlab="",ylab="",
     main=expression(paste("Response of ",y[1,"*,*",t]," to a one-unit increase in ",eta[2],sep="")))
abline(h=0,col="grey",lty=3)
plot(res.sim.1$Y[2,],las=1,
     type="l",lwd=3,xlab="",ylab="",
     main=expression(paste("Response of ",y[2,"*,*",t]," to a one-unit increase in ",eta[1],sep="")))
abline(h=0,col="grey",lty=3)
plot(res.sim.2$Y[2,],las=1,
     type="l",lwd=3,xlab="",ylab="",
     main=expression(paste("Response of ",y[2,"*,*",t]," to a one-unit increase in ",eta[2],sep="")))
abline(h=0,col="grey",lty=3)
```

<!-- \includegraphics[width=.9\linewidth]{figures/RcodesFigure_illustrIRF.pdf} -->

<!-- \begin{defn}[Autocovariance of order $j$] -->
<!-- The autocovariance of order $j$ of $y_t$ is $\mathbb{C}ov(y_t,y_{t-j})$. -->
<!-- \end{defn} -->

<!-- \begin{defn}[Covariance-stationary process] -->
<!-- Process $y_t$ is covariance-stationary if $\mathbb{E}(y_t)$ and all autocovariances of $y_t$ are finite and do not depend on $t$. -->
<!-- \end{defn} -->

Let's come back to the infinite MA case (Eq. \@ref(eq:InfMA)):
$$
y_t = \mu + \sum_{h=0}^\infty {\color{blue}\Psi_{h}} \eta_{t-h}.
$$
For $y_t$ to be covariance-stationary (and ergodic for the mean), it has to be the case that

\begin{equation}
\sum_{i=0}^\infty \|\Psi_i\| < \infty,(\#eq:condiInfiniteMA)
\end{equation}
where $\|A\|$ denotes a norm of the matrix $A$ (e.g. $\|A\|=\sqrt{tr(AA')}$).

This notably implies that if $y_t$ is stationary, then $\|\Psi_h\|\rightarrow 0$ when $h$ gets large.

A finite-order MA process is always covariance-stationary.

What should be satisfied by $\Phi_k$'s and $\Theta_k$'s for a VARMA-based process (Eq. \@ref{eq:VARMAstd)) to be stationary?
\begin{eqnarray}
y_t &=& c+ \underbrace{\Phi_1 y_{t-1} + \dots +\Phi_p y_{t-p}}_{{\color{blue}\mbox{AR component}}} +  (\#eq:VARMA2)\\
&&\underbrace{B \eta_t+ \Theta_1 B \eta_{t-1}+ \dots+ \Theta_q B \eta_{t-q}}_{{\color{red}\mbox{MA component}}} \nonumber\\
&\Leftrightarrow& \underbrace{(I - \Phi_1 L - \dots - \Phi_p L^p)}_{= \Phi(L)}y_t = c +  \underbrace{ {\color{red} (I - \Theta_1 L - \ldots - \Theta_q L^q)}}_{={\color{red}\Theta(L)}} B \eta_{t}. \nonumber
\end{eqnarray}

If $y_t$ is stationary, then it has to be that the roots of $\det(\Phi(z))=0$ are outside the unit circle.

Equivalently, the eigenvalues of 
$$
\left[\begin{array}{cccc}
\Phi_{1} & \Phi_{2} & \cdots & \Phi_{p}\\
I & 0 & \cdots & 0\\
0 & \ddots & 0 & 0\\
0 & 0 & I & 0\end{array}\right]
$$
have to be within the unit circle.

Let's derive the first two unconditional moments of a (covariance-stationary) VARMA process.

Based on Eq. \@ref{eq:VARMA2), we have $\mathbb{E}(\Phi(L)y_t)=c \Rightarrow \Phi(1)\mathbb{E}(y_t)=c$. Hence:
$$
\mathbb{E}(y_t) = (I - \Phi_1 - \dots - \Phi_p)^{-1}c.
$$
The autocovariances of $y_t$ can be deduced from the infinite MA representation (Eq. \@ref(eq:InfMA)). 

<!-- \hyperlink{appendix:getMAinf}{\beamergotobutton{methodology to get the $\Psi_k$'s}} -->

We have:
$$
\gamma_j \equiv \mathbb{C}ov(y_t,y_{t-j}) = \sum_{i=j}^\infty \Psi_i \Psi_{i-j}'.
$$
(NB: This infinite sum exists as soon as Eq. \@ref(eq:condiInfiniteMA) is satisfied.)

Conditional means and autocovariances can also be deduced from Eq. \@ref(eq:InfMA). For $0 \le h$ and $0 \le h_1 \le h_2$:

\begin{eqnarray*}
\mathbb{E}_t(y_{t+h}) &=& \mu + \sum_{k=0}^\infty \Psi_{k+h} \eta_{t-k} \\
\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &=& \sum_{k=0}^{h_1} \Psi_{k}\Psi_{k+h_2-h_1}'.
\end{eqnarray*}

The previous formula implies in particular that the forecasting error $y_{t+h} - \mathbb{E}_t(y_{t+h})$ has a variance equal to:
$$
\mathbb{V}ar_t(y_{t+h}) = \sum_{k=1}^{h} \Psi_{k}\Psi_{k}'.
$$
Because the $\eta_t$ are mutually and serially independent (and therefore uncorrelated), we have:
$$
\mathbb{V}ar(\Psi_k \eta_{t-k}) = \mathbb{V}ar\left(\sum_{i=1}^n \psi_{k,i} \eta_{i,t-k}\right)  = \sum_{i=1}^n \psi_{k,i}\psi_{k,i}',
$$
where $\psi_{k,i}$ denotes the $i^{th}$ column of $\Psi_k$.

This suggests the following decomposition of the variance of the forecast error (called  **variance decomposition**):
$$
\mathbb{V}ar_t(y_{t+h}) = \sum_{i=1}^n \underbrace{\sum_{k=1}^{h}  \psi_{k,i}\psi_{k,i}'}_{\mbox{Contribution of $\eta_{i,t}$}}.
$$

The estimation of VAR models is covered in Section \@ref(Section:BasicsVAR). In this case, the $\Phi_k$ matrices can be consistently estimated by simple OLS regressions.

If there is a MA component, OLS regressions yield biased estimates (even for asymptotically large samples).

Assume $y_t$ follows a VARMA(1,1) model. We have:
$$
y_{i,t} = \phi_i y_{t-1} + \varepsilon_{i,t},
$$
where $\phi_i$ is the $i^{th}$ row of $\Phi_1$, and where $\varepsilon_{i,t}$ is a linear combination of $\eta_t$ and $\eta_{t-1}$.

Since $y_{t-1}$ (the regressor) is correlated to $\eta_{t-1}$, it is also correlated to $\varepsilon_{i,t}$.

The OLS regression of $y_{i,t}$ on $y_{t-1}$ yields a biased estimator of $\phi_i$.

Estimation methods of VARMA models will be presented in Section \@ref(Section:AlternII).








### VAR estimation

Let us start with the case where the shocks are Gaussian.

:::{.proposition #estimVARGaussian name="MLE of a Gaussian VAR"}
If $y_t$ follows a VAR(p) (see Definition \@ref(def:SVAR)), and if $\varepsilon_t \sim \,i.i.d.\,\mathcal{N}(0,\Omega)$, then the ML estimate of $\Pi$, denoted by $\hat{\Pi}$ (see Eq. \@ref(eq:PIVAR)), is given by
\begin{equation}
\hat{\Pi}=\left[\sum_{t=1}^{T}x_{t}x'_{t}\right]^{-1}\left[\sum_{t=1}^{T}y_{t}'x_{t}\right]= (\bv{X}'\bv{X})^{-1}\bv{X}'\bv{y},(\#eq:Pi)
\end{equation}
where $\bv{X}$ is the $T \times (np)$ matrix whose $t^{th}$ row is $x_t$ and where $\bv{y}$ is the $T \times n$ matrix whose $t^{th}$ row is $y_{t}'$.

That is, the $i^{th}$ column of $\hat{\Pi}$ ($b_i$, say) is the OLS estimate of $\beta_i$, where:
\begin{equation}
y_{i,t} = \beta_i'x_t + \varepsilon_{i,t},(\#eq:betayx)
\end{equation}
(i.e., $\beta_i' = [c_i,\phi_{i,1}',\dots,\phi_{i,p}']'$).

The ML estimate of $\Omega$, denoted by $\hat{\Omega}$, coincides with the sample covariance matrix of the $n$ series of the OLS residuals in Eq. \@ref(eq:betayx), i.e.:
\begin{equation}
\hat{\Omega} = \frac{1}{T} \sum_{i=1}^T \hat{\varepsilon}_t\hat{\varepsilon}_t',\quad\mbox{with } \hat{\varepsilon}_t= y_t - \hat{\Pi}'x_t.
\end{equation}

The asymptotic distributions of these estimators are the ones resulting from standard OLS formula.
:::


:::{.proof}
See Appendix \@ref(estimVARGaussian).
:::


As stated by Proposition \@ref(OLSVAR), when the shocks are not Gaussian, then the OLS regressions still provide consistent estimates of the model parameters. However, since $x_t$ correlates to $\varepsilon_s$ for $s<t$, the OLS estimator $\bv{b}_i$ of $\boldsymbol\beta_i$ is biased in small sample. (That is also the case for the ML estimator.)

Indeed, denoting by $\boldsymbol\varepsilon_i$ the $T \times 1$ vector of $\varepsilon_{i,t}$'s, and using the notations of $b_i$ and $\beta_i$ introduced in Proposition \@ref(prp:estimVARGaussian), we have:
\begin{equation}
\bv{b}_i = \beta_i + (\bv{X}'\bv{X})^{-1}\bv{X}'\boldsymbol\varepsilon_i.(\#eq:olsar1)
\end{equation}
We have non-zero correlation between $x_t$ and $\varepsilon_{i,s}$ for $s<t$ and, therefore, $\mathbb{E}[(\bv{X}'\bv{X})^{-1}\bv{X}'\boldsymbol\varepsilon_i] \ne 0$.

However, when $y_t$ is covariance stationary, then $\frac{1}{n}\bv{X}'\bv{X}$ converges to a positive definite matrix $\bv{Q}$, and $\frac{1}{n}X'\boldsymbol\varepsilon_i$ converges to 0. Hence $\bv{b}_i \overset{p}{\rightarrow} \beta_i$. More precisely:

:::{.proposition #OLSVAR name="Asymptotic distribution of the OLS estimate of $\beta_i$"}
If $y_t$ follows a VAR model, as defined in Definition \@ref(def:SVAR), we have:
$$
\sqrt{T}(\bv{b}_i-\beta_i) =  \underbrace{\left[\frac{1}{T}\sum_{t=p}^T x_t x_t' \right]^{-1}}_{\overset{p}{\rightarrow} \bv{Q}^{-1}}
\underbrace{\sqrt{T} \left[\frac{1}{T}\sum_{t=1}^T x_t\varepsilon_{i,t} \right]}_{\overset{d}{\rightarrow} \mathcal{N}(0,\sigma_i^2\bv{Q})},
$$
where $\sigma_i = \mathbb{V}ar(\varepsilon_{i,t})$ and where $\bv{Q} = \mbox{plim }\frac{1}{T}\sum_{t=p}^T x_t x_t'$ is given by:
\begin{equation}
\bv{Q} = \left[
\begin{array}{ccccc}
1 & \mu' &\mu' & \dots & \mu' \\
\mu & \gamma_0 + \mu\mu' & \gamma_1 + \mu\mu' & \dots & \gamma_{p-1} + \mu\mu'\\
\mu & \gamma_1 + \mu\mu' & \gamma_0 + \mu\mu' & \dots & \gamma_{p-2} + \mu\mu'\\
\vdots &\vdots &\vdots &\dots &\vdots \\
\mu & \gamma_{p-1} + \mu\mu' & \gamma_{p-2} + \mu\mu' & \dots & \gamma_{0} + \mu\mu'
\end{array}
\right].(\#eq:Qols)
\end{equation}
:::

:::{.proof}
See Appendix \@ref(OLSVAR).
:::

The following proposition extends the previous proposition and includes covariances between different $\beta_i$'s as well as the asymptotic distribution of the ML estimates of $\Omega$.

:::{.proposition #OLSVAR2 name="Asymptotic distribution of the OLS estimates"}
If $y_t$ follows a VAR model, as defined in Definition \@ref(def:SVAR), we have:
\begin{equation}
\sqrt{T}\left[
\begin{array}{c}
vec(\hat\Pi - \Pi)\\
vec(\hat\Omega - \Omega)
\end{array}
\right]
\sim \mathcal{N}\left(0,
\left[
\begin{array}{cc}
\Omega \otimes \bv{Q}^{-1} & 0\\
0 & \Sigma_{22}
\end{array}
\right]\right),(\#eq:asymptPi)
\end{equation}
where the component of $\Sigma_{22}$ corresponding to the covariance between $\hat\sigma_{i,j}$ and $\hat\sigma_{k,l}$ (for $i,j,l,m \in \{1,\dots,n\}^4$) is equal to $\sigma_{i,l}\sigma_{j,m}+\sigma_{i,m}\sigma_{j,l}$.
:::

:::{.proof}
See @Hamilton_1994, Appendix of Chapter 11.
:::

Naturally, in practice, $\Omega$ is replaced by $\hat{\Omega}$, $Q$ is replaced with $\hat{\bv{Q}} = \frac{1}{T}\sum_{t=p}^T x_t x_t'$ and $\Sigma$ with the matrix whose components are of the form $\hat\sigma_{i,l}\hat\sigma_{j,m}+\hat\sigma_{i,m}\hat\sigma_{j,l}$, where the $\hat\sigma_{i,l}$'s are the components of $\hat\Omega$.


The simplicity of the VAR framework and the tractability of its MLE open the way to convenient econometric testing. Let's illustrate this with the likelihood ratio test. The maximum value achieved by the MLE is
$$
\log\mathcal{L}(Y_{T};\hat{\Pi},\hat{\Omega}) = -\frac{Tn}{2}\log(2\pi)+\frac{T}{2}\log\left|\hat{\Omega}^{-1}\right| -\frac{1}{2}\sum_{t=1}^{T}\left[\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\right].
$$
The last term is:
\begin{eqnarray*}
\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t} &=& \mbox{Tr}\left[\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\right] = \mbox{Tr}\left[\sum_{t=1}^{T}\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t}'\right]\\
&=&\mbox{Tr}\left[\hat{\Omega}^{-1}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t}'\right] = \mbox{Tr}\left[\hat{\Omega}^{-1}\left(T\hat{\Omega}\right)\right]=Tn.
\end{eqnarray*}
Therefore, the optimized log-likelihood is simply obtained by:
\begin{equation}
\log\mathcal{L}(Y_{T};\hat{\Pi},\hat{\Omega})=-(Tn/2)\log(2\pi)+(T/2)\log\left|\hat{\Omega}^{-1}\right|-Tn/2.(\#eq:optimzedLogL)
\end{equation}

Assume that we want to test the null hypothesis that a set of variables follows a VAR($p_{0}$) against the alternative
specification of $p_{1}$ ($>p_{0}$).

Let us denote by $\hat{L}_{0}$ and $\hat{L}_{1}$ the maximum log-likelihoods obtained with $p_{0}$ and $p_{1}$ lags, respectively.

Under the null hypothesis ($H_0$: $p=p_0$), we have:
\begin{eqnarray*}
2\left(\hat{L}_{1}-\hat{L}_{0}\right)&=&T\left(\log\left|\hat{\Omega}_{1}^{-1}\right|-\log\left|\hat{\Omega}_{0}^{-1}\right|\right)  \sim \chi^2(n^{2}(p_{1}-p_{0})).
\end{eqnarray*}


**Block exogeneity**

Let's decompose $y_t$ into two subvectors $y^{(1)}_{t}$ ($n_1 \times 1$) and $y^{(2)}_{t}$ ($n_2 \times 1$), with $y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']$ (and therefore $n=n_1 +n_2$), such that:
$$
\left[
\begin{array}{c}
y^{(1)}_{t}\\
y^{(2)}_{t}
\end{array}
\right] = \left[
\begin{array}{cc}
\Phi^{(1,1)} & \Phi^{(1,2)}\\
\Phi^{(2,1)} & \Phi^{(2,2)}
\end{array}
\right]
\left[
\begin{array}{c}
y^{(1)}_{t-1}\\
y^{(2)}_{t-1}
\end{array}
\right] + \varepsilon_t.
$$
One can easily test for block exogeneity of $y_t^{(2)}$ (say). The null assumption ca n be expressed as $\Phi^{(2,1)}=0$ and $\Sigma^{(2,1)}=0$.

**Lag selection**

In a VAR, adding lags consumes numerous degrees of freedom: with $p$ lags, each of the $n$ equations in the VAR contains $n\times p$ coefficients plus the intercept term.

Adding lags improve in-sample fit, but is likely to result in over-parameterization and affect the **out-of-sample** prediction performance.

To select appropriate lag length, so-called **selection criteria** can be used (see Definition \@ref(def:infocriteria)). These criteria have to be minimized. That is, the best specification is the one giving the lowest criteria.

In the context of VAR models, using Eq. \@ref(eq:optimzedLogL), we have:
\begin{eqnarray*}
AIC & = & cst + \log\left|\hat{\Omega}\right|+\frac{2}{T}N\\
BIC & = & cst + \log\left|\hat{\Omega}\right|+\frac{\log T}{T}N,
\end{eqnarray*}
where $N=p \times n^{2}$.

**Companion Form and Stability of a VAR process**

Let us introduce vector $y_{t}^{*}$, whihc stacks the last $p$ values of $y_t$:
$$
y_{t}^{*}=\left[\begin{array}{cccc}
y'_{t} & y'_{t-1} & \ldots & y'_{t-p+1}\end{array}\right]^{'},
$$
Eq. \@ref(eq:yVAR) can then be rewritten in its companion form:
\begin{equation}
y_{t}^{*} =
\underbrace{\left[\begin{array}{c}
c\\
0\\
\vdots\\
0\end{array}\right]}_{=c^*}+
\underbrace{\left[\begin{array}{cccc}
\Phi_{1} & \Phi_{2} & \cdots & \Phi_{p}\\
I & 0 & \cdots & 0\\
0 & \ddots & 0 & 0\\
0 & 0 & I & 0\end{array}\right]}_{=\Phi}
y_{t-1}^{*}+
\underbrace{\left[\begin{array}{c}
\varepsilon_{t}\\
0\\
\vdots\\
0\end{array}\right]}_{\varepsilon_t^*}(\#eq:ystarVAR)
\end{equation}

Matrices $\Phi$ and $\Sigma^* = \mathbb{V}ar(\varepsilon_t^*)$ are of dimension $np \times np$. $\Sigma^*$ is filled with zeros, except the $n\times n$ upper-left block that is equal to $\Sigma = \mathbb{V}ar(\varepsilon_t)$.

We then have:
\begin{eqnarray*}
y_{t}^{*} & = & c^{*}+\Phi\left(c^{*}+\Phi y_{t-2}^{*}+\varepsilon_{t-1}^{*}\right)+\varepsilon_{t}^{*} \nonumber \\
& = & c^{*}+\varepsilon_{t}^{*}+\Phi(c^{*}+\varepsilon_{t-1}^{*})+\ldots+\Phi^{k}(c^{*}+\varepsilon_{t-k}^{*})+\Phi^k y_{t-k}^{*}.
\end{eqnarray*}

If the eigenvalues of $\Phi$ are strictly within the unit circle, then $\Phi^k$ geometrically decays to the zero matrix and we get the following Wold decomposition for $y_t$:
\begin{eqnarray}
y_{t}^{*}  & = & c^{*}+\varepsilon_{t}^{*}+\Phi(c^{*}+\varepsilon_{t-1}^{*})+\ldots+\Phi^{k}(c^{*}+\varepsilon_{t-k}^{*})+\ldots \nonumber \\
& = & \mu^{*} +\varepsilon_{t}^{*}+\Phi\varepsilon_{t-1}^{*}+\ldots+\Phi^{k}\varepsilon_{t-k}^{*}+\ldots,(\#eq:VARstar)
\end{eqnarray}
where $\mu^* = (I - \Phi)^{-1} c^*$.

(It can also be seen that $\mu^{*} = [\mu',\dots,\mu']'$, where $\mu = (I - \Phi_1 - \dots - \Phi_p)^{-1}c$).

The unconditional variance of $y_t$ can be derived from Eq. \@ref(eq:VARstar), exploiting the fact that the $\varepsilon_{t}^{*}$ are serially uncorrelated:
$$
\mathbb{V}ar(y_t^*)=\Omega^*+\Phi\Omega^*\Phi'+\ldots+\Phi^{k}\Omega^*\Phi'^{k}+\ldots,
$$
with $\mathbb{V}ar(\varepsilon_t^*)=\Omega^*$.

The unconditional variance of $y_t$ is the upper-left $n\times n$ block of matrix $\mathbb{V}ar(y_t^*)$.

Eq. \@ref(eq:VARstar) also implies that the $\Psi_k$ matrices defining the IRFs  (see Eq. \@ref(eq:InfMA)) are given by: $\Psi_k = \widetilde{\Phi^k}B$, where $\widetilde{\Phi^k}$ is the upper-left matrix block of $\Phi^k$.

**Granger Causality**

@Granger_1969 developed a method to explore **causal relationships** among variables. The approach consists in determining whether the past values of $y_{1,t}$ can help explain the current $y_{2,t}$ (beyond the information already included in the past values of $y_{2,t}$).

Formally, let us denote three information sets:
\begin{eqnarray*}
\mathcal{I}_{1,t} & = & \left\{ y_{1,t},y_{1,t-1},\ldots\right\} \\
\mathcal{I}_{2,t} & = & \left\{ y_{2,t},y_{2,t-1},\ldots\right\} \\
\mathcal{I}_{t} & = & \left\{ y_{1,t},y_{1,t-1},\ldots y_{2,t},y_{2,t-1},\ldots\right\}.
\end{eqnarray*}
We say that $y_{1,t}$ Granger-causes $y_{2,t}$ if
$$
\mathbb{E}\left[y_{2,t}\mid \mathcal{I}_{2,t-1}\right]\neq \mathbb{E}\left[y_{2,t}\mid \mathcal{I}_{t-1}\right].
$$

To get the intuition behind the testing procedure, consider the following
bivariate VAR($p$) process:
\begin{eqnarray*}
y_{1,t} & = & c_1+\Sigma_{i=1}^{p}\Phi_i^{(11)}y_{1,t-i}+\Sigma_{i=1}^{p}\Phi_i^{(12)}y_{2,t-i}+\varepsilon_{1,t}\\
y_{2,t} & = & c_2+\Sigma_{i=1}^{p}\Phi_i^{(21)}y_{1,t-i}+\Sigma_{i=1}^{p}\Phi_i^{(22)}y_{2,t-i}+\varepsilon_{2,t},
\end{eqnarray*}
where $\Phi_k^{(ij)}$ denotes the element $(i,j)$ of $\Phi_k$.

Then, $y_{1,t}$ is said not to Granger-cause $y_{2,t}$ if
$$
\Phi_1^{(21)}=\Phi_2^{(21)}=\ldots=\Phi_p^{(21)}=0.
$$
Therefore the hypothesis testing is
$$
\begin{cases}
H_{0}: & \Phi_1^{(21)}=\Phi_2^{(21)}=\ldots=\Phi_p^{(21)}=0\\
H_{1}: & \Phi_1^{(21)}\neq0\mbox{ or }\Phi_2^{(21)}\neq0\mbox{ or}\ldots\Phi_p^{(21)}\neq0.\end{cases}
$$
Loosely speaking, we reject $H_{0}$ if some of the coefficients on the lagged $y_{1,t}$'s are statistically significant. Formally, this can be tested using the $F$-test or asymptotic chi-square test. The $F$-statistic is
$$
F=\frac{(RSS-USS)/p}{USS/(T-2p-1)},
$$
where RSS is the Restricted sum of squared residuals and USS is the Unrestricted sum of squared residuals. Under $H_{0}$, the $F$-statistic is distributed as $\mathcal{F}(p,T-2p-1)$.

Note that $pF\underset{T \rightarrow \infty}{\rightarrow}\chi^{2}(p)$. Therefore, for large samples and under $H_0$:
$$
F \sim \chi^{2}(p)/p.
$$

**Factor-Augmented VAR (FAVAR)**


See [Blog by Joao B. Duarte](https://colab.research.google.com/github/jbduarte/blog/blob/master/_notebooks/2020-04-24-FAVAR-Replication.ipynb#scrollTo=-ArJCCP-WUvl)

VAR models are subject to the curse of dimensionality: If $n$, is large, then the number of parameters (in $n^2$) explodes.

In the case where one suspects that the $y_{i,t}$'s are mainly driven by a small number of random sources, a **factor structure** may be imposed (@Bernanke_Boivin_Eliasz_2005).

Let us denote by $f_t$ a $k$-dimensional vector of latent factors accounting for important shares of the variances of the $y_{i,t}$'s (with $k \ll n$) and by $x_t$ is a small $q$-dimensional subset of $y_t$ (with $q \ll n$). The following factor structure is posited:
$$
y_t = \Lambda^f f_t + \Lambda^x x_t + e_t,
$$
where the $e_t$ are ``small'' serially and mutually i.i.d. error terms.

The model is complemented by positing a VAR dynamics for $[f_t',x_t']'$:
\begin{equation}
\left[\begin{array}{c}f_t\\x_t\end{array}\right] = \Phi(L)\left[\begin{array}{c}f_{t-1}\\x_{t-1}\end{array}\right] + v_t.(\#eq:FAVAR)
\end{equation}

$f_t$ e.g. $\equiv$ the first $k$ principal components of $y_t$.

Standard identification techniques of structural shocks can be employed in Eq. \@ref(eq:FAVAR): Cholesky approach can be used for instance if the last component of $x_t$ is the short-term interest rate and if it is assumed that a MP shock has no contemporaneous impact on other macro-variables (in $y_t$).


## Identification Problem and Standard Identification Techniques

**The Identification Issue***

In the previous section, we have seen how to estimate $\Omega$ and the $\Phi_k$ matrices in the context of a VAR model. But the IRFs are functions of $B$ and the $\Phi_k$'s, not of $\Omega$ the $\Phi_k$'s. We have $\Omega = BB'$, but this is not sufficient to recover $B$.

Indeed, seen a system of equations whose unknowns are the $b_{i,j}$'s (components of $B$), the system  $\Omega = BB'$ contains only $n(n+1)/2$ linearly independent equations. Example for $n=2$:
\begin{eqnarray*}
&&\left[
\begin{array}{cc}
\omega_{11} & \omega_{12} \\
\omega_{12} & \omega_{22}
\end{array}
\right] = \left[
\begin{array}{cc}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{array}
\right]\left[
\begin{array}{cc}
b_{11} & b_{21} \\
b_{12} & b_{22}
\end{array}
\right]\\
&\Leftrightarrow&\left[
\begin{array}{cc}
\omega_{11} & \omega_{12} \\
\omega_{12} & \omega_{22}
\end{array}
\right] = \left[
\begin{array}{cc}
b_{11}^2+b_{12}^2 & \color{red}{b_{11}b_{21}+b_{12}b_{22}} \\
\color{red}{b_{11}b_{21}+b_{12}b_{22}} & b_{22}^2 + b_{21}^2
\end{array}
\right].
\end{eqnarray*}

We have 3 linearly independent equations but 4 unknowns. Therefore, $B$ is not identified based on second-order moments. Additional restrictions are required to identify $B$.

This section covers two standard identification schemes: **short-run** and **long-run** restrictions:

1. a **short-run restriction (SRR)** prevents a structural shock from affecting an endogenous variable contemporaneously.
* Easy to implement: the appropriate entries of $B$ are set to 0.
* Particular case: **Cholesky, or recursive approach**.
* Examples: @BERNANKE198649, @Sims_1986, @Gali_1992, @RubioRamirez_et_al_2010.

2. a **long-run restriction (LRR)** prevents a structural shock from having a cumulative impact on one of the endogenous variables. 
* Additional computations are required to implement this. One needs to compute the cumulative effect of one of the structural shocks $u_{t}$ on one of the endogenous variable.
* Examples: @Blanchard_Quah_1989, @Faust_Leeper_1997, @Gali_1999, @Erceg_et_al_2005, @NBERc11177.

The two approaches can be combined (see, e.g., @Gerlach_Smets_1995).

**A Simple Example**

Consider the following stylized economic dynamics:
\begin{equation}
\begin{array}{clll}
g_{t}&=& \bar{g}-\lambda(i_{t-1}-\mathbb{E}_{t-1}\pi_{t})+ \underbrace{{\color{blue}\sigma_d \eta_{d,t}}}_{\mbox{demand shock}}& (\mbox{IS curve})\\
\Delta \pi_{t} & = & \beta (g_{t} - \bar{g})+ \underbrace{{\color{blue}\sigma_{\pi} \eta_{\pi,t}}}_{\mbox{cost push shock}} & (\mbox{Phillips curve})\\
i_{t} & = & \rho i_{t-1} + \left[ \gamma_\pi \mathbb{E}_{t}\pi_{t+1}  + \gamma_g (g_{t} - \bar{g}) \right]\\
&& \qquad \qquad+\underbrace{{\color{blue}\sigma_{mp} \eta_{mp,t}}}_{\mbox{Mon. Pol. shock}} & (\mbox{Taylor rule}),
\end{array}(\#eq:systemI)
\end{equation}
where:
\begin{equation}
\eta_t = 
\left[
\begin{array}{c}
\eta_{\pi,t}\\
\eta_{d,t}\\
\eta_{mp,t}
\end{array}
\right]
\sim i.i.d.\,\mathcal{N}(0,I).(\#eq:covU)
\end{equation}

Vector $\eta_t$ is a vector of Gaussian {\color{blue}structural shocks}, mutually and serially independent.

On date $t$:

* $g_t$ is contemporaneously affected by $\eta_{d,t}$ only;
* $\pi_t$ is contemporaneously affected by $\eta_{\pi,t}$ and $\eta_{d,t}$;
* $i_t$ is contemporaneously affected by $\eta_{mp,t}$, $\eta_{\pi,t}$ and $\eta_{d,t}$.

System \@ref(eq:systemI) could be rewritten in the form:
\begin{equation}
\left[\begin{array}{c}
d_t\\
\pi_t\\
i_t
\end{array}\right]
= \Phi(L)
\left[\begin{array}{c}
d_{t-1}\\
\pi_{t-1}\\
i_{t-1} +
\end{array}\right] +\underbrace{\underbrace{
\left[
\begin{array}{ccc}
0 & \bullet & 0 \\
\bullet & \bullet & 0 \\
\bullet & \bullet & \bullet
\end{array}
\right]}_{=B} \eta_t}_{=\varepsilon_t}(\#eq:BBBB)
\end{equation}

This is the **reduced-form** of the model. This representation suggests three additional restrictions on the entries of $B$; the latter matrix is therefore identified (up to the signs of its columns) as soon as $\Omega = BB'$ is known.

There are particular cases in which some well-known matrix decompositions of $\mathbb{V}ar(\varepsilon_t)=\Omega_\varepsilon$ can be used to easily estimate some specific SVAR.

Consider the following context:

* A first shock (say, $\eta_{n_1,t}$) can affect instantaneously
(i.e., on date $t$) only one of the endogenous variable (say, $y_{n_1,t}$);
* A second shock (say, $\eta_{n_2,t}$) can affect instantaneously
(i.e., on date $t$) the first two endogenous variables (say, $y_{n_1,t}$
and $y_{n_2,t}$);
* ...

This implies (1) that column $n_1$ of $B$ has only 1 non-zero entry (this is the $n_1^{th}$ entry), (2) that column $n_2$ of $B$ has 2 non-zero entries (the $n_1^{th}$ and the $n_2^{th}$ ones), ...

Without loss of generality, we can set $n_1=n$, $n_2=n-1$, ... In this context, matrix $B$ is lower triangular.

The Cholesky decomposition of $\Omega_{\varepsilon}$ then provides an appropriate estimate of $B$, that is a lower triangular matrix $B$ that is such that:
$$
\Omega_\varepsilon = BB'.
$$

For instance, @DEDOLA20051543 estimate 5 structural VAR models for the US, the UK, Germany, France and Italy to analyse the monetary-policy transmission mechanisms. They estimate SVAR(5) models over the period 1975-1997. The shock-identification scheme is based on Cholesky decompositions, the ordering of the endogenous variables being: the industrial production, the consumer price index, a commodity price index, the short-term rate, monetary aggregate and the effective exchange rate (except for the US). This ordering implies that monetary policy reacts to the shocks affecting
the first three variables but that the latter react to monetary policy shocks with a one-period lag.

Importantly, the Cholesky approach can be useful if you are essentially interested in one structural shock. This was the case, e.g., of @Christiano_Eichenbaum_Evans_1996. Their identification is based on the following relationship between $\varepsilon_t$ and $\eta_t$:
$$
\left[\begin{array}{c}
\boldsymbol\varepsilon_{S,t}\\
\varepsilon_{r,t}\\
\boldsymbol\varepsilon_{F,t}
\end{array}\right] =
\left[\begin{array}{ccc}
B_{SS} & 0 & 0 \\
B_{rS} & B_{rr} & 0 \\
B_{FS} & B_{Fr} & B_{FF}
\end{array}\right]
\left[\begin{array}{c}
\boldsymbol\eta_{S,t}\\
\eta_{r,t}\\
\boldsymbol\eta_{F,t}
\end{array}\right],
$$
where $S$, $r$ and $F$ respectively correspond to *slow-moving variables*, the policy variable (short-term rate) and *fast-moving variables*. While $\eta_{r,t}$ is scalar, $\boldsymbol\eta_{S,t}$ and $\boldsymbol\eta_{F,t}$ may be vectors. The space spanned by $\boldsymbol\varepsilon_{S,t}$ is the same as that spanned by $\boldsymbol\eta_{S,t}$. As a result, because $\varepsilon_{r,t}$ is a linear combination of $\eta_{r,t}$ and $\boldsymbol\eta_{S,t}$ (which are $\perp$), it comes that the $B_{rr}\eta_{r,t}$'s are the (population) residuals in the regression of $\varepsilon_{r,t}$ on $\boldsymbol\varepsilon_{S,t}$. Because $\mathbb{V}ar(\eta_{r,t})=1$, $B_{rr}$ is given by the square root of the variance of  $B_{rr}\eta_{r,t}$. $B_{F,r}$ is finally obtained by regressing the components of $\boldsymbol\varepsilon_{F,t}$ on $\eta_{r,t}$.

An equivalent approach consists in computing the Cholesky decomposition of $BB'$ and keep only the column corresponding to the policy variable.

**Long-run restrictions**

A second type of restriction relates to the long-run influence of a shock on an endogenous variable. Let us consider for instance a structural shock that is assumed to have no "long-run influence" on GDP. How to express this? The long-run change in GDP can be expressed as $GDP_{t+h} - GDP_t$, with $h$ large. Note further that:
$$
GDP_{t+h} - GDP_t = \Delta GDP_{t+h} +\Delta GDP_{t+h-1} + \dots + \Delta GDP_{t+1}.
$$
Hence, the fact that a given structural shock ($\eta_{i,t}$, say) has no long-run influence on GDP means that
$$
\lim_{h\rightarrow\infty}\frac{\partial GDP_{t+h}}{\partial \eta_{i,t}} = \lim_{h\rightarrow\infty} \frac{\partial}{\partial \eta_{i,t}}\left(\sum_{k=1}^h \Delta  GDP_{t+k}\right)= 0.
$$

This can be easily formulated as a function of $B$ when $y_t$ (including $\Delta GDP_t$) follows a VAR(MA) process.

As was shown previously (Eq. \@ref(eq:ystarVAR)), one can always write a VAR($p$) as a VAR(1). Consequently, let us focus on the VAR(1) case:
\begin{eqnarray}
y_{t} &=& c+\Phi y_{t-1}+\varepsilon_{t}\\
& = & c+\varepsilon_{t}+\Phi(c+\varepsilon_{t-1})+\ldots+\Phi^{k}(c+\varepsilon_{t-k})+\ldots \nonumber \\
& = & \mu +\varepsilon_{t}+\Phi\varepsilon_{t-1}+\ldots+\Phi^{k}\varepsilon_{t-k}+\ldots \\
& = & \mu +B\eta_{t}+\Phi B\eta_{t-1}+\ldots+\Phi^{k}B\eta_{t-k}+\ldots,
\end{eqnarray}

The sequence of shocks $\{\eta_t\}$ determines the sequence $\{y_t\}$. What if $\{\eta_t\}$ is replaced by $\{\tilde{\eta}_t\}$, where $\tilde{\eta}_t=\eta_t$ if $t \ne s$ and $\tilde{\eta}_s=\eta_s + \gamma$?

Assume $\{\tilde{y}_t\}$ is the associated "perturbated" sequence. We have $\tilde{y}_t = y_t$ if $t<s$. For $t \ge s$, the Wold decomposition of $\{\tilde{y}_t\}$ implies:
$$
\tilde{y}_t = y_t + \Phi^{t-s} B \gamma.
$$

Therefore, the cumulative impact of $\gamma$ on $\tilde{y}_t$ will be (for $t \ge s$):
\begin{eqnarray}
(\tilde{y}_t - y_t) +  (\tilde{y}_{t-1} - y_{t-1}) + \dots +  (\tilde{y}_s - y_s) &=& \nonumber \\
(Id + \Phi + \Phi^2 + \dots + \Phi^{t-s}) B \gamma.&& (\#eq:cumul)
\end{eqnarray}

Consider a shock on $\eta_{1,t}$, with a magnitude of $1$. This shock is $\gamma = [1,0,\dots,0]'$. Given Eq. \@ref{eq:cumul), the long-run cumulative effect of this shock on the endogenous variables is given by:
$$
(Id+\Phi+\ldots+\Phi^{k}+\ldots)B\left[\begin{array}{c}
1\\
0\\
\vdots\\
0\end{array}\right],
$$
that is the first column of $\Theta \equiv (I+\Phi+\ldots+\Phi^{k}+\ldots)B$.

In this context, consider the following long-run restriction: *"$j^{th}$ structural shock has no cumulative impact on the $i^{th}$ endogenous variable"* is equivalent to
$$
\Theta_{ij}=0,
$$
where $\Theta_{ij}$ is the element $(i,j)$ of $\Theta$.

@Blanchard_Quah_1989 implement long-run restrictions in a small-scale VAR. Two variables are considered: GDP and unemployment. Consequently, the VAR is affected by two types of shocks. The authors want to identify **supply shocks** (that can have a permanent effect on output) and **demand shocks** (that cannot have a permanent effect on output).

The motivation of the authors regarding their long-run restrictions can be obtained from a traditional Keynesian view of fluctuations. The authors propose a variant of a model from @Fischer_1977:
\begin{eqnarray}
Y_{t} & = & M_{t}-P_{t}+a.\theta_{t}(\#eq:demand)\\
Y_{t} & = & N_{t}+\theta_{t}(\#eq:prodfunct)\\
P_{t} & = & W_{t}-\theta_{t}(\#eq:PS)\\
W_{t} & = & W\mid\left\{ \mathbb{E}_{t-1}N_{t}=\overline{N}\right\}. (\#eq:WS)
\end{eqnarray}
To close the model, the authors assume the following dynamics for the money supply and the productivity:
\begin{eqnarray*}
M_{t} & = & M_{t-1}+\varepsilon_{t}^{d}\\
\theta_{t} & = & \theta_{t-1}+\varepsilon_{t}^{s}.
\end{eqnarray*}
In this context, it can be shown that
\begin{eqnarray*}
\Delta Y_{t} & = & (\varepsilon_{t}^{d}-\varepsilon_{t-1}^{d})+a.(\varepsilon_{t}^{s}-\varepsilon_{t-1}^{s})+\varepsilon_{t}^{s}\\
u_{t} & = & -\varepsilon_{t}^{d}-a\varepsilon_{t}^{s}
\end{eqnarray*}
Then, it appears that the demand shocks have no long-run cumulative impact on $\Delta Y_{t}$, the GDP growth, i.e. no long-term impact on output $Y_t$. The vector of endogenous variables is $y_t = [\Delta Y_{t} \quad u_{t}]'$ where $\Delta Y_{t}$ denotes the GDP growth. Estimation data are quarterly, and span the period from 1950:2 to 1987:4; 8 lags are used in the VAR model.



### Sign restrictions {#Signs}



Let us go back to the VAR(1) case:\[ y_{t}=c+\Phi
y_{t-1}+\varepsilon_{t}.\] 

Remember: any $VAR(p)$ can be
written in the form of a VAR(1) companion $VAR(1)$.

The **residuals
$\varepsilon_{t}$ are assumed to be some linear combinations of
the \textcolor{blue}{structural shocks $\eta_{t}$}, that is:\[
\varepsilon_{t}=B\eta_{t}.\]

$\eta_t$ is a vector of Gaussian **structural shocks**, mutually and serially independent:
\[\eta_t = \sim i.i.d.\,\mathcal{N}(0,\Omega_\eta), \quad \mbox{ with }
\Omega_\eta = Id.
\]

$B$ has to satisfy
\[\Omega_{\varepsilon}=B\Omega_{\eta}B'=BB',\] which corresponds to
$n(n+1)/2$ restrictions ($<n^2$ required to identify $B$).

$\Rightarrow$ We need some restrictions on $B$.

So far **Short-run restrictions**

$B$ is lower triangular. There exists a
unique lower triangular matrix $P$ (Cholesky decomposition) such
that $PP'=\Omega_{\varepsilon}$

$B=P$: identification!

**Long-run restrictions**

Cumulative effect of the shock $\eta_{t}$ from time $t$ to
$t+\infty$:
\[\bar y_{t,t+\infty}=\sum_{j=0}^{+\infty}\Phi^jB\eta_{t}=\underbrace{(I-\Phi)^{-1}B}_{D}\eta_{t}\]

Note that
\[DD'= (I -\Phi)^{-1}BB'(I
-\Phi)^{-1'} = \underbrace{(I -\Phi)^{-1}\Omega_{\varepsilon}(I
-\Phi)^{-1'}}_{\text{known}}\]

There exists a unique lower triangular matrix $P$ such that $PP' =(I-\Phi)^{-1}\Omega_{\varepsilon}(I -\Phi)^{-1'}$. If we assume $D$ is lower triangular, then $D=P$.

$B=(I-\Phi)P$: identification!


Limits: Restrictions are either short-run or long-run

What about intermediate horizons? Forecast error variance maximization

Zero restrictions = very strong assumptions

Macro variables typically comove within a quarter (or worse, a year) \

**Sign restrictions**

In the zero short-run restriction identification we used the fact that $\Omega_{\varepsilon} = BB'$ and $\Omega_{\varepsilon} = PP'$ where the lower triangular $P$ matrix is the Cholesky decomposition of $\Omega_{\varepsilon}$.

An **orthonormal matrix** $Q$ is a matrix such that \[QQ' = I,\] i.e., all columns (rows) of $Q$ are are
orthogonal and unit vectors:
\[q_i'q_j=0\text{ if }i\neq j\text{ and }q_i'q_j=1\text{ if }i= j,\]
where $q_i$ is the $i^{th}$ column of $Q$.

For a given random {\color{red}orthonormal matrix} we have that \[\Omega_{\varepsilon} = PP' = PQQ'P'  =\mathcal{P}\mathcal{P}'\] where $\mathcal{P}=PQ$ is generally not triangular anymore.

$B = \mathcal{P}$ is clearly a valid solution to the identification problem. But $Q$ is a random matrix... is the solution $B = \mathcal{P}$ plausible? Identification is achieved by checking whether the impulse responses implied by $Q$ satisfy a set of a priori (and possibly theory-driven) sign restrictions. We can draw as many $Q$ as we want and construct a distribution of the solutions that satisfy the sign
restrictions.

Important issue: What does the set of orthonormal matrices $Q$ look like and how can we manage to truly draw $Q$s randomly from that set?

In linear algebra, a rotation matrix is a matrix that is used to perform a rotation in Euclidean space.

For example, in $\mathbb{R}^2$,
\[Q_x=\begin{pmatrix}\cos(x)&\cos\left(x+\frac{\pi}{2}\right)\\
\sin(x)&\sin\left(x+\frac{\pi}{2}\right)\end{pmatrix}=\begin{pmatrix}\cos(x)&-\sin(x)\\
\sin(x)&\cos(x)\end{pmatrix}\]
applies an $x$ angle counter-clockwise rotation to a vector.

$Q_x=\begin{pmatrix}\cos(x)&-\sin(x)\\
\sin(x)&\cos(x)\end{pmatrix}$ is orthonormal (its vectors lie on the unit circle and are orthogonal).

It is parametrized by $x$. $x\in[0,2\pi]$ spans the entire space of rotation matrices in $\mathbb{R}^2$.

By drawing $x$ randomly from $U[0,2\pi]$, we draw randomly from the set of $2\times2$ rotation matrices.

It is not always possible to parametrize a rotation matrix (high-dimentional VARs).

How do we do then?


@Arias_et_al_2018 provide a procedure. Note: any square matrix $X$ may be decomposed as $X=QR$ where $Q$ is an orthogonal matrix and $R$ is an upper diagonal matrix ($QR$ decomposition).

1. Draw a random matrix $X$ by drawing each element from independent standard normal distribution.
2. Let $X = QR$ be the $QR$ decomposition of $X$ with the diagonal of $R$ normalized to be
positive. The random matrix $Q$ is orthogonal and is a draw from the uniform distribution over the set of orthogonal matrices.


**Algorithm**

1. Draw a random orthonormal matrix $Q$
2. Compute $B = PQ$ where $P$ is the Cholesky decomposition of the reduced form residuals $\Omega_{\varepsilon}$
3. Compute the impulse response associated with $B$ \[ y_{t,t+k}=\Phi^kB\] or the cumulated response \[\bar y_{t,t+k}=\sum_{j=0}^{k}\Phi^jB\]
4. Are the sign restrictions satisfied?
a. **Yes**. Store the impulse response
b. **No**. Discard the impulse response
5. Perform N replications and report the median impulse response (and its confidence intervals)


Note: to take into account the uncertainty in $B$ and $\Phi$, you can draw $B$ and $\Phi$ in steps 2 and 3 using an inference method.

**Advantages:**

* Relatively agnostic
* Flexible (can impose sign restrictions on any variable, at any horizon)


Example: @Uhlig_2005 "What are the effects of monetary policy on output? Results from an agnostic
identification procedure,." US monthly data from 1965.I to 2003.XII

Before asking what are the effects of a monetary policy shocks we should be asking, what is a **monetary policy shock?** 
In the inflation targeting era a monetary policy shock is an increase in the policy rate that is ordered last in a Cholesky decomposition? has no permanent effect on output?  ....?

According to conventional wisdom, monetary contractions should 

* Raise the federal funds rate
* Lower prices
* Decrease non-borrowed reserves
* Reduce real output

Standard identification schemes do not fully accomplish the 4 points above 

* Liquidity puzzle: when identifying monetary policy shocks as surprise increases in the stock of money, interest rates tend to go up, not down 
* Price puzzle: after a contractionary monetary policy shock, even with interest rates going up and money supply going down, inflation goes up rather than down

Successful identification needs to deliver results matching the conventional wisdom

@Uhlig_2005's assumption: a contractionary monetary policy shock does not lead to

* Increases in prices
* Increase in nonborrowed reserves
* Decreases in the federal funds rate

How about output? Since is the response of interest, we leave it un-restricted.


Another approach

If $B$ satisfies $BB'=\Omega_\varepsilon$, the vectors of $B$ are called **impulse vectors**

Let $b$ be en impulse vector, there exists a unit-length vector $q$ (i.e. $q'q=1$) so that \[b=Pq\]

$b$ describes the response of the variables to a shock that is a linear combination (with weights given by $q$) of the shocks associated to the Cholesky decomposition. It's a "candidate shock" for the sign restrictions.

Note that $b$ is associated to a unique unit-length vector $q$. To span the
potential $b$s, it is enough to span the $q$s.

Let $\psi_k^i$ be the vector response at horizon $k$ to the $i^{th}$ shock in the
Cholesky decomposition of $\Omega_\varepsilon$. The $\psi_k^i$ are the columns of $\Psi_k$, defined as follows:
\[\Psi_k=\Phi^kP.\]

The impulse response $\psi_k(q)$ associated to $q$ is then given by
\[\psi_k(q)=\sum_{i=1}^m q_i\psi_k^i=\Psi_kq\]

1. Compute $\Psi_k=\Phi^kP$ (or $\sum_{j=0}^k\Psi_k$) using $P$,
the Cholesky decomposition of $\Omega_\varepsilon$.
2. Draw a random orthonormal vector $q$
3. Compute the impulse response associated with $q$ \[ y_{t,t+k}=\Psi_kq\] or the cumulated response \[\bar y_{t,t+k}=\left(\sum_{j=0}^k\Psi_k\right)q\]
4. Are the sign restrictions satisfied?

a. **Yes**. Store the impulse response
b. **No**. Discard the impulse response

5. Perform N times (starting from 2.) and report the median impulse response (and its confidence intervals)

How to draw a unit-length vector?


1. Draw a vector $u$ from the standard normal distribution on
$\mathbb{R}^m$.
2. Compute $q = u/||u||$.

What if we identify $l>1$ shocks? We need to draw $q_1$,...,$q_l$ that are orthonormal.

* For $1\le j\le l$, draw $u_j\in \mathbb{R}^{m+1-j}$ from a standard normal distribution and set $w_j = u_j/||u_j||$. 
* Define $\begin{pmatrix}q_1&...&q_l\end{pmatrix}$ recursively by $q_j = K_jw_j$ for any matrix $K_j$ whose columns form an orthonormal basis for the null space of the matrix \[M_j = \begin{pmatrix} q_1&...&q_{j-1}\end{pmatrix}'\] ($q_j$ will be orthogonal to $\begin{pmatrix} q_1&...&q_{j-1}\end{pmatrix}$)


Characteristics of sign restrictions: there is not a unique IRF (set-identified, not point-identified). 

By drawing the rotation matrices ($Q$) from a uniform distribution and taking the median, we adopt an agnostic approach: "pure sign restriction approach" 


Alternative approach: "Penalty-function approach" (PFA, @Uhlig_2005, present in @Danne_2015's package)

Penalizes sign violation, rewards correct sign (but also magnitude)

Selects the "best" rotation matrix (equivalently, the ``best'' IRF).

**PFA**



Define the **penalty function**:
\[\begin{array}{llll}f(x)&=&x&\text{ if }x\le0\\
&&100.x&\text{ if }x>0\end{array}\] which penalizes positive
responses and rewards negative responses.

Let $\psi_k^j(q)$ be the impulse response of variable $j$. The $\psi_k^j(q)$s are the elements of $\psi_k(q)=\Psi_kq$.

Let $\sigma_j$ be the standard deviation of variable $j$. Let $\iota_{j,k}=1$ if we restrict the response of variable $j$ at the $k^th$ horizon to be negative, $\iota_{j,k}=-1$ if we restrict it to be positive, and $\iota_{j,k}=0$ if there is no restriction. The total penalty is \[\mathbf{P}(q)=\sum_{j=1}^m\sum_{k=0}^Kf\left(\iota_{j,k}\frac{\psi_k^j(q)}{\sigma_j}\right)\]

We are looking for a solution to
\[\begin{array}{ll}&\min_q \mathbf{P}(q)\\
&\\
\text{s.t. }&q'q=1\end{array}\]

Here you can use standard minimization routines in R or Matlab.

**Combining zero and sign restrictions**

Sometimes we need to combine more then one restriction approach. For instance:
* one shock satisfies both zero and sign restrictions
* some shocks can be identified with zero restrictions (SR or LR), others with sign restrictions
* Some shocks satisfy the same zero restrictions (e.g. no LR effect on output) but can be distinguished from each other through sign restrictions

We must make independent draws from the set of all structural parameters satisfying the zero
restrictions. How to do that? 

@Arias_et_al_2018: Idea: impose zero restrictions on $B$, then check signs 

Remember, $B=PQ$ is a candidate impact IRF. For each structural shock $j$, define the $m$-column matrices

$Z_j$ (zero restrictions) and \$S_j$ (sign restrictions).

Each row corresponds to one zero (or sign) restriction. 

$Z_j$ has $m-j$ rows at most (i.e. $m-j$ zero restriction at most).

Example: In a 4-variable VAR, we want to impose that the first structural shock has no effect on variable 1, affects positively variable 2 and negatively variable 3 on impact:
\[Z_1 = \begin{pmatrix}1 & 0 & 0 & 0\end{pmatrix} \]
\[S_1 = \begin{pmatrix}0 & 1 & 0 & 0\\
0 & 0 & -1 & 0\end{pmatrix} \]

For both zero and sign restrictions to be satisfied, we must have that \[Z_jb_j=0\] \[S_jb_j>0\] where $b_j$ is the $j^{th}$ column of $B$, i.e. the impact effect of the $j^{th}$ structural shock.

**Algorithm**

1. For $1\le j\le m$, draw $u_j\in \mathbb{R}^{m+1-j-z_j}$, where $z_j$ is the number of zero restrictions imposed on the $j^{th}$ shock, from a standard normal distribution and set $w_j = u_j/||u_j||$.
2. Define $Q= \begin{pmatrix}q_1&...&q_m\end{pmatrix}$ recursively by $q_j = K_jw_j$ for any matrix $K_j$ whose columns form an orthonormal basis for the null space of the matrix \[M_j =
\begin{pmatrix} q_1&...&q_{j-1}&{\color{blue}(Z_jP)'}\end{pmatrix}'\] ($q_j$ will be orthogonal to $\begin{pmatrix} q_1&...&q_{j-1}\end{pmatrix}$ and satisfy the zero restriction) 
3. Set $B=PQ$.
4. Check sign restrictions ($S_jb_j>0$ for all $j$?).

Perform N replications and report the median impulse response (and its confidence intervals)


**Forecast error variance maximization**

See also @BARSKY2011273: *The news shock is identified as the shock orthogonal to the innovation in current utilization-adjusted TFP that best explains variation in future TFP.*

@Uhlig_2004: We can write $y_t$ as \[y_t=\sum_{h=0}^{+\infty}\Phi^h\varepsilon_{t-h}\] where $\varepsilon_t$ are the residuals.

Let $P$ be the Cholesky decomposition $\Omega_\varepsilon=PP'$.

With the Cholesky decomposition
\[\begin{array}{lll}y_t&=&\sum_{h=0}^{+\infty}\Phi^h\underbrace{\varepsilon_{t-h}}_{P\eta_{t-h}}\\
&=&\sum_{h=0}^{+\infty}\underbrace{\Phi^hP}_{\Psi_h}\eta_{t-h}\\
&=&\sum_{h=0}^{+\infty}\Psi_h\eta_{t-h}\end{array}\] where $\Psi_h=\Phi^hP$ are the impulse responses at horizon $h$ to the shocks identified in this decomposition.

Let $Q$ be an orthogonal matrix, an alternative decomposition is \[\begin{array}{lll}y_t&=&\sum_{h=0}^{+\infty}\Psi_h\underbrace{\eta_{t-h}}_{Q\tilde \eta_{t-h}}\\
&=&\sum_{h=0}^{+\infty}\underbrace{\Psi_hQ}_{\tilde\Psi_h}\tilde
\eta_{t-h}\\
&=&\sum_{h=0}^{+\infty}\tilde\Psi_h\tilde \eta_{t-h}\end{array}\]
where $\tilde\Psi_h=\Phi^hPQ$ are the impulse responses at horizon $h$ to the shocks identified in this alternative decomposition, and $\tilde \eta_{t-h}=Q'\eta_{t-h}$ are the corresponding shocks.

The $h$-step ahead prediction error of $y_{t+h}$ given all the data up to and including $t-1$ is given by
\[e_{t+h}(h)=y_{t+h}-E_{t-1}(y_{t+h})=\sum_{j=0}^h\tilde \Psi_h\tilde \eta_{t+h-j}\]

The variance-covariance matrix of $e_{t+h}(h)$ is \[\Omega(h)=\sum_{j=0}^h\tilde \Psi_j\tilde \Psi_j'=\sum_{j=0}^h \Psi_j \Psi_j'\]

We can decompose $\Omega(h)$ into the contribution of each shock $l$: \[\Omega(h)=\sum_{l=1}^m\Omega(h,l)\]
with \[\Omega(h,l)=\sum_{j=0}^h(\Psi_jq_l)(\Psi_jq_l)'\]

This method aims at finding an **impulse vector $b$**, so that this impulse vector explains as much as possible of the sum of the $h$-step ahead prediction error variance of some
variable $i$, say, for prediction horizons $h = \underline{h} \le
\overline{h}$.

Formally then, the task is to explain as much as possible of the variance
\[\sigma^2(\underline{h},\overline{h})=\sum_{h=\underline{h}}^{\overline{h}} \Omega(h)_{ii}\] with a single impulse vector.

This amounts to finding a vector $q_1$ of unit length, maximizing
\[\sigma^2(\underline{h},\overline{h},q_1)=\sum_{h=\underline{h}}^{\overline{h}} \Omega(h,1)_{ii}\]

Calculate
\[\begin{array}{lll}\sigma^2(\underline{h},\overline{h},q_1)&=&\sum_{h=\underline{h}}^{\overline{h}}
\Omega(h,1)_{ii}\\
&=& \sum_{h=\underline{h}}^{\overline{h}}\sum_{l=0}^{h}\left((\Psi_lq_1)(\Psi_lq_1)'\right)_{ii}\\
&=& \sum_{h=\underline{h}}^{\overline{h}}\sum_{l=0}^{h}trace\left(E_{ii}(\Psi_lq_1)(\Psi_lq_1)'\right)\\
&=& q_1'Sq_1\\
\end{array}\]
where
\[\begin{array}{lll}S&=&\sum_{h=\underline{h}}^{\overline{h}}\sum_{l=0}^{h}\Psi_l'E_{ii}\Psi_l\\
&=&\sum_{l=0}^{\overline{h}}(\overline{h}+1-max(\underline{h},l))\Psi_l'E_{ii}\Psi_l\\
&=&\sum_{l=0}^{\overline{h}}(\overline{h}+1-max(\underline{h},l))\Psi_{l,i}'\Psi_{l,i}\\
\end{array}\]
where $\Psi_{l,i}$ denotes row $i$ of $\Psi_{l}$, i.e. the response of variable $i$ at horizon $l$.


The maximization problem subject to the side constraint $q_1'q_1=1$ can be written as a Lagrangian, \[L=q_1'Sq_1-\lambda(q_1'q_1-1)\] with the first-order condition
\[Sq_1=\lambda q_1\] and the side constraint $q_1'q_1=1$.

From this equation, we see that the solution $q_1$ is an eigenvector of $S$ with eigenvalue $\lambda$.

We also see that \[\sigma^2(\underline{h},\overline{h},q_1)=\lambda\]

Thus, to maximize this variance, we need to find the eigenvector with the maximal eigenvalue $\lambda$, i.e., we need to find the first principal component.

The impulse vector $b$ is then found as \[b=Pq_1\].


### Non-Gaussian Shocks {#NonGaussian}

In this section, we show that the non-identification of the structural shocks ($\eta_t$) is very specific to the Gaussian case. We propose consistent estimation approaches in the context of non-Gaussian shocks. In a first part, we focus on non-Gaussian SVAR models; in a second part, we discuss the case of non-Gaussian SVARMA models.

**Non-Gaussian SVAR models**

The estimation of (S)VARs is more common than that of (S)VARMAs. (Simpler estimation of $\Phi_k$'s). We have seen in what precedes that, in the Gaussian case, we cannot identify $B$ in the Gaussian case. That is, even if we observe an infinite number of i.i.d. $B \eta_t$, we cannot recover $B$ is the $\eta_t$'s are Gaussian. Indeed, if $\eta_t \sim \mathcal{N}(0,Id)$, then the distribution of $\varepsilon_t \equiv B \eta_t$ is $\mathcal{N}(0,BB')$.

Hence $\Omega = B B'$ is observed (in the population), but for any orthogonal matrix $Q$ (i.e. $QQ'=Id$), we also have $BQ \eta_t \sim \mathcal{N}(0,\Omega)$.


Example: Bivariate Gaussian case (Eq. \@ref(eq:VARMA111), with $\Theta_1=0$)

$\left[\begin{array}{c}\eta_{1,t}\\ \eta_{2,t}\end{array}\right]\sim \mathcal{N}(0,Id)$,
$B = \left[\begin{array}{cc}
1 & 2 \\
-1 & 1
\end{array}\right]$ and
$Q = \left[\begin{array}{cc}
\cos(\pi/3) & -\sin(\pi/3) \\
\sin(\pi/3) & \cos(\pi/3)
\end{array}\right]$ (rotation).

$\Rightarrow$ Distribution of $B \eta_t$ versus that of $BQ\eta_t$?


```{r simulGaussian, eval=FALSE}
theta.angle <- pi/3
Q <- matrix(c(cos(theta.angle),sin(theta.angle),-sin(theta.angle),cos(theta.angle)),2,2)
#nb.sim <- 10^4
nb.sim <- 10^2
distri.1 <- list(type=c("gaussian"),name="Panel (a) Gaussian",name.4.table="Gaussian")
distri.2 <- list(type=c("mixt.gaussian"),mu=0,sigma=5,p=.03,name="Panel (b) Mixture of Gaussian",name.4.table="Mixture of Gaussian")
distri.3 <- list(type=c("student"),df=c(5),name="Panel (c) Student (df: 5)",name.4.table="Student (df: 5)")
distri.4 <- list(type=c("student"),df=c(10),name="Panel (d) Student (df: 10)",name.4.table="Student (df: 10)")
x.lim <- c(-7,7)
y.lim <- c(-5,5)
nb.points <- 100
x.points <- seq(x.lim[1],x.lim[2],length.out=nb.points)
y.points <- x.points
all.x <- c(matrix(x.points,nb.points,nb.points))
all.y <- c(t(matrix(x.points,nb.points,nb.points)))
eps <- cbind(all.x,all.y)
par(plt=c(.25,.9,.25,.8))
eta.1 <- simul.distri(distri.1,nb.sim)
eta.2 <- simul.distri(distri.1,nb.sim)
epsilon.C <- cbind(eta.1,eta.2) %*% t(C)
epsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)
Model$distri <- list(type=c("gaussian","gaussian"),df=c(NaN,NaN))
par(mfrow=c(1,2))
plot(epsilon.C[,1],epsilon.C[,2],pch=19,
     xlim=x.lim,ylim=y.lim,col="#00000044",
     xlab=expression(epsilon[1]),
     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,
     main=expression(paste("Distribution of ",epsilon[t]," = ",B,eta[t],sep="")))
z <- matrix(exp(g(eps,Model)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.01,.1,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)
plot(epsilon.CQ[,1],epsilon.CQ[,2],pch=19,
     xlim=x.lim,ylim=y.lim,col="#00000044",
     xlab=expression(epsilon[1]),
     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,
     main=expression(paste("Distribution of ",epsilon[t]," = ",BQ,eta[t],sep="")))
z <- matrix(exp(g(eps,Model)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.01,.1,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)
```

```{r preMadeFigureICA, fig.align = 'center', out.width = "95%", fig.cap = "XXXX.", echo=FALSE}
knitr::include_graphics("images/Figure_A.png")
```


```{r preMadeFigureICA2, fig.align = 'center', out.width = "95%", fig.cap = "XXXX.", echo=FALSE}
library(AEC)
nb.sim <- 12
Phi <- array(NaN,c(n,n,1))
Phi[,,1] <- matrix(c(.5,-.4,.3,.7),2,2)
p <- dim(Phi)[3]
Theta <- array(NaN,c(n,n,1))
Theta[,,1] <- 0*(-matrix(c(2,1,0,.5),2,2))
q <- dim(Theta)[3]
Mu <- rep(0,n)
C <- matrix(c(1,-1,2,1),2,2)
Model <- list(
  Mu = Mu,
  Phi = Phi,
  Theta = Theta,
  C = C,
  distri = distri
)
Y0 <- rep(0,n)
eta0 <- c(1,0)
res.sim.1C <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)
eta0 <- c(0,1)
res.sim.2C <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)
theta.angle <- pi/3
Q <- matrix(c(cos(theta.angle),sin(theta.angle),-sin(theta.angle),cos(theta.angle)),2,2)
Model$C <- C %*% Q
Y0 <- rep(0,n)
eta0 <- c(1,0)
res.sim.1CQ <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)
eta0 <- c(0,1)
res.sim.2CQ <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)
par(plt=c(.1,.95,.25,.8))
par(mfrow=c(2,2))
plot(res.sim.1C$Y[1,],las=1,
     ylim=c(-1.5,3),
     type="b",lwd=2,xlab="",ylab="",
     main=expression(paste("Response of ",y[1,"*,*",t]," to a one-unit increase in ",eta[1],sep="")))
lines(res.sim.1CQ$Y[1,],col="red",type="b",lwd=2,pch=0)
abline(h=0,col="grey",lty=3)
legend("topright",
       c(expression(paste(epsilon[t]," = ",B,eta[t],sep="")),
         expression(paste(epsilon[t]," = ",BQ,eta[t],sep=""))),
       lty=c(1,1), # gives the legend appropriate symbols (lines)
       lwd=c(3,3), # line width
       pch=c(21,22),
       pt.bg = "white",
       col=c("black","red"),
       seg.len=4,
       cex=1.3,bg="white",title="Model:")
plot(res.sim.2C$Y[1,],las=1,
     ylim=c(-1.5,3),
     type="b",lwd=2,xlab="",ylab="",
     main=expression(paste("Response of ",y[1,"*,*",t]," to a one-unit increase in ",eta[2],sep="")))
lines(res.sim.2CQ$Y[1,],col="red",type="b",lwd=2,pch=0)
abline(h=0,col="grey",lty=3)
plot(res.sim.1C$Y[2,],las=1,
     ylim=c(-1.5,2),
     type="b",lwd=2,xlab="",ylab="",
     main=expression(paste("Response of ",y[2,"*,*",t]," to a one-unit increase in ",eta[1],sep="")))
lines(res.sim.1CQ$Y[2,],col="red",type="b",lwd=2,pch=0)
abline(h=0,col="grey",lty=3)
plot(res.sim.2C$Y[2,],las=1,
     ylim=c(-1.5,2),
     type="b",lwd=2,xlab="",ylab="",
     main=expression(paste("Response of ",y[2,"*,*",t]," to a one-unit increase in ",eta[2],sep="")))
lines(res.sim.2CQ$Y[2,],col="red",type="b",lwd=2,pch=0)
abline(h=0,col="grey",lty=3)
```



External restrictions (economic hypotheses) are needed to identify $B$ (see previous sections). But such restrictions may not be necessary if the structural shocks are not Gaussian.

That is, the identification problem is very specific to normally-distributed $\eta_t$'s.

[Rigobon (2003)](https://www.mitpressjournals.org/doi/abs/10.1162/003465303772815727?journalCode=rest)
[Normandin and Phaneuf (2004)](https://www.sciencedirect.com/science/article/abs/pii/S0304393204000698)
[Lanne and Lutkepohl (2008)](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1538-4616.2008.00151.x)


Example: Bivariate Gaussian + Student (5) case:

$\eta_{1,t} \sim \mathcal{N}(0,1)$, $\eta_{2,t} \sim t(5)$,
$B = \left[\begin{array}{cc}
1 & 2 \\
-1 & 1
\end{array}\right]$ and
$Q = \left[\begin{array}{cc}
\cos(\pi/3) & -\sin(\pi/3) \\
\sin(\pi/3) & \cos(\pi/3)
\end{array}\right]$.

Distribution of $B \eta_t$ versus that of $BQ\eta_t$?



```{r simulGaussianStudent, eval=FALSE}
theta.angle <- pi/3
Q <- matrix(c(cos(theta.angle),sin(theta.angle),-sin(theta.angle),cos(theta.angle)),2,2)
#nb.sim <- 10^4
nb.sim <- 10^2
distri.1 <- list(type=c("gaussian"),name="Panel (a) Gaussian",name.4.table="Gaussian")
distri.2 <- list(type=c("mixt.gaussian"),mu=0,sigma=5,p=.03,name="Panel (b) Mixture of Gaussian",name.4.table="Mixture of Gaussian")
distri.3 <- list(type=c("student"),df=c(5),name="Panel (c) Student (df: 5)",name.4.table="Student (df: 5)")
distri.4 <- list(type=c("student"),df=c(10),name="Panel (d) Student (df: 10)",name.4.table="Student (df: 10)")
x.lim <- c(-7,7)
y.lim <- c(-5,5)
nb.points <- 100
x.points <- seq(x.lim[1],x.lim[2],length.out=nb.points)
y.points <- x.points
all.x <- c(matrix(x.points,nb.points,nb.points))
all.y <- c(t(matrix(x.points,nb.points,nb.points)))
eps <- cbind(all.x,all.y)
par(plt=c(.25,.9,.25,.8))
eta.1 <- simul.distri(distri.1,nb.sim)
eta.2 <- simul.distri(distri.3,nb.sim)
epsilon.C <- cbind(eta.1,eta.2) %*% t(C)
epsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)
Model$distri <- list(type=c("gaussian","student"),df=c(NaN,5))
par(mfrow=c(1,2))
plot(epsilon.C[,1],epsilon.C[,2],pch=19,
     xlim=x.lim,ylim=y.lim,col="#00000044",
     xlab=expression(epsilon[1]),
     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,
     main=expression(paste("Distribution of ",epsilon[t]," = ",B,eta[t],sep="")))
z <- matrix(exp(g(eps,Model)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.01,.1,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)
plot(epsilon.CQ[,1],epsilon.CQ[,2],pch=19,
     xlim=x.lim,ylim=y.lim,col="#00000044",
     xlab=expression(epsilon[1]),
     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,
     main=expression(paste("Distribution of ",epsilon[t]," = ",BQ,eta[t],sep="")))
z <- matrix(exp(g(eps,Model)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.01,.1,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)
```

```{r preMadeFigureICAGaussianStudent, fig.align = 'center', out.width = "95%", fig.cap = "XXXX.", echo=FALSE}
knitr::include_graphics("images/Figure_C.png")
```



NB: In both cases, we have $\mathbb{V}ar(\varepsilon_t)=BB'$.


Example: Bivariate Student (5) case

$\eta_{1,t} \sim t(5)$, $\eta_{2,t} \sim t(5)$,
$B = \left[\begin{array}{cc}
1 & 2 \\
-1 & 1
\end{array}\right]$ and
$Q = \left[\begin{array}{cc}
\cos(\pi/3) & -\sin(\pi/3) \\
\sin(\pi/3) & \cos(\pi/3)
\end{array}\right]$.

$\Rightarrow$ Distribution of $B \eta_t$ versus that of $BQ\eta_t$?



```{r simulStudentStudent, eval=FALSE}
theta.angle <- pi/3
Q <- matrix(c(cos(theta.angle),sin(theta.angle),-sin(theta.angle),cos(theta.angle)),2,2)
#nb.sim <- 10^4
nb.sim <- 10^2
distri.1 <- list(type=c("gaussian"),name="Panel (a) Gaussian",name.4.table="Gaussian")
distri.2 <- list(type=c("mixt.gaussian"),mu=0,sigma=5,p=.03,name="Panel (b) Mixture of Gaussian",name.4.table="Mixture of Gaussian")
distri.3 <- list(type=c("student"),df=c(5),name="Panel (c) Student (df: 5)",name.4.table="Student (df: 5)")
distri.4 <- list(type=c("student"),df=c(10),name="Panel (d) Student (df: 10)",name.4.table="Student (df: 10)")
x.lim <- c(-7,7)
y.lim <- c(-5,5)
nb.points <- 100
x.points <- seq(x.lim[1],x.lim[2],length.out=nb.points)
y.points <- x.points
all.x <- c(matrix(x.points,nb.points,nb.points))
all.y <- c(t(matrix(x.points,nb.points,nb.points)))
eps <- cbind(all.x,all.y)
par(plt=c(.25,.9,.25,.8))
eta.1 <- simul.distri(distri.3,nb.sim)
eta.2 <- simul.distri(distri.3,nb.sim)
epsilon.C <- cbind(eta.1,eta.2) %*% t(C)
epsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)
Model$distri <- list(type=c("student","student"),df=c(5,5))
par(mfrow=c(1,2))
plot(epsilon.C[,1],epsilon.C[,2],pch=19,
     xlim=x.lim,ylim=y.lim,col="#00000044",
     xlab=expression(epsilon[1]),
     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,
     main=expression(paste("Distribution of ",epsilon[t]," = ",B,eta[t],sep="")))
z <- matrix(exp(g(eps,Model)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.01,.1,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)
plot(epsilon.CQ[,1],epsilon.CQ[,2],pch=19,
     xlim=x.lim,ylim=y.lim,col="#00000044",
     xlab=expression(epsilon[1]),
     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,
     main=expression(paste("Distribution of ",epsilon[t]," = ",BQ,eta[t],sep="")))
z <- matrix(exp(g(eps,Model)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.01,.1,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)
```


NB: In both cases, we have $\mathbb{V}ar(\varepsilon_t)=BB'$.


```{r preMadeFigureICAStudentStudent, fig.align = 'center', out.width = "95%", fig.cap = "XXXX.", echo=FALSE}
knitr::include_graphics("images/Figure_D.png")
```


**Relationship with Signal Processing Literature**

Task to be performed:

you observe $n$ linear combinations of $n$ **independent signals**; you want to estimate these linear combinations ($\Leftrightarrow$ recover independent signals).

Without loss of generality, we can assume that $BB' = Id$ (i.e. $B$ is orthogonal)

(If not the case, i.e. if $\mathbb{V}ar(\varepsilon_t)=\Omega \ne Id$, pre-multiply the data by $\Omega^{-1/2}$).

**Classical signal processing problem: Independent Component Analysis (ICA)**

Find $B$ such that $\varepsilon_t = B \eta_t$ (or $\eta_t= B' \varepsilon_t $) given that
i. you observe the $\varepsilon_t$'s,
ii. The components of $\eta_t$ are independent,
iii. $BB'=Id$ ($B$ is orthogonal).


```{r ThreePlotsCode, fig.asp = .8,eval=FALSE,echo=FALSE}
library(AEC)
indic.solve.C <- 0
distri.1 <- list(type=c("gaussian"),name="Panel (a) Gaussian",name.4.table="Gaussian")
distri.2 <- list(type=c("mixt.gaussian"),mu=0,sigma=5,p=.03,name="Panel (b) Mixture of Gaussian",name.4.table="Mixture of Gaussian")
distri.3 <- list(type=c("student"),df=c(5),name="Panel (c) Student (df: 5)",name.4.table="Student (df: 5)")
distri.4 <- list(type=c("student"),df=c(10),name="Panel (d) Student (df: 10)",name.4.table="Student (df: 10)")
x.lim <- c(-7,7)
y.lim <- c(-5,5)
nb.points <- 100
x.points <- seq(x.lim[1],x.lim[2],length.out=nb.points)
y.points <- x.points
all.x <- c(matrix(x.points,nb.points,nb.points))
all.y <- c(t(matrix(x.points,nb.points,nb.points)))
eps <- cbind(all.x,all.y)

C <- diag(2)
Model.aux <- Model
Model.aux$C <- C

# Gaussian
par(plt=c(.15,.9,.15,.85))
eta.1 <- simul.distri(distri.1,nb.sim)
eta.2 <- simul.distri(distri.1,nb.sim)
epsilon.C <- cbind(eta.1,eta.2) %*% t(C)
epsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)
Model.aux$distri <- list(type=c("gaussian","gaussian"),df=c(NaN,NaN))
par(mfrow=c(1,3))
z <- matrix(exp(g(eps,Model.aux)),nb.points,nb.points)
max.z <- max(z)
levels <- c(.002,.02,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="black",lwd=2,
        main=expression(paste(eta[1*','*t]," and ",eta[2*','*t]," are ",N(0,1),sep="")),
        xlab=expression(epsilon[1*','*t]),
        ylab=expression(epsilon[2*','*t]))

legend("topright", # places a legend at the appropriate place c("Health","Defense"), # puts text in the legend
       c(expression(paste(eta[t],sep="")),
         expression(paste(epsilon[t]," = ",B,eta[t],sep=""))),
       lty=c(1,1), # gives the legend appropriate symbols (lines)
       lwd=c(2,2),
       col=c("black","red"),
       seg.len=4,
       bg="white",bty = "n")

z <- matrix(exp(g(eps,Model.aux)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.002,.02,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)

# t-Student + Gaussian

eta.1 <- simul.distri(distri.1,nb.sim)
eta.2 <- simul.distri(distri.3,nb.sim)
epsilon.C <- cbind(eta.1,eta.2) %*% t(C)
epsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)

Model.aux$distri <- list(type=c("gaussian","student"),df=c(NaN,5))
Model.aux$C <- C

z <- matrix(exp(g(eps,Model.aux)),nb.points,nb.points)
max.z <- max(z)
levels <- c(.002,.02,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="black",lwd=2,
        main=expression(paste(eta[1*','*t]," is ",N(0,1)," and ",eta[2*','*t]," is ",t(5),sep="")),
        xlab=expression(epsilon[1*','*t]),
        ylab=expression(epsilon[2*','*t]))

Model.aux$C <- C %*% Q
z <- matrix(exp(g(eps,Model.aux)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.002,.02,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)

# t-Student + t-Student
eta.1 <- simul.distri(distri.3,nb.sim)
eta.2 <- simul.distri(distri.3,nb.sim)
epsilon.C <- cbind(eta.1,eta.2) %*% t(C)
epsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)

Model.aux$distri <- list(type=c("student","student"),df=c(5,5))
Model.aux$C <- C

z <- matrix(exp(g(eps,Model.aux)),nb.points,nb.points)
max.z <- max(z)
levels <- c(.002,.02,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="black",lwd=2,
        main=expression(paste(eta[1*','*t]," and ",eta[2*','*t]," are ",t(5),sep="")),
        xlab=expression(epsilon[1*','*t]),
        ylab=expression(epsilon[2*','*t]))

Model.aux$C <- C %*% Q
z <- matrix(exp(g(eps,Model.aux)),nb.points,nb.points)
par(new=TRUE)
max.z <- max(z)
levels <- c(.002,.02,.3,.6,.9)*max.z
contour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col="red",lwd=2)
```


```{r ThreePlots, fig.align = 'center', out.width = "95%", fig.cap = "XXXX.", echo=FALSE}
knitr::include_graphics("images/Figure_E.png")
```

In all cases, we have $\mathbb{V}ar(\varepsilon_t)=\mathbb{V}ar(\eta_t)=Id$. Assume you observe the distribution of $\varepsilon_t$ (in \color{red}{red}). How to rotate $\varepsilon_t$ to get obtain *independent signals* ($\eta_t = B'\varepsilon_t$)?

NB: $\varepsilon_{1,t}$ and $\varepsilon_{2,t}$ are not independent.

For instance: We have $\mathbb{E}(\varepsilon_{2,t}|\varepsilon_{1,t}>4)<0$ (whereas $\mathbb{E}(\eta_{2,t}|\eta_{1,t}>4)=0$).

:::{.hypothesis #NonGauss}
We have:

i. The shocks $\eta_t$ are i.i.d. with $\mathbb{E}(\eta_t) = 0$ and $\mathbb{V}ar(\eta_t) = Id.$
ii. The components $\eta_{1,t}, \ldots, \eta_{n,t}$ are mutually independent.
iii We have
$$
\boxed{Y_t = B_0 \eta_t,}
$$
with $\mathbb{E}(Y_t) = Id$ (i.e. $B_0$ is orthogonal).
:::

:::{.theorem #EK2004 name="Eriksson, Koivunen (2004)"}
If Hypothesis \@ref(hyp:NonGauss) is satisfied and if at most one of the components of $\eta$ is Gaussian, then matrix $B_0$ is identifiable up to the post multiplication by $DP$, where $P$ is a permutation matrix and $D$ is a diagonal matrix whose diagonal entries are 1 or $-1$.}
:::

**The PML approach**

We introduce a set of p.d.f. $g_i (\eta_i), i=1,\ldots,n,$ and consider the **pseudo log-likelihood function**:
\begin{equation}
\log \mathcal{L}_T (B) = \sum^T_{t=1} \sum^n_{i=1} \log g_i (b'_i Y_t),(\#eq:pseudolog)
\end{equation}
where $b_i$ is the $i^{th}$ column of matrix $B$ (or $b'_i$ is the $i^{th}$ row of $B^{-1}$ since $B^{-1}=B'$).

The log-likelihood function \@ref(eq:pseudolog) is computed as if the errors $\eta_{i,t}$ had the p.d.f.  $g_i (\eta_i)$.

A **pseudo maximum likelihood (PML) estimator** of matrix $B$ maximizes the pseudo log-likelihood function:
\begin{equation}
\widehat{B_T} = \arg \max_B \sum^T_{t=1} \sum^n_{i=1} \log g_i (b'_i Y_t),(\#eq:optimprob)
\end{equation}
\centerline{$s.t. \;B'B = Id.$}

The restrictions $B'B = Id$ can be eliminated by parameterizing $B$.

**Cayley's representation:**

Any orthogonal matrix with no eigenvalue equal to $-1$ can be written as
\begin{equation}
  B(A) = (Id+A) (Id-A)^{-1},
\end{equation}
where $A$ is a skew symmetric (or antisymmetric) matrix, such that $A'=-A$.

There is a one-to-one relationship with $A$, since:
\begin{equation}
  A = (B(A)+Id)^{-1} (B(A)-Id).
\end{equation}

PML estimator of matrix $B$: $\widehat{B_T} = B(\hat{A}_T),$ where:
\begin{equation}
  \hat{A}_T = \arg \max_{a_{i,j}, i>j} \sum^T_{t=1} \sum^n_{i=1} \log g_i [b_i (A)' Y_t].(\#eq:optimprob2)
\end{equation}

**Asymptotic properties of the PML approach**

:::{.hypothesis #NonGauss2}
We have:

i. The functions $\log g_i$, $i=1,\ldots,n$, are twice continuously differentiable.
ii. $sup_{B: B'B = Id} \left|\sum^n_{i=1} \log g_i (b'_i y)\right| \leq h(y),$ where $\mathbb{E}_0 [h (Y)] < \infty$.
:::

:::{.hypothesis #NonGauss3 name="Identification from the asymptotic FOC"}
The only solutions of the system of equations:
$$
\left\{
\begin{array}{l} \mathbb{E}_0 \left[b'_j Y_t \frac{d\log g_i}{d\eta} (b'_i Y_t)\right] = 0,\;  i \neq j, \\
B' B = Id,
\end{array}
\right.
$$
are the elements of $\mathcal{P}_0 \equiv \mathcal{P}(B_0)$, which is the set of matrices obtained by permutation and sign change of the columns of $B_0$.
:::


:::{.hypothesis #NonGauss4 name="Local concavity"}
The asymptotic objective function is locally concave in a neighbourhood of a matrix $B$ of $\mathcal{P}(B_0)$, which is the case if and only if
$$
\mathbb{E}_0 \left[ \frac{d^2 \log g_i (\eta_{i,t})}{d\eta^2} + \frac{d^2 \log g_j (\eta_{j,t})}{d\eta^2} - \eta_{j,t} \frac{d\log g_j (\eta_{j,t})}{d\eta}- \eta_{i,t} \frac{d\log g_i (\eta_{i,t})}{d\eta} \right] < 0, \forall i<j,
$$
where $\eta_{i,t}$ is the $i^{th}$ component of the $\eta_t$ associated with this particular element $B$ of $\mathcal{P}(B_0)$.
:::

This condition is in particular satisfied under the following set of conditions: derived in Hyvarinen (1997) XXX
\begin{equation}
\mathbb{E}_0 \left[\frac{d^2 \log g_i(\eta_{i,t})}{d\eta^2} - \eta_{i,t} \frac{d\log g_i(\eta_{i,t})}{d\eta}\right] <0,\quad  i=1,\ldots, n. (\#eq:HKO)
\end{equation}

Hyperbolic secant and the subgaussian distributions (see table on next slide): either one, or the other satisfy the inequality \@ref(eq:HKO) [Hyvarinen, Karhunen, Oja, 2001 XXXX].

XXXXXXXXXXXXXXX
<!-- \begin{table} -->
<!-- \begin{center} -->
<!-- \caption{Zero-mean unit-variance distributions}\label{tab:distri}%\vspace{-0.2in} -->
<!-- \begin{tabular}{lccccc} -->
<!-- \hline -->
<!-- & $\log g(x)$ & $\dfrac{d \log g(x)}{d x}$ & $\dfrac{d^2 \log g(x)}{d x^2}$ -->
<!-- \\ -->
<!-- %& (a) & (b) & (c) \\ -->
<!-- \hline -->
<!-- \\ -->
<!-- Gaussian & $cst - x^2/2$& $-x$ &$-1$\\%&$0$\\ -->
<!-- \\ -->
<!-- Student $t(\nu>4)$ & $-\dfrac{1-\nu}{2}\log\left( 1 +\dfrac{x^2}{\nu-2} \right)$&$-\dfrac{x(1+\nu)}{\nu - 2 + x^2}$\\ %$- (1+\nu)  \dfrac{\nu - 2 - x^2}{\nu - 2 + x^2}$&$\dfrac{6}{\nu-4}$ if $\nu>4$\\ -->
<!-- Hyperbolic secant &$cst - \log\left( \cosh\left\{\dfrac{\pi}{2}x\right\} \right)$&$-\dfrac{\pi}{2}\tanh\left(\dfrac{\pi}{2}x\right)$&$ -->
<!-- -\left(\dfrac{\pi}{2}\dfrac{1}{\cosh\left(\dfrac{\pi}{2}x\right)}\right)^2 -->
<!-- $\\%$2$\\ -->
<!-- Subgaussian &$cst + \pi x^2 + \log \left(\cosh\left\{\dfrac{\pi}{2}x\right\}\right)$& -->
<!-- $2\pi x+\dfrac{\pi}{2}\tanh\left(x \dfrac{\pi}{2}\right)$& -->
<!-- $2\pi + \left(\dfrac{\pi}{2}\dfrac{1}{\cosh\left(\dfrac{\pi}{2}x\right)}\right)^2$\\%$\dfrac{-2\pi^2 + 8\pi - 8}{\pi^2}<0$\\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \end{center} -->
<!-- \end{table} -->

Note: Except for the Gaussian distribution, we have $E[d^2 \log g(X)/d \varepsilon^2 - X d\log g(X)/d \varepsilon] < 0$ (i.e. Assumption\,4 is satisfied) when these pseudo distributions coincide to the distribution of $X$. The subGaussian distribution is a mixture of Gaussian distributions: $X$ is drawn from this distribution if it is equal to $BY - (1-B)Y$, where $B$ is drawn from a Bernoulli distribution of parameter $1/2$ and $Y \sim \mathcal{N}(\sqrt{(\pi-2)/\pi},2/\pi)$.


Under Hypotheses \@ref(NonGauss)-\@ref(NonGauss4), the PML estimator $\widehat{B_T}$ of $B_0$ is consistent (in $\mathcal{P}_0$) and asymptotically normal, with speed of convergence $1/\sqrt{T}$.

The asymptotic variance-covariance matrix of $vec \sqrt{T} (\widehat{B_T} - B_0)$ is $A^{-1} \left[\begin{array}{cc} \Gamma & 0 \\ 0 & 0 \end{array} \right] (A')^{-1}$, where matrices $A$ and $\Gamma$ are detailed in Gouri\'eroux, Monfort and Renne (2017) XXX.

The potential misspecification of pseudo-distributions $g_i$ has no effect on the consistency of these specific PML estimators.

The previous proposition can be exploited to build a test whose null hypothesis is:

*$H_0$: $B$ belongs to $\mathcal{P}_0$, where $\mathcal{P}_0$ is the set of orthogonal matrices obtained by permuting and changing the signs of the columns of a given orthogonal matrix $B_0$.*


**Application: Structural VAR model**

Three dependent variables: inflation ($\pi_t$), economic activity ($z_t$) and the nominal  short-term interest rate ($r_t$).

Structural shocks are posited monetary-policy, demand and supply shocks.

$W_t$: set of information made of the past values of $y_t= [\pi_t,z_t,r_t]$, that is $\{y_{t-1},y_{t-2},\dots\}$, and of exogenous variables $\{x_{t},x_{t-1},\dots\}$.

The reduced-form VAR model reads:
$$
y_t  = \underbrace{\mu + \sum_{i=1}^{p} \Phi_i y_{t-i} + \Theta x_t}_{a(W_t;\theta)} + u_t
$$
where the $u_t$'s are assumed to be serially independent, with zero mean and variance-covariance matrix $\Sigma$.

U.S. data. 1959:IV to 2015:I at the quarterly frequency ($T=224$). Source: Federal Reserve Economic Database (FRED).

Two different measures of economic activity are considered: the output gap and the unemployment gap. Change in the log of oil prices added as an exogenous variable ($x_t$).

$\mu$, $\Phi_i$, $\Theta$ and $\Sigma$ are consistently estimated by OLS.

Jarque-Bera tests support the hypothesis of non-normality for all residuals.

We want to estimate the orthogonal matrix $B$ such that $u_t=SB \eta_t$, where

* $S$ results from the Cholesky decomposition of $\Sigma$ and
* the components of $\eta_t$ are independent, zero-mean with unit variance.

The PML approach is applied on standardized VAR residuals given by:
$$
\hat{S}_T^{-1}\underbrace{[y_t - a(W_t;\hat\theta_T)]}_{\mbox{VAR residuals}}.
$$
By construction of $\hat{S}_T^{-1}$, it comes that the covariance matrix of these residuals is $Id$.

Pseudo density functions: Distinct and asymmetric mixtures of Gaussian distributions.

Once $B$ has been estimated, it remains to associate the structural shocks (monetary-policy, supply or demand) with the different components of $\eta_{t}$.

* Contractionary **monetary-policy shocks**: negative impact on real activity and on inflation.
* **Supply shock**: influences of opposite signs on economic activity and on inflation.
* **Demand shock**: influences of same signs on economic activity and on inflation.

Important: This method does not rely on the assumption that specific impacts  (e.g. contemporaneous or long-run) are null.

<!-- \begin{frame}{} -->
<!-- \begin{footnotesize} -->
<!-- \begin{figure} -->
<!-- \includegraphics[width=.8\linewidth]{figures/ICA/figSVAR1.pdf} -->
<!-- \end{figure} -->
<!-- \end{footnotesize} -->
<!-- \end{frame} -->

<!-- \begin{frame}{} -->
<!-- \begin{footnotesize} -->
<!-- \begin{figure} -->
<!-- \includegraphics[width=.8\linewidth]{figures/ICA/figSVAR2.pdf} -->
<!-- \end{figure} -->
<!-- \end{footnotesize} -->
<!-- \end{frame} -->

Comparison of the previous IRFs with those stemming from "recursive" identification approaches based on specific short-run restrictions (SRRs).

SRRs approach (Section\,\ref{Section:Standard}) are based on the assumptions that

a. $Cov(\eta_t)=Id$,
b. the $k^{th}$ structural shock does not contemporaneously affects the first $k-1$ endogenous variables and
c. the contemporaneous effect of the $k^{th}$ structural shock on the $k^{th}$ dependent variable is positive.

Under these assumptions, the structural shocks are given by $S^{-1}u_t$.

SRR approaches assume --potentially wrongly-- that the contemporaneous impacts of some structural shocks on given variables are null.

The null hypothesis of these tests is $H_0= (P \in \mathcal{P}(Id))$.
The null hypothesis $H_0$ stating that the true value of $B$ belongs to $\mathcal{P}_0$ is not standard since it is a finite union of simple hypotheses $H_{0,j} = (B = B_{j,0})$.

**First testing procedure:**

* Define the Wald statistics $\hat\xi_{j,T}$, $j \in J$:
\begin{equation}
\hat\xi_{j,T} = T [vec\hat{B}_T-vec B_{j,0}]'\hat{A}_T'
\left[
\begin{array}{cc}
\hat{\Omega}^{-1}_T & 0\\
0&0
\end{array}
\right]\hat{A}_T
[vec\hat{B}_T-vec B_{j,0}],
\end{equation}
$\hat{A}_T$ and $\hat{\Omega}_T$ being consistent estimators of the matrices $A$ and $\Omega$.

Since the dimension of the asymptotic distribution of $\sqrt{T}[vec\hat{B}_T-vec B_{j,0}]$ is $\frac{1}{2}n(n-1)$, the asymptotic distribution of $\hat\xi_{j,T}$ under $H_{0,j}$ is $\chi^2\left(\frac{1}{2}n(n-1)\right)$.

* Define $\hat\xi_T = \underset{j \in J}{\min} \hat\xi_{j,T}$ as the test statistic for $H_0$.
* Under $H_0$, $\hat{B}_T$ converges to $B_{j_0,0}$ (say).
* By the asymptotic properties of the Wald statistics for simple hypotheses:
\begin{equation}
\hat\xi_{j_0,T} \overset{D}{\rightarrow} \chi^2\left(\frac{n(n-1)}{2}\right) \quad \mbox{and}\quad  \hat\xi_{j,T} \rightarrow \infty, \mbox{ if } j \ne j_0.
\end{equation}


Under the null hypothesis, $\hat\xi_T = \underset{j}{\min}$ $\hat\xi_{j,T}$ is asymptotically equal to $\hat\xi_{j_0,T}$  and its asymptotic distribution, $\chi^2\left(\frac{1}{2}n(n-1)\right)$, does not depend on $j_0$. Therefore $\hat\xi_T$ is asymptotically a pivotal statistic for the null hypothesis $H_0$ and the test of critical region $\hat\xi_T \ge \chi^2_{1-\alpha}\left(\frac{1}{2}n(n-1)\right)$ is of asymptotic level $\alpha$ and is consistent.


**Second testing procedure**

Define $B_{0,T} = \underset{B \in \mathcal{P}_0}{\mbox{Argmin }} d(\hat{B}_T,B)$ where $d$ is any distance, for instance the Euclidean one.

Under the null hypothesis $H_0$: $(B \in \mathcal{P}_0)$, $\hat{B}_T$ converges almost surely to an element of $\mathcal{P}_0$ denoted by $B_{j_0,0}$ and it is also the case for $B_{0,T}$ since, asymptotically, we have $B_{0,T}=B_{j_0,0}$.

Moreover:
$$
\sqrt{T}(\hat{B}_T - B_{0,T})=\sqrt{T}(\hat{B}_T - B_{j_0,0}) + \sqrt{T}(B_{j_0,0} - B_{0,T}),
$$
and, since $B_{0,T}$ is almost surely asymptotically equal to $B_{j_0,0}$, the asymptotic distribution of $\sqrt{T}(\hat{B}_T - B_{0,T})$ under $H_0$ is the same as that of $\sqrt{T}(\hat{B}_T - B_{j_0,0})$.

This implies that
$$
\tilde\xi_{T} = T [vec\hat{B}_T-vec B_{0,T}]'\hat{A}_T'
\left[
\begin{array}{cc}
\hat{\Omega}^{-1}_T & 0\\
0&0
\end{array}
\right]\hat{A}_T
[vec\hat{B}_T-vec B_{0,T}]
$$
is asymptotically distributed as $\chi^2\left(\frac{1}{2}n(n-1)\right)$ under $H_0$.

An advantage of this second method is that it necessitates the computation of only one Wald test statistic.

We consider two specific SRR schemes:
* In both of them, it is assumed that the monetary-policy shock has no contemporaneous impact on $\pi_t$ and $y_t$.
* In SRR Scheme 1: Inflation is contemporaneously impacted by one structural shock only.
* In SRR Scheme 2: Economic activity is contemporaneously impacted by one structural shock only.
\end{itemize}
* When economic activity is measured by means of the output gap, both SRRs are rejected at the 5\% level.


**SVARMA**

In what precedes, we have seen that, if $y_t$ follows a VAR
\begin{eqnarray*}
y_t &=& \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p}  +   B \eta_{t},
\end{eqnarray*}
and if the components of $\eta_t$ are non-Gaussian i.i.d. shocks,

then model parameters are identifiable (and can be consistently estimated).

What about Structural VARMAs?
\begin{eqnarray*}
&&y_t = \underbrace{\Phi_1 y_{t-1} + \dots +\Phi_p y_{t-p}}_{{\color{blue}\mbox{AR component}}} + \underbrace{B \eta_t+ \Theta_1 B \eta_{t-1}+ \dots+ \Theta_q B \eta_{t-q}}_{{\color{red}\mbox{MA component}}}\\
&\Leftrightarrow&\underbrace{(I - \Phi_1 L - \dots - \Phi_p L^p)}_{= \Phi(L)}y_t =  \underbrace{ {\color{red} (I - \Theta_1 L - \ldots - \Theta_q L^q)}}_{={\color{red}\Theta(L)}} B \eta_{t}
\end{eqnarray*}

Still may have identification problems with $B$ (solved if $\eta_t$ non-Gaussian). Additional identification issue (w.r.t. SVAR case):

There are $2^n$ different $\Theta(L)$ yielding the exact same second-order properties for $y_t$.
%The SVARMA process may have a {\color{red}non-invertible MA matrix polynomial $\Theta(L)$} but, still, has the same second-order properties as a VARMA process in which the MA matrix polynomial is invertible (called fundamental representation).

**Univariate MA(1) Case**

The two MA(1) processes:
\begin{equation}
y_t = \varepsilon_t + \theta \varepsilon_{t-1}
\end{equation}
and
\begin{equation}
y^*_t = \theta \varepsilon_t + \varepsilon_{t-1},
\end{equation}
have the same second-order properties (same variances, same auto-correlations).

Hence, if $\varepsilon_t $ is Gaussian, $y_t$ and $y^*_t$ are observationally equivalent. However, different IRFs: $(1,\theta,0,\dots)$ versus $(\theta,1,0,\dots)$. One of the 2 processes is said to be **fundamental**, or **invertible** (if $|\theta|<1$):

It is the one that is such that $\varepsilon_t$ can be deduced from past values of $y_t$ only.

The other one is **nonfundamental** ($\varepsilon_t$ cannot be deduced from past  values of $y_t$).

An MA process is said to be **invertible**, or **fundamental**, if we can obtain $\eta_t$ as a function of past values of $y_t$.

In the context of a univariate MA(1),
$$
y_t = \eta_t + \theta \eta_{t-1} = (1 + \theta L) \eta_t, \quad \eta_t \sim \,i.i.d,
$$
this is the case iff $|\theta|<1$. Indeed:
$$
\begin{array}{lllll}
\mbox{When $|\theta|<1$, } \qquad \eta_t &=& \sum^\infty_{h=0} \theta^h y_{t-h} & \mbox{(invertible case)}\\
\\
\mbox{When $|\theta|>1$, } \qquad \eta_t &=& - \sum^\infty_{h=0} \dfrac{1}{\theta^{h+1}} y_{t+h+1} & \mbox{(non-invertible case)}.
\end{array}
$$
Hence, when $|\theta|<1$ (respect. $|\theta|>1$), $\eta_t$ is a function of past (respect. future) values of $y_t$.

Context of a VMA(1): $y_t = \eta_t + \Theta \eta_{t-1}$. Process $y_t$ is invertible if the eigenvalues of $\Theta$ are strictly within the unit circle.

Consider the following (nonfundamental) VARMA(1,1) models:
$$
y_t = \left[\begin{array}{cc}
0.6 & 0.4 \\
-0.2 & 0.9
\end{array}\right]
y_{t-1} + B \eta_t - \Theta_1 B \eta_{t-1},
$$
with
$$
B = \left[\begin{array}{cc}
1.00 & 0.30 \\
0.50 & 1.00
\end{array}\right],\quad \Theta_1 = \left[\begin{array}{cc}
-2.00 & 0.50 \\
-0.20 & -0.70
\end{array}\right].
$$

It is possible to compute the three other $\Theta_1$'s yielding to the same second-order properties (Lippi and Reichlin, 1994). Different IRFs (next slide, Model 2 is the fundamental one).

If the $\eta_t$'s are Gaussian, the resulting four models observationally equivalent.

Remark: The previous identification problem is still present here. That is: For a given $\Theta_1$, $B$ can be replaced by $BQ$, same second-order properties.

Hence, for Gaussian SVARMA models, two identification issues.

Standard estimation toolboxes return fundamental processes. However, in various contexts, no theoretical reasons to rule out **non-fundamentalness** (productivity shocks with lagged impacts, news/noise shocks, non-observability, rational expectations, prediction errors). 


* **Lagged impact (Lippi and Reichlin, 1993)**
Suppose that the productivity process, denoted by $y_t$, is given by:
$$
y_t = \varepsilon_t + \theta \varepsilon_{t-1},
$$
where $\varepsilon_t$ denotes the productivity shock. The impact of the productivity shock may be maximal with a lag, i.e. $\theta > 1$. The MA(1) process is then non-fundamental.
\end{block}

* **Rational expectations** 
Simple example of Hansen and Sargent (1991). The economic variable $y_t$ is defined as:
$$
y_t = E_t (\Sigma^\infty_{h=0} \beta^h w_{t+h}), \; \mbox{with }\; w_t = \varepsilon_t - \theta \varepsilon_{t-1},\; 0 < \beta <1, \; |\theta|<1.
$$
If the information set available to the economic agent at date $t$ is $I_t = (\varepsilon_t,
\varepsilon_{t-1}, \ldots)$:
$$
  y_t  = (1-\beta \theta) \varepsilon_t - \theta \varepsilon_{t-1}.
$$
The abs. value of the root of the moving average polynomial is larger or smaller than 1.

* **Advanced indicator (noise / news shocks) and  information structure.**

Consider a process $(x_t)$ that summarizes ``fundamentals'' (technology, preferences, endowments, or government policy) s.t.:
\begin{equation}
x_t = a(L)u_{t},
\end{equation}
where process $(u_t)$ is a strong white noise. On date $t$, the consumer observes $x_t$ as well as a *noisy* signal about the future value of fundamentals:
\begin{equation}
s_t = x_{t+1} + v_t,
\end{equation}
where $(v_t)$ is also a strong white noise, independent of $(u_t)$ (incremental information about future fundamentals, Barsky and Sims, 2012). Moving average representation:
\begin{equation}
\left[
\begin{array}{c}
x_t\\
s_t
\end{array}
\right] = 
\left[
\begin{array}{cc}
La(L) & 0\\
a(L) & 1
\end{array}
\right]
\left[
\begin{array}{c}
u^a_t\\
v_t
\end{array}
\right],\quad \mbox{where $u^a_t = u_{t+1}$.}
\end{equation}

The determinant of the moving average polynomial has a root equal to zero, which is within the unit circle $\Rightarrow$ non-fundamentalness.

Standard estimation toolboxes may lead to misspecified IRFs.

Gouri\'eroux, Monfort and Renne (2019) XXX show that both identification issues ["Q" (or **static issue**) + fundamentalness regime (or **dynamic issue**)}] disappear when the structural shocks are non-Gaussian.

Moreover: they develop consistent parametric and semi-parametric estimation methods when the MA part of the process may be non-fundamental, they illustrate the functioning and performances of these methods by applying them on both simulated and real data.

If  $\mathbb{V}ar(\varepsilon_t)=1$ and  $\mathbb{V}ar(\varepsilon^*_t)=\frac{1}{4}$, then the following two MA processes:
\begin{eqnarray*}
y_t &=& \varepsilon_t + \frac{1}{2} \varepsilon_{t-1}\\
y^*_t &=&  \varepsilon^*_t + 2 \varepsilon^*_{t-1},
\end{eqnarray*}
have the same second-order properties.

Processes $y_t$ and $y_t^*$ are observationally equivalent if the shocks are Gaussian.

Next slide: For each of the following four distributions, we compare the distributions of $(y_{t-1},y_t)$ and of $(y^*_{t-1},y^*_t)$.


```{r FourDistriSVARMA, fig.align = 'center', out.width = "95%", fig.cap = "XXXX.", echo=FALSE}
knitr::include_graphics("images/Figure_distri4MC.png")
```

```{r FourDistriSVARMA2, fig.align = 'center', out.width = "95%", fig.cap = "XXXX.", echo=FALSE}
knitr::include_graphics("images/Figure_simulSVARMA.png")
```


The fact that SVARMA parameterization is identified in non-Gaussian context already present in the literature [Chan, Ho and Tong, 2006, Biometrika] XXX

GMR (2019) propose two consistent estimation approaches of potentially-non-fundamental SVARMA($p$,$q$):

1. Semi-parametric approach (2SLS-GMM). Two steps:
    a. Consistent estimates of $\Pi= [\Phi_1,\dots,\Phi_p]$ can be obtained by applying two-stage least squares (2SLS).
    b. Estimate $\Theta(L)$ by fitting higher-order moments of MA components.

2. Maximum-Likelihood approach. Key ingredient: algorithm to recover $\eta_t$'s from the $y_t$'s, whatever the (non)fundamentalness regime.

First step of the 2SLS-GMM approach:

Consistent estimates of $\Pi= [\Phi_1,\dots,\Phi_p]$ obtained by 2SLS, using $y_{t-2},\dots,y_{t-k-1}$ ($k \ge p$) as instruments $\Rightarrow$ Consistent estimates $\hat{Z}_t$ of $\Theta(L)\varepsilon_t$ ($\hat{Z}_t \equiv \hat{\Phi}(L)y_t$).
\end{block}

Second step of the 2SLS-GMM approach: Using the Taylor expansion of the log-Laplace transf. of $(Z_t,Z_{t-1})$, it can be shown that:
\begin{equation*}
\begin{array}{ccll}
E[(u'Z_t + v'Z_{t-1})^2] &=& \sum_{j=1}^{n} [(u'B_{0j})^2 + (u'B_{1j}+v'B_{0j})^2 + (v'B_{1j})^2] & \mbox{(order 2)}\\
E[(u'Z_t + v'Z_{t-1})^3] &=& \sum_{j=1}^{n} \kappa_{3j}[(u'B_{0j})^3 + (u'B_{1j}+v'B_{0j})^3 + (v'B_{1j})^3] & \mbox{(order 3)}\\
E[(u'Z_t + v'Z_{t-1})^4] &=& \sum_{j=1}^{n} \kappa_{4j}[(u'B_{0j})^4 + (u'B_{1j}+v'B_{0j})^4 + (v'B_{1j})^4]\\
&& +3\left(\sum_{j=1}^{n}[(u'B_{0j})^2 + (u'B_{1j}+v'B_{0j})^2 + (v'B_{1j})^2]\right)^2 & \mbox{(order 4)},
\end{array}
\end{equation*}
where $Z_t := \Phi(L)Y_t = B_0 \eta_t + B_1 \eta_{t-1}$ (i.e. $C=B_0$ and $B_1 \equiv - \Theta C$), and where the  $\kappa_{3j}$'s and $\kappa_{4j}$'s are the third-order and fourth-order cumulants of $\eta_{j,t}$. 

The previous formula are used to express moment restrictions of the form
$$
E\left[h(Y_t,Y_{t-1},\dots,Y_{t-p-1};\alpha,\beta)\right]=0.
$$

If $r$ is the dimension of $h(Y_t,Y_{t-1},\dots,Y_{t-p-1};\alpha,\beta)$, then we have the order condition $r \ge 2n^2+2n$ (if both third-order an fourth-order moments are used).

:::{.hypothesis #StrongSVARMA name="Strong stationary SVARMA process"}
We have:

i. The errors $\varepsilon_t$ are i.i.d. and such that $\mathbb{E}(\varepsilon_t)=0$ and $\mathbb{E}(\| \varepsilon_t \|^2)<\infty$.
ii. $\varepsilon_t = B \eta_t$, where the components $\eta_{j,t}$ are zero, unit variance, mutually independent.
iii. All the roots of $\det(\Phi (L))$ have a modulus strictly larger than 1 (stationarity)
iv. The roots of $\det(\Theta(z))$ are not on the unit circle.
v. The components of the first row of $B$ are positive and in increasing order.
vi. Each component of $\eta_t$ has a non-zero $r^{th}$ cumulant, with $r \ge 3$, and a finite moment of order $s$, where $s$ is an even integer greater than, or equal to, $r$.
:::

Assumption vi is satisfied by most "usual" non-Gaussian distributions.

**Maximum Likelihood approach** (Univariate case $y_t = \varepsilon_t - \theta \varepsilon_{t-1}$)}

\begin{eqnarray*}
\mbox{When $|\theta|<1$, } \qquad \varepsilon_t &=& \sum^\infty_{h=0} \theta^h y_{t-h} = \underbrace{\sum^{t-1}_{h=0} \theta^h y_{t-h}}_{\mbox{truncated value }\hat{\varepsilon}_t(\theta)} + \mbox{ trunc. error}\\
\mbox{When $|\theta|>1$, } \qquad \varepsilon_t &=& - \sum^\infty_{h=0} \frac{1}{\theta^{h+1}} y_{t+h+1} = \underbrace{- \sum^{T-t-1}_{h=0} \frac{1}{\theta^{h+1}} y_{t+h+1}}_{\mbox{truncated value }\hat{\varepsilon}_t(\theta)} + \mbox{ trunc. error}.
\end{eqnarray*}

Truncated log-likelihood function: $\log \mathcal{L}(y_1,\dots,y_T;\theta, \gamma) = \sum^T_{t=1} \log g \left(\hat{\varepsilon}_t(\theta); \gamma\right)$, where $g(.;\gamma)$ is the (parametric) p.d.f. of $\varepsilon_t$.
\end{block}


Maximum Likelihood approach (Multivariate case)

We propose an algorithm ($\mathcal{E}$) to get estimates of the $\varepsilon_t$'s ($t \in \{1,\dots,T\}$):
\begin{eqnarray*}
\mathcal{E}: &&\\
&& Y_{-p+1}^T,\Phi_1,\dots,\Phi_p,\Theta
\quad \mapsto \quad \mathcal{E}(Y_{-p+1}^T,\Phi_1,\dots,\Phi_p,\Theta),
\end{eqnarray*}
where $Y_{-p+1}^T=\{Y_{-p+1},\dots,Y_1,\dots,Y_T\}$.

Algorithm $\mathcal{E}$ is based on the Schur decomposition of $\Theta$ and on a mixture of forward and backward recursions. 

$\mathcal{E}(Y_{-p+1}^T,\Phi_1,\dots,\Phi_p,\Theta)$ converges to $\varepsilon_t$ in mean square.

Truncated log-likelihood:
\begin{equation}
\log \mathcal{L} (Y_{-p+1}^T;\Phi,\Theta, \Gamma) = - T \sum_{k=1}^n \log |\lambda_k| \mathbb{I}_{\{|\lambda_k| \ge 1\}} + \sum_{t=1}^{T} \log g(\mathcal{E}(Y_{-p+1}^T,\Phi,\Theta), \Gamma), (\#eq:VARMAapproxML)
\end{equation}
where the $\lambda_k$'s are the eigenvalues of $\Theta$ and where $g(.,\Gamma)$ is the p.d.f. of $\varepsilon_t$.


**Monte Carlo experiments**

VMA(1) process: $y_t = C \eta_t - \Theta C \eta_{t-1}$, with
\begin{equation}
C = \left[
\begin{array}{cc}
0 & 1 \\
1 & 0.5
\end{array}
\right] \quad \mbox{and} \quad
\Theta = \left[
\begin{array}{cc}
-0.5 & 0 \\
1 & -2
\end{array}
\right], (\#eq:CTHETA)
\end{equation}
$\eta_{1,t}$ is drawn from a mixture of Gaussian (skewness of 2, excess kurtosis of 6), $\eta_{2,t}$, is drawn from a Student's distribution with 6 df.

The data generating process is non-fundamental.

Three sample sizes: $T=100$, 200 and 500. %Pseudo ML: in the (truncated) log-likelihood, the distributions of both $\eta_{1,t}$ and $\eta_{2,t}$ are mixtures of Gaussian.

Better small-sample performances for MLE than for 2SLS-GMM. 

**Relation with the Heteroskedasticity Identification**


In some cases, where the $\varepsilon_t$'s are heteroskedastic, the $B$ matrix can be identified

[Rigobon (2003)](\href{https://www.mitpressjournals.org/doi/10.1162/003465303772815727), 

[Lanne, Lutkepohl and Maciejowska (2010)](https://www.sciencedirect.com/science/article/pii/S0165188909001481\#!)

Consider the case where we still have $\varepsilon_t = B \eta_t$ but where $\eta_t$'s variance conditionally depends on a regime $s_t \in \{1,\dots,M\}$. That is:
$$
\mathbb{V}ar(\eta_{k,t}|s_t) = \lambda_{s_t,k} \quad \mbox{for } k \in \{1,\dots,n\}
$$

Denoting by $\Lambda_i$ the diagonal matrix whose diagonal entries are the $\lambda_{i,k}$'s, it comes that:
$$
\mathbb{V}ar(\eta_{t}|s_t) = \Lambda_{s_t},\quad \mbox{and}\quad \mathbb{V}ar(\varepsilon_{t}|s_t) = B\Lambda_{s_t}B'.
$$

Without loss of generality, it can be assumed that $\Lambda_1=Id$.

In this context, $B$ is identified, apart from sign reversal of its columns if for all $k \ne j \in \{1,\dots,n\}$, there is a regime $i$ s.t. $\lambda_{i,k} \ne \lambda_{i,j}$. [Prop.1 in Lanne, L\"utkepohl and Maciejowska (2010)](https://www.sciencedirect.com/science/article/pii/S0165188909001481\#!).

Bivariate regime case ($M=2$): $B$ identified if the $\lambda_{2,k}$'s are all different. That is, identification is ensured if "there is sufficient heterogeneity in the volatility changes" [L\"utkepohl and Netsunajev (2017)(https://www.sciencedirect.com/science/article/pii/S2452306216300223).

If the regimes $s_t$ are exogenous and serially independent, then this situation is consistent with the "non-Gaussian" situation described in this section.







# Estimation Methods


Context and Objective: 

* You observe a sample $\bv{y}=\{y_1,\dots,y_n\}$.
* You know that these data have been generated by a model parameterized by $\theta_0 \in \mathbb{R}^K$.


## Generalized Method of Moments (GMM) {#secGMM}

### Framework

We denote by $x_i$ a $p \times 1$ vector of (stationary) variables; by $\theta$ an $K \times 1$ vector of parameters, and by $h(x_t;\theta)$ a continuous $r \times 1$ vector-valued function.

We denote by $\theta_0$ the true value of $\theta$ and we assume that $\theta_0$ satisfies:
$$
\mathbb{E}[h(x_t;\theta_0)] = 0.
$$

We denote by $\underline{x_t}$ the information contained in the current and past observations of $x_t$, that is: $\underline{x_t} = \{x_t,x_{t-1},\dots,x_1\}$. We denote by $g(\underline{x_T};\theta)$ the sample average of $h(x_t;\theta)$, i.e.:
$$
g(\underline{x_T};\theta) = \frac{1}{T} \sum_{t=1}^{T} h(x_t;\theta).
$$

Intuition behind GMM: Choose $\theta$ so as to make the sample moment as close as possible to 0.

:::{.definition #GMM}
A GMM estimator of $\theta_0$ is given by:
$$
\hat{\theta}_T = \mbox{argmin}_{\theta} \quad g(\underline{x_T};\theta)'\, W_T \, g(\underline{x_T};\theta),
$$
where $W_T$ is a positive definite matrix (that may depend on $\underline{x_T}$).
:::

If $a = r$ (the dimension of $\theta$ is the same as that of $h(x_t;\theta)$, or $g(\underline{x_T};\theta)$), $\hat{\theta}_T$ is such that:
$$
g(\underline{x_T};\hat{\theta}_T) = 0.
$$
Under regularity and identification conditions:
$$
\hat{\theta}_{T} \overset{p}{\rightarrow} \theta_0,
$$
i.e. $\forall \varepsilon>0$, $\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\theta}_{T} - \theta_0|>\varepsilon) = 0$.

**Optimal weighting matrix**. The GMM estimator achieving the minimum asymptotic variance is obtained when $W_n$ is the inverse of matrix $S$, the latter being defined by:
$$
S = Asy.\mathbb{V}ar\left(\sqrt{n}g(\underline{x_n};\hat{\theta}_n)\right).
$$
The intuition behind this result is the same that underlies Generalized Least Squares (see Section \@ref(GLS)), that is: it is beneficial to use a criterion in which the weights are inversely proportional to the variances of the moments. 

If $h(x_i;\theta_0)$ is not correlated to $h(x_j;\theta_0)$, for $i \ne j$, then we have:
$$
S = \mathbb{V}ar(h(x_i;\theta_0)),
$$
which can be approximated by
$$
\hat{\Gamma}_{0,n}=\frac{1}{n}\sum_{i=1}^{n} h(x_i;\hat{\theta}_n)h(x_{i};\hat{\theta}_n)'.
$$

In a time series context, we often have correlation between $x_i$ and $x_{t+k}$, for finite $k$. In this case, and if the time series $\{x_i\}$ is covariance stationary (see \@ref(LRvariance)), then we have:
$$
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
$$
where $\Gamma_\nu := \mathbb{E}[h(x_i;\theta_0) h(x_{i-\nu};\theta_0)']$.

For $\nu \ge 0$, let us define $\hat{\Gamma}_{\nu,n}$ by:
$$
\hat{\Gamma}_{\nu,n} = \frac{1}{n} \sum_{i=\nu + 1}^{n} h(x_i;\hat{\theta}_n)h(x_{i-\nu};\hat{\theta}_n)',
$$
then $S$ can be approximated by:
\begin{equation}
\hat{\Gamma}_{0,n} + \sum_{\nu=1}^{q}[1-\nu/(q+1)](\hat{\Gamma}_{\nu,n}+\hat{\Gamma}_{\nu,n}').	(\#eq:Shat)
\end{equation}


**Asymptotic distribution of $\hat\theta_n$**

We have:
\begin{equation}
\sqrt{n}(\hat\theta_n - \theta_0) \overset{\mathcal{L}}{\rightarrow} \mathcal{N}(0,V),(\#eq:asymptGMM)
\end{equation}
where $V = (DS^{-1}D')^{-1}$.

$V$ can be approximated by
\begin{equation}
\hat{V}_n = (\hat{D}_T\hat{S}_n^{-1}\hat{D}_n')^{-1},(\#eq:VGMM)
\end{equation}
where $\hat{S}_n$ is given by Eq. \@ref(eq:Shat) and
$$
\hat{D}'_n := \left.\frac{\partial g(\underline{x_n};\theta)}{\partial \theta'}\right|_{\theta = \hat\theta_n}.
$$


### Testing hypotheses in the GMM framework {#overidentif}

A first important test is the one concerning the validity of the moment restrictions. Assume that the number of restrictions imposed is larger than the number of parameters to estimate ($r>K$). In this case, we say that our restrictions are over-identifiying.

Under correct specification, we asymptotically have:
$$
\sqrt{n}g(\underline{x_n};\theta_0)  \sim \mathcal{N}(0,S).
$$
As a result, it comes that:
$$
J_n = \left(\sqrt{n}g(\underline{x_n};\theta_0)\right)'S^{-1}\left(\sqrt{n}g(\underline{x_n};\theta_0)\right)
$$
asymptotically follows a $\chi^2$ distribution. The number of degrees of freedom is equal to $r-K$. (Indeed, for $r=K$, it comes that $J=0$.) That is, asymptotically:
$$
J_n \sim \chi^2(r-K).
$$
XXX Sargan-Hansen test @Sargan_1958 and @Hansen_1982 XXX


The GMM framework also allows to easily test linear restrictions on the parameters. Fist, given Eq. \@ref(eq:asymptGMM), Wald tests (see Eq. \@ref(eq:W1) in Section \@ref(Ftest)) are readily available. Second, one can also resort to a test equivalent to the *likelihood ratio tests* (see Definition \@ref(def:LR)). More precisely, consider an unconstrained model and a constrained version of this model, the number of restrictions being equal to $k$. If the two models are estimated by considering the same moment constraints, and the same weighting matrix (using Eq. \@ref(eq:VGMM), based on the unrestricted model), then we have that:
$$
n \left[(g(\underline{x_n};\hat{\theta}^*_n)-g(\underline{x_n};\hat{\theta}_n)\right] \sim \chi^2(k),
$$
where $\hat{\theta}^*_n$ is the constrained estimate of $\theta_0$.

### Example: Estimation of the Stochastic Discount Factor (s.d.f.)

Under the no-arbitrage assumption, there exists a random variable $\mathcal{M}_{t,t+1}$ such that
$$
\mathbb{E}_t(\mathcal{M}_{t,t+1}R_{t+1})=1
$$
for any (gross) asset return $R_t$. In the following, $R_t$ denotes a $n_r$-dimensional vector of gross  returns.

We consider the following specification of the s.d.f.:
\begin{equation}
\mathcal{M}_{t,t+1} = 1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1})), (\#eq:sdf)
\end{equation}
where $F_t$ is a vector of factors. Eq. \@ref(eq:sdf) then reads:
$$
\mathbb{E}_t([1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1}))]R_{t+1})=1.
$$

Assume that the date-$t$ information set is $\mathcal{I}_t=\{\textbf{z}_t,\mathcal{I}_{t-1}\}$, where $\textbf{z}_t$ is a vector of variables observed on date $t$. (We then have $\mathbb{E}_t(\bullet) \equiv \mathbb{E}(\bullet|\mathcal{I}_t)$.)

We can use $\textbf{z}_t$ as an instrument. Indeed, we have:
\begin{eqnarray}
&&\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]) \nonumber \\
&=&\mathbb{E}(\mathbb{E}_t\{z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]\})\nonumber\\
&=&\mathbb{E}(z_{i,t} \underbrace{\mathbb{E}_t\{\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1\}}_{=0})=0.(\#eq:momF)
\end{eqnarray}
We have then converted a conditional moment condition into a unconditional one (which we need to implement the theory above). However, at that stage, we cannot still not directly use the GMM formulas because of the conditional expectation $\mathbb{E}_t(F_{t+1})$ that appears in $\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1])=0$.

To go further, let us assume that:
$$
\mathbb{E}_t(F_{t+1}) = \textbf{b}_F \textbf{z}_t.
$$
We can then easily estimate matrix $\textbf{b}_F$ (of dimension $n_F \times n_z$) by OLS. Note here that these OLS can be seen as a special GMM case. Indeed, as was done in Eq. \@ref(eq:momF), we can show that, for the $j^{th}$ component of $F_t$, we have:
$$
\mathbb{E}( [F_{j,t+1} - \textbf{b}_{F,j} \textbf{z}_t]\textbf{z}_{t})=0,
$$
where $\textbf{b}_{F,j}$ denotes the $j^{th}$ row of $\textbf{b}_{F}$. This yields the OLS formula.

At that stage, we count on the following moment restrictions to estimate $\textbf{b}_M$:
$$
\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \textbf{b}_F \textbf{z}_t\}R_{t+1}-R_{t+1}+1])=0.
$$
Specifically, the number of restrictions is $n_R \times n_z$. Let us implement this approach in the U.S. context, using data extracted from the [FRED database](https://fred.stlouisfed.org). In factor $F_t$, we use the changes in the VIX and in the personal consumption expenditures. The returns ($R_t$) are based on the Wilshire 5000 Price Index (a stock price index) and on the ICE BofA BBB US Corporate Index Total Return Index (a bond return index).


```{r gmm1}
library(fredr)
fredr_set_key("df65e14c054697a52b4511e77fcfa1f3")
start_date <- as.Date("1990-01-01"); end_date <- as.Date("2022-01-01")
f <- function(ticker){
  fredr(series_id = ticker,
        observation_start = start_date,observation_end = end_date,
        frequency = "m",aggregation_method = "avg")
}
vix <- f("VIXCLS") # VIX
pce <- f("PCE") # Personal consumption expenditures
sto <- f("WILL5000PRFC") # Wilshire 5000 Full Cap Price Index
bdr <- f("BAMLCC0A4BBBTRIV") # ICE BofA BBB US Corporate Index Total Return Index
T <- dim(vix)[1]
dvix <- c(vix$value[3:T]/vix$value[2:(T-1)]) # change in VIX t+1
dpce <- c(pce$value[3:T]/pce$value[2:(T-1)]) # change in PCE t+1
dsto <- c(sto$value[3:T]/sto$value[2:(T-1)]) # return t+1
dbdr <- c(bdr$value[3:T]/bdr$value[2:(T-1)]) # return t+1
dvix_1 <- c(vix$value[2:(T-1)]/vix$value[1:(T-2)]) # change in VIX t
dpce_1 <- c(pce$value[2:(T-1)]/pce$value[1:(T-2)]) # change in PCE t
dsto_1 <- c(sto$value[2:(T-1)]/sto$value[1:(T-2)]) # return t
dbdr_1 <- c(bdr$value[2:(T-1)]/bdr$value[1:(T-2)]) # return t
```

Define the matrices containing the $F_{t+1}$, $\textbf{z}_t$, and $R_{t+1}$ vectors: 

```{r gmm2}
F_tp1 <- cbind(dvix,dpce)
Z     <- cbind(1,dvix_1,dpce_1,dsto_1,dbdr_1)
b_F <- t(solve(t(Z) %*% Z) %*% t(Z) %*% F_tp1)
F_innov <- F_tp1 - Z %*% t(b_F)
R_tp1 <- cbind(dsto,dbdr)
n_F <- dim(F_tp1)[2]; n_R <- dim(R_tp1)[2]; n_z <- dim(Z)[2]
```

Function `f_aux` compute the $h(x_t;\theta)$ and the $g(\underline{x_T};\theta)$; function `f2beMin` is the function to be minimized.

```{r gmm3}
f_aux <- function(theta){
  b_M <- matrix(theta[1:n_F],ncol=1)
  R_aux <- matrix(F_innov %*% b_M,T-2,n_R) * R_tp1 - R_tp1 + 1
  H <- (R_aux %x% matrix(1,1,n_z)) * (matrix(1,1,n_R) %x% Z)
  g <- matrix(apply(H,2,mean),ncol=1)
  return(list(g=g,H=H))
}
f2beMin <- function(theta,W){# function to be minimized
  res <- f_aux(theta)
  return(t(res$g) %*% W %*% res$g)
}
```

Now, let's minimize this function. We consider 5 iterations (where $W$ is updated).

```{r gmm4}
theta <- c(rep(0,n_F)) # inital value
for(i in 1:5){# recursion on W
  res <- f_aux(theta)
  W <- solve(1/T * t(res$H) %*% res$H)
  res.optim <- optim(theta,f2beMin,W=W,
                     method="BFGS", # could be "Nelder-Mead"
                     control=list(trace=FALSE,maxit=200),hessian=TRUE)
  theta <- res.optim$par
}
```

Finally, let's compute the standard deviation of the parameter estimates.

```{r gmm5}
eps <- .0001
g0 <- f_aux(theta)$g
D <- NULL
for(i in 1:length(theta)){
  theta.i <- theta
  theta.i[i] <- theta.i[i] + eps
  gi <- f_aux(theta.i)$g
  D <- cbind(D,(gi-g0)/eps)
}
V <- 1/T * solve(t(D) %*% W %*% D)
std.dev <- sqrt(diag(V));t.stud <- theta/std.dev
cbind(theta,std.dev,t.stud)
```

The Hansen statistic can be used to test the model. If the model is correct, we have:
$$
T g(\underline{x_T};\theta)'\, S^{-1} \, g(\underline{x_T};\theta) \sim \,i.i.d.\,\chi^2(J - K),
$$
where $J$ is the number of moment contraints ($n_z \times n_r$ here) and $K$ is the number of estimated parameters ($=n_F$ here).

```{r gmm6}
g <- f_aux(theta)$g
Hanse_stat <- T * t(g) %*% W %*% g
pvalue <- pchisq(q = Hanse_stat,df = n_R*n_z - n_F)
```



## Maximum Likelihood Estimation {#secMLE}

Intuition behind the Maximum Likelihood Estimation: Estimator = the value of $\theta$ that is such that the probability of having observed $\bv{y}$ is the highest possible.

Assume that the time periods between the arrivals of two customers in a shop, denoted by $y_i$, are i.i.d. and follow an exponential distribution, i.e. $y_i \sim \mathcal{E}(\lambda)$.

You have observed these arrivals for some time, thereby constituting a sample $\{y_1,\dots,y_n\}$. You want to estimate $\lambda$ (i.e. in that case, the vector of parameters is simply $\theta = \lambda$).

The density of $Y$ is $f(y;\lambda) = \dfrac{1}{\lambda}\exp(-y/\lambda)$. Fig. \@ref(fig:MLE1) represents that density functions for different values of $\lambda$.

Your 200 observations are reported at the bottom of Fig. \@ref(fig:MLE1) (red).
You build the histogram and report it on the same chart.

```{r MLE1, echo=FALSE, warning=FALSE, fig.cap="The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations", fig.asp = .6, out.width = "90%", fig.align = 'center'}
y <- rexp(200,rate = 1/3)
y.save <- y
hist(y,xlim=c(0,12),ylim=c(0,1),
     freq = FALSE,breaks = 20,xlab="",ylab="",main="",col="white")
x <- seq(0,12,by=.01)
lambda = 1
par(new=TRUE)
plot(x,1/lambda * exp(-x/lambda),type="l",lwd=2,
     ylim=c(0,1),
     xlab="Time period between two arrivals",
     ylab="Density")
rug(y,col="red")
lambda = 3
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="blue")
lambda = 5
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="red")
lambda = 7
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="green")

legend("topright",
       c(expression(paste(lambda," = 1",sep="")),
         expression(paste(lambda," = 3",sep="")),
         expression(paste(lambda," = 5",sep="")),
         expression(paste(lambda," = 7",sep=""))),
       lty=c(1,1,1,1), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2,2,2), # line width
       col=c("black","blue","red","green"),
       seg.len=3)
```

What is your estimate of $\lambda$?

Now, assume that you have only four observations: $y_1=1.1$, $y_2=2.2$, $y_3=0.7$ and $y_4=5.0$.
What was the probability of observing, for a small $\varepsilon$,

*	$1.1-\varepsilon \le Y_1 < 1.1+\varepsilon$, 
*	$2.2-\varepsilon \le Y_2 < 2.2+\varepsilon$, 
*	$0.7-\varepsilon \le Y_3 < 0.7+\varepsilon$ and
*	$5.0-\varepsilon \le Y_4 < 5.0+\varepsilon$?

Because the $y_i$s are i.i.d., this probability is $\prod_{i=1}^4(2\varepsilon f(y_i,\lambda))$.
The next plot shows the probability (divided by $16\varepsilon^4$) as a function of $\lambda$.

```{r MLE2, echo=FALSE, warning=FALSE, fig.cap="Proba. that $y_i-\\varepsilon \\le Y_i < y_i+\\varepsilon$, $i \\in \\{1,2,3,4\\}$. The vertical red line indicates the maximum of the function.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
log.f <- function(y,lambda){
  return(sum(log(1/lambda*exp(-y/lambda))))
}
y <- c(1.1,2.2,0.7,5)
all.log.f <- NULL
all.lambda <- seq(.1,10,by=.1)
for(lambda in all.lambda){
  all.log.f <- c(all.log.f,log.f(y,lambda))
}
par(plt=c(.15,.95,.2,.85),mfrow=c(1,2))
plot(all.lambda,exp(all.log.f),type="l",lwd=2,xlab=expression(lambda),
     ylab="Probability",main="(a) Probability")
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
plot(all.lambda,all.log.f,type="l",lwd=2,ylim=c(-20,-5),
     xlab=expression(lambda),ylab="Probability",main="(b) log(Probability)")
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
```


Back to the example with 200 observations:

```{r MLE3, echo=FALSE, warning=FALSE, fig.cap="Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
y <- y.save
all.log.f <- NULL
all.lambda <- seq(.1,10,by=.1)
for(lambda in all.lambda){
  all.log.f <- c(all.log.f,log.f(y,lambda))
}
par(mfrow=c(1,1),plt=c(.15,.95,.2,.85))
plot(all.lambda,all.log.f,type="l",lwd=2,xlab=expression(lambda),
     ylab="log(Probability)",ylim=c(-500,-400))
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
```



### Notations

$f(y;\boldsymbol\theta)$ denotes the probability density function (p.d.f.) of a random variable $Y$ which depends on a set of parameters $\boldsymbol\theta$.

Density of $n$ independent and identically distributed (i.i.d.) observations of $Y$:
$$
f(y_1,\dots,y_n;\boldsymbol\theta) = \prod_{i=1}^n f(y_i;\boldsymbol\theta).
$$
$\bv{y}$ denotes the vector of observations; $\bv{y} = \{y_1,\dots,y_n\}$.

***
:::{.definition #likelihood name="Likelihood function"}
$\mathcal{L}: \boldsymbol\theta \rightarrow  \mathcal{L}(\boldsymbol\theta;\bv{y})=f(y_1,\dots,y_n;\boldsymbol\theta)$ is the **likelihood function**.
:::
***

We often work with $\log \mathcal{L}$, the **log-likelihood function**.

:::{.example #normal name="Gaussian distribution"}
If $y_i \sim \mathcal{N}(\mu,\sigma^2)$, then
$$
\log \mathcal{L}(\boldsymbol\theta;\bv{y}) = - \frac{1}{2}\sum_{i=1}^n\left( \log \sigma^2 + \log 2\pi + \frac{(y_i-\mu)^2}{\sigma^2} \right).
$$
:::


***
:::{.definition #score name="Score"}
The score $S(y;\boldsymbol\theta)$ is given by $\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta}$.
:::
***

If $y_i \sim \mathcal{N}(\mu,\sigma^2)$ (Example \@ref(exm:normal)), then
$$
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} =
\left[\begin{array}{c}
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \mu}\\
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \sigma^2}
\end{array}\right] =
\left[\begin{array}{c}
\dfrac{y-\mu}{\sigma^2}\\
\frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right)
\end{array}\right].
$$

***
:::{.proposition #score name="Score expectation"}
The expectation of the score is zero.
:::
***

:::{.proof}
We have:
\begin{eqnarray*}
\mathbb{E}\left(\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\right) &=&
\int \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta) dy \\
&=& \int \frac{\partial f(y;\boldsymbol\theta)/\partial \boldsymbol\theta}{f(y;\boldsymbol\theta)} f(y;\boldsymbol\theta) dy =
\frac{\partial}{\partial \boldsymbol\theta} \int f(y;\boldsymbol\theta) dy =
\partial 1 /\partial \boldsymbol\theta = 0,
\end{eqnarray*}
which gives the result.
:::

:::{.definition #Fisher name="Fisher information matrix"}
The **information matrix** is (minus) the the expectation of the second derivatives of the log-likelihood function:
$$
\mathcal{I}_Y(\boldsymbol\theta) = - \mathbb{E} \left( \frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right).
$$
:::

***
:::{.proposition #Fisher}
We have $\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E} \left[ \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)
\left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)' \right] = \mathbb{V}ar[S(Y;\boldsymbol\theta)]$.
:::
***

:::{.proof}
We have $\frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = \frac{\partial^2 f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\frac{1}{f(Y;\boldsymbol\theta)} - \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta'}$. The expectation of the first right-hand side term is $\partial^2 1 /(\partial \boldsymbol\theta \partial \boldsymbol\theta') = \bv{0}$, which gives the result.
:::

:::{.example}
If $y_i \sim\,i.i.d.\, \mathcal{N}(\mu,\sigma^2)$, let $\boldsymbol\theta = [\mu,\sigma^2]'$ then
$$
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} = \left[\frac{y-\mu}{\sigma^2} \quad \frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right) \right]'
$$
and
$$
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E}\left( \frac{1}{\sigma^4}
\left[
\begin{array}{cc}
\sigma^2&y-\mu\\
y-\mu & \frac{(y-\mu)^2}{\sigma^2}-\frac{1}{2}
\end{array}\right]
\right)=
\left[
\begin{array}{cc}
1/\sigma^2&0\\
0 & 1/(2\sigma^4)
\end{array}\right].
$$
:::


***
:::{.proposition #additiv name="Additive property of the Info. mat."}
The information matrix resulting from two independent experiments is the sum of the information matrices:
$$
\mathcal{I}_{X,Y}(\boldsymbol\theta) = \mathcal{I}_X(\boldsymbol\theta) + \mathcal{I}_Y(\boldsymbol\theta).
$$
:::
***
:::{.proof} Immediately obtained from the definition (see Def. \@ref{def:Fisher}).
:::

***
:::{.theorem #FDCR name="Fr\\'echet-Darmois-Cram\\'er-Rao bound"}
Consider an unbiased estimator of $\boldsymbol\theta$ denoted by $\hat{\boldsymbol\theta}(Y)$.

The variance of the random variable $\boldsymbol\omega'\hat{\boldsymbol\theta}$ (which is a linear combination of the components of $\hat{\boldsymbol\theta}$) is larger than:
$$
(\boldsymbol\omega'\boldsymbol\omega)^2/(\boldsymbol\omega' \mathcal{I}_Y(\boldsymbol\theta) \boldsymbol\omega).
$$
:::
***

:::{.proof}
The Cauchy-Schwarz inequality implies that $\sqrt{\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y))\mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))} \ge |\boldsymbol\omega'\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)]\boldsymbol\omega |$. Now, $\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)] = \int_y \hat{\boldsymbol\theta}(y) \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta)dy =
\frac{\partial}{\partial \boldsymbol\theta}\int_y \hat{\boldsymbol\theta}(y) f(y;\boldsymbol\theta)dy = \bv{I}$ because $\hat{\boldsymbol\theta}$ is unbiased. 

Therefore $\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y)) \ge \mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))^{-1} (\boldsymbol\omega'\boldsymbol\omega)^2$. Prop. \@ref(prp:Fisher) leads to the result.
:::

:::{.definition #identif name="Identifiability"}
The vector of parameters $\boldsymbol\theta$ is identifiable if, for any other vector $\boldsymbol\theta^*$:
$$
\boldsymbol\theta^* \ne \boldsymbol\theta \Rightarrow \mathcal{L}(\boldsymbol\theta^*;\bv{y}) \ne \mathcal{L}(\boldsymbol\theta;\bv{y}).
$$
:::

:::{.definition #MLEest name="Maximum Likelihood Estimator (MLE)"}
The maximum likelihood estimator (MLE) is the vector $\boldsymbol\theta$ that maximizes the likelihood function. Formally:
\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\bv{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\bv{y}).(\#eq:MLEestimator)
\end{equation}
:::

:::{.definition #likFunction name="Likelihood equation"}
Necessary condition for maximizing the likelihood function:
\begin{equation}
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta} = \bv{0}.
\end{equation}
:::

:::{.hypothesis #MLEregularity name="Regularity assumptions"}

We have:

i. $\boldsymbol\theta \in \Theta$ where $\Theta$ is compact.

ii. $\boldsymbol\theta_0$ is identified.

iii. The log-likelihood function is continuous in $\boldsymbol\theta$.

iv. $\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))$ exists.

v. The log-likelihood function is such that $(1/n)\log\mathcal{L}(\boldsymbol\theta;\bv{y})$ converges almost surely to $\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))$, uniformly in $\boldsymbol\theta \in \Theta$.

vi. The log-likelihood function is twice continuously differentiable in an open neighborood of $\boldsymbol\theta_0$.

vii. The matrix $\bv{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \quad \mbox{(Fisher Information matrix)}$ exists and is nonsingular.
:::

***
:::{.proposition #MLEproperties name="Properties of MLE"}
Under regularity conditions (Assumptions \@ref(hyp:MLEregularity)), the MLE is:

a. **Consistent**: $\mbox{plim}\quad  \boldsymbol\theta_{MLE} = \theta_0$ ($\theta_0$ is the true vector of parameters).

b. **Asymptotically normal**: $\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\bv{I}(\boldsymbol\theta_0)^{-1})$, where
$$
\bv{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0). \quad \mbox{(Fisher Info. matrix)}
$$

c. **Asymptotically efficient**: $\boldsymbol\theta_{MLE}$ is asymptotically efficient and achieves the Fr\'echet-Darmois-Cram\'er-Rao lower bound for consistent estimators.

d. **Invariant**: The MLE of $g(\boldsymbol\theta_0)$ is $g(\boldsymbol\theta_{MLE})$ if $g$ is a continuous and continuously differentiable function.
:::
***

:::{.proof}
See Appendix \@ref(MLEproperties).
:::


Note that (b) also writes:
\begin{equation}
\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0)^{-1}). (\#eq:normMLE)
\end{equation}


The asymptotic covariance matrix of the MLE is:
$$
[\bv{I}(\boldsymbol\theta_0)]^{-1} = \left[- \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \right]^{-1}.
$$
A direct (analytical) evaluation of this expectation is often out of reach.

It can however be estimated by, either:
\begin{eqnarray}
\hat{\bv{I}}_1^{-1} &=&  \left( - \frac{\partial^2 \log \mathcal{L}({\boldsymbol\theta_{MLE}};\bv{y})}{\partial {\boldsymbol\theta} \partial {\boldsymbol\theta}'}\right)^{-1}, (\#eq:III1)\\
\hat{\bv{I}}_2^{-1} &=&  \left( \sum_{i=1}^n \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta}} \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta'}} \right)^{-1}.  (\#eq:I2)
\end{eqnarray}

Asymptotically, we have $(\hat{\bv{I}}_1^{-1})\hat{\bv{I}}_2=Id$, that is, the two formulas provide the same result.

In case of (suspected) misspecification, one can use the so-called *sandwich estimator* of the covariance matrix.[^footnoteSandwich] This covariance matrix is given by:
$$
\hat{\bv{I}}_3^{-1} = \hat{\bv{I}}_2^{-1} \hat{\bv{I}}_1 \hat{\bv{I}}_2^{-1}.
$$

[^footnoteSandwich]: See, e.g., [Charles Geyer's lectures notes](https://www.stat.umn.edu/geyer/5601/notes/sand.pdf).



### To sum up -- MLE in practice

* A parametric model (depending on the vector of parameters $\boldsymbol\theta$ whose "true" value is $\boldsymbol\theta_0$) is specified.

* i.i.d. sources of randomness are identified.

* The density associated to one observation $y_i$ is computed analytically (as a function of $\boldsymbol\theta$): $f(y;\boldsymbol\theta)$.

* The log-likelihood is $\log \mathcal{L}(\boldsymbol\theta;\bv{y}) = \sum_i \log f(y_i;\boldsymbol\theta)$.

* The MLE estimator results from the optimization problem (this is Eq. \@ref(eq:MLEestimator)):
\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\bv{y}).
\end{equation}

* We have: $\boldsymbol\theta_{MLE} \sim \mathcal{N}(\theta_0,\bv{I}(\boldsymbol\theta_0)^{-1})$, where $\bv{I}(\boldsymbol\theta_0)^{-1}$ is estimated by means of Eq. \@ref(eq:III1) or Eq. \@ref(eq:I2). Most of the time, this computation is numerical.


### Example: MLE estimation of a mixture of Gaussian distribution

Consider the returns of the SMI index. Let's assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. $f(x;\boldsymbol\theta)$, with $\boldsymbol\theta = [\mu_1,\sigma_1,\mu_2,\sigma_2,p]'$, is given by:
$$
p \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) + (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right).
$$
(See [p.d.f. of mixtures of Gaussian dist.](https://jrenne.shinyapps.io/density/))

The maximum likelihood estimate is $\boldsymbol\theta_{MLE}=[0.30,1.40,-1.45,3.61,0.87]'$.

The first two entries of the diagonal of $\hat{\bv{I}}_1^{-1}$ are $0.00528$ and $0.00526$. They are the estimates of $\mathbb{V}ar(\mu_{1,MLE})$ and of $\mathbb{V}ar(\sigma_{1,MLE})$, respectively.

95\% confidence intervals for $\mu_1$ and $\sigma_1$ are, respectively:
$$
0.30 \pm 1.96\underbrace{\sqrt{0.00528}}_{=0.0726} \quad \mbox{ and } \quad 1.40 \pm 1.96\underbrace{\sqrt{0.00526}.}_{=0.0725}
$$

```{r smiData, echo=TRUE, warning=FALSE, fig.cap="Time series of SMI weekly returns (source: Yahoo Finance).", fig.asp = .6, out.width = "90%", fig.align = 'center'}
smi <- read.csv("https://raw.githubusercontent.com/jrenne/Data4courses/master/SMI/SMI.csv",
                dec = ".",header = TRUE, na.strings = "null")
smi$Date <- as.Date(smi$Date,"%m/%d/%y")
T <- dim(smi)[1]
h <- 5 # holding period (one week)
smi$r <- c(rep(NaN,h),
           100*c(log(smi$Close[(1+h):T]/smi$Close[1:(T-h)])))
indic.dates <- seq(1,T,by=5)  # weekly returns
smi <- smi[indic.dates,]
smi <- smi[complete.cases(smi),]
par(mfrow=c(1,1))
plot(smi$Date,smi$r,type="l",xlab="",ylab="in percent")
abline(h=0,col="blue")
abline(h=mean(smi$r,na.rm = TRUE)+2*sd(smi$r,na.rm = TRUE),lty=3,col="blue")
abline(h=mean(smi$r,na.rm = TRUE)-2*sd(smi$r,na.rm = TRUE),lty=3,col="blue")
```

```{r smi_mle}
f <- function(theta,y){ # Likelihood function
  mu.1 <- theta[1]; mu.2 <- theta[2]
  sigma.1 <- theta[3]; sigma.2 <- theta[4]
  p <- exp(theta[5])/(1+exp(theta[5]))
  res <- p*1/sqrt(2*pi*sigma.1^2)*exp(-(y-mu.1)^2/(2*sigma.1^2)) + 
    (1-p)*1/sqrt(2*pi*sigma.2^2)*exp(-(y-mu.2)^2/(2*sigma.2^2))
  return(res)
}
log.f <- function(theta,y){ #log-Likelihood function
  return(-sum(log(f(theta,y))))
}
res.optim <- optim(c(0,0,0.5,1.5,.5),
                   log.f,
                   y=smi$r,
                   method="BFGS", # could be "Nelder-Mead"
                   control=list(trace=FALSE,maxit=100),hessian=TRUE)
theta <- res.optim$par
theta
```
Now, let us compute estimates of the covariance matrix of the MLE:

```{r smiCovar}
# Hessian approach:
I.1 <- solve(res.optim$hessian)
# Outer-product of gradient approach:
log.f.0 <- log(f(theta,smi$r))
epsilon <- .00000001
d.log.f <- NULL
for(i in 1:length(theta)){
  theta.i <- theta
  theta.i[i] <- theta.i[i] + epsilon
  log.f.i <- log(f(theta.i,smi$r))
  d.log.f <- cbind(d.log.f,
                   (log.f.i - log.f.0)/epsilon)
}
I.2 <- solve(t(d.log.f) %*% d.log.f)
# Misspecification-robust approach (sandwich formula):
I.3 <- I.1 %*% solve(I.2) %*% I.1
cbind(diag(I.1),diag(I.2),diag(I.3))
```
According to the first (respectively third) type of estimate for the covariance matrix, a 95\% confidence interval for $\mu_1$ is [`r c(round(theta[1]+qnorm(.025)*sqrt(I.1[1,1]),3),round(theta[1]+qnorm(.975)*sqrt(I.1[1,1]),3))`] (resp. [`r c(round(theta[1]+qnorm(.025)*sqrt(I.3[1,1]),3),round(theta[1]+qnorm(.975)*sqrt(I.3[1,1]),3))`]).

```{r smidistri, echo=TRUE, warning=FALSE, fig.cap="Comparison of different estimates of the distribution of returns.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
x <- seq(-5,5,by=.01)
plot(x,f(theta,x),type="l",lwd=2,xlab="returns, in percent",ylab="",
     ylim=c(0,1.4*max(f(theta,x))))
lines(density(smi$r),type="l",lwd=2,lty=3)
lines(x,dnorm(x,mean=mean(smi$r),sd = sd(smi$r)),col="red",lty=2,lwd=2)
rug(smi$r,col="blue")

legend("topleft",
       c("Kernel estimate (non-parametric)","Estimated mixture of Gaussian distr. (MLE, parametric)","Normal distribution"),
       lty=c(3,1,2), # gives the legend appropriate symbols (lines)
       lwd=c(2), # line width
       col=c("black","black","red"), # gives the legend lines the correct color and width
       pt.bg=c(1),
       pt.cex = c(1),
       bg="white",
       seg.len = 4
)
```

<!-- \begin{figure} -->
<!-- \caption{Density of 5-day returns on SMI index} -->
<!-- \includegraphics[width=.9\linewidth]{../../figures/Figure_kernel_smi.pdf} -->
<!-- \label{fig:illuskernel_smi} -->
<!-- \begin{tiny} -->
<!-- Gaussian kernel, $h=0.5$ (in percent). -->

<!-- The data spans the period from 2 June 2006 to 23 February 2016 at the daily frequency. -->

<!-- Left-hand plot: the blue lines indicates $\mu \pm 2 \sigma$, where $\mu$ is the sample mean of the returns and $\sigma$ is their sample standard deviation. Right-hand plot: the red dotted line is the density $\mathcal{N}(\mu,\sigma^2)$ -->
<!-- \end{tiny} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


<!-- \begin{frame}{} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[MLE estim. of a mixture of Gaussian distri. (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{Estimated density (vs kernel-based estimate)} -->
<!-- \includegraphics[width=1\linewidth]{../../figures/Figure_kernel_smiMLE.pdf} -->
<!-- \label{fig:MLE1} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


### Test procedures {#TestMLE}

The objective is to test the following parameter restrictions:
\begin{equation}
\boxed{H_0: \underbrace{h(\boldsymbol\theta)}_{r \times 1}=0.}
\end{equation}

Three tests:

*  Likelihood Ratio (LR) test,
* Wald (W) test,
* Lagrange Multiplier (LM) test.

While the LR necessitates to estimate both restricted and unrestricted models, the Wald and LM tests requires the estimation of the restricted model only.

Here is the rationale behind these tests:

* LR: If $h(\boldsymbol\theta)=0$, then imposing this restriction during the estimation (restricted estimator) should not result in a large decrease in the likelihood function (w.r.t the unrestricted estimation). 
* Wald: If $h(\boldsymbol\theta)=0$, then $h(\hat{\boldsymbol\theta})$ should not be far from $0$ (even if these restrictions are not imposed during the MLE).
* LM: If $h(\boldsymbol\theta)=0$, then the gradient of the likelihood function should be small when evaluated at the restricted estimator.


A graphical presentation of the tests is proposed in [Buse (1982)](http://hedibert.org/wp-content/uploads/2014/04/LR-W-LM-Tests-Buse1982.pdf)



<!-- :::{.proof} -->
<!-- (sketch of proof) The restricted estimation could be done by maximizing the Lagrangian function $\log\mathcal{L}(\boldsymbol\theta) + \lambda' h(\boldsymbol\theta)$. The first order conditions associated to $\boldsymbol\theta$ are $(\partial \log\mathcal{L}(\boldsymbol\theta) / \partial \boldsymbol\theta) = - (\partial h(\boldsymbol\theta) / \partial \theta) \lambda$. Under $H_0$, the expectation of the latter term is 0. Besides, the covariance matrix of the score is the information matrix. -->
<!-- ::: -->

***
:::{.proposition #Walddistri name="Asymptotic distribution of the Wald statistic"}
Under regularity conditions (Assumptions \@ref(hyp:MLEregularity)) and under $H_0: h(\boldsymbol\theta)=0$, the Wald statistic, defined by:
$$
\xi^W = h(\hat{\boldsymbol\theta})' \mathbb{V}ar[h(\hat{\boldsymbol\theta})]^{-1} h(\hat{\boldsymbol\theta}),
$$
where
\begin{equation}
\mathbb{V}ar[h(\hat{\boldsymbol\theta})] = \left(\frac{\partial h(\hat{\boldsymbol\theta})}{\partial \boldsymbol\theta'} \right) \mathbb{V}ar[\hat{\boldsymbol\theta}]
\left(\frac{\partial h(\hat{\boldsymbol\theta})'}{\partial \boldsymbol\theta} \right),(\#eq:varinWald)
\end{equation}

is asymptotically $\chi^2(r)$, where the number of degrees of freedom $r$ corresponds to the dimension of $h(\boldsymbol\theta)$.

The Wald test, defined by the critical region
$$
\{\xi^W \ge \chi^2_{1-\alpha}(r)\},
$$
where $\chi^2_{1-\alpha}(r)$ denotes the quantile of level $1-\alpha$ of the $\chi^2(r)$ distribution, has asymptotic level $\alpha$ and is consistent.
:::
***
:::{.proof}
See Appendix \@ref(Walddistri).
:::

In practice, in Eq. \@ref(eq:varinWald), $\mathbb{V}ar[\hat{\boldsymbol\theta}]$ is replaced by an estimate given, e.g., by  Eq. \@ref(eq:III1) or Eq. \@ref(eq:I2).



See Defs. \@ref(dfn:asmyptlevel) and  \@ref(def:asmyptconsisttest) for asymptotic levels and consistency of tests.

Remark: The Wald test requires the maximisation of the unconstrained likelihood function (but not of the constrained one).



***
:::{.proposition #LMdistri name="Asymptotic distribution of the LM test statistic"}
Under regularity conditions (Assumptions \@ref(hyp:MLEregularity)) and under $H_0: h(\boldsymbol\theta)=0$, the LM statistic
\begin{equation}
\xi^{LM} =
\left(\left.\frac{\partial \log \mathcal{L}(\boldsymbol\theta)}{\partial \boldsymbol\theta'}\right|_{\boldsymbol\theta = \hat{\boldsymbol\theta}^0}  \right)
[\bv{I}(\hat{\boldsymbol\theta}^0)]^{-1}
\left(\left.\frac{\partial \log \mathcal{L}(\boldsymbol\theta)}{\partial \boldsymbol\theta }\right|_{\boldsymbol\theta = \hat{\boldsymbol\theta}^0}  \right), (\#eq:xiLM)
\end{equation}
where $\hat{\boldsymbol\theta}^0$ is the restricted MLE estimator, is $\chi^2(r)$.

The test defined by the critical region:
$$
\{\xi^{LM} \ge \chi^2_{1-\alpha}(r)\}
$$
has asymptotic level $\alpha$ and is consistent (see Defs. \@ref(dfn:asmypt_level) and  \@ref(def:asmypt_consist_test)).

This test is called *Score* or *Lagrange Multiplier (LM)* test.
:::
***

:::{.proof}
See Appendix \@ref(LMdistri).
:::


:::{.definition #LR name="Likelihood Ratio test statistics"}
The likelihood ratio associated to a restriction of the form $H_0: h(\theta)=0$ is given by:
$$
LR = \frac{\mathcal{L}_R(\boldsymbol\theta;\bv{y})}{\mathcal{L}_U(\boldsymbol\theta;\bv{y})} \quad (\in [0,1]),
$$
where $\mathcal{L}_R$ (respectively $\mathcal{L}_U$) is the likelihood function that imposes (resp. that does not impose) the restriction.
:::

***
:::{.proposition #equivLRLM name="Asymptotic equivalence of LR and LM tests"}
Under the null hypothesis $H_0$, we have, asymptotically:
$$
\xi^{LM} \approx \xi^{LR}.
$$
:::
***
:::{.proof}
See Appendix \@ref(equivLRLM).
:::


***
:::{.proposition #LRdistri name="Asymptotic distribution of the LR test statistic"}
Under regularity conditions (Assumptions \@ref(hyp:MLEregularity)) and under $H_0: h(\boldsymbol\theta)=0$, the asymptotic distribution of $-2\log LR$ is $\chi^2(r)$, where the number of degrees of freedom $r$ corresponds to the number of restrictions imposed.
:::
***
:::{.proof}
This is directly deduced from Propositions \@ref(prp:LMdistri) and \@ref(prp:equivLRLM).
:::


***
:::{.proposition #equivWaldLM name="Asymptotic equivalence of W and LM tests"}
Under the null hypothesis $H_0$, we have, asymptotically:
$$
\xi^{LM} \approx \xi^{W}.
$$
:::
***
:::{.proof}
See Appendix \@ref(equivWaldLM).
:::


## Bayesian approach

### Introduction

An excellent introduction to Bayesian methods is proposed by [Martin Haugh, 2017](http://www.columbia.edu/~mh2078/MonteCarlo/MCMC_Bayes.pdf).

As suggested by the name of this approach, the starting point is the Bayes formula:
$$
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \& B)}{\mathbb{P}(B)},
$$
where $A$ and $B$ are two "events". For instance, $A$ may be: parameter $\alpha$ (conceived as something stochastic) lies in interval $[a,b]$. Assume that you are interested in the probability of occurrence of $A$. Without any specific information (or "unconditionally"), this probability if $\mathbb{P}(A)$. Your evaluation of this probability can only be better if you are provided with any additional form of information. Typically, if the event $B$ tends to occur simultaneously with $A$, then knowledge of $B$ can be useful. The Bayes formula says how this additional information (on $B$) can be used to "update" the probability of event $A$. 

In our case, this intuition will work as follows: assume that you know the form of the data-generating process (DGP). That is, you know the structure of the model used to draw some stochastic data; you also know the type of distributions used to generate these data. However, you do not know the numerical values of all the parameters characterizing the DGP. Let us denote by $\theta$ the vector of unknown parameters. While these parameters are not known exactly, assume that we have --even without having observed any data-- some **priors** on their distribution. Then, as was the case in the example above (with $A$ and $B$), the observation of data can only reduce the uncertainty associated $\theta$. Loosely speaking, combining the priors and the observations of data generated by the model should result in "thinner" distributions for the components of $\theta$.

Let us formalize this intuition. Define the prior by $f_\theta(\theta)$ and the model realizations (the "data") by vector $\bv{x}$. The joint distribution of $(\bv{x},\theta)$ is given by:
$$
f_{X,\theta}(\bv{x},\theta) = f_{X|\theta}(\bv{x},\theta)f_\theta(\theta),
$$
and, symmetrically, by
$$
f_{X,\theta}(\bv{x},\theta) = f_{\theta|X}(\theta,\bv{x})f_X(\bv{x}),
$$
where $f_{\theta|X}(\cdot,\bv{x})$, the distribution of the parameters conditional on the observations is called **posterior** distribution.

The last two equations imply that:
\begin{equation}
f_{\theta|X}(\theta,\bv{x}) = \frac{f_{X|\theta}(\bv{x},\theta)f_{\theta}(\theta)}{f_X(\bv{x})}.(\#eq:post1)
\end{equation}
Note that $f_X$ is the marginal (or unconditional) distribution of $\bv{x}$, that can be written:
\begin{equation}
f_X(\bv{x}) = \int f_{X|\theta}(\bv{x},\theta)f_\theta(\theta) d \theta.
\end{equation}

Eq. \@ref(eq:post1) is sometimes rewritten as follows:
\begin{equation}
f_{\theta|X}(\theta,\bv{x}) \propto f_{\theta,X}(\theta,\bv{x}) := f_{X|\theta}(\bv{x},\theta)f_\theta(\theta), (\#eq:post2)
\end{equation}
where $\propto$ means, loosely speaking, "*proportional to*". In rare instances, starting from given priors, one can analytically compute the posterior distribution $f_\theta(\theta,\bv{x})$. However, in most cases, this is out of reach. One then has to resort to numerical approaches to compute the posterior distribution. Monte Carlo Markov Chains (MCMC) is one of them.


According to the Bernstein-von Mises Theorem, Bayesian and MLE estimators have the same large sample properties. (In particular, the Bayesian approach also achieve the FDCR bound, see Theorem \@ref(thm:FDCR).) The intuition behind this result is that the influence of the prior diminishes with increasing sample sizes.

### Monte-Carlo Markov Chains

MCMC techniques aim at using simulations to approach a distribution whose distribution is difficult to obtain analytically. Indeed, in some circumstances, one can draw in a distribution even if we do not know its analytical expression.

:::{.definition #MC name="Markov Chain"}
The sequence of variables $\{y_i\}$ follows a (first-order) Markov Chain is it satisfies:
$$
f(y_i|y_{i-1},y_{i-2},\dots) = f(y_i|y_{i-1}).
$$
:::

The Metropolis-Hastings (MH) algorithm is a specific MCMC approach that allows to generate samples of $\theta$'s whose distribution approximately corresponds to the posterior distribution of Eq. \@ref(eq:post1).

The MH algorithm is a recursive algorithm. That is, one can draw the $i^{th}$ value of $\theta$, denoted by $\theta_i$, if one has already drawn $\theta_{i-1}$. Assume we have $\theta_{i-1}$. We obtain a value for $\theta_i$ by implementing the following steps:

1. Draw $\tilde{\theta}_i$ from the conditional distribution $Q_{\tilde{\theta}|\theta}(\cdot.,\theta_{i-1})$, called **proposal distribution**.
2. Draw $u$ in a uniform distribution on $[0,1]$.
3. Compute
\begin{equation}
\alpha(\tilde{\theta}_i,\theta_{i-1}):= \min\left(\frac{f_{\theta,X}(\tilde{\theta}_i,\bv{x})}{f_{\theta,X}(\theta_{i-1},\bv{x})}\times\frac{Q_{\tilde{\theta}|\theta}(\theta_{i-1},\tilde{\theta}_i)}{Q_{\tilde{\theta}|\theta}(\tilde{\theta}_i,\theta_{i-1})},1\right),(\#eq:alphaXXX)
\end{equation}
where $f_{\theta,X}$ is given in Eq. \@ref(eq:post2).
4. If $u<\alpha(\tilde{\theta}_i,\theta_{i-1})$, then take $\theta_i = z$, otherwise we set $\theta_i$ to $\theta_{i-1}$.

It can be shown that, the distribution of the draws converges to the posterior distribution. That is, after a sufficiently large number of iterations, the draws can be considered to be drawn from the posterior distribution.[^footnoteproofMCMC]

[^footnoteproofMCMC]: The proof of this claim is based on the fact that, if $\theta_{i-1}$ is drawn from the posterior distribution, then it is also the case for $\theta_i$.

To get some insights into the algorithm, consider the case of a **symmetric proposal distribution**, that is:
\begin{equation}
Q_{\tilde{\theta}|\theta}(\tilde{\theta}_i,\theta_{i-1})=Q_{\tilde{\theta}|\theta}(\theta_{i-1},\tilde{\theta}_i).(\#eq:symmQ)
\end{equation}
We then have:
\begin{equation}
\alpha(\tilde{\theta},\theta_{i-1})= \min\left(\frac{q(\tilde{\theta},x)}{q(\theta_{i-1},x)},1\right). (\#eq:hypoQ)
\end{equation}
Remember that, up to the marginal distribution of the data ($f_X(\bv{x})$), $f_{\theta,X}(\tilde{\theta},\bv{x})$ is the probability of observing $x$ conditional on having a model parameterized by $\tilde\theta$. Then, under Eq. \@ref(eq:hypoQ), it appears that if this probability is larger for $\tilde\theta$ than for $\theta_{i-1}$ (in which case $\tilde\theta$ seems "more consistent with the observations $\bv{x}$" than $\theta_{i-1}$), we accept $\theta_i$. By contrast, if $f_{\theta,X}(\tilde{\theta},\bv{x})<f_{\theta,X}(\theta_{i-1},\bv{x})$, then we do not necessarily accept the proposed value $\tilde{\theta}$, especially if $f_{\theta,X}(\tilde{\theta},\bv{x})\ll f_{\theta,X}(\theta_{i-1},\bv{x})$ (in which case $\tilde\theta$ seems far less consistent with the observations $\bv{x}$ than $\theta_{i-1}$, and $\alpha(\tilde{\theta},\theta_{i-1})$ is small).

The choice of the **proposal distribution** $Q_{\tilde\theta|\theta}$ is crucial to get a rapid convergence of the algorithm. Looking at Eq. \@ref(eq:alphaXXX), it is easily seen that the optimal choice would be $Q_{\tilde\theta|\theta}(\cdot,\theta_i)=f_{\theta|X}(\cdot,\bv{x})$. In that case, we would have $\alpha(\tilde{\theta}_i,\theta_{i-1})\equiv 1$ (see Eq. \@ref(eq:alphaXXX)). We would then accept all draws from the proposal distribution, as this distribution would directly be the posterior distribution. Of course, this situation is not realistoc as the objective of the algorithm is precisely to approximate the posterior distribution.

A common choice for $Q$ is a multivariate normal distribution. If $\theta$ is of dimension $K$, we can for instance use:
$$
Q(\tilde\theta,\theta)= \frac{1}{\left(\sqrt{2\pi\sigma^2}\right)^K}\exp\left(-\frac{1}{2}\sum_{j=1}^K\frac{(\tilde\theta_j-\theta_j)^2}{\sigma^2}\right),
$$
which is an example of symmetric proposal distribution (see Eq. \@ref(eq:symmQ)). Equivalently, we then have:
$$
\tilde\theta = \theta + \varepsilon,
$$
where $\varepsilon$ is a $K$-dimensional vector of independent zero-mean normal disturbances of variance $\sigma^2$.[^footnoteSigmaMCMC] One then has to determine an appropriate value for $\sigma$. If it is too low, then $\alpha$ will be close to 1 (as $\tilde{\theta}_i$ will be close to $\theta_{i-1}$), and we will accept very often the proposed value ($\tilde{\theta}_i$). This seems to be a favourable situation. But it may not be. Indeed, it means that it will take a large number of iterations to explore the whole distribution of $\theta$. What if $\sigma$ is very large? In this case, it is likely that the porposed values ($\tilde{\theta}_i$) will often result in poor likelihoods; The probability of acceptance will then be low and the Markov chain may be blocked at its initial value. Therefore, intermediate values of $\sigma^2$ have to be determined. The acceptance rate (i.e., the average value of $\alpha(\tilde{\theta},\theta_{i-1})$) can be used as a guide for that. Indeed, a literature explores the optimal values for such acceptance rate (in order to obtain the best possible fit of the posterior for a minimum nuber ofalgorithm iterations). In particular, following @Roberts_Gelman_Gilks_1997, people often target acceptance rate of the order of magnitude of 20\%.


[^footnoteSigmaMCMC]: We could also have different variances for the different components of $\theta$. However, this may lead to complicated settings. A useful practice consists in looking for model (re)parametrization --based, e.g., on exponential and/or logistic functions-- that are such that the components of $\theta$ are all expected to be of the order of magnitude of the unity.

It is important to note that, to implement this approach, one only has to be able to compute the joint p.d.f. $q(\theta,x)=f_{X|\theta}(x,\theta)f_\theta(\theta)$ (Eq. \@ref(eq:post2)). That is, as soon as one can evaluate the likelihood ($f_{X|\theta}(x,\theta)$) and the prior ($f_\theta(\theta)$), we can employ this methodology.

### Example: AR(1) specification

In the following example, we employ MCMC in order to estimate the posterior distributions of the three parameters defining an AR(1) model (see Section XXX). The specification is as follows:
$$
y_t = \mu + \rho y_{t-1} + \sigma \varepsilon_{t}, \quad \varepsilon_t \sim \,i.i.d.\,\mathcal{N}(0,1).
$$
Hence, we have $\theta = [\mu,\rho,\sigma]$. Let us first simulate the process on $T$ periods:

```{r MCMC1}
mu <- .6; rho <- .8; sigma <- 2.5 # true model specification
T <- 20 # number of observations
y0 <- mu/(1-rho)
Y <- NULL
for(t in 1:T){
  if(t==1){
    y <- y0
  }
  y <- mu + rho*y + sigma * rnorm(1)
  Y <- c(Y,y)
}
plot(Y,type="l",xlab="time t",ylab=expression(y[t]))
```

Next, let us code the likelihood function, i.e. $f_{X|\theta}(\bv{x},\theta)$. For $\rho$ (which is expected to be between 0 and 1), we use a logistic transformation. For $\sigma$, that is expected to be positive, we use an exponential transformation.

```{r MCMC2}
likelihood <- function(param,Y){
  mu  <- param[1]
  rho <- exp(param[2])/(1+exp(param[2]))
  sigma <- exp(param[3])
  MU <- mu/(1-rho)
  SIGMA2 <- sigma^2/(1-rho^2)
  L <- 1/sqrt(2*pi*SIGMA2)*exp(-(Y[1]-MU)^2/(2*SIGMA2))
  Y1 <- Y[2:length(Y)]
  Y0 <- Y[1:(length(Y)-1)]
  aux <- 1/sqrt(2*pi*sigma^2)*exp(-(Y1-mu-rho*Y0)^2/(2*sigma^2))
  L <- L * prod(aux)
  return(L)
}
```

Next define function `rQ` that draws from the (Gaussian) proposal distribution, as well as function `Q`, that computes $Q_{\tilde\theta|\theta}(\tilde\theta,\theta)$:

```{r MCMC3}
#param0 <- c(mu,log(rho/(1-rho)),sigma)
#likelihood(param0,Y)

rQ <- function(x,a){
  n <- length(x)
  y <- x + a * rnorm(n)
  return(y)
}
Q <- function(y,x,a){
  q <- 1/sqrt(2*pi*a^2)*exp(-(y - x)^2/(2*a^2))
  return(prod(q))
}
```

We consider Gaussian priors:

```{r MCMC4}
prior <- function(param,means_prior,stdv_prior){
  f <- 1/sqrt(2*pi*stdv_prior^2)*exp(-(param - means_prior)^2/(2*stdv_prior^2))
  return(prod(f))
}
```

Function `p_tilde` corresponds to $f_{\theta,X}$:
```{r MCMC5}
p_tilde <- function(param,Y,means_prior,stdv_prior){
  p <- likelihood(param,Y) * prior(param,means_prior,stdv_prior)
  return(p)
}
```

We can now define function $\alpha$ (Eq. \@ref(eq:alphaXXX)):
```{r MCMC6}
alpha <- function(y,x,means_prior,stdv_prior,a){
  aux <- p_tilde(y,Y,means_prior,stdv_prior)/
    p_tilde(x,Y,means_prior,stdv_prior) * Q(y,x,a)/Q(x,y,a)
  alpha_proba <- min(aux,1)
  return(alpha_proba)
}
```

Now, all is set for us to write the MCMC function: 
```{r MCMC7}
MCMC <- function(Y,means_prior,stdv_prior,a,N){
  x <- means_prior
  all_theta <- NULL
  count_accept <- 0
  for(i in 1:N){
    y <- rQ(x,a)
    alph <- alpha(y,x,means_prior,stdv_prior,a)
    #print(alph)
    u <- runif(1)
    if(u < alph){
      count_accept <- count_accept + 1
      x <- y
    }
    all_theta <- rbind(all_theta,x)
  }
  print(paste("Acceptance rate:",toString(round(count_accept/N,3))))
  return(all_theta)
}
```

let us specify the Gaussian priors:

```{r MCMC8, fig.cap="The upper line of plot compares prior (black) and posterior (red) distributions. The vertical dashed blue lines indicate the true values of the parameters. The second row of plots show the sequence of $\\theta_i$s generated by the MCMC algorithm. These sequences are the ones used to produce the posterior distributions (red lines) in the upper plots.", fig.asp = .6, out.width = "100%"}
true_values <- c(mu,log(rho/(1-rho)),log(sigma))
means_prior <- c(1,0,0) # as if we did not know the true values
stdv_prior <- rep(2,3)
resultMCMC <- MCMC(Y,means_prior,stdv_prior,a=.45,N=20000)

par(mfrow=c(2,3))
for(i in 1:length(means_prior)){
  m <- means_prior[i]
  s <- stdv_prior[i]
  x <- seq(m-3*s,m+3*s,length.out = 100)
  par(mfg=c(1,i))
  aux <- density(resultMCMC[,i])
  par(plt=c(.15,.95,.15,.85))
  plot(x,dnorm(x,m,s),type="l",xlab="",ylab="",main=paste("Parameter",i),
       ylim=c(0,max(aux$y)))
  lines(aux$x,aux$y,col="red",lwd=2)
  abline(v=true_values[i],lty=2,col="blue")
  par(mfg=c(2,i))
  plot(resultMCMC[,i],1:length(resultMCMC[,i]),xlim=c(min(x),max(x)),
       type="l",xlab="",ylab="")
}
```

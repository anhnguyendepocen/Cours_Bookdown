% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Advanced Econometrics},
  pdfauthor={Jean-Paul Renne},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Advanced Econometrics}
\author{Jean-Paul Renne}
\date{2022-08-19}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\newcommand{\bv}[1]{\mathbf{#1}}

\hypertarget{prerequisites}{%
\chapter{Prerequisites}\label{prerequisites}}

This is a \emph{sample} book written in \textbf{Markdown}. You can use anything that Pandoc's Markdown supports, e.g., a math equation \(a^2 + b^2 = c^2\).

The \textbf{bookdown} package can be installed from CRAN or Github:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\CommentTok{\# or the development version}
\CommentTok{\# devtools::install\_github("rstudio/bookdown")}
\end{Highlighting}
\end{Shaded}

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading \texttt{\#}.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): \url{https://yihui.name/tinytex/}.

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

You can label chapter and section titles using \texttt{\{\#label\}} after them, e.g., we can reference Chapter \ref{intro}. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \ref{methods}.

Figures and tables with captions will be placed in \texttt{figure} and \texttt{table} environments, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, .}\DecValTok{1}\NormalTok{, .}\DecValTok{1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(pressure, }\AttributeTok{type =} \StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{AdvECTS_files/figure-latex/nice-fig-1} 

}

\caption{Here is a nice figure!}\label{fig:nice-fig}
\end{figure}

Reference a figure by its code chunk label with the \texttt{fig:} prefix, e.g., see Figure \ref{fig:nice-fig}. Similarly, you can reference tables generated from \texttt{knitr::kable()}, e.g., see Table \ref{tab:nice-tab}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
  \FunctionTok{head}\NormalTok{(iris, }\DecValTok{20}\NormalTok{), }\AttributeTok{caption =} \StringTok{\textquotesingle{}Here is a nice table!\textquotesingle{}}\NormalTok{,}
  \AttributeTok{booktabs =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:nice-tab}Here is a nice table!}
\centering
\begin{tabular}[t]{rrrrl}
\toprule
Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\
\midrule
5.1 & 3.5 & 1.4 & 0.2 & setosa\\
4.9 & 3.0 & 1.4 & 0.2 & setosa\\
4.7 & 3.2 & 1.3 & 0.2 & setosa\\
4.6 & 3.1 & 1.5 & 0.2 & setosa\\
5.0 & 3.6 & 1.4 & 0.2 & setosa\\
\addlinespace
5.4 & 3.9 & 1.7 & 0.4 & setosa\\
4.6 & 3.4 & 1.4 & 0.3 & setosa\\
5.0 & 3.4 & 1.5 & 0.2 & setosa\\
4.4 & 2.9 & 1.4 & 0.2 & setosa\\
4.9 & 3.1 & 1.5 & 0.1 & setosa\\
\addlinespace
5.4 & 3.7 & 1.5 & 0.2 & setosa\\
4.8 & 3.4 & 1.6 & 0.2 & setosa\\
4.8 & 3.0 & 1.4 & 0.1 & setosa\\
4.3 & 3.0 & 1.1 & 0.1 & setosa\\
5.8 & 4.0 & 1.2 & 0.2 & setosa\\
\addlinespace
5.7 & 4.4 & 1.5 & 0.4 & setosa\\
5.4 & 3.9 & 1.3 & 0.4 & setosa\\
5.1 & 3.5 & 1.4 & 0.3 & setosa\\
5.7 & 3.8 & 1.7 & 0.3 & setosa\\
5.1 & 3.8 & 1.5 & 0.3 & setosa\\
\bottomrule
\end{tabular}
\end{table}

You can write citations, too. For example, we are using the \textbf{bookdown} package \citep{R-bookdown} in this sample book, which was built on top of R Markdown and \textbf{knitr} \citep{xie2015}.

Below is an example borrowed from \href{https://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.htm}{Petersen}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sandwich)}
\DocumentationTok{\#\# Petersen\textquotesingle{}s data}
\FunctionTok{data}\NormalTok{(}\StringTok{"PetersenCL"}\NormalTok{, }\AttributeTok{package =} \StringTok{"sandwich"}\NormalTok{)}
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ PetersenCL)}

\DocumentationTok{\#\# clustered covariances}
\DocumentationTok{\#\# one{-}way}
\FunctionTok{vcovCL}\NormalTok{(m, }\AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ firm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               (Intercept)             x
## (Intercept)  4.490702e-03 -6.473517e-05
## x           -6.473517e-05  2.559927e-03
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vcovCL}\NormalTok{(m, }\AttributeTok{cluster =}\NormalTok{ PetersenCL}\SpecialCharTok{$}\NormalTok{firm) }\DocumentationTok{\#\# same}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               (Intercept)             x
## (Intercept)  4.490702e-03 -6.473517e-05
## x           -6.473517e-05  2.559927e-03
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# one{-}way with HC2}
\FunctionTok{vcovCL}\NormalTok{(m, }\AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ firm, }\AttributeTok{type =} \StringTok{"HC2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               (Intercept)             x
## (Intercept)  4.494487e-03 -6.592912e-05
## x           -6.592912e-05  2.568236e-03
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# two{-}way}
\FunctionTok{vcovCL}\NormalTok{(m, }\AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ firm }\SpecialCharTok{+}\NormalTok{ year)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               (Intercept)             x
## (Intercept)  4.233313e-03 -2.845344e-05
## x           -2.845344e-05  2.868462e-03
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vcovCL}\NormalTok{(m, }\AttributeTok{cluster =}\NormalTok{ PetersenCL[, }\FunctionTok{c}\NormalTok{(}\StringTok{"firm"}\NormalTok{, }\StringTok{"year"}\NormalTok{)]) }\DocumentationTok{\#\# same}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               (Intercept)             x
## (Intercept)  4.233313e-03 -2.845344e-05
## x           -2.845344e-05  2.868462e-03
\end{verbatim}

XXXX

Sargan-Hansen () test. \citet{Sargan_1958} and \citet{Hansen_1982}

Durbin-Wu-Hausman test: \citet{Durbin_1954} / \citet{Wu_1973} / \citet{Hausman_1978}

\href{https://www.hds.utc.fr/~tdenoeux/dokuwiki/_media/en/kleibzeil_-_aer.pdf}{Use R!} is an excellent tutorial.
(notably for \texttt{plm} and the Arellano-Bond example, 140 UK firms)

Program evaluation (very good survey): \citet{Abadie_Cattaneo_2018}
Mostly harmless: \citet{angrist_mostly_2008}

Diff-in-Diff: \href{https://towardsdatascience.com/analyze-causal-effect-using-diff-in-diff-model-85e07b17e7b7}{Card and Krueger (1994)}

XXXX

\hypertarget{linear-regressions}{%
\chapter{Linear Regressions}\label{linear-regressions}}

\hypertarget{specification}{%
\section{Specification}\label{specification}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}
\protect\hypertarget{def:essai}{}\label{def:essai}A linear regression is a model defined through:
\[
y_i = \boldsymbol\beta'{\bf x}_{i} + \varepsilon_i,
\]
where \({\bf x}_{i}=[x_{i,1},\dots,x_{i,K}]'\) is a vector of dimension \(K \times 1\).
\end{definition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For entity \(i\), the \(x_{i,k}\)'s, for \(k \in \{1,\dots,K\}\), are explanatory variables. If one wants to have an intercept in the specification, then set \(x_{i,1}=1\) for all \(i\), and \(\beta_1\) then corresponds to the intercept.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{hypothesis}[Full rank]
\protect\hypertarget{hyp:fullrank}{}\label{hyp:fullrank}There is no exact linear relationship among the independent variables (the \(x_{i,k}\)s, for a given \(i \in \{1,\dots,n\}\)).
\end{hypothesis}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{hypothesis}[Conditional mean-zero assumption]
\protect\hypertarget{hyp:exogeneity}{}\label{hyp:exogeneity}\begin{equation}
\mathbb{E}(\boldsymbol\varepsilon|\mathbf{X}) = 0.
\end{equation}
\end{hypothesis}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Note that, in Hypothesis \ref{hyp:exogeneity}, \(\boldsymbol\varepsilon\) is a \(n\)-dimensional vector (where \(n\) is the sample size), and \(\mathbf{X}\) is the matrix containing all explanatory variables, of dimension \(n \times K\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:implicationExog}{}\label{prp:implicationExog}

Under Hypothesis \ref{hyp:exogeneity}:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  \(\mathbb{E}(\varepsilon_{i})=0\);
\item
  the \(x_{ij}\)s and the \(\varepsilon_{i}\)s are uncorrelated, i.e.~\(\forall i,\,j \quad \mathbb{C}orr(x_{ij},\varepsilon_{i})=0\).
\end{enumerate}

\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}

Let us prove (i) and (ii):

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  By the law of iterated expectations:
  \[
  \mathbb{E}(\boldsymbol\varepsilon)=\mathbb{E}(\mathbb{E}(\boldsymbol\varepsilon|\mathbf{X}))=\mathbb{E}(0)=0.
  \]
\item
  \(\mathbb{E}(x_{ij}\varepsilon_i)=\mathbb{E}(\mathbb{E}(x_{ij}\varepsilon_i|\mathbf{X}))=\mathbb{E}(x_{ij}\underbrace{\mathbb{E}(\varepsilon_i|\mathbf{X})}_{=0})=0\).\(\square\)
\end{enumerate}

\end{proof}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{hypothesis}[Homoskedasticity]
\protect\hypertarget{hyp:homoskedasticity}{}\label{hyp:homoskedasticity}\[
\forall i, \quad \mathbb{V}ar(\varepsilon_i|\mathbf{X}) = \sigma^2.
\]
\end{hypothesis}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{AdvECTS_files/figure-latex/heteroskedasticity-1} 

}

\caption{This is the caption}\label{fig:heteroskedasticity}
\end{figure}

Panel (b) of Figure \ref{fig:heteroskedasticity} corresponds to a situation of heteroskedasticity. Let us be more specific. In the two plots, we have \(X_i \sim \mathcal{N}(0,1)\) and \(\varepsilon^*_i \sim \mathcal{N}(0,1)\). In Panel (a) (homoskedasticity): \(Y_i = 2 + 2X_i + \varepsilon^*_i\). In Panel (b) (heteroskedasticity): \(Y_i = 2 + 2X_i + \left(2\mathbb{1}_{\{X_i<0\}}+0.2\mathbb{1}_{\{X_i\ge0\}}\right)\varepsilon^*_i\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load data into R}
\FunctionTok{data}\NormalTok{(Salaries, }\AttributeTok{package =} \StringTok{"carData"}\NormalTok{)}
\CommentTok{\# first six rows of the data}
\FunctionTok{head}\NormalTok{(Salaries)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        rank discipline yrs.since.phd yrs.service  sex salary
## 1      Prof          B            19          18 Male 139750
## 2      Prof          B            20          16 Male 173200
## 3  AsstProf          B             4           3 Male  79750
## 4      Prof          B            45          39 Male 115000
## 5      Prof          B            40          41 Male 141500
## 6 AssocProf          B             6           6 Male  97000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Regression:}
\NormalTok{eq }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{Salaries)}
\FunctionTok{summary}\NormalTok{(eq)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = salary ~ ., data = Salaries)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -65248 -13211  -1775  10384  99592 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    65955.2     4588.6  14.374  < 2e-16 ***
## rankAssocProf  12907.6     4145.3   3.114  0.00198 ** 
## rankProf       45066.0     4237.5  10.635  < 2e-16 ***
## disciplineB    14417.6     2342.9   6.154 1.88e-09 ***
## yrs.since.phd    535.1      241.0   2.220  0.02698 *  
## yrs.service     -489.5      211.9  -2.310  0.02143 *  
## sexMale         4783.5     3858.7   1.240  0.21584    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 22540 on 390 degrees of freedom
## Multiple R-squared:  0.4547, Adjusted R-squared:  0.4463 
## F-statistic:  54.2 on 6 and 390 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{2}\NormalTok{,.}\DecValTok{95}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(salary}\SpecialCharTok{/}\DecValTok{1000}\SpecialCharTok{\textasciitilde{}}\NormalTok{yrs.since.phd,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"years since PhD"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"Salary"}\NormalTok{,}\AttributeTok{data=}\NormalTok{Salaries,}\AttributeTok{las=}\DecValTok{1}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(salary}\SpecialCharTok{/}\DecValTok{1000}\SpecialCharTok{\textasciitilde{}}\NormalTok{yrs.since.phd,}\AttributeTok{data=}\NormalTok{Salaries),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{AdvECTS_files/figure-latex/exmpSalarayPhD-1} 

}

\caption{Salary versus years after PhD}\label{fig:exmpSalarayPhD}
\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{hypothesis}[Uncorrelated residuals]
\protect\hypertarget{hyp:noncorrelResid}{}\label{hyp:noncorrelResid}\[
\forall i \ne j, \quad \mathbb{C}ov(\varepsilon_i,\varepsilon_j|\mathbf{X})=0.
\]
\end{hypothesis}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:Sigma}{}\label{prp:Sigma}If \ref{hyp:homoskedasticity} and \ref{hyp:noncorrelResid} hold, then:
\[
\mathbb{V}ar(\boldsymbol\varepsilon|\mathbf{X})= \sigma^2 Id,
\]
where \(Id\) is the \(n \times n\) identity matrix.
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{hypothesis}[Normal distribution]
\protect\hypertarget{hyp:normality}{}\label{hyp:normality}\[
\forall i, \quad \varepsilon_i \sim \mathcal{N}(0,\sigma^2).
\]
\end{hypothesis}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{least-square-estimation}{%
\section{Least square estimation}\label{least-square-estimation}}

For a given vector of coefficients \(\mathbf{b}=[b_1,\dots,b_K]'\), the sum of square residuals is:
\[
f(\mathbf{b}) =\sum_{i=1}^n \left(y_i - \sum_{j=1}^K x_{i,j} b_j \right)^2 = \sum_{i=1}^n (y_i - \mathbf{x}_i' \mathbf{b})^2.
\]
Minimizing the sum of squared residuals amounts to minimizing:
\[
f(\mathbf{b}) = (\mathbf{y} - \mathbf{X}\mathbf{b})'(\mathbf{y} - \mathbf{X}\mathbf{b}).
\]

We have:
\[
\frac{\partial f}{\partial \mathbf{b}}(\mathbf{b}) = - 2 \mathbf{X}'\mathbf{y} + 2 \mathbf{X}'\mathbf{X}\mathbf{b}.
\]
Necessary first-order condition (FOC):
\begin{equation}
\mathbf{X}'\mathbf{X}\mathbf{b} = \mathbf{X}'\mathbf{y}.\label{eq:OLSFOC}
\end{equation}
Under Assumption \ref{hyp:fullrank}, \(\mathbf{X}'\mathbf{X}\) is invertible. Hence:
\[
\boxed{\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y}.}
\]
Vector \(\mathbf{b}\) minimises the sum of squared residuals. (\(f\) is a non-negative quadratic function, it admits a minimum.)

The estimated residuals are:
\begin{equation}
\mathbf{e} = \mathbf{y} - \mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{y} = \mathbf{M} \mathbf{y}\label{eq:Mres}
\end{equation}
where \(\mathbf{M} := \mathbf{I} - \mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\) is called the \textbf{residual maker} matrix. Let us further define a \textbf{projection matrix} by \(\mathbf{P}=\mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\). These matrices \(\mathbf{M}\) and \(\mathbf{P}\) are such that:

\begin{itemize}
\tightlist
\item
  \(\mathbf{M} \mathbf{X} = \mathbf{0}\): if one regresses one of the explanatory variables on \(\mathbf{X}\), the residuals are null.
\item
  \(\mathbf{M}\mathbf{y}=\mathbf{M}\boldsymbol\varepsilon\) (because \(\mathbf{y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\varepsilon\) and \(\mathbf{M} \mathbf{X} = \mathbf{0}\)).
\item
  The fitted values are:
  \begin{equation}
  \hat{\mathbf{y}}=\mathbf{X} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{y} = \mathbf{P} \mathbf{y},\label{eq:Proj}
  \end{equation}
  i.e., \(\hat{\mathbf{y}}\) is the projection of the vector \(\mathbf{y}\) onto the vectorial space spanned by the columns of \(\mathbf{X}\).
\item
  It can be shown that each column \(\tilde{\mathbf{x}}_k\) of \(\mathbf{X}\) is orthogonal to \(\mathbf{e}\). \(\Rightarrow\) If intercepts are included in the regression (\(x_{i,1} \equiv 1\)), the average of the residuals is null.
\end{itemize}

Here are some properties of \(\mathbf{M}\) and \(\mathbf{P}\):

\begin{itemize}
\tightlist
\item
  \(\mathbf{M}\) is symmetric (\(\mathbf{M} = \mathbf{M}'\)) and \textbf{idempotent} (\(\mathbf{M} = \mathbf{M}^2 = \mathbf{M}^k\) for \(k>0\)).
\item
  \(\mathbf{P}\) is symmetric and idempotent.
\item
  \(\mathbf{P}\mathbf{X} = \mathbf{X}\).
\item
  \(\mathbf{P} \mathbf{M} = \mathbf{M} \mathbf{P} = 0\).
\item
  \(\mathbf{y} = \mathbf{P}\mathbf{y} + \mathbf{M}\mathbf{y}\) (decomposition of \(\mathbf{y}\) into two orthogonal parts).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}[Properties of the OLS estimator]
\protect\hypertarget{prp:propOLS}{}\label{prp:propOLS}

We have:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  Under Assumptions \ref{hyp:fullrank} and \ref{hyp:exogeneity}, the OLS estimator is linear and unbiased.
\item
  Under Hypotheses \ref{hyp:fullrank} to \ref{hyp:noncorrelResid}, the conditional covariance matrix of \(\mathbf{b}\) is: \(\mathbb{V}ar(\mathbf{b}|\mathbf{X}) = \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\).
\end{enumerate}

\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}

Under Hypothesis \ref{hyp:fullrank}, \(\mathbf{X}'\mathbf{X}\) can be inverted. We have:
\[
\mathbf{b} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y} = \boldsymbol\beta + (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon}.
\]

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  Let us consider the expectation of the last term, i.e.~\(\mathbb{E}((\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon})\). Using the law of iterated expectations, we obtain:
  \[
  \mathbb{E}((\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon}) = \mathbb{E}(\mathbb{E}[(\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon}|\mathbf{X}]) = \mathbb{E}((\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbb{E}[\mathbf{\varepsilon}|\mathbf{X}]).
  \]
  By Hypothesis \ref{hyp:exogeneity}, we have \(\mathbb{E}[\mathbf{\varepsilon}|\mathbf{X}]=0\). Hence \(\mathbb{E}((\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{\varepsilon}) =0\) and result (i) follows.
\item
  \(\mathbb{V}ar(\mathbf{b}|\mathbf{X}) = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbb{E}(\boldsymbol\varepsilon\boldsymbol\varepsilon'|\mathbf{X}) \mathbf{X} (\mathbf{X}'\mathbf{X})^{-1}\).
  By Prop. \ref{prp:Sigma}, if \ref{hyp:homoskedasticity} and \ref{hyp:noncorrelResid} hold, then we have \(\mathbb{E}(\boldsymbol\varepsilon\boldsymbol\varepsilon'|\mathbf{X})=\mathbb{V}ar(\boldsymbol\varepsilon|\mathbf{X})=\sigma^2 Id\). The result follows. \(\square\)
\end{enumerate}

\end{proof}

\hypertarget{bivariate-case}{%
\subsection{Bivariate case}\label{bivariate-case}}

Consider a bivariate situation, where we regress\(y_i\) on a constant and an explanatory variable \(w_i\). We have \(K=2\), and \(\mathbf{X}\) is a \(n \times 2\) matrix whose \(i^{th}\) row is \([x_{i,1},x_{i,2}]\), with \(x_{i,1}=1\) (to account for the intercept) and with \(w_i = x_{i,2}\) (say).

We have:
\begin{eqnarray*}
\mathbf{X}'\mathbf{X} &=& 
\left[\begin{array}{cc}
n & \sum_i w_i \\
\sum_i w_i & \sum_i w_i^2
\end{array}
\right],\\
(\mathbf{X}'\mathbf{X})^{-1} &=& 
\frac{1}{n\sum_i w_i^2-(\sum_i w_i)^2}
\left[\begin{array}{cc}
\sum_i w_i^2 & -\sum_i w_i \\
-\sum_i w_i & n
\end{array}
\right],\\
(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y} &=& 
\frac{1}{n\sum_i w_i^2-(\sum_i w_i)^2}
\left[\begin{array}{c}
\sum_i w_i^2\sum_i y_i -\sum_i w_i \sum_i w_iy_i \\
-\sum_i w_i \sum_i y_i + n \sum_i w_i y_i
\end{array}
\right]\\
&=& \frac{1}{\frac{1}{n}\sum_i(w_i - \bar{w})^2}
\left[\begin{array}{c}
\frac{\bar{y}}{n}\sum_i w_i^2 -\frac{\bar{w}}{n}\sum_i w_iy_i \\
\frac{1}{n}\sum_i (w_i-\bar{w})(y_i-\bar{y})
\end{array}
\right].
\end{eqnarray*}

It can be seen that the second element of \(\mathbf{b}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}\) is:
\[
b_2 = \frac{\overline{\mathbb{C}ov(W,Y)}}{\overline{\mathbb{V}ar(W)}},
\]
where \(\overline{\mathbb{C}ov(W,Y)}\) and \(\overline{\mathbb{V}ar(W)}\) are sample estimates.

Since there is a constant in the regression, we have \(b_1 = \bar{y} - b_2 \bar{w}\).

\hypertarget{gauss-markow-theorem}{%
\subsection{Gauss Markow Theorem}\label{gauss-markow-theorem}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{theorem}[Gauss-Markov Theorem]
\protect\hypertarget{thm:GaussMarkov}{}\label{thm:GaussMarkov}Under Assumptions \ref{hyp:fullrank} to \ref{hyp:noncorrelResid}, for any vector \(w\), the minimum-variance linear unbiased estimator of \(w' \boldsymbol\beta\) is \(w' \mathbf{b}\), where \(\mathbf{b}\) is the least squares estimator. (BLUE: Best Linear Unbiased Estimator.)
\end{theorem}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
Consider \(\mathbf{b}^* = C \mathbf{y}\), another linear unbiased estimator of \(\boldsymbol\beta\). Since it is unbiased, we must have \(\mathbb{E}(C\mathbf{y}|\mathbf{X}) = \mathbb{E}(C\mathbf{X}\boldsymbol\beta + C\boldsymbol\varepsilon|\mathbf{X}) = \boldsymbol\beta\). We have \(\mathbb{E}(C\boldsymbol\varepsilon|\mathbf{X})=C\mathbb{E}(\boldsymbol\varepsilon|\mathbf{X})=0\) (by \ref{hyp:exogeneity}).

Therefore \(\mathbf{b}^*\) is unbiased if \(\mathbb{E}(C\mathbf{X})\boldsymbol\beta=\boldsymbol\beta\). This has to be the case for any \(\boldsymbol\beta\), which implies that we must have \(C\mathbf{X}=\mathbf{I}\).\textbackslash{}

Let us compute \(\mathbb{V}ar(\mathbf{b^*}|\mathbf{X})\). For this, we introduce \(D = C - (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\), which is such that \(D\mathbf{y}=\mathbf{b}^*-\mathbf{b}\). The fact that \(C\mathbf{X}=\mathbf{I}\) implies that \(D\mathbf{X} = \mathbf{0}\).

We have \(\mathbb{V}ar(\mathbf{b^*}|\mathbf{X}) = \mathbb{V}ar(C \mathbf{y}|\mathbf{X}) =\mathbb{V}ar(C \boldsymbol\varepsilon|\mathbf{X}) = \sigma^2CC'\) (by Assumptions \ref{hyp:homoskedasticity} and \ref{hyp:noncorrelResid}, see Prop. \ref{prp:Sigma}). Using \(C=D+(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\) and exploiting the fact that \(D\mathbf{X} = \mathbf{0}\) leads to:
\[
\mathbb{V}ar(\mathbf{b^*}|\mathbf{X}) =\sigma^2\left[(D+(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}')(D+(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}')'\right] = \mathbb{V}ar(\mathbf{b}|\mathbf{X}) + \sigma^2 \mathbf{D}\mathbf{D}'.
\]
Therefore, we have \(\mathbb{V}ar(w'\mathbf{b^*}|\mathbf{X})=w'\mathbb{V}ar(\mathbf{b}|\mathbf{X})w + \sigma^2 w'\mathbf{D}\mathbf{D}'w\ge w'\mathbb{V}ar(\mathbf{b}|\mathbf{X})w=\mathbb{V}ar(w'\mathbf{b}|\mathbf{X})\). \(\square\)
\end{proof}

\hypertarget{frish-waugh}{%
\subsection{Frish-Waugh}\label{frish-waugh}}

Consider the linear least square regression of \(\mathbf{y}\) on \(\mathbf{X}\). We introduce the notations:

\begin{itemize}
\tightlist
\item
  \(\mathbf{b}^{\mathbf{y}/\mathbf{X}}\): OLS estimates of \(\boldsymbol\beta\),
\item
  \(\mathbf{M}^{\mathbf{X}}\): residual-maker matrix of any regression on \(\mathbf{X}\),
\item
  \(\mathbf{P}^{\mathbf{X}}\): projection matrix of any regression on \(\mathbf{X}\).
\end{itemize}

Consider the case where we have two sets of explanatory variables: \(\mathbf{X} = [\mathbf{X}_1,\mathbf{X}_2]\). With obvious notations: \(\mathbf{b}^{\mathbf{y}/\mathbf{X}}=[\mathbf{b}_1',\mathbf{b}_2']'\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{theorem}[Frisch-Waugh Theorem]
\protect\hypertarget{thm:FW}{}\label{thm:FW}We have:
\[
\mathbf{b}_2 = \mathbf{b}^{\mathbf{M^{\mathbf{X}_1}y}/\mathbf{M^{\mathbf{X}_1}\mathbf{X}_2}}.
\]
\end{theorem}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
The minimization of the least squares leads to (these are first-order conditions, see Eq. \eqref{eq:OLSFOC}):
\[
\left[ \begin{array}{cc} \mathbf{X}_1'\mathbf{X}_1 & \mathbf{X}_1'\mathbf{X}_2 \\ \mathbf{X}_2'\mathbf{X}_1 & \mathbf{X}_2'\mathbf{X}_2\end{array}\right]
\left[ \begin{array}{c} \mathbf{b}_1 \\ \mathbf{b}_2\end{array}\right] =
\left[ \begin{array}{c} \mathbf{X}_1' \mathbf{y} \\ \mathbf{X}_2' \mathbf{y} \end{array}\right].
\]
Use the first-row block of equations to solve for \(\mathbf{b}_1\) first; it comes as a function of \(\mathbf{b}_2\). Then use the second set of equations to solve for \(\mathbf{b}_2\), which leads to:
\[
\mathbf{b}_2 = [\mathbf{X}_2'\mathbf{X}_2 - \mathbf{X}_2'\mathbf{X}_1(\mathbf{X}_1'\mathbf{X}_1)\mathbf{X}_1'\mathbf{X}_2]^{-1}\mathbf{X}_2'(Id - \mathbf{X}_1(\mathbf{X}_1'\mathbf{X}_1)\mathbf{X}_1')\mathbf{y}=[\mathbf{X}_2' \mathbf{M}^{\mathbf{X}_1}\mathbf{X}_2]^{-1}\mathbf{X}_2'\mathbf{M}^{\mathbf{X}_1}\mathbf{y}.
\]
Using the fact that \(\mathbf{M}^{\mathbf{X}_1}\) is idempotent and symmetric leads to the result.\qed
\end{proof}

This suggests a second way of estimating \(\mathbf{b}_2\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regress \(Y\) on \(X_1\), regress \(X_2\) on \(X_1\).
\item
  Regress the former residuals on the latter.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/jrenne/Data4courses/master/parapluie/data4parapluie.csv"}\NormalTok{)}
\NormalTok{dummies }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(Data[,}\DecValTok{4}\SpecialCharTok{:}\DecValTok{14}\NormalTok{])}
\NormalTok{eq\_all }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(parapluie}\SpecialCharTok{\textasciitilde{}}\NormalTok{precip}\SpecialCharTok{+}\NormalTok{dummies,}\AttributeTok{data=}\NormalTok{Data)}
\FunctionTok{summary}\NormalTok{(eq\_all)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Estimate Std. Error    t value     Pr(>|t|)
## (Intercept)  -2.9534478 3.40601605 -0.8671268 0.3893855798
## precip        0.1300055 0.03594492  3.6167985 0.0006192876
## dummiesX1    -8.5990682 3.30046623 -2.6054101 0.0115966151
## dummiesX2   -13.3290386 3.30657128 -4.0310755 0.0001613897
## dummiesX3    -7.9829582 3.25949898 -2.4491366 0.0173091333
## dummiesX4    -3.3923533 3.27605582 -1.0354992 0.3046614769
## dummiesX5    -3.7038158 3.25710094 -1.1371511 0.2600724511
## dummiesX6    -3.3606412 3.27334327 -1.0266694 0.3087670632
## dummiesX7    -7.3158812 3.28491529 -2.2271141 0.0297682836
## dummiesX8    -7.7172773 3.26128164 -2.3663327 0.0212677987
## dummiesX9    -4.6491997 3.26005024 -1.4261129 0.1591057652
## dummiesX10   -5.1091987 3.25143961 -1.5713651 0.1214457733
## dummiesX11    1.9807700 3.25942144  0.6077060 0.5457145714
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{deseas\_parapluie }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(parapluie}\SpecialCharTok{\textasciitilde{}}\NormalTok{dummies,}\AttributeTok{data=}\NormalTok{Data)}\SpecialCharTok{$}\NormalTok{residuals}
\NormalTok{deseas\_precip    }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(precip}\SpecialCharTok{\textasciitilde{}}\NormalTok{dummies,}\AttributeTok{data=}\NormalTok{Data)}\SpecialCharTok{$}\NormalTok{residuals}
\NormalTok{eq\_frac }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(deseas\_parapluie}\SpecialCharTok{\textasciitilde{}}\NormalTok{deseas\_precip)}
\FunctionTok{summary}\NormalTok{(eq\_frac)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    Estimate Std. Error       t value     Pr(>|t|)
## (Intercept)   -3.265931e-16 0.60898084 -5.362946e-16 1.0000000000
## deseas_precip  1.300055e-01 0.03300004  3.939557e+00 0.0001907741
\end{verbatim}

When \(b_2\) is scalar (and then \(\mathbf{X}_2\) is of dimension \(n \times 1\)), Theorem \ref{thm:FW} leads to:
\[
b_2 = \frac{\mathbf{X}_2'M^{\mathbf{X}_1}\mathbf{y}}{\mathbf{X}_2'M^{\mathbf{X}_1}\mathbf{X}_2} \quad \text{(partial regression coefficient)}.
\]

\hypertarget{goodness-of-fit}{%
\subsection{Goodness of fit}\label{goodness-of-fit}}

Define the total variation in \(y\) as the sum of squared deviations:
\[
TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2.
\]
We have:
\[
\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e} = \hat{\mathbf{y}} + \mathbf{e}
\]
In the following, we assume that the regression includes a constant (i.e.~for all \(i\), \(x_{i,1}=1\)). Denote by \(\mathbf{M}^0\) the matrix that transforms observations into deviations from sample means. Using that \(\mathbf{M}^0 \mathbf{e} = \mathbf{e}\) and that \(\mathbf{X}' \mathbf{e}=0\), we have:
\begin{eqnarray*}
\underbrace{\mathbf{y}'\mathbf{M}^0\mathbf{y}}_{\mbox{Total sum of sq.}} &=& (\mathbf{X}\mathbf{b} + \mathbf{e})' \mathbf{M}^0 (\mathbf{X}\mathbf{b} + \mathbf{e})\\
&=& \underbrace{\mathbf{b}' \mathbf{X}' \mathbf{M}^0 \mathbf{X}\mathbf{b}}_{\mbox{"Explained" sum of sq.}} + \underbrace{\mathbf{e}'\mathbf{e}}_{\mbox{Sum of sq. residuals}}\\
TSS &=& Expl.SS + SSR.
\end{eqnarray*}

\begin{equation}
\boxed{\mbox{Coefficient of determination} = \frac{Expl.SS}{TSS} = 1 - \frac{SSR}{TSS} = 1 - \frac{\mathbf{e}'\mathbf{e}}{\mathbf{y}'\mathbf{M}^0\mathbf{y}}.}\label{eq:RR2}
\end{equation}

It can be shown {[}Greene, 2012, Section 3.5{]} that:
\[
\mbox{Coefficient of determination} = \frac{[\sum_{i=1}^n(y_i - \bar{y})(\hat{y_i} - \bar{y})]^2}{\sum_{i=1}^n(y_i - \bar{y})^2 \sum_{i=1}^n(\hat{y_i} - \bar{y})^2}.
\]
\(\Rightarrow\) \(R^2\) is the sample squared correlation between \(y\) and the (regression-implied) \(y\)'s predictions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{3}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{2}\NormalTok{,.}\DecValTok{85}\NormalTok{))}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N)}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N)}
\NormalTok{Y }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ eps}
\FunctionTok{plot}\NormalTok{(X,Y,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}\AttributeTok{main=}\StringTok{"(a) Low R2"}\NormalTok{)}
\NormalTok{Y }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ .}\DecValTok{1}\SpecialCharTok{*}\NormalTok{eps}
\FunctionTok{plot}\NormalTok{(X,Y,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}\AttributeTok{main=}\StringTok{"(b) High R2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{AdvECTS_files/figure-latex/R2-1.pdf}

The \textbf{partial correlation} between \(y\) and \(z\), controlling for some variables \(\mathbf{X}\) is the sample correlation between \(y^*\) and \(z^*\), where the latter two variables are the residuals in regressions of \(y\) on \(\mathbf{X}\) and of \(z\) on \(\mathbf{X}\), respectively.

This correlation is denoted by \(r_{yz}^\mathbf{X}\). By definition, we have:
\begin{equation}
r_{yz}^\mathbf{X} = \frac{\mathbf{z^*}'\mathbf{y^*}}{\sqrt{(\mathbf{z^*}'\mathbf{z^*})(\mathbf{y^*}'\mathbf{y^*})}}.\label{eq:pc}
\end{equation}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}[Change in SSR when a variable is added]
\protect\hypertarget{prp:chgeR2}{}\label{prp:chgeR2}We have:
\begin{equation}
\mathbf{u}'\mathbf{u} = \mathbf{e}'\mathbf{e} - c^2(\mathbf{z^*}'\mathbf{z^*}) \qquad (\le \mathbf{e}'\mathbf{e}) \label{eq:uu}
\end{equation}
where (i) \(\mathbf{u}\) and \(\mathbf{e}\) are the residuals in the regressions of \(\mathbf{y}\) on \([\mathbf{X},\mathbf{z}]\) and of \(\mathbf{y}\) on \(\mathbf{X}\), respectively, (ii) \(c\) is the regression coefficient on \(\mathbf{z}\) in the former regression and where \(\mathbf{z}^*\) are the residuals in the regression of \(\mathbf{z}\) on \(\mathbf{X}\).
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
The OLS estimates \([\mathbf{d}',\mathbf{c}]'\) in the regression of \(\mathbf{y}\) on \([\mathbf{X},\mathbf{z}]\) satisfies (first-order cond., Eq. \eqref{eq:OLSFOC})
\[
\left[ \begin{array}{cc} \mathbf{X}'\mathbf{X} & \mathbf{X}'\mathbf{z} \\ \mathbf{z}'\mathbf{X} & \mathbf{z}'\mathbf{z}\end{array}\right]
\left[ \begin{array}{c} \mathbf{d} \\ \mathbf{c}\end{array}\right] =
\left[ \begin{array}{c} \mathbf{X}' \mathbf{y} \\ \mathbf{z}' \mathbf{y} \end{array}\right].
\]
Hence, in particular \(\mathbf{d} = \mathbf{b} - (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{z}\mathbf{c}\), where \(\mathbf{b}\) is the OLS of \(\mathbf{y}\) on \(\mathbf{X}\). Substituting in \(\mathbf{u} = \mathbf{y} - \mathbf{X}\mathbf{d} - \mathbf{z}c\), we get \(\mathbf{u} = \mathbf{e} - \mathbf{z}^*c\). We therefore have:
\begin{equation}
\mathbf{u}'\mathbf{u} = (\mathbf{e} - \mathbf{z}^*c)(\mathbf{e} - \mathbf{z}^*c)= \mathbf{e}'\mathbf{e} + c^2(\mathbf{z^*}'\mathbf{z^*}) - 2 c\mathbf{z^*}'\mathbf{e}.\label{eq:uuu}
\end{equation}
Now \(\mathbf{z^*}'\mathbf{e} = \mathbf{z^*}'(\mathbf{y} - \mathbf{X}\mathbf{b}) = \mathbf{z^*}'\mathbf{y}\) because \(\mathbf{z}^*\) are the residuals in an OLS regression on \(\mathbf{X}\). Since \(c = (\mathbf{z^*}'\mathbf{z^*})^{-1}\mathbf{z^*}'\mathbf{y^*}\) (by an application of Theorem \ref{thm:FW}), we have \((\mathbf{z^*}'\mathbf{z^*})c = \mathbf{z^*}'\mathbf{y^*}\) and, therefore, \(\mathbf{z^*}'\mathbf{e} = (\mathbf{z^*}'\mathbf{z^*})c\). Inserting this in Eq. \eqref{eq:uuu} leads to the results. \(\square\)
\end{proof}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}[Change in the coefficient of determination when a variable is added]
\protect\hypertarget{prp:chgeInR2}{}\label{prp:chgeInR2}Denoting by \(R_W^2\) the coefficient of determination in the regression of \(\mathbf{y}\) on some variable \(\mathbf{W}\), we have:
\[
R_{\mathbf{X},\mathbf{z}}^2 = R_{\mathbf{X}}^2 + (1-R_{\mathbf{X}}^2)(r_{yz}^\mathbf{X})^2,
\]
where \(r_{yz}^\mathbf{X}\) is the coefficient of partial correlation.
\end{proposition}

\begin{proof}
Let's use the same notations as in Prop. @ref\{prp:chgeR2\}. Theorem \ref{thm:FW} implies that \(c = (\mathbf{z^*}'\mathbf{z^*})^{-1}\mathbf{z^*}'\mathbf{y^*}\). Using this in Eq. \eqref{eq:uu} gives \(\mathbf{u}'\mathbf{u} = \mathbf{e}'\mathbf{e} - (\mathbf{z^*}'\mathbf{y^*})^2/(\mathbf{z^*}'\mathbf{z^*})\). Using the definition of the partial correlation (Eq. \eqref{eq:pc}), we get \(\mathbf{u}'\mathbf{u} = \mathbf{e}'\mathbf{e}\left(1 - (r_{yz}^\mathbf{X})^2\right)\). The results is obtained by dividing both sides of the previous equation by \(\mathbf{y}'\mathbf{M}_0\mathbf{y}\). \(\square\)
\end{proof}

The previous theorem shows that we necessarily increase the \(R^2\) if we add variables, \textbf{even if they are irrelevant}.

The \textbf{adjusted \(R^2\)}, denoted by \(\bar{R}^2\), is a fit measure that penalizes large numbers of regressors:
\begin{equation*}
\boxed{\bar{R}^2 = 1 - \frac{\mathbf{e}'\mathbf{e}/(n-K)}{\mathbf{y}'\mathbf{M}^0\mathbf{y}/(n-1)} = 1 - \frac{n-1}{n-K}(1-R^2).}
\end{equation*}

\hypertarget{inference-and-prediction}{%
\subsection{Inference and Prediction}\label{inference-and-prediction}}

Under the normality assumption (Assumption \ref{hyp:normality}), we know the distribution of \(\mathbf{b}\) (conditional on \(\mathbf{X}\)). Indeed, \((\mathbf{b}|\mathbf{X}) \equiv (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y}\) is multivariate Gaussian:
\begin{equation}
\mathbf{b}|\mathbf{X} \sim \mathcal{N}(\beta,\sigma^2(\mathbf{X}'\mathbf{X})^{-1}).\label{eq:distriBcondi}
\end{equation}

Problem: In practice, we do not know \(\sigma^2\) (\textbf{population parameter}).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:expects2}{}\label{prp:expects2}Under \ref{hyp:fullrank} to \ref{hyp:noncorrelResid}, an unbiased estimate of \(\sigma^2\) is given by:
\begin{equation}
s^2 = \frac{\mathbf{e}'\mathbf{e}}{n-K}.\label{eq:s2}
\end{equation}
(It is sometimes denoted by \(\sigma^2_{OLS}\).)
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
\(\mathbb{E}(\mathbf{e}'\mathbf{e}|\mathbf{X})=\mathbb{E}(\boldsymbol{\varepsilon}'\mathbf{M}\boldsymbol{\varepsilon}|\mathbf{X})=\mathbb{E}(\mbox{Tr}(\boldsymbol{\varepsilon}'\mathbf{M}\boldsymbol{\varepsilon})|\mathbf{X})) =\mbox{Tr}(\mathbf{M}\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|\mathbf{X}))=\sigma^2 \mbox{Tr}(\mathbf{M})\). (Note that we have \(\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|\mathbf{X})=\sigma^2Id\) by Assumptions \ref{hyp:homoskedasticity} and \ref{hyp:noncorrelResid}, see Prop. \ref{prp:Sigma}.) Finally: \(\mbox{Tr}(\mathbf{M})=n-\mbox{Tr}(\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}')=n-\mbox{Tr}((\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X})=n-\mbox{Tr}(Id_{K\times K})\). \(\square\)
\end{proof}

Two results will prove important to perform hypothesis testing:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  We know the distribution of \(s^2\) (Prop. \ref{prp:s2distri}).
\item
  \(s^2\) and \(\mathbf{b}\) are independent random variables (Prop. \ref{prp:indeps2b}).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:s2distri}{}\label{prp:s2distri}Under \ref{hyp:fullrank} to \ref{hyp:normality}, we have: \(\dfrac{s^2}{\sigma^2} | \mathbf{X} \sim \chi^2(n-K)/(n-K)\).
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
We have \(\mathbf{e}'\mathbf{e}=\boldsymbol\varepsilon'\mathbf{M}\boldsymbol\varepsilon\). \(\mathbf{M}\) is an idempotent symmetric matrix. Therefore it can be decomposed as \(PDP'\) where \(D\) is a diagonal matrix and \(P\) is an orthogonal matrix. As a result \(\mathbf{e}'\mathbf{e} = (P'\boldsymbol\varepsilon)'D(P'\boldsymbol\varepsilon)\), i.e.~\(\mathbf{e}'\mathbf{e}\) is a weighted sum of independent squared Gaussian variables (the entries of \(P'\boldsymbol\varepsilon\) are independent because they are Gaussian -- under \ref{hyp:normality} -- and uncorrelated). The variance of each of these i.i.d. Gaussian variable is \(\sigma^2\). Because \(\mathbf{M}\) is an idempotent symmetric matrix, its eigenvalues are either 0 or 1, and its rank equals its trace. Further, its trace is equal to \(n-K\) (see proof of Eq. \eqref{eq:s2}). Therefore \(D\) has \(n-K\) entries equal to 1 and \(K\) equal to 0.Hence, \(\mathbf{e}'\mathbf{e} = (P'\boldsymbol\varepsilon)'D(P'\boldsymbol\varepsilon)\) is a sum of \(n-K\) squared independent Gaussian variables of variance \(\sigma^2\). Therefore \(\frac{\mathbf{e}'\mathbf{e}}{\sigma^2} = (n-K)\frac{s^2}{\sigma^2}\) is a sum of \(n-k\) squared i.i.d. standard normal variables. \(\square\)
\end{proof}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:indeps2b}{}\label{prp:indeps2b}Under Hypotheses \ref{hyp:fullrank} to \ref{hyp:normality}, \(\mathbf{b}\) and \(s^2\) are independent.
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
We have \(\mathbf{b}=\boldsymbol\beta + [\mathbf{X}'{\mathbf{X}}]^{-1}\mathbf{X}\boldsymbol\varepsilon\) and \(s^2 = \boldsymbol\varepsilon' \mathbf{M} \boldsymbol\varepsilon/(n-K)\). Hence \(\mathbf{b}\) is an affine combination of \(\boldsymbol\varepsilon\) and \(s^2\) is a quadratic combination of the same Gaussian shocks. One can write \(s^2\) as \(s^2 = (\mathbf{M}\boldsymbol\varepsilon)' \mathbf{M} \boldsymbol\varepsilon/(n-K)\) and \(\mathbf{b}\) as \(\boldsymbol\beta + \mathbf{T}\boldsymbol\varepsilon\). Since \(\mathbf{T}\mathbf{M}=0\), \(\mathbf{T}\boldsymbol\varepsilon\) and \(\mathbf{M}\boldsymbol\varepsilon\) are independent (because two uncorrelated Gaussian variables are independent), therefore \(\mathbf{b}\) and \(s^2\), which are functions of respective independent variables, are independent. \(\square\)
\end{proof}

Under Hypotheses \ref{hyp:fullrank} to \ref{hyp:normality}, let us consider \(b_k\), the \(k^{th}\) entry of \(\mathbf{b}\):
\[
b_k | \mathbf{X} \sim \mathcal{N}(\beta_k,\sigma^2 v_k),
\]
where \(v_k\) is the k\(^{th}\) component of the diagonal of \((\mathbf{X}'\mathbf{X})^{-1}\).

Besides, we have (Prop. \ref{prp:s2distri}):
\[
\frac{(n-K)s^2}{\sigma^2} | \mathbf{X} \sim \chi ^2 (n-K).
\]

As a result (using Props. \ref{prp:s2distri} and \ref{prp:indeps2b}), we have:
\begin{equation}
\boxed{t_k = \frac{\frac{b_k - \beta_k}{\sqrt{\sigma^2 v_k}}}{\sqrt{\frac{(n-K)s^2}{\sigma^2(n-K)}}} = \frac{b_k - \beta_k}{\sqrt{s^2v_k}} \sim t(n-K),}\label{eq:resultstudentt}
\end{equation}
where \(t(n-K)\) denotes a \(t\) distribution with \(n-K\) degrees of freedom.

Remark: \(\frac{b_k - \beta_k}{\sqrt{\sigma^2 v_k}} | \mathbf{X} \sim \mathcal{N}(0,1)\) and \(\frac{(n-K)s^2}{\sigma^2} | \mathbf{X} \sim \chi ^2 (n-K)\). These two distributions do not depend on \(\mathbf{X}\) \(\Rightarrow\) the \emph{marginal distribution} of \(t_k\) is also \(t\).

Note that \(s^2 v_k\) is not exactly the conditional variance of \(b_k\): The variance of \(b_k\) conditional on \(\mathbf{X}\) is \(\sigma^2 v_k\). However \(s^2 v_k\) is an unbiased estimate of \(\sigma^2 v_k\) (by Prop. \ref{prp:expects2}).

The previous result (Eq. \eqref{eq:resultstudentt}) can be extended to any linear combinations of elements of \(\mathbf{b}\) (Eq. \eqref{eq:resultstudentt} is for its \(k^{th}\) component only).

Let us consider \(\boldsymbol\alpha'\mathbf{b}\), the OLS estimate of \(\boldsymbol\alpha'\boldsymbol\beta\). From Eq. \eqref{eq:distriBcondi}, we have:
\[
\boldsymbol\alpha'\mathbf{b} | \mathbf{X} \sim \mathcal{N}(\boldsymbol\alpha'\boldsymbol\beta,\sigma^2 \boldsymbol\alpha'(\mathbf{X}'\mathbf{X})^{-1}\boldsymbol\alpha).
\]
Therefore:
\[
\frac{\boldsymbol\alpha'\mathbf{b} - \boldsymbol\alpha'\boldsymbol\beta}{\sqrt{\sigma^2 \boldsymbol\alpha'(\mathbf{X}'\mathbf{X})^{-1}\boldsymbol\alpha}} | \mathbf{X} \sim \mathcal{N}(0,1).
\]
Using the same approach as the one used to derive Eq. \eqref{eq:resultstudentt}, one can show that Props. \ref{prp:s2distri} and \ref{prp:indeps2b} imply that:
\begin{equation}
\boxed{\frac{\boldsymbol\alpha'\mathbf{b} - \boldsymbol\alpha'\boldsymbol\beta}{\sqrt{s^2\boldsymbol\alpha'(\mathbf{X}'\mathbf{X})^{-1}\boldsymbol\alpha}} \sim t(n-K).}\label{eq:resultstudentt2}
\end{equation}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{2}\NormalTok{,.}\DecValTok{95}\NormalTok{))}
\NormalTok{xx }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{3.5}\NormalTok{,}\FloatTok{3.5}\NormalTok{,}\AttributeTok{by=}\NormalTok{.}\DecValTok{01}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(xx,}\FunctionTok{dnorm}\NormalTok{(xx),}\AttributeTok{xlab=}\StringTok{"X"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{""}\NormalTok{,}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(xx,}\FunctionTok{dt}\NormalTok{(xx,}\AttributeTok{df=}\DecValTok{3}\NormalTok{),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(xx,}\FunctionTok{dt}\NormalTok{(xx,}\AttributeTok{df=}\DecValTok{7}\NormalTok{),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(xx,}\FunctionTok{dt}\NormalTok{(xx,}\AttributeTok{df=}\DecValTok{20}\NormalTok{),}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{3}\NormalTok{,}\AttributeTok{lty=}\DecValTok{3}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{,}
       \FunctionTok{c}\NormalTok{(}\StringTok{"N(0,1)"}\NormalTok{,}\StringTok{"t(3)"}\NormalTok{,}\StringTok{"t(7)"}\NormalTok{,}\StringTok{"t(20)"}\NormalTok{),}
       \AttributeTok{lty=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), }\CommentTok{\# gives the legend appropriate symbols (lines)       }
       \AttributeTok{lwd=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), }\CommentTok{\# line width}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{,}\StringTok{"red"}\NormalTok{,}\StringTok{"red"}\NormalTok{,}\StringTok{"blue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{AdvECTS_files/figure-latex/chartStudent-1.pdf}
\caption{\label{fig:chartStudent}The chart shows that the higher the degree of freedom, the closer the distribution of \(t( u)\) gets to the normal distribution.}
\end{figure}

\hypertarget{confidence-interval-of-beta_k}{%
\subsection{\texorpdfstring{Confidence interval of \(\beta_k\)}{Confidence interval of \textbackslash beta\_k}}\label{confidence-interval-of-beta_k}}

Assume we want to compute a (symmetrical) confidence interval \([I_{d,1-\alpha},I_{u,1-\alpha}]\) that is such that \(\mathbb{P}(\beta_k \in [I_{d,1-\alpha},I_{u,1-\alpha}])=1-\alpha\). In particular, we want to have: \(\mathbb{P}(\beta_k < I_{d,1-\alpha})=\frac{\alpha}{2}\).

For this purpose, we make use of \(t_k = \frac{b_k - \beta_k}{\sqrt{s^2v_k}} \sim t(n-K)\) (Eq. \eqref{eq:resultstudentt}).

We have:
\[
\mathbb{P}(\beta_k < I_{d,1-\alpha})=\frac{\alpha}{2} \Leftrightarrow
\]
\begin{eqnarray*}
\mathbb{P}\left(\frac{b_k - \beta_k}{\sqrt{s^2v_k}} > \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} \Leftrightarrow \mathbb{P}\left(t_k > \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} \Leftrightarrow&&\\
1 - \mathbb{P}\left(t_k \le \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}}\right)=\frac{\alpha}{2} \Leftrightarrow \frac{b_k - I_{d,1-\alpha}}{\sqrt{s^2v_k}} = \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right),&&
\end{eqnarray*}
where \(\Phi_{t(n-K)}(\alpha)\) is the c.d.f. of the \(t(n-K)\) distribution (Table @ref\{tab:Studenttable\}).

Doing the same for \(I_{u,1-\alpha}\), we obtain:
\begin{eqnarray*}
&&[I_{d,1-\alpha},I_{u,1-\alpha}] =\\
&&\left[b_k - \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\sqrt{s^2v_k},b_k + \Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\sqrt{s^2v_k}\right].
\end{eqnarray*}

\hypertarget{example}{%
\subsection{Example}\label{example}}

The following example is based on the \href{https://hrs.isr.umich.edu/about}{HRS dataset (Health and Retirement Study)}. We only consider only a subset of this large dataset, focusing on a few variables, and for year 2018 (wave 14). This \href{https://raw.githubusercontent.com/jrenne/Data4courses/master/HRS_RAND/reduced_version/prepare_small_HRS.R}{R script} builds the reduced dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reducedHRS }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/jrenne/Data4courses/master/HRS\_RAND/reduced\_version/reducedHRS.csv"}\NormalTok{)}
\NormalTok{eq }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(riearn}\SpecialCharTok{\textasciitilde{}}\NormalTok{raedyrs}\SpecialCharTok{+}\NormalTok{ragey\_b}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(ragey\_b}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}\NormalTok{rfemale,}\AttributeTok{data=}\NormalTok{reducedHRS)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(eq))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = riearn ~ raedyrs + ragey_b + I(ragey_b^2) + rfemale, 
##     data = reducedHRS)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -82512 -29447  -8144  18083 394724 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -1.336e+05  3.245e+04  -4.116 3.92e-05 ***
## raedyrs       5.216e+03  2.384e+02  21.876  < 2e-16 ***
## ragey_b       4.758e+03  1.086e+03   4.382 1.20e-05 ***
## I(ragey_b^2) -4.441e+01  9.097e+00  -4.882 1.09e-06 ***
## rfemale      -1.499e+04  1.498e+03 -10.007  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 50130 on 4540 degrees of freedom
## Multiple R-squared:  0.1173, Adjusted R-squared:  0.1165 
## F-statistic: 150.8 on 4 and 4540 DF,  p-value: < 2.2e-16
\end{verbatim}

The last two columns give the test statistic and p-values associated to the test whose null hypothesis is:
\[
H_0: \beta_k=0.
\]
The \textbf{t-statistics}, that is \(b_k/\sqrt{s^2 v_k}\), is the test statistic of the test. Under \(H_0\), the t-statistic is \(t(n-K)\) (see Eq. \eqref{eq:resultstudentt}). Hence, the \textbf{critical region} for the test of size \(\alpha\) is:
\[
\left]-\infty,-\Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right)\right] \cup \left[\Phi^{-1}_{t(n-K)}\left(1-\frac{\alpha}{2}\right),+\infty\right[.
\]
The \textbf{p-value} is defined as the probability that \(|Z| > |t|\), where \(t\) is the (computed) t statistics and where \(Z \sim t(n-K)\). That is, the p-value is given by \(2(1 - \Phi_{t(n-K)}(|t_k|))\).

See \href{https://jrenne.shinyapps.io/tests/}{this webpage} for details regarding the link between critical regions, p-value, and test outcomes.

\hypertarget{set-of-linear-restrictions}{%
\subsection{Set of linear restrictions}\label{set-of-linear-restrictions}}

We consider the following model:
\[
\mathbf{y} = \mathbf{X}\boldsymbol\beta + \boldsymbol\varepsilon, \quad \varepsilon \sim i.i.d. \mathcal{N}(0,\sigma^2).
\]
and we want to test for the \emph{joint} validity of a set of restrictions involving the components of \(\boldsymbol\beta\) in a linear way.

Set of linear restrictions:
\begin{equation}\label{eq:restrictions}
\begin{array}{ccc}
r_{1,1} \beta_1 + \dots + r_{1,K} \beta_K &=& q_1\\
\vdots && \vdots\\
r_{J,1} \beta_1 + \dots + r_{J,K} \beta_K &=& q_J,
\end{array}
\end{equation}
that can be written in matrix form:
\begin{equation}
\mathbf{R}\boldsymbol\beta = \mathbf{q}.
\end{equation}

Defin the \textbf{Discrepancy vector} \(\mathbf{m} = \mathbf{R}\mathbf{b} - \mathbf{q}\). Under the null hypothesis:
\begin{eqnarray*}
\mathbb{E}(\mathbf{m}|\mathbf{X}) &=& \mathbf{R}\boldsymbol\beta - \mathbf{q} = 0 \quad \mbox{and} \\
\mathbb{V}ar(\mathbf{m}|\mathbf{X}) &=& \mathbf{R} \mathbb{V}ar(\mathbf{b}|\mathbf{X}) \mathbf{R}'.
\end{eqnarray*}
Under Hypotheses \ref{hyp:fullrank} to \ref{hyp:noncorrelResid}, \(\mathbb{V}ar(\mathbf{m}|\mathbf{X}) = \sigma^2 \mathbf{R} (\mathbf{X}'\mathbf{X})^{-1} \mathbf{R}'\) (see Prop. \ref{prp:propOLS}).

Consider the test:
\begin{equation}
\boxed{H_0: \mathbf{R}\boldsymbol\beta - \mathbf{q} = 0 \mbox{ against } H_1: \mathbf{R}\boldsymbol\beta - \mathbf{q} \ne 0.}\label{eq:H0Ftest}
\end{equation}

We could perform a \textbf{Wald test}. Under \ref{hyp:fullrank} to \ref{hyp:normality} --we need the normality assumption-- and under \(H_0\), it can be shown that we have:
\begin{equation}
W = \mathbf{m}'\mathbb{V}ar(\mathbf{m}|\mathbf{X})^{-1}\mathbf{m} \sim \chi^2(J). \label{eq:W1}
\end{equation}
However, \(\sigma^2\) is unknown. Hence we cannot compute \(W\).

We can however approximate it be replacing \(\sigma^2\) by \(s^2\). The distribution of this new statistic is not \(\chi^2(J)\) any more;
it is an \textbf{\(\mathcal{F}\) distribution}, and the test is called \textbf{\(F\) test}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:Ftest1}{}\label{prp:Ftest1}Under Hypotheses \ref{hyp:fullrank} to \ref{hyp:normality} and if Eq. \eqref{eq:H0Ftest} holds, we have:
\begin{equation}
F = \frac{W}{J}\frac{\sigma^2}{s^2} = \frac{\mathbf{m}'(\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}')^{-1}\mathbf{m}}{s^2J} \sim \mathcal{F}(J,n-K),\label{eq:defFstatistics}
\end{equation}
where \(\mathcal{F}\) is the distribution of the F-statistic.
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
According to Eq. \eqref{eq:W1}, \(W/J \sim \chi^2(J)/J\). Moreover, the denominator (\(s^2/\sigma^2\)) is \(\sim \chi^2(n-K)\). Therefore, \(F\) is the ratio of a r.v. distributed as \(\chi^2(J)/J\) and another distributed as \(\chi^2(n-K)/(n-K)\). It remains to verify that these r.v. are independent.

Under \(H_0\), we have \(\mathbf{m} = \mathbf{R}(\mathbf{b}-\boldsymbol\beta) = \mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon\).
Therefore \(\mathbf{m}'(\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}')^{-1}\mathbf{m}\) is of the form \(\boldsymbol\varepsilon'\mathbf{T}\boldsymbol\varepsilon\) with \(\mathbf{T}=\mathbf{D}'\mathbf{C}\mathbf{D}\) where \(\mathbf{D}=\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\) and \(\mathbf{C}=(\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}')^{-1}\). Under Hypotheses \ref{hyp:fullrank} to \ref{hyp:noncorrelResid}, the covariance between \(\mathbf{T}\boldsymbol\varepsilon\) and \(\mathbf{M}\boldsymbol\varepsilon\) is \(\sigma^2\mathbf{T}\mathbf{M} = \mathbf{0}\). Therefore, under \ref{hyp:normality}, these variables are Gaussian variables with 0 covariance. Hence they are independent. \(\square\)
\end{proof}

Remark: For large \(n-K\), the \(\mathcal{F}_{J,n-K}\) distribution converges to \(\mathcal{F}_{J,\infty}=\chi^2(J)/J\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:Ftest}{}\label{prp:Ftest}The F-statistic defined by Eq. \eqref{eq:defFstatistics} is also equal to:
\begin{equation}
F = \frac{(R^2-R_*^2)/J}{(1-R^2)/(n-K)} =  \frac{(SSR_{restr}-SSR_{unrestr})/J}{SSR_{unrestr}/(n-K)},\label{eq:defFstatistics2}
\end{equation}
where \(R_*^2\) is the coef. of determination (Eq. \ref(eq:RR2)) of the ``restricted regression'\,' \emph{(SSR: sum of squared residuals.)}
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
Let's denote by \(\mathbf{e}_*=\mathbf{y}-\mathbf{X}\mathbf{b}_*\) the vector of residuals associated to the \emph{restricted regression} (i.e.~\(\mathbf{R}\mathbf{b}_*=\mathbf{q}\)).
We have \(\mathbf{e}_*=\mathbf{e} - \mathbf{X}(\mathbf{b}_*-\mathbf{b})\). Using \(\mathbf{e}'\mathbf{X}=0\), we get \(\mathbf{e}_*'\mathbf{e}_*=\mathbf{e}'\mathbf{e} + (\mathbf{b}_*-\mathbf{b})'\mathbf{X}'\mathbf{X}(\mathbf{b}_*-\mathbf{b}) \ge \mathbf{e}'\mathbf{e}\).

By Prop. @ref(prp:constrained\_LS), we know that \(\mathbf{b}_*-\mathbf{b}=-(\mathbf{X}'\mathbf{X})^{-1} \mathbf{R}'\{\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}'\}^{-1}(\mathbf{R}\mathbf{b} - \mathbf{q})\). Therefore:
\[
\mathbf{e}_*'\mathbf{e}_* - \mathbf{e}'\mathbf{e} = (\mathbf{R}\mathbf{b} - \mathbf{q})'[\mathbf{R}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{R}']^{-1}(\mathbf{R}\mathbf{b} - \mathbf{q}).
\]
This implies that the F statistic defined in Prop. \ref{prp:Ftest1} is also equal to:
\[
\frac{(\mathbf{e}_*'\mathbf{e}_* - \mathbf{e}'\mathbf{e})/J}{\mathbf{e}'\mathbf{e}/(n-K)}. \square
\]
\end{proof}

The null hypothesis \(H_0\) (Eq. \eqref{eq:H0Ftest}) of the F-test is rejected if \(F\) --defined by Eq. \eqref{eq:defFstatistics} or \eqref{eq:defFstatistics2}-- is higher than \(\mathcal{F}_{1-\alpha}(J,n-K)\). (Hence, this test is a one-sided test.)

\hypertarget{common-pitfalls}{%
\subsection{Common pitfalls}\label{common-pitfalls}}

\hypertarget{multicollinearity}{%
\subsection{Multicollinearity}\label{multicollinearity}}

Consider the model: \(y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i\), where all variables are zero-mean and \(\mathbb{V}ar(\varepsilon_i)=\sigma^2\). We have
\[
\mathbf{X}'\mathbf{X} = \left[ \begin{array}{cc}
\sum_i x_{i,1}^2 & \sum_i x_{i,1} x_{i,2} \\
\sum_i x_{i,1} x_{i,2} & \sum_i x_{i,2}^2
\end{array}\right],
\]
therefore:
\begin{eqnarray*}
(\mathbf{X}'\mathbf{X})^{-1} &=& \frac{1}{\sum_i x_{i,1}^2\sum_i x_{i,2}^2 - (\sum_i x_{i,1} x_{i,2})^2} \left[ \begin{array}{cc}
\sum_i x_{i,2}^2 & -\sum_i x_{i,1} x_{i,2} \\
-\sum_i x_{i,1} x_{i,2} & \sum_i x_{i,1}^2
\end{array}\right].
\end{eqnarray*}
The inverse of the upper-left parameter of \((\mathbf{X}'\mathbf{X})^{-1}\) is:
\begin{equation}
\sum_i x_{i,1}^2 - \frac{(\sum_i x_{i,1} x_{i,2})^2}{\sum_i x_{i,2}^2} = \sum_i x_{i,1}^2(1 - correl_{1,2}^2),\label{eq:multicollin}
\end{equation}
where \(correl_{1,2}\) is the sample correlation between \(\mathbf{x}_{1}\) and \(\mathbf{x}_{2}\).

Hence, the closer to one \(correl_{1,2}\), the higher the variance of \(b_1\) (recall that the variance of \(b_1\) is the upper-left component of \(\sigma^2(\mathbf{X}'\mathbf{X})^{-1}\)).

\hypertarget{omitted-variables}{%
\subsection{Omitted variables}\label{omitted-variables}}

Consider the following model, called ``True model'\,':
\[
\mathbf{y} = \underbrace{\mathbf{X}_1}_{n \times K_1}\underbrace{\boldsymbol\beta_1}_{K_1 \times 1} + \underbrace{\mathbf{X}_2}_{n\times K_2}\underbrace{\boldsymbol\beta_2}_{K_2 \times 1} + \boldsymbol\varepsilon
\]
Then, if one computes \(\mathbf{b}_1\) by regressing \(\mathbf{y}\) on \(\mathbf{X}_1\) only, we get:
\[
\mathbf{b}_1 = (\mathbf{X}_1'\mathbf{X}_1)^{-1}\mathbf{X}_1'\mathbf{y} = \boldsymbol\beta_1 + (\mathbf{X}_1'\mathbf{X}_1)^{-1}\mathbf{X}_1'\mathbf{X}_2\boldsymbol\beta_2 + 
(\mathbf{X}_1'\mathbf{X}_1)^{-1}\mathbf{X}_1'\boldsymbol\varepsilon.
\]

Hence, we obtain the omitted-variable formula:
\[
\boxed{\mathbb{E}(\mathbf{b}_1|\mathbf{X}) = \boldsymbol\beta_1 + \underbrace{(\mathbf{X}_1'\mathbf{X}_1)^{-1}(\mathbf{X}_1'\mathbf{X}_2)}_{K_1 \times K_2}\boldsymbol\beta_2}
\]
(each column of \((\mathbf{X}_1'\mathbf{X}_1)^{-1}(\mathbf{X}_1'\mathbf{X}_2)\) are the OLS regressors obtained when regressing the columns of \(\mathbf{X}_2\) on \(\mathbf{X}_1\)).

\begin{example}
\protect\hypertarget{exm:wageeduc}{}\label{exm:wageeduc}Consider the ``true model'\,':
\begin{equation}
wage_i = \beta_0 +\beta_1 edu_i + \beta_2 ability_i + \varepsilon_i, \quad \varepsilon_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\varepsilon^2)
\end{equation}
Further, we assume that the \(edu\) variable is correlated to the \(ability\). Specifically:
\[
edu_i = \alpha_0 +\alpha_1 ability_i + \eta_i, \quad \eta_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\eta^2).
\]
Assume we mistakingly run the regression omitting the \(ability\) variable:
\begin{equation}
wage_i = \gamma_0 +\gamma_1 edu_i + \xi_i.
\end{equation}
It can be seen that \(\xi_i = \varepsilon_i - (\beta_2/\alpha_1) \eta_i \sim i.i.d.\,\mathcal{N}(0,\sigma_\varepsilon^2+(\beta_2/\alpha_1)^2\sigma_\eta^2)\) and that the population regression coefficient is \(\gamma_1 = \beta_1 + \beta_2/\alpha_1 \ne \beta_1\).
\end{example}

\begin{example}
\protect\hypertarget{exm:CASchools}{}\label{exm:CASchools}

Let us use the \href{https://rdrr.io/cran/AER/man/CASchools.html}{California Test Score dataset} (in the package \texttt{AER}). Assume we want to measure the effect of the students-to-teacher ratio (`\texttt{str}) on student test scores (\texttt{testscr}). The folowing regressions show that the effect is lower when controls are added.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AER); }\FunctionTok{data}\NormalTok{(}\StringTok{"CASchools"}\NormalTok{)}
\NormalTok{CASchools}\SpecialCharTok{$}\NormalTok{str }\OtherTok{\textless{}{-}}\NormalTok{ CASchools}\SpecialCharTok{$}\NormalTok{students}\SpecialCharTok{/}\NormalTok{CASchools}\SpecialCharTok{$}\NormalTok{teachers}
\NormalTok{CASchools}\SpecialCharTok{$}\NormalTok{testscr }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{5} \SpecialCharTok{*}\NormalTok{ (CASchools}\SpecialCharTok{$}\NormalTok{math }\SpecialCharTok{+}\NormalTok{ CASchools}\SpecialCharTok{$}\NormalTok{read)}
\NormalTok{eq }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(testscr}\SpecialCharTok{\textasciitilde{}}\NormalTok{str,}\AttributeTok{data=}\NormalTok{CASchools)}
\FunctionTok{summary}\NormalTok{(eq)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Estimate Std. Error   t value      Pr(>|t|)
## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242
## str          -2.279808  0.4798255 -4.751327  2.783308e-06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eq }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(testscr}\SpecialCharTok{\textasciitilde{}}\NormalTok{str}\SpecialCharTok{+}\NormalTok{lunch,}\AttributeTok{data=}\NormalTok{CASchools)}
\FunctionTok{summary}\NormalTok{(eq)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Estimate Std. Error    t value      Pr(>|t|)
## (Intercept) 702.9113020 4.70024626 149.547760  0.000000e+00
## str          -1.1172255 0.24035528  -4.648225  4.498554e-06
## lunch        -0.5997501 0.01676439 -35.775242 3.709097e-129
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eq }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(testscr}\SpecialCharTok{\textasciitilde{}}\NormalTok{str}\SpecialCharTok{+}\NormalTok{lunch}\SpecialCharTok{+}\NormalTok{english,}\AttributeTok{data=}\NormalTok{CASchools)}
\FunctionTok{summary}\NormalTok{(eq)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Estimate Std. Error    t value     Pr(>|t|)
## (Intercept) 700.1499572 4.68568672 149.423126 0.000000e+00
## str          -0.9983090 0.23875428  -4.181324 3.535873e-05
## lunch        -0.5473454 0.02159885 -25.341418 2.303048e-86
## english      -0.1215735 0.03231728  -3.761872 1.928369e-04
\end{verbatim}

\end{example}

\hypertarget{irrelevant-variable}{%
\subsection{Irrelevant variable}\label{irrelevant-variable}}

Consider the \emph{True model}:
\[
\mathbf{y} = \mathbf{X}_1\boldsymbol\beta_1 + \boldsymbol\varepsilon,
\]
while the \emph{Estimated model} is:
\[
\mathbf{y} = \mathbf{X}_1\boldsymbol\beta_1 + \mathbf{X}_2\boldsymbol\beta_2 + \boldsymbol\varepsilon
\]

The estimates are unbiased. However, adding irrelevant explanatory variables increases the variance of the estimate of \(\boldsymbol\beta_1\) (compared to the case where one uses the correct explanatory variables). This is the case unless the correlation between \(\mathbf{X}_1\) and \(\mathbf{X}_2\) is null, see Eq. \eqref{eq:multicollin}.

In other words, the estimator is \emph{inefficient}, i.e., there exists an alternative consistent estimator whose variance is lower. The inefficiency problem can have serious consequences when testing hypotheses of type \(H_0: \beta_1 = 0\) due to the loss of power, so we might infer that they are no relevant variables when they truly are (Type-II error; False Negative).

\hypertarget{large-sample-properties}{%
\section{Large Sample Properties}\label{large-sample-properties}}

Even if we relax the normality assumption (Hypothesis \ref{hyp:normality}), we can approximate the finite-sample behavior of the estimators by using \emph{large-sample} or \emph{asymptotic properties}.

To begin with, we proceed under Hypothesis \ref{hyp:fullrank} to \ref{hyp:noncorrelResid}. (We will see later how to deal with --partial-- relaxations of Hypothesis \ref{hyp:homoskedasticity} and \ref{hyp:noncorrelResid}.)

Under regularity assumptions, under \ref{hyp:fullrank} to \ref{hyp:noncorrelResid}, even if the residuals are not normally-distributed, the least square estimators can be \emph{asymptotically normal} and inference can be performed as in small samples when \ref{hyp:fullrank} to \ref{hyp:normality} hold. This derives from Prop. \ref{prp:asymptOLS} (below). The F-test (Prop. \ref(prp:Ftest)) and the t-test can then be performed.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:asymptOLS}{}\label{prp:asymptOLS}Under Assumptions \ref{hyp:fullrank} to \ref{hyp:noncorrelResid}, and assuming further that:
\begin{equation}
Q = \mbox{plim}_{n \rightarrow \infty} \frac{\mathbf{X}'\mathbf{X}}{n},\label{eq:Qasympt}
\end{equation}
and that the \((\mathbf{x}_i,\varepsilon_i)\)s are independent (across entities \(i\)), we have:
\begin{equation}
\sqrt{n}(\mathbf{b} - \boldsymbol\beta)\overset{d} {\rightarrow} \mathcal{N}\left(0,\sigma^2Q^{-1}\right).\label{eq:convgceOLS}
\end{equation}
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
Since \(\mathbf{b} = \boldsymbol\beta + \left( \frac{\mathbf{X}'\mathbf{X}}{n}\right)^{-1}\left(\frac{\mathbf{X}'\boldsymbol\varepsilon}{n}\right)\), we have: \(\sqrt{n}(\mathbf{b} - \boldsymbol\beta) = \left( \frac{\mathbf{X}'\mathbf{X}}{n}\right)^{-1} \left(\frac{1}{\sqrt{n}}\right)\mathbf{X}'\boldsymbol\varepsilon\). Since \(f:A \rightarrow A^{-1}\) is a continuous function (for \(A \ne \mathbf{0}\)), \(\mbox{plim}_{n \rightarrow \infty} \left(\frac{\mathbf{X}'\mathbf{X}}{n}\right)^{-1} = \mathbf{Q}^{-1}\). Let us denote by \(V_i\) the vector \(\mathbf{x}_i \varepsilon_i\). Because the \((\mathbf{x}_i,\varepsilon_i)\)s are independent, the \(V_i\)s are independent as well. Their covariance matrix is \(\sigma^2\mathbb{E}(\mathbf{x}_i \mathbf{x}_i')=\sigma^2Q\). Applying the multivariate central limit theorem on the \(V_i\)s gives \(\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n \mathbf{x}_i \varepsilon_i\right) = \left(\frac{1}{\sqrt{n}}\right)\mathbf{X}'\boldsymbol\varepsilon \overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2Q)\). An application of Slutsky's theorem then leads to the results. \(\square\)
\end{proof}

In practice, \(\sigma^2\) is estimated with \(\frac{\mathbf{e}'\mathbf{e}}{n-K}\) (Eq. \eqref{eq:s2}) and \(\mathbf{Q}^{-1}\) with \(\left(\frac{\mathbf{X}'\mathbf{X}}{n}\right)^{-1}\).

Eqs. \eqref{eq:Qasympt} and \eqref{eq:convgceOLS} respectively correspond to convergences in probability and in distribution.

\hypertarget{instrumental-variables}{%
\section{Instrumental Variables}\label{instrumental-variables}}

Here, we want to relax Hypothesis \ref{hyp:exogeneity} --conditional mean zero assumption, implying in particular that \(\mathbf{x}_i\) and \(\varepsilon_i\) are uncorrelated.

We consider the following model:
\begin{equation}
y_i = \mathbf{x_i}'\boldsymbol\beta + \varepsilon_i, \quad \mbox{where } \mathbb{E}(\varepsilon_i)=0  \mbox{ and } \mathbf{x_i}\not\perp \varepsilon_i.\label{eq:modelIV}
\end{equation}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}
\protect\hypertarget{def:instruments}{}\label{def:instruments}

The \(L\)-dimensional random variable \(\mathbf{z}_i\) is a \textbf{valid set of instruments} if:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \(\mathbf{z}_i\) is correlated to \(\mathbf{x}_i\);
\item
  we have \(\mathbb{E}(\boldsymbol\varepsilon|\mathbf{Z})=0\) and
\item
  the orthogonal projections of the \(\mathbf{x}_i\)s on the \(\mathbf{z}_i\)s are not multicollinear.
\end{enumerate}

\end{definition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Example. Let us make the assumption \(\mathbf{x}_i\not\perp \varepsilon_i\) (in \eqref{eq:modelIV}) more precise:
\begin{equation}
\mathbb{E}(\varepsilon_i)=0 \quad \mbox{and} \quad \mathbb{E}(\varepsilon_i \mathbf{x_i})=\boldsymbol\gamma.\label{eq:exmIV}
\end{equation}
By the law of large numbers, \(\mbox{plim}_{n \rightarrow \infty} \mathbf{X}'\boldsymbol\varepsilon / n = \boldsymbol\gamma\). If \(\mathbf{Q}_{xx} := \mbox{plim } \mathbf{X}'\mathbf{X}/n\), the OLS estimator is not consistent because
\[
\mathbf{b} = \boldsymbol\beta + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon \overset{p}{\rightarrow} \boldsymbol\beta + \mathbf{Q}_{xx}^{-1}\boldsymbol\gamma \ne \boldsymbol\beta.
\]

If \(\mathbf{z}_i\) is a valid set of instruments, we have:
\[
\mbox{plim}\left( \frac{\mathbf{Z}'\mathbf{y}}{n} \right) =\mbox{plim}\left( \frac{\mathbf{Z}'(\mathbf{X}\boldsymbol\beta + \boldsymbol\varepsilon)}{n} \right) = \mbox{plim}\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)\boldsymbol\beta
\]
Indeed, by the law of large numbers, \(\frac{\mathbf{Z}'\boldsymbol\varepsilon}{n} \overset{p}{\rightarrow}\mathbb{E}(\mathbf{z}_i\varepsilon_i)=0\).

If \(L = K\), the matrix \(\frac{\mathbf{Z}'\mathbf{X}}{n}\) is of dimension \(K \times K\) and we have:
\[
\left[\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)\right]^{-1}\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{y}}{n} \right) = \boldsymbol\beta.
\]
By continuity of the inverse funct.: \(\left[\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)\right]^{-1}=\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)^{-1}\).
The Slutsky Theorem further implies that
\[
\mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)^{-1} \mbox{plim }\left( \frac{\mathbf{Z}'\mathbf{y}}{n} \right)  = \mbox{plim }\left( \left( \frac{\mathbf{Z}'\mathbf{X}}{n} \right)^{-1} \frac{\mathbf{Z}'\mathbf{y}}{n} \right).
\]
Hence \(\mathbf{b}_{iv}\) is consistent if it is defined by:
\[
\boxed{\mathbf{b}_{iv} = (\mathbf{Z}'\mathbf{X})^{-1}\mathbf{Z}'\mathbf{y}.}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:IV}{}\label{prp:IV}If \(\mathbf{z}_i\) is a \(L\)-dimensional random variable that constitutes a valid set of instruments (see Def. \ref{def:instruments}) and if \(L=K\), then the asymptotic distribution of \(\mathbf{b}_{iv}\) is:
\[
\mathbf{b}_{iv} \overset{d}{\rightarrow} \mathcal{N}\left(\boldsymbol\beta,\frac{\sigma^2}{n}\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\right]^{-1}\right)
\]
where \(\mbox{plim } \mathbf{Z}'\mathbf{Z}/n =: \mathbf{Q}_{zz}\), \(\mbox{plim } \mathbf{Z}'\mathbf{X}/n =: \mathbf{Q}_{zx}\), \(\mbox{plim } \mathbf{X}'\mathbf{Z}/n =: \mathbf{Q}_{xz}\).
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
The proof is very similar to that of Prop. \ref{prp:asymptOLS}, the starting point being that \(\mathbf{b}_{iv} = \boldsymbol\beta + (\mathbf{Z}'\mathbf{X})^{-1}\mathbf{Z}'\boldsymbol\varepsilon\). \qed
\end{proof}

When \(L=K\), we have:
\[
\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\right]^{-1}=Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}
\]
In practice, to estimate \(\mathbb{V}ar(\mathbf{b}_{iv}) = \frac{\sigma^2}{n}Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}\), we replace \(\sigma^2\) by:
\[
s_{iv}^2 = \frac{1}{n}\sum_{i=1}^{n} (y_i - \mathbf{x}_i'\mathbf{b}_{iv})^2
\]

And when \(L > K\)? Idea: First regress \(\mathbf{X}\) on the space spanned by \(\mathbf{Z}\) and then regress \(\mathbf{y}\) on the fitted values \(\hat{\mathbf{X}}:=\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{X}\). That is \(\mathbf{b}_{iv} = (\hat{\mathbf{X}}'\hat{\mathbf{X}})^{-1}\hat{\mathbf{X}}'\mathbf{y}\):
\begin{equation}
\boxed{\mathbf{b}_{iv} = [\mathbf{X}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{X}]^{-1}\mathbf{X}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{Y}.} \label{eq:IV}
\end{equation}

In this case, Prop. \ref{prp:IV} still holds, with \(\mathbf{b}_{iv}\) given by Eq. \eqref{eq:IV}.

\(\mathbf{b}_{iv}\) is also the result of the regression of \(\mathbf{y}\) on \(\mathbf{X^*}\), where the columns of \(\mathbf{X}^*\) are the (othogonal) projections of those of \(\mathbf{X}\) on \(\mathbf{Z}\), i.e.~\(\mathbf{X^*} = \mathbf{P^{Z}X}\) (using the notations introduced in Eq. \eqref{eq:Proj}). Hence the other names of this estimator: \textbf{Two-Stage Least Squares (TSLS)}.

If the instruments do not properly satisfy Condition (a) in Def. \ref{def:instruments} (i.e.~if \(\mathbf{x}_i\) and \(\mathbf{z}_i\) are only loosely related), the instruments are said to be \textbf{weak}.

Relevant citation: \citet{Andrews_Stock_Sun_2019}.

This problem is for instance discussed in \href{http://scholar.harvard.edu/files/stock/files/testing_for_weak_instruments_in_linear_iv_regression.pdf}{Stock and Yogo (2003)}. See also \href{https://www.pearson.com/us/higher-education/product/Stock-Introduction-to-Econometrics-3rd-Edition/9780138009007.html}{Stock and Watson} pp.,489-490.

The Hausman test can be used to test if IV necessary. IV techniques are required if \(\mbox{plim}_{n \rightarrow \infty} \mathbf{X}'\boldsymbol\varepsilon / n \ne 0\). \href{http://www.jstor.org/stable/1913827?seq=1\#page_scan_tab_contents}{Hausman (1978)} proposes a test of the efficiency of estimators. Under the null hypothesis two estimators, \(\mathbf{b}_0\) and \(\mathbf{b}_1\), are consistent but \(\mathbf{b}_0\) is (asymptotically) efficient relative to \(\mathbf{b}_1\). Under the alternative hypothesis, \(\mathbf{b}_1\) (IV in the present case) remains consistent but not \(\mathbf{b}_0\) (OLS in the present case).

The test statistic is:
\[
H = (\mathbf{b}_1 - \mathbf{b}_0)' MPI(\mathbb{V}ar(\mathbf{b}_1) - \mathbb{V}ar(\mathbf{b}_0))(\mathbf{b}_1 - \mathbf{b}_0),
\]
where \(MPI\) is the Moore-Penrose pseudo-inverse. Under the null hypothesis, \(H \sim \chi^2(q)\), where \(q\) is the rank of \(\mathbb{V}ar(\mathbf{b}_1) - \mathbb{V}ar(\mathbf{b}_0)\).

\begin{example}
\protect\hypertarget{exm:priceElasticity}{}\label{exm:priceElasticity}\textbf{Estimation of price elasticity}

See e.g.~\href{http://www.who.int/tobacco/economics/2_2estimatingpriceincomeelasticities.pdf?ua=1}{WHO and estimation of tobacco price elasticity of demand}.

We want to estimate what is the effect on demand of an \emph{exogenous increase} in prices of cigarettes (say).

The model is:
\begin{eqnarray*}
\underbrace{q^d_t}_{\mbox{log(demand)}} &=& \alpha_0 + \alpha_1 \underbrace{\times p_t}_{\mbox{log(price)}} + \alpha_2 \underbrace{\times w_t}_{\mbox{income}} + \varepsilon_t^d\\
\underbrace{q^s_t}_{\mbox{log(supply)}} &=& \gamma_0 + \gamma_1 \times p_t + \gamma_2 \underbrace{\times \mathbf{y}_t}_{\mbox{cost factors}} + \varepsilon_t^s,
\end{eqnarray*}
where \(\mathbf{y}_t\), \(w_t\), \(\varepsilon_t^s \sim \mathcal{N}(0,\sigma^2_s)\) and \(\varepsilon_t^d \sim \mathcal{N}(0,\sigma^2_d)\) are independent.

Equilibrium: \(q^d_t = q^s_t\). This implies that prices are \textbf{endogenous}:
\[
p_t = \frac{\alpha_0 + \alpha_2 w_t + \varepsilon_t^d - \gamma_0 - \gamma_2 \mathbf{y}_t - \varepsilon_t^s}{\gamma_1 - \alpha_1}.
\]
In particular we have \(\mathbb{E}(p_t \varepsilon_t^d) = \frac{\sigma^2_d}{\gamma_1 - \alpha_1} \ne 0\) \(\Rightarrow\) Regressing by OLS \(q_t^d\) on \(p_t\) gives biased estimates (see Eq. \eqref{eq:exmIV}).
\end{example}

\begin{figure}
\centering
\includegraphics{AdvECTS_files/figure-latex/figureIV-1.pdf}
\caption{\label{fig:figureIV}This figure illustrates the situation prevailing when estimating a price-elasticity (and the price is endogenous).}
\end{figure}

\href{https://rpubs.com/wsundstrom/t_ivreg}{Estimation of the price elasticity of cigarette demand}. Instrument: real tax on cigarettes arising from the state's general sales tax. Presumption: in states with a larger general sales tax, cigarette prices are higher, but the general tax is not determined by other forces affecting \(\varepsilon_t^d\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"CigarettesSW"}\NormalTok{, }\AttributeTok{package =} \StringTok{"AER"}\NormalTok{)}
\NormalTok{CigarettesSW}\SpecialCharTok{$}\NormalTok{rprice  }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(CigarettesSW, price}\SpecialCharTok{/}\NormalTok{cpi)}
\NormalTok{CigarettesSW}\SpecialCharTok{$}\NormalTok{rincome }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(CigarettesSW, income}\SpecialCharTok{/}\NormalTok{population}\SpecialCharTok{/}\NormalTok{cpi)}
\NormalTok{CigarettesSW}\SpecialCharTok{$}\NormalTok{tdiff   }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(CigarettesSW, (taxs }\SpecialCharTok{{-}}\NormalTok{ tax)}\SpecialCharTok{/}\NormalTok{cpi)}

\DocumentationTok{\#\# model }
\NormalTok{fm }\OtherTok{\textless{}{-}} \FunctionTok{ivreg}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(rprice) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(rincome) }\SpecialCharTok{|} \FunctionTok{log}\NormalTok{(rincome) }\SpecialCharTok{+}\NormalTok{ tdiff }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(tax}\SpecialCharTok{/}\NormalTok{cpi),}
            \AttributeTok{data =}\NormalTok{ CigarettesSW, }\AttributeTok{subset =}\NormalTok{ year }\SpecialCharTok{==} \StringTok{"1995"}\NormalTok{)}
\NormalTok{eq.no.IV }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(rprice) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(rincome),}
               \AttributeTok{data =}\NormalTok{ CigarettesSW, }\AttributeTok{subset =}\NormalTok{ year }\SpecialCharTok{==} \StringTok{"1995"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fm, }\AttributeTok{vcov =}\NormalTok{ sandwich, }\AttributeTok{diagnostics =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## ivreg(formula = log(packs) ~ log(rprice) + log(rincome) | log(rincome) + 
##     tdiff + I(tax/cpi), data = CigarettesSW, subset = year == 
##     "1995")
## 
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -0.6006931 -0.0862222 -0.0009999  0.1164699  0.3734227 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    9.8950     0.9288  10.654 6.89e-14 ***
## log(rprice)   -1.2774     0.2417  -5.286 3.54e-06 ***
## log(rincome)   0.2804     0.2458   1.141     0.26    
## 
## Diagnostic tests:
##                  df1 df2 statistic p-value    
## Weak instruments   2  44   228.738  <2e-16 ***
## Wu-Hausman         1  44     3.823  0.0569 .  
## Sargan             1  NA     0.333  0.5641    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.1879 on 45 degrees of freedom
## Multiple R-Squared: 0.4294,  Adjusted R-squared: 0.4041 
## Wald test: 17.25 on 2 and 45 DF,  p-value: 2.743e-06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(eq.no.IV)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log(packs) ~ log(rprice) + log(rincome), data = CigarettesSW, 
##     subset = year == "1995")
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.59077 -0.07856 -0.00149  0.11860  0.35442 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   10.3420     1.0227  10.113 3.66e-13 ***
## log(rprice)   -1.4065     0.2514  -5.595 1.24e-06 ***
## log(rincome)   0.3439     0.2350   1.463     0.15    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.1873 on 45 degrees of freedom
## Multiple R-squared:  0.4327, Adjusted R-squared:  0.4075 
## F-statistic: 17.16 on 2 and 45 DF,  p-value: 2.884e-06
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fm2 }\OtherTok{\textless{}{-}} \FunctionTok{ivreg}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(rprice) }\SpecialCharTok{|}\NormalTok{ tdiff, }\AttributeTok{data =}\NormalTok{ CigarettesSW, }\AttributeTok{subset =}\NormalTok{ year }\SpecialCharTok{==} \StringTok{"1995"}\NormalTok{)}
\FunctionTok{anova}\NormalTok{(fm, fm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Model 1: log(packs) ~ log(rprice) + log(rincome) | log(rincome) + tdiff + 
##     I(tax/cpi)
## Model 2: log(packs) ~ log(rprice) | tdiff
##   Res.Df    RSS Df Sum of Sq      F Pr(>F)
## 1     45 1.5880                           
## 2     46 1.6668 -1 -0.078748 1.3815  0.246
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sem)}
\FunctionTok{data}\NormalTok{(}\StringTok{"CollegeDistance"}\NormalTok{, }\AttributeTok{package =} \StringTok{"AER"}\NormalTok{)}
\NormalTok{simple.ed}\FloatTok{.1}\NormalTok{s}\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(education }\SpecialCharTok{\textasciitilde{}}\NormalTok{ urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ distance,}
                  \AttributeTok{data =}\NormalTok{ CollegeDistance)}
\NormalTok{CollegeDistance}\SpecialCharTok{$}\NormalTok{ed.pred}\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(simple.ed}\FloatTok{.1}\NormalTok{s)}
\NormalTok{simple.ed}\FloatTok{.2}\NormalTok{s}\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ ed.pred ,}
                  \AttributeTok{data =}\NormalTok{ CollegeDistance)}

\NormalTok{simple.comp}\OtherTok{\textless{}{-}} \FunctionTok{encomptest}\NormalTok{(wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ ed.pred ,}
\NormalTok{                         wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ education ,}
                         \AttributeTok{data =}\NormalTok{ CollegeDistance)}
\NormalTok{fsttest}\OtherTok{\textless{}{-}} \FunctionTok{encomptest}\NormalTok{(education }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tuition }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ urban ,}
\NormalTok{                     education }\SpecialCharTok{\textasciitilde{}}\NormalTok{ distance ,}
                     \AttributeTok{data =}\NormalTok{ CollegeDistance)}

\NormalTok{eqOLS }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ education,}
            \AttributeTok{data=}\NormalTok{CollegeDistance)}

\FunctionTok{summary}\NormalTok{(eqOLS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = wage ~ urban + gender + ethnicity + unemp + education, 
##     data = CollegeDistance)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3484 -0.8408  0.1808  0.8119  3.9875 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(>|t|)    
## (Intercept)        8.641490   0.157008  55.039   <2e-16 ***
## urbanyes           0.070117   0.044727   1.568   0.1170    
## genderfemale      -0.085242   0.037069  -2.300   0.0215 *  
## ethnicityafam     -0.556056   0.052167 -10.659   <2e-16 ***
## ethnicityhispanic -0.544007   0.048670 -11.177   <2e-16 ***
## unemp              0.133101   0.006711  19.834   <2e-16 ***
## education          0.005369   0.010362   0.518   0.6044    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.268 on 4732 degrees of freedom
## Multiple R-squared:  0.1098, Adjusted R-squared:  0.1087 
## F-statistic: 97.27 on 6 and 4732 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(simple.ed}\FloatTok{.2}\NormalTok{s)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = wage ~ urban + gender + ethnicity + unemp + ed.pred, 
##     data = CollegeDistance)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1692 -0.8294  0.1502  0.8482  3.9537 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(>|t|)    
## (Intercept)       -0.359032   1.412087  -0.254  0.79931    
## urbanyes           0.046144   0.044691   1.033  0.30188    
## genderfemale      -0.070753   0.036978  -1.913  0.05576 .  
## ethnicityafam     -0.227240   0.072984  -3.114  0.00186 ** 
## ethnicityhispanic -0.351291   0.057021  -6.161 7.84e-10 ***
## unemp              0.139163   0.006748  20.622  < 2e-16 ***
## ed.pred            0.647099   0.100592   6.433 1.38e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.263 on 4732 degrees of freedom
## Multiple R-squared:  0.1175, Adjusted R-squared:  0.1163 
## F-statistic:   105 on 6 and 4732 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eqTSLS }\OtherTok{\textless{}{-}} \FunctionTok{tsls}\NormalTok{(wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ education,}
               \SpecialCharTok{\textasciitilde{}}\NormalTok{ urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ distance,}
               \AttributeTok{data=}\NormalTok{CollegeDistance)}

\NormalTok{eqTSLS }\OtherTok{\textless{}{-}} \FunctionTok{ivreg}\NormalTok{(wage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ education}\SpecialCharTok{|}
\NormalTok{                  urban }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ ethnicity }\SpecialCharTok{+}\NormalTok{ unemp }\SpecialCharTok{+}\NormalTok{ distance,}
                \AttributeTok{data=}\NormalTok{CollegeDistance)}

\FunctionTok{summary}\NormalTok{(eqTSLS, }\AttributeTok{vcov =}\NormalTok{ sandwich, }\AttributeTok{diagnostics =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## ivreg(formula = wage ~ urban + gender + ethnicity + unemp + education | 
##     urban + gender + ethnicity + unemp + distance, data = CollegeDistance)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -5.20896 -1.14578 -0.02361  1.33303  4.77571 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(>|t|)    
## (Intercept)       -0.35903    1.91755  -0.187   0.8515    
## urbanyes           0.04614    0.05926   0.779   0.4362    
## genderfemale      -0.07075    0.04974  -1.422   0.1550    
## ethnicityafam     -0.22724    0.09539  -2.382   0.0172 *  
## ethnicityhispanic -0.35129    0.07577  -4.636 3.64e-06 ***
## unemp              0.13916    0.00934  14.899  < 2e-16 ***
## education          0.64710    0.13691   4.727 2.35e-06 ***
## 
## Diagnostic tests:
##                   df1  df2 statistic  p-value    
## Weak instruments    1 4732     50.19 1.60e-12 ***
## Wu-Hausman          1 4731     40.30 2.38e-10 ***
## Sargan              0   NA        NA       NA    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.706 on 4732 degrees of freedom
## Multiple R-Squared: -0.6118, Adjusted R-squared: -0.6138 
## Wald test: 57.08 on 6 and 4732 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{general-regression-model}{%
\section{General Regression Model}\label{general-regression-model}}

We want to relax the assumption according to which the disturbances are uncorrelated with each other (Hypothesis @ref(hyp:noncorrel\_resid)) or the homoskedasticity Hypothesis \ref{hyp:homoskedasticity}.

We replace the latter two assumptions by the general formulation:
\begin{eqnarray}
\mathbb{E}(\boldsymbol\varepsilon \boldsymbol\varepsilon'| \mathbf{X}) &=& \boldsymbol\Sigma. \label{eq:assumGLS2}
\end{eqnarray}

Note that Eq. (\eqref{eq:assumGLS2}) is more general than Hypothesis \ref{hyp:homoskedasticity} and @ref(hyp:noncorrel\_resid) because the diagonal entries of \(\boldsymbol\Sigma\) may be different (not the case under Hypothesis \ref{hyp:homoskedasticity}), and the non-diagonal entries of \(\boldsymbol\Sigma\) can be \(\ne 0\) (contrary to Hypothesis \ref{hyp:noncorrelResid}).

\begin{definition}
\protect\hypertarget{def:GRM}{}\label{def:GRM}Hypothesis \ref{hyp:fullrank} and \ref{hyp:exogeneity}, together with Eq. \eqref{eq:assumGLS2}, form the \textbf{General Regression Model} (GRM) framework.
\end{definition}

Note that a regression model where Hypotheses \ref{hyp:fullrank} to \ref{hyp:noncorrelResid} hold is a specific case of the GRM framework.

The GRM context notably allows to model \textbf{heteroskedasticity} and \textbf{autocorrelation}.

\begin{itemize}
\tightlist
\item
  Heteroskedasticity:
  \begin{equation}
  \boldsymbol\Sigma = \left[  \begin{array}{cccc}
  \sigma_1^2 & 0 & \dots & 0 \\
  0 & \sigma_2^2 &  & 0 \\
  \vdots && \ddots& \vdots \\
  0 & \dots & 0 & \sigma_n^2
  \end{array} \right]. \label{eq:heteroskedasticity}
  \end{equation}
\item
  Autocorrelation:
  \begin{equation}
  \boldsymbol\Sigma = \sigma^2 \left[ \begin{array}{cccc}
  1 & \rho_{2,1} & \dots & \rho_{n,1} \\
  \rho_{2,1} & 1 &  & \vdots \\
  \vdots && \ddots& \rho_{n,n-1} \\
  \rho_{n,1} & \rho_{n,2} & \dots & 1
  \end{array} \right]. \label{eq:autocorrelation}
  \end{equation}
\end{itemize}

\begin{example}
\protect\hypertarget{exm:autocorrelaaa}{}\label{exm:autocorrelaaa}Autocorrelation is, in particular, a recurrent problem when time-series data are used (see Section @ref(section:TS\}).

In a time-series context, subscript \(i\) refers to a date. Assume for instance that:
\begin{equation}
y_i = \mathbf{x}_i' \boldsymbol\beta + \varepsilon_i \label{eq:usual}
\end{equation}
with
\begin{equation}
\varepsilon_i = \rho \varepsilon_{i-1} + v_i, \quad v_i \sim \mathcal{N}(0,\sigma_v^2).\label{eq:usual2}
\end{equation}
In this case, we are in the GRM context, with:
\begin{equation}\label{eq:SigmaAutocorrel}
\boldsymbol\Sigma =\frac{ \sigma_v^2}{1 - \rho^2} \left[    \begin{array}{cccc}
1 & \rho & \dots & \rho^{n-1} \\
\rho & 1 &  & \vdots \\
\vdots && \ddots& \rho \\
\rho^{n-1} & \rho^{n-2} & \dots & 1
\end{array} \right].
\end{equation}
\end{example}

\hypertarget{generalized-least-squares}{%
\subsection{Generalized Least Squares}\label{generalized-least-squares}}

Assume \(\boldsymbol\Sigma\) is known (``feasible GLS'\,'). Because \(\boldsymbol\Sigma\) is symmetric positive, it admits a spectral decomposition of the form \(\boldsymbol\Sigma = \mathbf{C} \boldsymbol\Lambda \mathbf{C}'\), where \(\mathbf{C}\) is an orthogonal matrix (i.e.~\(\mathbf{C}\mathbf{C}'=Id\)) and \(\boldsymbol\Lambda\) is a diagonal matrix (the diagonal entries are the eigenvalues of \(\boldsymbol\Sigma\)).

We have \(\boldsymbol\Sigma = (\mathbf{P}\mathbf{P}')^{-1}\) with \(\mathbf{P} = \mathbf{C}\boldsymbol\Lambda^{-1/2}\).

Consider the transformed model:
\[
\mathbf{P}'\mathbf{y} = \mathbf{P}'\mathbf{X}\boldsymbol\beta + \mathbf{P}'\boldsymbol\varepsilon \quad \mbox{or} \quad \mathbf{y}^* = \mathbf{X}^*\boldsymbol\beta + \boldsymbol\varepsilon^*.
\]
The variance of \(\boldsymbol\varepsilon^*\) is \(\mathbf{I}\). In the transformed model, OLS is BLUE (Gauss-Markow Theorem \ref{thm:GaussMarkov}).

The \textbf{Generalized least squares} estimator of \(\boldsymbol\beta\) is:
\begin{equation}
\boxed{\mathbf{b}_{GLS} = (\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{y}}.\label{eq:betaGLS}
\end{equation}
We have:
\[
\mathbb{V}ar(\mathbf{b}_{GLS}|\mathbf{X}) = (\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{X})^{-1}.
\]

When \(\boldsymbol\Sigma\) is unknown, the GLS estimator is said to be \emph{infeasible}. Some structure is required. Assume \(\boldsymbol\Sigma\) admits a parametric form \(\boldsymbol\Sigma(\theta)\). The estimation becomes \emph{feasible} (FGLS) if one replaces \(\boldsymbol\Sigma(\theta)\) by \(\boldsymbol\Sigma(\hat\theta)\).

If \(\hat\theta\) is a consistent estimator of \(\theta\), then the FGLS is asymptotically efficient (see Example \ref{exm:autocorrelaa}).

By contrast, when \(\boldsymbol\Sigma\) has no obvious structure: the OLS (or IV) is the only estimator available. It remains unbiased, consistent, and asymptotically normally distributed, but not efficient. Standard inference procedures are not appropriate any longer.

Autocorrelation in the time-series context. Consider the case presented in Example \ref{exm:autocorrelaaa}. Because the OLS estimate \(\mathbf{b}\) of \(\boldsymbol\beta\) is consistent, the estimates \(e_i\)s of the \(\varepsilon_i\)s also are. Consistent estimators of \(\rho\) and \(\sigma_v\) are then obtained by regressing the \(e_i\)s on the \(e_{i-1}\)s. Using these estimates in Eq. \eqref{eq:SigmaAutocorrel} provides a consistent estimate of \(\boldsymbol\Sigma\).

See \href{https://www.tandfonline.com/doi/abs/10.1080/01621459.1949.10483290}{Cochrane and Orcutt (2012)}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:AsymptOLSGRM}{}\label{prp:AsymptOLSGRM}Conditionally on \(\mathbf{X}\), we have:
\begin{equation}
\mathbb{V}ar(\mathbf{b}|\mathbf{X}) = \frac{1}{n}\left(\frac{1}{n}\mathbf{X}'\mathbf{X}\right)^{-1}\left(\frac{1}{n}\mathbf{X}'\boldsymbol\Sigma\mathbf{X}\right)\left(\frac{1}{n}\mathbf{X}'\mathbf{X}\right)^{-1}.\label{eq:xsx}
\end{equation}
Under Hypothesis \ref{hyp:normality}, since \(\mathbf{b}\) is linear in \(\boldsymbol\varepsilon\), we have:
\begin{equation}
\mathbf{b}|\mathbf{X} \sim \mathcal{N}\left(\boldsymbol\beta,\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\mathbf{X}'\boldsymbol\Sigma\mathbf{X}\right)\left(\mathbf{X}'\mathbf{X}\right)^{-1}\right).
\end{equation}
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Note that the variance of the estimator is not \(\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\) any more, so using \(s^2 (\mathbf{X}'\mathbf{X})^{-1}\) for inference may be misleading.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:XXX}{}\label{prp:XXX}If \(\mbox{plim }(\mathbf{X}'\mathbf{X}/n)\) and \(\mbox{plim }(\mathbf{X}'\boldsymbol\Sigma\mathbf{X}/n)\) are finite positive definite matrices, then \(\mbox{plim }(\mathbf{b})=\boldsymbol\beta\).
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
We have \(\mathbb{V}ar(\mathbf{b})=\mathbb{E}[\mathbb{V}ar(\mathbf{b}|\mathbf{X})]+\mathbb{V}ar[\mathbb{E}(\mathbf{b}|\mathbf{X})]\). Since \(\mathbb{E}(\mathbf{b}|\mathbf{X})=\boldsymbol\beta\), \(\mathbb{V}ar[\mathbb{E}(\mathbf{b}|\mathbf{X})]=0\). Eq. \eqref{eq:xsx} implies that \(\mathbb{V}ar(\mathbf{b}|\mathbf{X}) \rightarrow 0\). Hence \(\mathbf{b}\) converges in mean square and therefore in probability. \(\square\)
\end{proof}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:AsymptGRM}{}\label{prp:AsymptGRM}If \(Q_{xx}=\mbox{plim }(\mathbf{X}'\mathbf{X}/n)\) and \(Q_{x\boldsymbol\Sigma x}=\mbox{plim }(\mathbf{X}'\boldsymbol\Sigma\mathbf{X}/n)\) are finite positive definite matrices, then:
\[
\sqrt{n}(\mathbf{b}-\boldsymbol\beta) \overset{d}{\rightarrow} \mathcal{N}(0,Q_{xx}^{-1}Q_{x\boldsymbol\Sigma x}Q_{xx}^{-1}).
\]
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:AsymptIVGRM}{}\label{prp:AsymptIVGRM}If regressors and IV variables are ``well-behaved'\,', then:
\[
\mathbf{b}_{iv} \overset{a}{\sim} \mathcal{N}(\boldsymbol\beta,\mathbf{V}_{iv}),
\]
where
\[
\mathbf{V}_{iv} = \frac{1}{n}(\mathbf{Q}^*)\mbox{ plim }\left( \frac{1}{n} \mathbf{Z}'\boldsymbol\Sigma \mathbf{Z}\right)(\mathbf{Q}^*)',
\]
with
\[
\mathbf{Q}^* = [\mathbf{Q}_{xz}\mathbf{Q}_{zz}^{-1}\mathbf{Q}_{zx}]^{-1}\mathbf{Q}_{xz}\mathbf{Q}_{zz}^{-1}.
\]
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For practical purposes, one needs to have estimates of \(\boldsymbol\Sigma\) in Props. \ref{prp:AsymptOLSGRM}, \ref{prp:AsymptGRM} or \ref{prp:AsymptIVGRM}.

Idea: instead of estimating \(\boldsymbol\Sigma\) (dimension \(n \times n\)) directly, one can estimate \(\frac{1}{n}\mathbf{X}'\boldsymbol\Sigma\mathbf{X}\), of dimension \(K \times K\) (or \(\frac{1}{n}\mathbf{Z}'\boldsymbol\Sigma\mathbf{Z}\) in the IV case). Indeed, this is this expression (\(\mathbf{X}'\boldsymbol\Sigma\mathbf{X}\)) that eventually appears in the formulas -- for instance in Eq. \eqref{eq:xsx}.

We have:
\begin{equation}
\frac{1}{n}\mathbf{X}'\boldsymbol\Sigma\mathbf{X} = \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\sigma_{i,j}\mathbf{x}_i\mathbf{x}'_j. \label{eq:GeneralXSigmaX}
\end{equation}

\textbf{Robust estimation of asymptotic covariance matrices} look for estimates of the previous matrix. Their computation is based on the fact that if \(\mathbf{b}\) is consistent, then the \(e_i\)s are consistent (pointwise) estimators of the \(\varepsilon_i\)s.

\begin{example}
\protect\hypertarget{exm:heteroskedasticity}{}\label{exm:heteroskedasticity}\textbf{Heteroskedasticity}.

This is the case of Eq. \eqref{eq:heteroskedasticity}.

We then need to estimate \(\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}^2\mathbf{x}_i\mathbf{x}'_i\). \href{http://www.jstor.org/stable/1912934}{White (1980)}: Under general conditions:
\begin{equation}
\mbox{plim}\left( \frac{1}{n}\sum_{i=1}^{n}\sigma_{i}^2\mathbf{x}_i\mathbf{x}'_i \right) = 
\mbox{plim}\left( \frac{1}{n}\sum_{i=1}^{n}e_{i}^2\mathbf{x}_i\mathbf{x}'_i \right). \label{eq:white}
\end{equation}
The estimator of \(\frac{1}{n}\mathbf{X}'\boldsymbol\Sigma\mathbf{X}\) therefore is:
\begin{equation}
\frac{1}{n}\mathbf{X}'\mathbf{E}^2\mathbf{X},\label{eq:White}
\end{equation}
where \(\mathbf{E}\) is an \(n \times n\) diagonal matrix whose diagonal elements are the estimated residuals \(e_i\).

Illustration: Figure \ref{fig:exmpSalarayPhD}.
\end{example}

Let us illustrate the influence of heteroskedasticity using simulations.

We consider the following model:
\[
y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2).
\]
where the \(x_i\)s are i.i.d. \(t(4)\).

Here is a simulated sample (\(n=200\)) of this model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{200}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rt}\NormalTok{(n,}\AttributeTok{df=}\DecValTok{5}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ x}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(n)}
\FunctionTok{plot}\NormalTok{(x,y,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{AdvECTS_files/figure-latex/simulHeterosk-1.pdf}

We simulate 1000 samples of the same model with \(n=200\). For each sample, we compute the OLS estimate of \(\beta\) (=1). Using these 1000 estimates of \(b\), we construct an approximated \emph{(kernel-based) distribution of this OLS estimator} (in red on the figure).

For each of the 1000 OLS estimations, we employ \emph{the standard OLS variance formula (\(s^2 (\mathbf{X}'\mathbf{X})^{-1}\))} to estimate the variance of \(b\). The blue curve is a normal distribution centred on 1 and whose variance is the average of the 1000 previous variance estimates.

The variance of the simulated \(b\) is of 0.040 (that is the \emph{true} one); the average of the estimated variances based on the standard OLS formula is of 0.005 (\emph{bad} estimate); the average of the estimated variances based on the White robust covariance matrix is of 0.030 (better estimate).

The standard OLS formula for the variance of \(b\) overestimates the precision of this estimator.

For almost 50\% of the simulations, 1 is not included in the 95\% confidence interval of \(\beta\) when the computation of the interval is based on the standard OLS formula for the variance of \(b\).

When the White robust covariance matrix is used, 1 is not in the 95\% confidence interval of \(\beta\) for less than 10\% of the simulations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{200}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{XX }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rt}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{N,}\AttributeTok{df=}\DecValTok{5}\NormalTok{),n,N)}
\NormalTok{YY }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(XX }\SpecialCharTok{+}\NormalTok{ XX}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(n),n,N)}
\NormalTok{all\_b       }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{all\_V\_OLS   }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{all\_V\_White }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{  Y }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(YY[,j],}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(XX[,j],}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{)}
\NormalTok{  b }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{X) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{Y}
\NormalTok{  e }\OtherTok{\textless{}{-}}\NormalTok{ Y }\SpecialCharTok{{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ b}
\NormalTok{  S }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%} \FunctionTok{diag}\NormalTok{(}\FunctionTok{c}\NormalTok{(e}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%*\%}\NormalTok{ X}
\NormalTok{  V\_OLS   }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{X) }\SpecialCharTok{*} \FunctionTok{var}\NormalTok{(e)}
\NormalTok{  V\_White }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{n }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{solve}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n}\SpecialCharTok{*}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{X)) }\SpecialCharTok{\%*\%}\NormalTok{ S }\SpecialCharTok{\%*\%}\NormalTok{ (}\FunctionTok{solve}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n}\SpecialCharTok{*}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{X))}
  
\NormalTok{  all\_b       }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_b,b)}
\NormalTok{  all\_V\_OLS   }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_V\_OLS,V\_OLS)}
\NormalTok{  all\_V\_White }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_V\_White,V\_White)}
\NormalTok{\}}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{density}\NormalTok{(all\_b))}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{mean}\NormalTok{(all\_b),}\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\AttributeTok{by=}\NormalTok{.}\DecValTok{01}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x,}\FunctionTok{dnorm}\NormalTok{(x,}\AttributeTok{mean =} \DecValTok{1}\NormalTok{,}\AttributeTok{sd =} \FunctionTok{mean}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(all\_V\_OLS))),}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x,}\FunctionTok{dnorm}\NormalTok{(x,}\AttributeTok{mean =} \DecValTok{1}\NormalTok{,}\AttributeTok{sd =} \FunctionTok{mean}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(all\_V\_White))),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{AdvECTS_files/figure-latex/simulHeterosk2-1.pdf}

\hypertarget{heteroskedasticity-and-autocorrelation-hac}{%
\subsection{Heteroskedasticity and Autocorrelation (HAC)}\label{heteroskedasticity-and-autocorrelation-hac}}

This includes the cases of Eqs. \eqref{eq:heteroskedasticity} and \eqref{eq:autocorrelation}.

\href{http://www.jstor.org/stable/1913610}{Newey and West (1987)}: If the correlation between terms \(i\) and \(j\) gets sufficiently small when \(|i-j|\) increases:
\begin{eqnarray}
&&\mbox{plim} \left( \frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\sigma_{i,j}\mathbf{x}_i\mathbf{x}'_j \right) =  \\
&&\mbox{plim} \left( \frac{1}{n}\sum_{t=1}^{n}e_{t}^2\mathbf{x}_t\mathbf{x}'_t +
\frac{1}{n}\sum_{\ell=1}^{L}\sum_{t=\ell+1}^{n}w_\ell e_{t}e_{t-\ell}(\mathbf{x}_t\mathbf{x}'_{t-\ell} + \mathbf{x}_{t-\ell}\mathbf{x}'_{t})
\right) \nonumber \label{eq:NW}
\end{eqnarray}
where \(w_\ell = 1 - \ell/(L+1)\).

Let us illustrate the influence of autocorrelation using simulations.

We consider the following model:
\begin{equation}
y_i = x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,x_i^2),\label{eq:simul11}
\end{equation}
where the \(x_i\)s and the \(\varepsilon_i\)s are such that:
\begin{equation}
x_i = 0.8 x_{i-1} + u_i \quad and \quad \varepsilon_i = 0.8 \varepsilon_{i-1} + v_i, \label{eq:simul22}
\end{equation}
where the \(u_i\)s and the \(v_i\)s are i.i.d. \(\mathcal{N}(0,1)\).

Here is a simulated sample (\(n=200\)) of this model:

We simulate 1000 samples of the same model with \(n=200\).

For each sample, we compute the OLS estimate of \(\beta\) (=1).

Using these 1000 estimates of \(b\), we construct an approximated (kernel-based) distribution of this OLS estimator (in red on the figure).

For each of the 1000 OLS estimations, we employ the standard OLS variance formula (\(s^2 (\mathbf{X}'\mathbf{X})^{-1}\)) to estimate the variance of \(b\). The blue curve is a normal distribution centred on 1 and whose variance is the average of the 1000 previous variance estimates.

The variance of the simulated \(b\) is of 0.020 (that is the \emph{true} one); the average of the estimated variances based on the standard OLS formula is of 0.005 (\emph{bad} estimate); the average of the estimated variances based on the White robust covariance matrix is of 0.015 (\emph{better} estimate).

The standard OLS formula for the variance of \(b\) overestimates the precision of this estimator.

For about 35\% of the simulations, 1 is not included in the 95\% confidence interval of \(\beta\) when the computation of the interval is based on the standard OLS formula for the variance of \(b\).

When the Newey-West robust covariance matrix is used, 1 is not in the 95\% confidence interval of \(\beta\) for about 13\% of the simulations.

For the sake of comparison, let us consider a model with no auto-correlation (\(x_i \sim i.i.d. \mathcal{N}(0,2.8)\) and \(\varepsilon_i \sim i.i.d. \mathcal{N}(0,2.8)\)).

\hypertarget{how-to-detect-autocorrelation-in-residuals}{%
\subsection{How to detect autocorrelation in residuals?}\label{how-to-detect-autocorrelation-in-residuals}}

Consider the usual regression (say Eq. \eqref{eq:usual}).

The \textbf{Durbin-Watson test} is a typical autocorrelation test. Its test statistic is:
\[
DW = \frac{\sum_{i=2}^{n}(e_i - e_{i-1})^2}{\sum_{i=1}^{n}e_i^2}= 2(1 - r) - \underbrace{\frac{e_1^2 + e_n^2}{\sum_{i=1}^{n}e_i^2}}_{\overset{p}{\rightarrow} 0},
\]
where \(r\) is the slope in the regression of the \(e_i\)s on the \(e_{i-1}\)s, i.e.:
\[
r = \frac{\sum_{i=2}^{n}e_i e_{i-1}}{\sum_{i=1}^{n-1}e_i^2}.
\]
(\(r\) is a consistent estimator of \(\mathbb{C}or(\varepsilon_i,\varepsilon_{i-1})\), i.e.~\(\rho\) in Eq. \eqref{eq:usual2}.)

Critical values depend only on T and K: see e.g.~\href{http://web.stanford.edu/~clint/bench/dwcrit.htm}{tables} CHECK.

The one-sided test for \(H_0\): \(\rho=0\) against \(H_1\): \(\rho>0\) is carried out by comparing \(DW\) to values \(d_L(T, K)\) and \(d_U(T, K)\):
\[
\left\{
\begin{array}{ll}
\mbox{If $DW < d_L$,}&\mbox{ the null hypothesis is rejected;}\\
\mbox{if $DW > d_U$,}&\mbox{ the hypothesis is not rejected;}\\
\mbox{If $d_L \le DW \le d_U$,} &\mbox{ no conclusion is drawn.}
\end{array}
\right.
\]

\hypertarget{summary}{%
\section{Summary}\label{summary}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1399}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1062}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1580}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1192}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1606}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1244}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1917}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Under Assumptions \ref{hyp:fullrank}+
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\mathbf{b}\) normal in small sample (Eq. \eqref{eq:distriBcondi})
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\mathbf{b}\) is BLUE (Thm \ref{thm:GaussMarkov})
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\mathbf{b}\) unbiased in small sample (Prop. \ref{prp:propOLS})
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\mathbf{b}\) consistent (Prop. \ref{prp:XXX})\(^*\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\mathbf{b}\) \(\sim\) normal in large sample (Prop. \ref{prp:AsymptGRM})\(^*\)
\end{minipage} \\
\midrule()
\endhead
\rotatebox[origin=c]{90}{ Condit. mean-zero} & \ref{hyp:exogeneity} & X & X & X & X & X \\
\rotatebox[origin=c]{90}{ Homoskedasticity} & \ref{hyp:homoskedasticity} & X & X & & & \\
\rotatebox[origin=c]{90}{Uncorrelated residuals} & \ref{hyp:noncorrelResid} & X & X & & & \\
\rotatebox[origin=c]{90}{ Normality of disturbances} & \ref{hyp:normality} & X & & & & \\
\bottomrule()
\end{longtable}

\(^*\): see however Prop. \ref{prp:XXX} and Prop. \ref{prp:AsymptGRM} for additional hypotheses. Specifically \(\mathbf{X}'\mathbf{X}/n\) and \(\mathbf{X}'\boldsymbol{\Sigma}\mathbf{X}/n\) must converge in proba. to finite positive definite matrices (\(\boldsymbol\Sigma\) is defined in Eq. \eqref{eq:assumGLS2}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OtherTok{\textless{}{-}} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

Alors, combien vaut \texttt{a}? Answer: 1.

\hypertarget{clusters}{%
\section{Clusters}\label{clusters}}

\href{https://www.sciencedirect.com/science/article/pii/S0304407622000781\#da1}{MacKinnon, Nielsen, and Webb (2022)}

A nice reference is \citet{MACKINNON2022}

Another one is \citet{Cameron_Miller_2014}

See package \href{https://cran.r-project.org/web/packages/fwildclusterboot/vignettes/fwildclusterboot.html}{fwildclusterboot} for wild cluster bootstrap.

XXXXXX

Based on \citet{MACKINNON2022}:

We have:
\begin{equation}
\mathbf{b} = \boldsymbol\beta + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon.\label{eq:BBB}
\end{equation}
Consider a set \(\{n_1,n_2,\dots,n_G\}\) s.t. \(n=\sum_g n_g\), on which is based the following decomposition of \(\mathbf{X}\):
\[
\mathbf{X} = \left[
\begin{array}{c}
\mathbf{X}_1 \\
\mathbf{X}_1 \\
\vdots\\
\mathbf{X}_G
\end{array}
\right].
\]
With these notations, Eq. \eqref{eq:BBB} rewrites:
\begin{equation}
\mathbf{b} - \boldsymbol\beta = \left(\sum_{g=1}^G \mathbf{X}_g'\mathbf{X}_g\right)^{-1}\mathbf{X}'\sum_{g=1}^G \mathbf{s}_g,\label{eq:cluster1}
\end{equation}
where \(\mathbf{s}_g = \mathbf{X}_g'\boldsymbol\varepsilon_g\) denotes the score vector (of dimension \(K \times 1\)) associoated with the \(g^{th}\) cluster.

If the model is correctly specified then \(\mathbb{E}(\mathbf{s}_g))0\) for all clusters \(g\). Note that Eq. \eqref{eq:cluster1} is valid for any partition of \(\{1,\dots,n\}\). Nevertheless, dividing the sample into \textbf{clusters} really becomes meaningful if we assume that the following hypothesis holds:

\begin{hypothesis}
\protect\hypertarget{hyp:cluster}{}\label{hyp:cluster}We have:
\[
(i)\; \mathbb{E}(\mathbf{s}_g\mathbf{s}_g')=\Sigma_g,\quad (ii)\; \mathbb{E}(\mathbf{s}_g\mathbf{s}_q')=0,\;g \ne q.
\]
\end{hypothesis}

The real assumotion here is \((ii)\). The first one simply gives a notation for the covariance matrix of the score assiciated with the \(g^{th}\) cluster. Remark that these covariance matrices can differ across clusters. That is, cluster-based inference is robust against both heteroskedasticity and intra-cluster dependence without imposing any restrictions on the (unknown) form of either of them.

While the choice of clustering structure is sometimes debatable, the structure is generally assumed known in both theoretical and applied work.

Matrix \(\Sigma_g\) depends on the covariance structure of the \(\varepsilon\)'s. In particular, if \(\Omega_g = \mathbb{E}(\boldsymbol\varepsilon_g\boldsymbol\varepsilon_g'|\mathbf{X}_g)\), then we have \(\Sigma_g = \mathbb{E}(\mathbf{X}_g'\Omega_g\mathbf{X}_g)\).

Under Hypothesis \ref{hyp:cluster}, it comes that the covariance matrix of \(\mathbf{b}\) is:
\begin{equation}
\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\sum_{g=1}^G \Sigma_g\right)\left(\mathbf{X}'\mathbf{X}\right)^{-1}\label{eq:cluster2}
\end{equation}

Let us denote by \(\varepsilon_{g,i}\) the error associated with the \(i^{th}\) component of vector \(\boldsymbol\varepsilon_g\). Consider the special case where \(\mathbb{E}(\varepsilon_{g,i} \varepsilon_{g,j}|\mathbf{X}_g)=\sigma^2\mathbb{I}_{\{i=j\}}\), then Eq. \eqref{eq:cluster2} gives the standard expression \(\sigma^2\left(\mathbf{X}'\mathbf{X}\right)^{-1}\).

If we have \(\mathbb{E}(\varepsilon_{gi} \varepsilon_{gj}|\mathbf{X}_g)=\sigma_{gi}^2\mathbb{I}_{\{i=j\}}\), then we fall in the case addressed by the White formula (see Eq. \eqref{eq:White}), i.e.:
\[
\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\mathbf{X}'\left[  \begin{array}{cccc}
\sigma_1^2 & 0 & \dots & 0 \\
0 & \sigma_2^2 &  & 0 \\
\vdots && \ddots& \vdots \\
0 & \dots & 0 & \sigma_n^2
\end{array} \right]\mathbf{X}\right)\left(\mathbf{X}'\mathbf{X}\right)^{-1}.
\]
The natural way to estimate Eq. \eqref{eq:cluster2} consists in replacing the \(\Sigma_g\) by their sample equivalent, i.e.~\(\widehat{\Sigma}_g=\mathbf{X}_g'\mathbf{e}_g\mathbf{e}_g'\mathbf{X}_g\). Adding corrections for the degrees of freedom, this leads to the following estimate of the covariance matrix of \(\mathbf{b}\):
\begin{equation}
\frac{G(n-1)}{(G-1)(n-K)}\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\sum_{g=1}^G\widehat{\Sigma}_g\right) \left(\mathbf{X}'\mathbf{X}\right)^{-1}. \label{eq:AsymptCL}
\end{equation}
The previous estimate is CRCV1 in \citet{MACKINNON2022}.

Note that we indeed find the White estimator when \(G=n\) (see Eq. \eqref{eq:White}).

Remark, if only one cluster, and neglecting the degree-of-freedom correction, we would have, for \(G=1\):
\[
\left(\mathbf{X}'\mathbf{X}\right)^{-1}\left(\mathbf{X}'\mathbf{e}\mathbf{e}'\mathbf{X}\right) \left(\mathbf{X}'\mathbf{X}\right)^{-1} = 0
\]
because \(\mathbf{X}'\mathbf{e}=0\). Hence, large clusters not necessarily increase variance.

\hypertarget{two-way-clustering}{%
\subsection{Two-way clustering}\label{two-way-clustering}}

Let's add a second dimension to the data (e.g., time). There are now two partitions of the data: one through index \(g\), with \(g \in \{1,\dots,G\}\), and the other through index \(h\), with \(h \in \{1,\dots,H\}\). Accordingly, we denote by \(\mathbf{X}_{g,h}\) the submatrix of \(\mathbf{X}\) that contains the explanatory variables corresponding to clusters \(g\) and \(h\) (e.g., the firms of a given country \(g\) at a given date \(h\)). We also denote by \(\mathbf{X}_{g,\bullet}\) (respectively \(\mathbf{X}_{\bullet,h}\)) the submatrix of \(\mathbf{X}\) containing all explanatory variables pertaining to cluster \(g\), for all possible values of \(h\) (resp. to cluster \(h\), for all possible values of \(g\)).

Consider the follwing hypothesis:

\begin{hypothesis}
\protect\hypertarget{hyp:twowaycluster}{}\label{hyp:twowaycluster}We have:
\begin{eqnarray*}
&&\mathbb{E}(\mathbf{s}_{g,\bullet}\mathbf{s}_{g,\bullet}')=\Sigma_g,\quad \mathbb{E}(\mathbf{s}_{\bullet,h}\mathbf{s}_{\bullet,h}')=\Sigma^*_h,\quad \mathbb{E}(\mathbf{s}_{g,h}\mathbf{s}_{g,h}')=\Sigma_{g,h},\\ &&\mathbb{E}(\mathbf{s}_{g,h}\mathbf{s}_{q,k}')=0\;\mbox{if }g\neq q\mbox{ and }h \ne k.
\end{eqnarray*}
\end{hypothesis}

Under this assumption, the matrix of covariance of the scores is given by:
\[
\Sigma = \sum_{g=1}^G \Sigma_{g} + \sum_{h=1}^H \Sigma^*_{h} - \sum_{g=1}^G\sum_{h=1}^H \Sigma_{g,h}.
\]
The last term on the right-hand side must be subtracted in order to avoid double counting.

\begin{proof}
We have:
\begin{eqnarray*}
\Sigma &=& \sum_{g=1}^G\sum_{q=1}^G\sum_{h=1}^H\sum_{k=1}^H \mathbf{s}_{g,h}\mathbf{s}_{q,k}'\\
&=& \sum_{g=1}^G\underbrace{\left(\sum_{h=1}^H\sum_{k=1}^H \mathbf{s}_{g,h}\mathbf{s}_{g,k}'\right)}_{=\Sigma_g}+\sum_{h=1}^H\underbrace{\left(\sum_{g=1}^G\sum_{q=1}^G \mathbf{s}_{g,h}\mathbf{s}_{q,h}'\right)}_{=\Sigma^*_h}-\sum_{g=1}^G\sum_{h=1}^H \mathbf{s}_{g,h}\mathbf{s}_{g,h}',
\end{eqnarray*}
which gives the result.
\end{proof}

The asymptotic theory can be based on tow different approaches: (i) large number of clusters (common case), and (ii) fixed number of clusters but large number of observations in each cluster (see SUbsections 4.1 and 4.2 in \citet{MACKINNON2022}). The more variable the \(N_g\)'s, the less reliable asymptotic inference based on Eq. \eqref{eq:AsymptCL}, especially when a very few clusters are unusually large, or when the distribution of the data is heavy-tailed (has fewer moments). These issues are somehow mitigated when the clusters have an approximate factor structure.

In practice, \(\Sigma\) is estimated by:
\[
\widehat{\Sigma} = \sum_{g=1}^G \widehat{\mathbf{s}}_{g,\bullet}\widehat{\mathbf{s}}_{g,\bullet}' + \sum_{h=1}^H \widehat{\mathbf{s}}_{\bullet,h}\widehat{\mathbf{s}}_{\bullet,h} - \sum_{g=1}^G\sum_{h=1}^H \widehat{\mathbf{s}}_{g,h}\widehat{\mathbf{s}}_{g,h}',
\]
and we then use:
\[
\widehat{\mathbb{V}ar}(\mathbf{b}) = \left(\mathbf{X}'\mathbf{X}\right)^{-1}\widehat{\Sigma}\left(\mathbf{X}'\mathbf{X}\right)^{-1}.
\]

As an alternative to the asymptotic approximation to the distribution of a statistic of interest, one can resort to bootstrap approximation (see Section 5 of \citet{MACKINNON2022}). In R, the packge \texttt{fwildclusterboot} allows to implement such approaches (see, e.g., \href{https://cran.r-project.org/web/packages/fwildclusterboot/vignettes/fwildclusterboot.html}{this tutorial by Alexander Fischer}).

\hypertarget{shrinkage-method}{%
\section{Shrinkage method}\label{shrinkage-method}}

Chosing the right variables is often a compicated matter, especially in the presence of many potentially relevant covariates. Keeping a large number of covariates results in large standard deviations for the estimated parameters. In order to address this issue, shrinkage methods have been designed. The objective of these methods is to help to select of a limited number of variables (by shrinking the regression coefficients of the less useful variables towards zero). The two best-known shrinkage techniques are ridge regression and the lasso.

\citet{James2013} (Chapter 6.2)

Example: use credit data (interest rates on XXXX)?

\hypertarget{panel-regressions}{%
\chapter{Panel regressions}\label{panel-regressions}}

The standard panel situation is the following: we have a lot of entities (\(i \in \{1,\dots,n\}\)) and, for each entity, we observe different variables over a small number of periods (\(t \in \{1,\dots,T\}\)). This is a \emph{longitudinal dataset}.

The regression then reads:
\begin{equation}
y_{i,t} = \mathbf{x}'_{i,t}\underbrace{\boldsymbol\beta}_{K \times 1} + \underbrace{\mathbf{z}'_{i}\boldsymbol\alpha}_{\mbox{Individual effects}} + \varepsilon_{i,t}.\label{eq:panel1}
\end{equation}
Our objective is to estimate the previous equation.

Figure \ref{fig:simulPanel}. The model is \(y_i = \alpha_i + \beta x_{i,t} + \varepsilon_{i,t}\), \(t \in \{1,2\}\). On Panel (b), blue dots are for \(t=1\), red dots are for \(t=2\). The lines relate the dots associated to the same entity \(i\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{T }\OtherTok{\textless{}{-}} \DecValTok{2} \CommentTok{\# 2 periods}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{12} \CommentTok{\# 12 entities}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \DecValTok{5}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(n) }\CommentTok{\# draw fixed effects}
\NormalTok{x}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }\SpecialCharTok{{-}}\NormalTok{ .}\DecValTok{5}\SpecialCharTok{*}\NormalTok{alpha }\CommentTok{\# note: x\_i\textquotesingle{}s correlate to alpha\_i\textquotesingle{}s}
\NormalTok{x}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }\SpecialCharTok{{-}}\NormalTok{ .}\DecValTok{5}\SpecialCharTok{*}\NormalTok{alpha}
\NormalTok{beta }\OtherTok{\textless{}{-}} \DecValTok{5}\NormalTok{; sigma }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{3}
\NormalTok{y}\FloatTok{.1} \OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ x}\FloatTok{.1} \SpecialCharTok{+}\NormalTok{ sigma}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y}\FloatTok{.2} \OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ x}\FloatTok{.2} \SpecialCharTok{+}\NormalTok{ sigma}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(x}\FloatTok{.1}\NormalTok{,x}\FloatTok{.2}\NormalTok{) }\CommentTok{\# pooled x}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(y}\FloatTok{.1}\NormalTok{,y}\FloatTok{.2}\NormalTok{) }\CommentTok{\# pooled y}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(x,y,}\AttributeTok{col=}\StringTok{"black"}\NormalTok{,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"x"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"y"}\NormalTok{,}\AttributeTok{main=}\StringTok{"(a)"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x,y,}\AttributeTok{col=}\StringTok{"black"}\NormalTok{,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"x"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"y"}\NormalTok{,}\AttributeTok{main=}\StringTok{"(b)"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(x}\FloatTok{.1}\NormalTok{,y}\FloatTok{.1}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{)}
\FunctionTok{points}\NormalTok{(x}\FloatTok{.2}\NormalTok{,y}\FloatTok{.2}\NormalTok{,}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{pch=}\DecValTok{19}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n)\{}
  \FunctionTok{lines}\NormalTok{(}\FunctionTok{c}\NormalTok{(x}\FloatTok{.1}\NormalTok{[i],x}\FloatTok{.2}\NormalTok{[i]),}\FunctionTok{c}\NormalTok{(y}\FloatTok{.1}\NormalTok{[i],y}\FloatTok{.2}\NormalTok{[i]))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{AdvECTS_files/figure-latex/simulPanel-1} 

}

\caption{The data are the same for both panels. On Panel (b), blue dots are for $t=1$, red dots are for $t=2$. The lines relate the dots associated to the same entity $i$.}\label{fig:simulPanel}
\end{figure}

Let us know use the \href{https://rdrr.io/cran/AER/man/CigarettesSW.html}{Cigarette Consumption Panel dataset} of \href{https://www.pearson.com/en-gb/subject-catalog/p/Stock-Introduction-to-Econometrics-Global-Edition-4th-Edition/P200000005500/9781292264523}{Stock and Watson (2007)}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"CigarettesSW"}\NormalTok{, }\AttributeTok{package =} \StringTok{"AER"}\NormalTok{)}
\NormalTok{CigarettesSW}\SpecialCharTok{$}\NormalTok{rprice  }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(CigarettesSW, price}\SpecialCharTok{/}\NormalTok{cpi)}
\NormalTok{CigarettesSW}\SpecialCharTok{$}\NormalTok{rincome }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(CigarettesSW, income}\SpecialCharTok{/}\NormalTok{population}\SpecialCharTok{/}\NormalTok{cpi)}
\NormalTok{T }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{levels}\NormalTok{(CigarettesSW}\SpecialCharTok{$}\NormalTok{year))}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{levels}\NormalTok{(CigarettesSW}\SpecialCharTok{$}\NormalTok{state))}
\NormalTok{eq.pooled }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(rprice)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(rincome),}\AttributeTok{data=}\NormalTok{CigarettesSW)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(eq.pooled)}\SpecialCharTok{$}\NormalTok{coefficients)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Estimate Std. Error   t value     Pr(>|t|)
## (Intercept)  10.0671678  0.5156035 19.525020 1.158630e-34
## log(rprice)  -1.3341612  0.1353614 -9.856288 4.120472e-16
## log(rincome)  0.3181371  0.1361194  2.337191 2.157508e-02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eq.LSDV }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(rprice)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(rincome)}\SpecialCharTok{+}\NormalTok{state,}
              \AttributeTok{data=}\NormalTok{CigarettesSW)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(eq.LSDV)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Estimate Std. Error   t value     Pr(>|t|)
## (Intercept)   9.9543751  0.2641889  37.67901 3.156258e-36
## log(rprice)  -1.2103380  0.1138384 -10.63207 5.590562e-14
## log(rincome)  0.1209004  0.1901069   0.63596 5.279541e-01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CigarettesSW}\SpecialCharTok{$}\NormalTok{year }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(CigarettesSW}\SpecialCharTok{$}\NormalTok{year)}
\NormalTok{eq.LSDV2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(packs)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(rprice)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(rincome)}\SpecialCharTok{+}\NormalTok{state}\SpecialCharTok{+}\NormalTok{year,}
               \AttributeTok{data=}\NormalTok{CigarettesSW)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(eq.LSDV2)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Estimate Std. Error   t value     Pr(>|t|)
## (Intercept)   8.3597463  1.0485390  7.972757 3.776672e-10
## log(rprice)  -1.0559739  0.1490905 -7.082770 7.682380e-09
## log(rincome)  0.4974424  0.3042306  1.635084 1.090085e-01
\end{verbatim}

Cigarettes data from \href{https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Cigarette.html}{Stock and Watson (2003)}. Data for U.S. states, 2 years: 1985 and 1995. Each colour corresponds to a given State.

Airlines: cost versus fuel price

Airlines data from \href{https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Airline.html}{Greene (2003)}. Each colour corresponds to a given airline.

Notations:
\[
\mathbf{y}_i =
\underbrace{\left[
\begin{array}{c}
y_{i,1}\\
\vdots\\
y_{i,T}
\end{array}\right]}_{T \times 1}, \quad
\boldsymbol\varepsilon_i =
\underbrace{\left[
\begin{array}{c}
\varepsilon_{i,1}\\
\vdots\\
\varepsilon_{i,T}
\end{array}\right]}_{T \times 1}, \quad
\mathbf{x}_i =
\underbrace{\left[
\begin{array}{c}
\mathbf{x}_{i,1}'\\
\vdots\\
\mathbf{x}_{i,T}'
\end{array}\right]}_{T \times K}, \quad
\mathbf{X} =
\underbrace{\left[
\begin{array}{c}
\mathbf{x}_{1}\\
\vdots\\
\mathbf{x}_{n}
\end{array}\right]}_{(nT) \times K}.
\]
\[
\tilde{\mathbf{y}}_i =
\left[
\begin{array}{c}
y_{i,1} - \bar{y}_i\\
\vdots\\
y_{i,T} - \bar{y}_i
\end{array}\right], \quad
\tilde{\boldsymbol\varepsilon}_i =
\left[
\begin{array}{c}
\varepsilon_{i,1} - \bar{\varepsilon}_i\\
\vdots\\
\varepsilon_{i,T} - \bar{\varepsilon}_i
\end{array}\right],
\]
\[
\tilde{\mathbf{x}}_i =
\left[
\begin{array}{c}
\mathbf{x}_{i,1}' - \bar{\mathbf{x}}_i'\\
\vdots\\
\mathbf{x}_{i,T}' - \bar{\mathbf{x}}_i'
\end{array}\right], \quad
\tilde{\mathbf{X}} =
\left[
\begin{array}{c}
\tilde{\mathbf{x}}_{1}\\
\vdots\\
\tilde{\mathbf{x}}_{n}
\end{array}\right], \quad
\tilde{\mathbf{Y}} =
\left[
\begin{array}{c}
\tilde{\mathbf{y}}_{1}\\
\vdots\\
\tilde{\mathbf{y}}_{n}
\end{array}\right],
\]
where
\[
\bar{y}_i = \frac{1}{T} \sum_{t=1}^T y_{i,t}, \quad \bar{\varepsilon}_i = \frac{1}{T}\sum_{t=1}^T \varepsilon_{i,t} \quad \mbox{and} \quad \bar{\mathbf{x}}_i = \frac{1}{T}\sum_{t=1}^T \mathbf{x}_{i,t}.
\]

\hypertarget{three-standard-cases}{%
\subsection{Three standard cases}\label{three-standard-cases}}

\begin{itemize}
\tightlist
\item
  \textbf{Pooled regression}: \(\mathbf{z}_i \equiv 1\).
\item
  \textbf{Fixed Effects}: \(\mathbf{z}_i\) is unobserved, but correlates with \(\mathbf{x}_i\) \(\Rightarrow\) \(\mathbf{b}\) is biased and inconsistent in the OLS regression of \(\mathbf{y}\) on \(\mathbf{X}\) (omitted variable, see XXX).
\item
  \textbf{Random Effects}: \(\mathbf{z}_i\) is unobserved, but uncorrelated with \(\mathbf{x}_i\). The model writes:
  \[
  y_{i,t} = \mathbf{x}'_{i,t}\boldsymbol\beta + \alpha +  \underbrace{{\color{blue}u_i + \varepsilon_{i,t}}}_{\mbox{compound disturbance}},
  \]
  where \(\alpha = \mathbb{E}(\mathbf{z}'_{i}\boldsymbol\alpha)\) and \(u_i = \mathbf{z}'_{i}\boldsymbol\alpha - \mathbb{E}(\mathbf{z}'_{i}\boldsymbol\alpha) \perp \mathbf{x}_i\).
\end{itemize}

Least squares are consistent (but inefficient, see GLS XXXX).

\hypertarget{estimation-of-fixed-effects-models}{%
\section{Estimation of Fixed Effects Models}\label{estimation-of-fixed-effects-models}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{hypothesis}[Fixed-effect model]
\protect\hypertarget{hyp:FE}{}\label{hyp:FE}

We assume that:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  There is no perfect multicollinearity among the regressors.
\item
  \(\mathbb{E}(\varepsilon_{i,t}|\mathbf{X})=0\), for all \(i,t\).
\item
  We have:
  \[
  \mathbb{E}(\varepsilon_{i,t}\varepsilon_{j,s}|\mathbf{X}) =
  \left\{
  \begin{array}{cl}
  \sigma^2 & \mbox{if $i=j$ and $s=t$},\\
  0 & \mbox{otherwise.}
  \end{array}\right.
  \]
\end{enumerate}

\end{hypothesis}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

These assumptions are analogous to those introduced in the standard linear regression:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  \(\leftrightarrow\) \ref{hyp:fullrank}, (ii) \(\leftrightarrow\) \ref{hyp:exogeneity}, (iii) \(\leftrightarrow\) \ref{hyp:homoskedasticity} + \ref{hyp:noncorrelResid}.
\end{enumerate}

In matrix form, for a given \(i\), the model writes:
\[
\mathbf{y}_i = \mathbf{X}_i \boldsymbol\beta + \mathbf{1}\alpha_i + \boldsymbol\varepsilon_i,
\]
where \(\mathbf{1}\) is a \(T\)-dimensional vector of ones.

This is the \textbf{Least Square Dummy Variable (LSDV)} model:
\begin{equation}
\boxed{\mathbf{y} = [\mathbf{X} \quad \mathbf{D}]
\left[
\begin{array}{c}
\boldsymbol\beta\\
\boldsymbol\alpha
\end{array}
\right]
+ \boldsymbol\varepsilon,} \label{eq:LSDV}
\end{equation}
with:
\[
\mathbf{D} = \underbrace{ \left[\begin{array}{cccc}
\mathbf{1}&\mathbf{0}&\dots&\mathbf{0}\\
\mathbf{0}&\mathbf{1}&\dots&\mathbf{0}\\
&&\vdots&\\
\mathbf{0}&\mathbf{0}&\dots&\mathbf{1}\\
\end{array}\right]}_{(nT \times n)}.
\]

The linear regression (Eq. \eqref{eq:LSDV}) --with the dummy variables-- satisfies the Gauss-Markov conditions (Theorem \ref{thm:GaussMarkov}). Hence, in this context, the OLS estimator is the \textbf{best linear unbiased estimator}.

Denoting by \(\mathbf{Z}\) the matrix \([\mathbf{X} \quad \mathbf{D}]\), and by \(\mathbf{b}\) and \(\mathbf{a}\) the respective OLS estimates of \(\boldsymbol\beta\) and of \(\boldsymbol\alpha\), we have:
\begin{equation}
\boxed{
\left[
\begin{array}{c}
\mathbf{b}\\
\mathbf{a}
\end{array}
\right]
= [\mathbf{Z}'\mathbf{Z}]^{-1}\mathbf{Z}'\mathbf{y}.} \label{eq:bfixedeffects11}
\end{equation}

The asymptotical distribution of \([\mathbf{b}',\mathbf{a}']'\) derives from the standard OLS context: Prop. \ref{prp:asymptOLS} can be used after having replaced \(\mathbf{X}\) by \(\mathbf{Z}=[\mathbf{X} \quad \mathbf{D}]\).

We have:
\begin{equation}
\boxed{\left[
\begin{array}{c}
\mathbf{b}\\
\mathbf{a}
\end{array}
\right] \overset{d}{\rightarrow}
\mathcal{N}\left(
\left[
\begin{array}{c}
\boldsymbol\beta\\
\boldsymbol\alpha
\end{array}
\right],
\sigma^2 \frac{Q^{-1}}{nT}
\right)}
\end{equation}
where
\[
Q = \mbox{plim}_{nT \rightarrow \infty} \frac{1}{nT} \mathbf{Z}'\mathbf{Z}.
\]

In practice, an estimator of the covariance matrix of \([\mathbf{b}',\mathbf{a}']'\) is:
\[
s^2 \left( \mathbf{Z}'\mathbf{Z}\right)^{-1} \quad with \quad s^2 = \frac{\mathbf{e}'\mathbf{e}}{nT - K - n},
\]
where \(\mathbf{e}\) is the \((nT) \times 1\) vector of OLS residuals.

There is an alternative way of expressing the LSDV estimators.

It is based on matrix \(\mathbf{M_D}=\mathbf{I} - \mathbf{D}(\mathbf{D}'\mathbf{D})^{-1}\mathbf{D}'\), which acts as an operator that removes entity-specific means, i.e.:
\[
\tilde{\mathbf{Y}} = \mathbf{M_D}\mathbf{Y}, \quad \tilde{\mathbf{X}} = \mathbf{M_D}\mathbf{X} \quad and \quad \tilde{\boldsymbol\varepsilon} = \mathbf{M_D}\boldsymbol\varepsilon.
\]

With these notations, using Theorem \ref{thm:FW}, we get:
\begin{equation}
\boxed{\mathbf{b} = [\mathbf{X}'\mathbf{M_D}\mathbf{X}]^{-1}\mathbf{X}'\mathbf{M_D}\mathbf{y}.}\label{eq:bfixedeffects}
\end{equation}

This amounts to regressing the \(\tilde{y}_{i,t}\)s (\(= y_{i,t} - \bar{y}_i\)) on the \(\tilde{\mathbf{x}}_{i,t}\)s (\(=\mathbf{x}_{i,t} - \bar{\mathbf{x}}_i\)).

The estimates of \(\boldsymbol\alpha\) are given by:
\begin{equation}
\boxed{\mathbf{a} = (\mathbf{D}'\mathbf{D})^{-1}\mathbf{D}'(\mathbf{y} - \mathbf{X}\mathbf{b}),} \label{eq:a}
\end{equation}
which is obtained by developing the second row of
\[
\left[
\begin{array}{cc}
\mathbf{X}'\mathbf{X} & \mathbf{X}'\mathbf{D}\\
\mathbf{D}'\mathbf{X} & \mathbf{D}'\mathbf{D}
\end{array}\right]
\left[
\begin{array}{c}
\mathbf{b}\\
\mathbf{a}
\end{array}\right] =
\left[
\begin{array}{c}
\mathbf{X}'\mathbf{Y}\\
\mathbf{D}'\mathbf{Y}
\end{array}\right],
\]
which are the first-order conditions resulting from the least squares problem (see Eq. \eqref{eq:OLSFOC}).

XXXX Regression of the log of airline costs on log(output), log(fuel price) and capacity utilization of the fleet (Data from \href{https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Airline.html}{Greene (2003)}, see Fig. \ref{fig:airline}.

XXXX Regression of the number of cigarette packs (log) on real income (log) and real price of cigarettes (log)

Extension: Fixed time and group effects

Time effects are easily introduced:
\[
y_{i,t} = \mathbf{x}_i'\boldsymbol\beta + \alpha_i + \gamma_t + \varepsilon_{i,t}.
\]

The LSDV (Eq. \eqref{eq:LSDV}) can be extended:
\begin{equation}
\mathbf{y} = [\mathbf{X} \quad \mathbf{D} \quad \mathbf{C}]
\left[
\begin{array}{c}
\boldsymbol\beta\\
\boldsymbol\alpha\\
\boldsymbol\gamma
\end{array}
\right]
+ \boldsymbol\varepsilon, \label{eq:LSDV2}
\end{equation}
with:
\[
\mathbf{C} = \left[\begin{array}{cccc}
\boldsymbol{\delta}_1&\boldsymbol{\delta}_2&\dots&\boldsymbol{\delta}_{T-1}\\
\vdots&\vdots&&\vdots\\
\boldsymbol{\delta}_1&\boldsymbol{\delta}_2&\dots&\boldsymbol{\delta}_{T-1}\\
\end{array}\right],
\]
where the \(T\)-dimensional vector \(\boldsymbol\delta_t\) is
\[
[0,\dots,0,\underbrace{1}_{\mbox{t$^{th}$ entry}},0,\dots,0]'.
\]

XXXX Geo-located data from Airbnb, Z"urich

Source: \href{http://tomslee.net/airbnb-data-collection-get-the-data}{Airbnb, date 22 June 2017}. Regression of price for Entire home/apt on number of bedrooms, number of people that can be accommodated.

\hypertarget{estimation-of-random-effects-models}{%
\section{Estimation of random effects models}\label{estimation-of-random-effects-models}}

Here, the individual effects are assumed not to be correlated to other variables (the \(\mathbf{x}_i\)s).

\textbf{Random-effect models} write:
\[
y_{i,t}=\mathbf{x}'_{it}\boldsymbol\beta + (\alpha + \underbrace{u_i}_{\substack{\text{Random}\\\text{heterogeneity}}}) + \varepsilon_{i,t}
\]
with
\begin{eqnarray*}
\mathbb{E}(\varepsilon_{i,t}|\mathbf{X})&=&\mathbb{E}(u_{i}|\mathbf{X}) =0,\\
\mathbb{E}(\varepsilon_{i,t}\varepsilon_{j,s}|\mathbf{X}) &=&
\left\{
\begin{array}{cl}
\sigma_\varepsilon^2 & \mbox{ if $i=j$ and $s=t$},\\
0 & \mbox{ otherwise.}
\end{array}
\right.\\
\mathbb{E}(u_{i}u_{j}|\mathbf{X}) &=&
\left\{
\begin{array}{cl}
\sigma_u^2 & \mbox{ if $i=j$},\\
0 & \mbox{otherwise.}
\end{array}
\right.\\
\mathbb{E}(\varepsilon_{i,t}u_{j}|\mathbf{X})&=&0 \quad \text{for all $i$, $j$ and $t$}.
\end{eqnarray*}

Notation: \(\eta_{i,t} = u_i + \varepsilon_{i,t}\) and \(\boldsymbol\eta_i = [\eta_{i,1},\eta_{i,2},\dots,\eta_{i,T}]'\).

We have \(\mathbb{E}(\boldsymbol\eta_i |\mathbf{X}) = \mathbf{0}\) and \(\mathbb{V}ar(\boldsymbol\eta_i | \mathbf{X}) = \boldsymbol\Gamma\) where
\[
\boldsymbol\Gamma = \left[  \begin{array}{ccccc}
\sigma_\varepsilon^2+\sigma_u^2 & \sigma_u^2 & \sigma_u^2 & \dots & \sigma_u^2\\
\sigma_u^2 & \sigma_\varepsilon^2+\sigma_u^2 & \sigma_u^2 & \dots & \sigma_u^2\\
\vdots && \ddots && \vdots \\
\sigma_u^2 & \sigma_u^2 & \sigma_u^2 & \dots & \sigma_\varepsilon^2+\sigma_u^2\\
\end{array}
\right] = \sigma_\varepsilon^2\mathbf{I} + \sigma_u^2\mathbf{1}\mathbf{1}'.
\]

Denoting by \(\boldsymbol\Sigma\) the covariance matrix of \(\boldsymbol\eta = [\boldsymbol\eta_1',\dots,\boldsymbol\eta_n']'\), we have:
\[
\boldsymbol\Sigma = \mathbf{I} \otimes \boldsymbol\Gamma.
\]

If we knew \(\boldsymbol\Sigma\), we would apply (feasible) GLS (Eq. \eqref{eq:betaGLS}):
\[
\boldsymbol\beta = (\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\Sigma^{-1}\mathbf{y}
\]
(recall that this amounts to regressing \({\boldsymbol\Sigma^{-1/2}}'\mathbf{y}\) on \({\boldsymbol\Sigma^{-1/2}}'\mathbf{X}\)).

It can be checked that \(\boldsymbol\Sigma^{-1/2} = \mathbf{I} \otimes (\boldsymbol\Gamma^{-1/2})\) where
\[
\boldsymbol\Gamma^{-1/2} = \frac{1}{\sigma_\varepsilon}\left( \mathbf{I} - \frac{\theta}{T}\mathbf{1}\mathbf{1}'\right)
\]
with
\[
\theta = 1 - \frac{\sigma_\varepsilon}{\sqrt{\sigma_\varepsilon^2+T\sigma_u^2}}.
\]

Hence, if we knew \(\boldsymbol\Sigma\), we would transform the data as follows:
\[
\boldsymbol\Gamma^{-1/2}\mathbf{y}_i = \frac{1}{\sigma_\varepsilon}\left[\begin{array}{c}y_{i,1} - \theta\bar{y}_i\\y_{i,2} - \theta\bar{y}_i\\\vdots\\y_{i,T} - \theta\bar{y}_i\\\end{array}\right].
\]

What about when \(\boldsymbol\Sigma\) is unknown?

Idea: taking deviations from group means removes heterogeneity:
\[
y_{i,t} - \bar{y}_i = [\mathbf{x}_{i,t} - \bar{\mathbf{x}}_i]'\boldsymbol\beta + (\varepsilon_{i,t} - \bar{\varepsilon}_i).
\]

The previous equation can be consistently estimated by OLS

(the residuals are correlated across \(t\) within an entity but the OLS remain consistent though, see Prop. @ref\{prp:XXX)).

We have \(\mathbb{E}\left[\sum_{i=1}^{T}(\varepsilon_{i,t}-\bar{\varepsilon}_i)^2\right] = (T-1)\sigma_{\varepsilon}^2\).

The \(\varepsilon_{i,t}\)s are not observed but \(\mathbf{b}\) is a consistent estimator of \(\boldsymbol\beta\); adjustment for the degrees of freedom:
\[
\hat{\sigma}_e^2 = \frac{1}{nT-n-K}\sum_{i=1}^{n}\sum_{t=1}^{T}(e_{i,t} - \bar{e}_i)^2.
\]

And for \(\sigma_u^2\)? We can exploit the fact that OLS are consistent in the pooled regression:
\[
\mbox{plim }s^2_{pooled} = \mbox{plim }\frac{\mathbf{e}'\mathbf{e}}{nT-K-1} = \sigma_u^2 + \sigma_\varepsilon^2,
\]
and therefore use \(s^2_{pooled} - \hat{\sigma}_e^2\) as an approximation to \(\sigma_u^2\).

\hypertarget{example-macro-history-database}{%
\subsection{Example: Macro-History database}\label{example-macro-history-database}}

Date from \citet{JST_2017} \href{https://www.macrohistory.net}{see website}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{JST }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/jrenne/Data4courses/master/JST/JSTdatasetR6.csv"}\NormalTok{)}
\NormalTok{JST }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(JST,year}\SpecialCharTok{\textgreater{}}\DecValTok{1950}\NormalTok{)}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(JST)[}\DecValTok{1}\NormalTok{]}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{hpreal }\OtherTok{\textless{}{-}}\NormalTok{ JST}\SpecialCharTok{$}\NormalTok{hpnom}\SpecialCharTok{/}\NormalTok{JST}\SpecialCharTok{$}\NormalTok{cpi }\CommentTok{\# real house price index}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{chge\_ctry }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\ConstantTok{NaN}\NormalTok{,JST}\SpecialCharTok{$}\NormalTok{iso[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{N]}\SpecialCharTok{!=}\NormalTok{JST}\SpecialCharTok{$}\NormalTok{iso[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(N}\DecValTok{{-}1}\NormalTok{)])}\CommentTok{\# will be use to put NA\textquotesingle{}s when computing change in price}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{dhpreal }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(JST}\SpecialCharTok{$}\NormalTok{hpreal}\SpecialCharTok{/}\FunctionTok{c}\NormalTok{(}\ConstantTok{NaN}\NormalTok{,JST}\SpecialCharTok{$}\NormalTok{hpreal[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(N}\DecValTok{{-}1}\NormalTok{)]))}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{dhpreal[JST}\SpecialCharTok{$}\NormalTok{chge\_ctry}\SpecialCharTok{==}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{dhpreal[}\FunctionTok{abs}\NormalTok{(JST}\SpecialCharTok{$}\NormalTok{dhpreal)}\SpecialCharTok{\textgreater{}}\NormalTok{.}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN} \CommentTok{\# remove extreme price change}
\CommentTok{\# JST$ltrate\_1 \textless{}{-} c(NaN,JST$ltrate[1:(N{-}1)])}
\CommentTok{\# JST$ltrate\_1[JST$chge\_ctry==1] \textless{}{-} NaN}
\CommentTok{\# JST$stir\_1 \textless{}{-} c(NaN,JST$stir[1:(N{-}1)])}
\CommentTok{\# JST$stir\_1[JST$chge\_ctry==1] \textless{}{-} NaN}
\NormalTok{JST}\SpecialCharTok{$}\NormalTok{YEAR }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(JST}\SpecialCharTok{$}\NormalTok{year) }\CommentTok{\# to have time fixed effects}
\NormalTok{eq }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stir }\SpecialCharTok{+}\NormalTok{ ltrate }\SpecialCharTok{+}\NormalTok{ iso }\SpecialCharTok{+}\NormalTok{ YEAR,}\AttributeTok{data=}\NormalTok{JST)}
\NormalTok{eq }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(ltrate}\SpecialCharTok{{-}}\NormalTok{stir) }\SpecialCharTok{+}\NormalTok{ iso }\SpecialCharTok{+}\NormalTok{ YEAR,}\AttributeTok{data=}\NormalTok{JST)}
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{coeftest}\NormalTok{(eq)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      Estimate  Std. Error    t value     Pr(>|t|)
## (Intercept)      -0.009956382 0.020728411 -0.4803254 0.6310957161
## I(ltrate - stir) -0.004746647 0.001254511 -3.7836614 0.0001632386
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sandwich)}
\FunctionTok{library}\NormalTok{(lmtest)}
\NormalTok{eq }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(dhpreal }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(ltrate}\SpecialCharTok{{-}}\NormalTok{stir) }\SpecialCharTok{+}\NormalTok{ iso,}\AttributeTok{data=}\NormalTok{JST)}
\NormalTok{vcov\_cluster }\OtherTok{\textless{}{-}} \FunctionTok{vcovCL}\NormalTok{(eq, }\AttributeTok{cluster =}\NormalTok{ JST[, }\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{)])}
\FunctionTok{coeftest}\NormalTok{(eq, }\AttributeTok{vcov =}\NormalTok{ vcov\_cluster)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      Estimate  Std. Error   t value     Pr(>|t|)
## (Intercept)       0.028199744 0.001814513 15.541217 1.778760e-49
## I(ltrate - stir) -0.004732743 0.002349209 -2.014611 4.418367e-02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vcov\_cluster }\OtherTok{\textless{}{-}} \FunctionTok{vcovCL}\NormalTok{(eq, }\AttributeTok{cluster =}\NormalTok{ JST[, }\FunctionTok{c}\NormalTok{(}\StringTok{"iso"}\NormalTok{,}\StringTok{"YEAR"}\NormalTok{)])}
\FunctionTok{coeftest}\NormalTok{(eq, }\AttributeTok{vcov =}\NormalTok{ vcov\_cluster)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      Estimate  Std. Error   t value     Pr(>|t|)
## (Intercept)       0.028199744 0.002182053 12.923490 1.020962e-35
## I(ltrate - stir) -0.004732743 0.002720606 -1.739592 8.220498e-02
\end{verbatim}

\hypertarget{example-spatial-data}{%
\subsection{Example Spatial Data}\label{example-spatial-data}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rgdal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: sp
\end{verbatim}

\begin{verbatim}
## Please note that rgdal will be retired by the end of 2023,
## plan transition to sf/stars/terra functions using GDAL and PROJ
## at your earliest convenience.
## 
## rgdal: version: 1.5-30, (SVN revision 1171)
## Geospatial Data Abstraction Library extensions to R successfully loaded
## Loaded GDAL runtime: GDAL 3.4.2, released 2022/03/08
## Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/4.2/Resources/library/rgdal/gdal
## GDAL binary built with GEOS: FALSE 
## Loaded PROJ runtime: Rel. 8.2.1, January 1st, 2022, [PJ_VERSION: 821]
## Path to PROJ shared files: /Library/Frameworks/R.framework/Versions/4.2/Resources/library/rgdal/proj
## PROJ CDN enabled: FALSE
## Linking to sp version:1.4-6
## To mute warnings of possible GDAL/OSR exportToProj4() degradation,
## use options("rgdal_show_exportToProj4_warnings"="none") before loading sp or rgdal.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AEC)}
\FunctionTok{library}\NormalTok{(sandwich)}
\FunctionTok{library}\NormalTok{(lmtest)}
\FunctionTok{library}\NormalTok{(stargazer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Please cite as:
\end{verbatim}

\begin{verbatim}
##  Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.
\end{verbatim}

\begin{verbatim}
##  R package version 5.2.3. https://CRAN.R-project.org/package=stargazer
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(RColorBrewer)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(zurich\_districts,}\AttributeTok{col=}\StringTok{"gray"}\NormalTok{,}\AttributeTok{border=}\StringTok{"white"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{dim}\NormalTok{(airbnb)[}\DecValTok{1}\NormalTok{])\{}
  \FunctionTok{points}\NormalTok{(airbnb}\SpecialCharTok{$}\NormalTok{longitude[i],airbnb}\SpecialCharTok{$}\NormalTok{latitude[i],}\AttributeTok{pch=}\DecValTok{1}\NormalTok{,}\AttributeTok{cex=}\NormalTok{(airbnb}\SpecialCharTok{$}\NormalTok{price[i]}\SpecialCharTok{/}\DecValTok{200}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\includegraphics{AdvECTS_files/figure-latex/airbnb-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eq\_noFE }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price}\SpecialCharTok{\textasciitilde{}}\NormalTok{bedrooms}\SpecialCharTok{+}\NormalTok{accommodates,}\AttributeTok{data=}\NormalTok{airbnb)}
\NormalTok{eq\_FE }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price}\SpecialCharTok{\textasciitilde{}}\NormalTok{bedrooms}\SpecialCharTok{+}\NormalTok{accommodates}\SpecialCharTok{+}\NormalTok{neighborhood,}\AttributeTok{data=}\NormalTok{airbnb)}
\CommentTok{\# stargazer(}
\CommentTok{\#   eq\_FE,eq\_noFE, type = "text", column.labels = c("FE", "No FE"),}
\CommentTok{\#   omit = c("neighborhood"),}
\CommentTok{\#   omit.labels = c("District FE")\#,keep.stat = "n"}
\CommentTok{\# )}
\CommentTok{\# lmtest::coeftest(eq\_FE)[1:2,]}
\CommentTok{\# lmtest::coeftest(eq\_noFE)[1:2,]}

\CommentTok{\# Adjust standard errors}
\NormalTok{cov\_FE          }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(eq\_FE, }\AttributeTok{cluster =}\NormalTok{ airbnb[, }\FunctionTok{c}\NormalTok{(}\StringTok{"neighborhood"}\NormalTok{)])}
\NormalTok{robust\_se\_FE    }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cov\_FE))}
\NormalTok{cov\_noFE        }\OtherTok{\textless{}{-}} \FunctionTok{vcovHC}\NormalTok{(eq\_noFE, }\AttributeTok{cluster =}\NormalTok{ airbnb[, }\FunctionTok{c}\NormalTok{(}\StringTok{"neighborhood"}\NormalTok{)])}
\NormalTok{robust\_se\_noFE  }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(cov\_noFE))}

\CommentTok{\# Stargazer output (with and without RSE)}
\FunctionTok{stargazer}\NormalTok{(eq\_FE, eq\_noFE, eq\_FE, eq\_noFE, }\AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
          \AttributeTok{column.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"FE (no HAC)"}\NormalTok{, }\StringTok{"No FE (no HAC)"}\NormalTok{,}
                            \StringTok{"FE (with HAC)"}\NormalTok{, }\StringTok{"No FE (with HAC)"}\NormalTok{),}
          \AttributeTok{omit =} \FunctionTok{c}\NormalTok{(}\StringTok{"neighborhood"}\NormalTok{),}
          \AttributeTok{omit.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"District FE"}\NormalTok{),}\AttributeTok{keep.stat =} \StringTok{"n"}\NormalTok{,}
          \AttributeTok{se =} \FunctionTok{list}\NormalTok{(}\ConstantTok{NULL}\NormalTok{, }\ConstantTok{NULL}\NormalTok{, robust\_se\_FE, robust\_se\_noFE))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ======================================================================
##                                 Dependent variable:                   
##              ---------------------------------------------------------
##                                        price                          
##              FE (no HAC) No FE (no HAC) FE (with HAC) No FE (with HAC)
##                  (1)          (2)            (3)            (4)       
## ----------------------------------------------------------------------
## bedrooms      7.229***      5.629**       7.229***        5.629***    
##                (2.135)      (2.194)        (2.052)        (2.073)     
##                                                                       
## accommodates  16.426***    17.449***      16.426***      17.449***    
##                (1.284)      (1.323)        (1.431)        (1.428)     
##                                                                       
## Constant      95.118***    68.417***      95.118***      68.417***    
##                (5.323)      (3.223)        (5.664)        (3.527)     
##                                                                       
## ----------------------------------------------------------------------
## District FE      Yes           No            Yes             No       
## ----------------------------------------------------------------------
## Observations    1,321        1,321          1,321          1,321      
## ======================================================================
## Note:                                      *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot residuals}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{)\{}\CommentTok{\# with/without FE}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{plt=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,.}\DecValTok{8}\NormalTok{))}
  \ControlFlowTok{if}\NormalTok{(j}\SpecialCharTok{==}\DecValTok{1}\NormalTok{)\{}
\NormalTok{    residuals }\OtherTok{\textless{}{-}}\NormalTok{ eq\_FE}\SpecialCharTok{$}\NormalTok{residuals}
\NormalTok{    titl }\OtherTok{\textless{}{-}} \StringTok{"(a) With FE"}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{    residuals }\OtherTok{\textless{}{-}}\NormalTok{ eq\_noFE}\SpecialCharTok{$}\NormalTok{residuals}
\NormalTok{    titl }\OtherTok{\textless{}{-}} \StringTok{"(b) Without FE"}
\NormalTok{  \}}
  \FunctionTok{plot}\NormalTok{(zurich\_districts,}\AttributeTok{col=}\StringTok{"gray"}\NormalTok{,}\AttributeTok{border=}\StringTok{"white"}\NormalTok{,}\AttributeTok{main=}\NormalTok{titl)}
  \CommentTok{\# create categories based on resisuals:}
\NormalTok{  nb\_categ }\OtherTok{\textless{}{-}} \DecValTok{11}
\NormalTok{  categ }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{((nb\_categ}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{(residuals}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(residuals))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{max}\NormalTok{(residuals)}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(residuals)))}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{  colour }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\FunctionTok{brewer.pal}\NormalTok{(}\AttributeTok{n =}\NormalTok{ nb\_categ, }\AttributeTok{name =} \StringTok{"RdBu"}\NormalTok{),}\StringTok{"77"}\NormalTok{,}\AttributeTok{sep=}\StringTok{""}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{dim}\NormalTok{(airbnb)[}\DecValTok{1}\NormalTok{])\{}
    \FunctionTok{points}\NormalTok{(airbnb}\SpecialCharTok{$}\NormalTok{longitude[i],airbnb}\SpecialCharTok{$}\NormalTok{latitude[i],}\AttributeTok{pch=}\DecValTok{19}\NormalTok{,}
           \AttributeTok{cex=}\NormalTok{(}\FunctionTok{abs}\NormalTok{(residuals[i])}\SpecialCharTok{/}\DecValTok{100}\NormalTok{),}\AttributeTok{col=}\NormalTok{colour[categ[i]])}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\includegraphics{AdvECTS_files/figure-latex/airbnb-2.pdf}

\hypertarget{dynamic-panel-regressions}{%
\section{Dynamic Panel Regressions}\label{dynamic-panel-regressions}}

In what precedes, it has been assumed that there is no correlation between the observations indexed by \((i,t)\) and those indexed by \((j,s)\) as long as \(j \ne i\) or \(t \ne s\). If one suspects that the errors \(\varepsilon_{i,t}\) are correlated (across entities \(i\) for a given date \(t\), or across dates for a given entity, or both), then one should employ a robust covariance matrix (see Section \ref{clusters}).

In several cases, auto-correlation in the variable of interest may stem from an auto-regressive specification of the variable of interest. Eq. \eqref{eq:panel1} is then replaced by:
\begin{equation}
y_{i,t} = \rho y_{i,t-1} + \mathbf{x}'_{i,t}\underbrace{\boldsymbol\beta}_{K \times 1} + \underbrace{\alpha_i}_{\mbox{Individual effects}} + \varepsilon_{i,t}.\label{eq:paneldyn}
\end{equation}

In that case, even if the explanatory variables \(\mathbf{x}_{i,t}\) are uncorrelated to the errors \(\varepsilon_{i,t}\), we have that the additional \emph{explanatory variable} \(y_{i,t-1}\) correlates to the errors \(\varepsilon_{i,t-1},\varepsilon_{i,t-2},\dots,\varepsilon_{i,1}\). As a result, the LSDV estimate of the model parameters \(\{\rho,\boldsymbol\beta\}\) may be biased, even if \(n\) is large. To see this, notice that the LSDV regression amounts to regress \(\widetilde{\mathbf{y}}\) on \(\widetilde{\mathbf{X}}\) (see Eq. \eqref{eq:bfixedeffects}), where the elements of \(\widetilde{\mathbf{X}}\) are the explanatory variables to which we subtract their within-sample means. In particular, we have:
\[
\tilde{y}_{i,t-1} = y_{i,t-1} - \frac{1}{T} \sum_{s=1}^{T} y_{i,s-1},
\]
which correlates to the corresponding error, that is:
\[
\tilde{\varepsilon}_{i,t} = \varepsilon_{i,t} - \frac{1}{T} \sum_{s=1}^{T} \varepsilon_{i,s}.
\]

The previous equation shows that the \emph{within-group} estimator (LSDV) introduces all realisations of the \(\varepsilon_{i,t}\) errors into the transformed error term (\(\tilde{\varepsilon}_{i,t}\)). As a result, in large \(n\), fixed \(T\) panels, it is consistent only if all the right-hand side variables are strictly exogenous (i.e., do not correlate to past, present, and future errors \(\varepsilon_{i,t}\)).\footnote{The bias may vanish for large \(T\)'s.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{500}\NormalTok{;T }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{rho }\OtherTok{\textless{}{-}} \FloatTok{0.9}\NormalTok{;sigma }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{5}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho) }\SpecialCharTok{+}\NormalTok{ sigma}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{all\_y }\OtherTok{\textless{}{-}}\NormalTok{ y}
\ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{  eps }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ rho }\SpecialCharTok{*}\NormalTok{ y }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ sigma }\SpecialCharTok{*}\NormalTok{ eps}
\NormalTok{  all\_y }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(all\_y,y)}
\NormalTok{\}}
\NormalTok{y   }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{T,])}
\NormalTok{y\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{),])}
\NormalTok{D }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(n) }\SpecialCharTok{\%x\%} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,T}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{Z }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{c}\NormalTok{(y\_1),D)}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ Z) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ y}
\NormalTok{a }\OtherTok{\textless{}{-}}\NormalTok{ b[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(n}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)]}
\FunctionTok{plot}\NormalTok{(alpha,a)}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{),}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{AdvECTS_files/figure-latex/dynpanel1-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plot(all\_y[,1],type="l")}
\end{Highlighting}
\end{Shaded}

How to address this? One can resort to instrumental-variable regressions.

\citet{Anderson_Hsiao_1982} proposed a first-differenced Two Stage Least Squares (2SLS) estimator. This estimation is based on the following transformation of the model:
\begin{equation}
\Delta y_{i,t} = \rho \Delta y_{i,t-1} + (\Delta \mathbf{x}_{i,t})'\boldsymbol\beta + \Delta\varepsilon_{i,t}.\label{eq:paneldynFisrtDiff}
\end{equation}
The OLS estimates of the parameters are biased because \(\varepsilon_{i,t_1}\) (which is part of the error \(\Delta\varepsilon_{i,t}\)) is correlated to \(y_{i,t-1}\) (which is part of the ``explanatory variable,'' namely \(\Delta y_{i,t-1}\)). But consistent estimates can be obtained using 2SLS with instrumental variables that are both correlated with \(\Delta y_{i,t}\) and orthogonal to \(\Delta\varepsilon_{i,t}\). One can for instance use \(\{y_{i,t-2},\mathbf{x}_{i,t-2}\}\) as instruments. Note that this approach can be implemented only if there are more than 3 time observations per entity \(i\).

If the explanatory variables \(\mathbf{x}_{i,t}\) are assumed to be predetermined (i.e., do not contemporaneous correlate with the errors \(\varepsilon_{i,t}\)), then \(\mathbf{x}_{i,t-1}\) can be added to the instruments associated with \(\Delta y_{i,t}\). Further, if these variables (the \(\mathbf{x}_{i,t}\)'s) are assumed to be exogenous (i.e., do not contemporaneous correlate with any of the errors \(\varepsilon_{i,s}\), \(\forall s\)), then \(\mathbf{x}_{i,t}\) also is a valid instrument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dy   }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T,]) }\SpecialCharTok{{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{),])}
\NormalTok{Dy\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{),]) }\SpecialCharTok{{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{),])}
\NormalTok{y\_2  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(all\_y[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{),])}
\NormalTok{Z }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(y\_2,}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{)}
\NormalTok{Pz }\OtherTok{\textless{}{-}}\NormalTok{ Z }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ Z) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Z)}
\NormalTok{Dy\_hat  }\OtherTok{\textless{}{-}}\NormalTok{ Pz }\SpecialCharTok{\%*\%}\NormalTok{ Dy}
\NormalTok{Dy\_1hat }\OtherTok{\textless{}{-}}\NormalTok{ Pz }\SpecialCharTok{\%*\%}\NormalTok{ Dy\_1}
\NormalTok{rho\_TSLS }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Dy\_1hat) }\SpecialCharTok{\%*\%}\NormalTok{ Dy\_1hat) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Dy\_1hat) }\SpecialCharTok{\%*\%}\NormalTok{ Dy\_hat}
\end{Highlighting}
\end{Shaded}

For \(t=3\), \(y_{i,1}\) (and \(\mathbf{x}_{i,1}\)) is the only possible instrument. However, for \(t=4\), one could use \(y_{i,2}\) and \(y_{i,1}\) (as well as \(\mathbf{x}_{i,2}\) and \(\mathbf{x}_{i,1}\)). More generally, defining matrix \(Z_i\) as follows:
\[
Z_i = \left[
\begin{array}{ccccccccccccccccc}
\mathbf{z}_{i,1}' & 0 & \dots \\
0 & \mathbf{z}_{i,1}' & \mathbf{z}_{i,2}' & 0 & \dots \\
0 &0 &0 & \mathbf{z}_{i,1} & \mathbf{z}_{i,2}' & \mathbf{z}_{i,3}' & 0 & \dots \\
\vdots \\
0 & \dots &&&&&& 0 & \mathbf{z}_{i,1}' &  \dots &   \mathbf{z}_{i,T-2}'
\end{array}
\right],
\]
where \(\mathbf{z}_{i,t} = [y_{i,t},\mathbf{x}_{i,t}']'\), we have the moment conditions:\footnote{If \(\mathbf{x}_{i,t}\) is predetermined (exogenous), we can use \(\mathbf{z}_{i,t} = [y_{i,t},\mathbf{x}_{i,t+1},\mathbf{x}_{i,t}']'\) (respectively \(\mathbf{z}_{i,t} = [y_{i,t},\mathbf{x}_{i,t+2},\mathbf{x}_{i,t+1},\mathbf{x}_{i,t}']'\)).}
\[
\mathbb{E}(Z_i'\Delta  \mathbf{v}_i)=0,
\]

with \(\Delta\mathbf{v}_i = [ \Delta v_{i,3},\dots,\Delta v_{i,T}]'\).

These restrictions are used in the GMM approach employed by \citet{Arellano_Bond_1991}. Specifically, a GMM estimator of the model parameters is given by:
\[
\mbox{argmin}\;\left(\frac{1}{n} \sum_{i=1}^n Z_i' \Delta \mathbf{v}_i\right)'W_n\left(\frac{1}{n} \sum_{i=1}^n Z_i' \Delta \mathbf{v}_i\right),
\]
using the weighting matrix
\[
W_n = \left(\frac{1}{n}\sum_{i=1}^n Z_i'\widehat{\Delta\mathbf{v}_i}\widehat{\Delta\mathbf{v}_i}'Z_i\right)^{-1},
\]
where the \(\widehat{\Delta\mathbf{v}_i}\)'s are consistent esitmates of the \(\Delta\mathbf{v}_i\) that result from a preliminary estimation. Hence, this estimator is a two-step GMM one.

If the disturbances are homoskedastic, then it can be shown that an asymtotically equilvalent (efficient) GMM estimator can be obtained by using:
\[
W_{1,n} = \left(\frac{1}{n}Z_i'HZ_i\right)^{-1},
\]
where \(H\) is is \((T-2) \times (T-2)\) matrix of the form:
\[
H = \left[\begin{array}{ccccccc}
2 & -1 & 0 & \dots &0 \\
-1 & 2 & -1 &  & \vdots \\
0 & \ddots& \ddots & \ddots & 0 \\
\vdots &  & -1 & 2&-1\\
0&\dots & 0 & -1 & 2
\end{array}\right].
\]

It is straightforward to extend these GMM methods extend to cases where there is more than one lag of the dependent variable on the right-hand side of the equation or in cases where disturbances feature limited moving-average serial correlation.

The \texttt{pdynmc} package allows to run these GMM approaches (see \citet{Fritsch_et_al_2019}). The following lines of code allow to replicate the results of \citet{Arellano_Bond_1991}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pdynmc)}
\FunctionTok{data}\NormalTok{(EmplUK, }\AttributeTok{package =} \StringTok{"plm"}\NormalTok{)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ EmplUK}
\NormalTok{dat[,}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{7}\NormalTok{)]         }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(dat[,}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{7}\NormalTok{)])}
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{pdynmc}\NormalTok{(}\AttributeTok{dat =}\NormalTok{ dat, }\CommentTok{\# name of the dataset}
             \AttributeTok{varname.i =} \StringTok{"firm"}\NormalTok{, }\CommentTok{\# name of the cross{-}section identifier}
             \AttributeTok{varname.t =} \StringTok{"year"}\NormalTok{, }\CommentTok{\# name of the time{-}series identifiers}
             \AttributeTok{use.mc.diff =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# use moment conditions from equations in differences? (i.e. instruments in levels) }
             \AttributeTok{use.mc.lev =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# use moment conditions from equations in levels? (i.e. instruments in differences)}
             \AttributeTok{use.mc.nonlin =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# use nonlinear (quadratic) moment conditions?}
             \AttributeTok{include.y =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# instruments should be derived from the lags of the dependent variable?}
             \AttributeTok{varname.y =} \StringTok{"emp"}\NormalTok{, }\CommentTok{\# name of the dependent variable in the dataset}
             \AttributeTok{lagTerms.y =} \DecValTok{2}\NormalTok{, }\CommentTok{\# number of lags of the dependent variable}
             \AttributeTok{fur.con =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# further control variables (covariates) are included?}
             \AttributeTok{fur.con.diff =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# include further control variables in equations from differences ?}
             \AttributeTok{fur.con.lev =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# include further control variables in equations from level?}
             \AttributeTok{varname.reg.fur =} \FunctionTok{c}\NormalTok{(}\StringTok{"wage"}\NormalTok{, }\StringTok{"capital"}\NormalTok{, }\StringTok{"output"}\NormalTok{), }\CommentTok{\# covariate(s) {-}in the dataset{-} to treat as further controls}
             \AttributeTok{lagTerms.reg.fur =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\CommentTok{\# number of lags of the further controls}
             \AttributeTok{include.dum =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# A logical variable indicating whether dummy variables for the time periods are included (defaults to \textquotesingle{}FALSE\textquotesingle{}).}
             \AttributeTok{dum.diff =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# A logical variable indicating whether dummy variables are included in the equations in first differences (defaults to \textquotesingle{}NULL\textquotesingle{}).}
             \AttributeTok{dum.lev =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# A logical variable indicating whether dummy variables are included in the equations in levels (defaults to \textquotesingle{}NULL\textquotesingle{}).}
             \AttributeTok{varname.dum =} \StringTok{"year"}\NormalTok{,}
             \AttributeTok{w.mat =} \StringTok{"iid.err"}\NormalTok{, }\CommentTok{\# One of the character strings c(\textquotesingle{}"iid.err"\textquotesingle{}, \textquotesingle{}"identity"\textquotesingle{}, \textquotesingle{}"zero.cov"\textquotesingle{}) indicating the type of weighting matrix to use (defaults to \textquotesingle{}"iid.err"\textquotesingle{})}
             \AttributeTok{std.err =} \StringTok{"corrected"}\NormalTok{,}
             \AttributeTok{estimation =} \StringTok{"onestep"}\NormalTok{, }\CommentTok{\# One of the character strings c(\textquotesingle{}"onestep"\textquotesingle{}, \textquotesingle{}"twostep"\textquotesingle{}, \textquotesingle{}"iterative"\textquotesingle{}). Denotes the number of iterations of the parameter procedure (defaults to \textquotesingle{}"twostep"\textquotesingle{}).}
             \AttributeTok{opt.meth =} \StringTok{"none"} \CommentTok{\# numerical optimization procedure. When no nonlinear moment conditions are employed in estimation, closed form estimates can be computed by setting the argument to \textquotesingle{}"none"}
\NormalTok{             )}
\FunctionTok{summary}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Dynamic linear panel estimation (onestep)
## Estimation steps: 1
## 
## Coefficients:
##             Estimate Std.Err.rob z-value.rob Pr(>|z.rob|)    
## L1.emp      0.686226    0.144594       4.746      < 2e-16 ***
## L2.emp     -0.085358    0.056016      -1.524      0.12751    
## L0.wage    -0.607821    0.178205      -3.411      0.00065 ***
## L1.wage     0.392623    0.167993       2.337      0.01944 *  
## L0.capital  0.356846    0.059020       6.046      < 2e-16 ***
## L1.capital -0.058001    0.073180      -0.793      0.42778    
## L2.capital -0.019948    0.032713      -0.610      0.54186    
## L0.output   0.608506    0.172531       3.527      0.00042 ***
## L1.output  -0.711164    0.231716      -3.069      0.00215 ** 
## L2.output   0.105798    0.141202       0.749      0.45386    
## 1979        0.009554    0.010290       0.929      0.35289    
## 1980        0.022015    0.017710       1.243      0.21387    
## 1981       -0.011775    0.029508      -0.399      0.68989    
## 1982       -0.027059    0.029275      -0.924      0.35549    
## 1983       -0.021321    0.030460      -0.700      0.48393    
## 1976       -0.007703    0.031411      -0.245      0.80646    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
##  41 total instruments are employed to estimate 16 parameters
##  27 linear (DIF) 
##  8 further controls (DIF) 
##  6 time dummies (DIF) 
##  
## J-Test (overid restrictions):  70.82 with 25 DF, pvalue: <0.001
## F-Statistic (slope coeff):  528.06 with 10 DF, pvalue: <0.001
## F-Statistic (time dummies):  14.98 with 6 DF, pvalue: 0.0204
\end{verbatim}

\hypertarget{estimation-methods}{%
\chapter{Estimation Methods}\label{estimation-methods}}

Context and Objective:

\begin{itemize}
\tightlist
\item
  You observe a sample \(\mathbf{y}=\{y_1,\dots,y_n\}\).
\item
  You know that these data have been generated by a model parameterized by \(\theta_0 \in \mathbb{R}^K\).
\end{itemize}

\hypertarget{generalized-method-of-moments-gmm}{%
\section{Generalized Method of Moments (GMM)}\label{generalized-method-of-moments-gmm}}

\hypertarget{framework}{%
\subsection{Framework}\label{framework}}

We denote by \(x_t\) a \(p \times 1\) vector of (stationary) variables observed at date \(t\); by \(\theta\) an \(a \times 1\) vector of parameters, and by \(h(x_t;\theta)\) a continuous \(r \times 1\) vector-valued function.

We denote by \(\theta_0\) the true value of \(\theta\) and we assume that \(\theta_0\) satisfies:
\[
\mathbb{E}[h(x_t;\theta_0)] = 0.
\]

We denote by \(\underline{x_t}\) the information contained in the current and past observations of \(x_t\), that is: \(\underline{x_t} = \{x_t,x_{t-1},\dots,x_1\}\). We denote by \(g(\underline{x_T};\theta)\) the sample average of \(h(x_t;\theta)\), i.e.:
\[
g(\underline{x_T};\theta) = \frac{1}{T} \sum_{t=1}^{T} h(x_t;\theta).
\]

Intuition behind GMM: Choose \(\theta\) so as to make the sample moment as close as possible to 0.

\begin{definition}
\protect\hypertarget{def:GMM}{}\label{def:GMM}A GMM estimator of \(\theta_0\) is given by:
\[
\hat{\theta}_T = \mbox{argmin}_{\theta} \quad g(\underline{x_T};\theta)'\, W_T \, g(\underline{x_T};\theta),
\]
where \(W_T\) is a positive definite matrix (that may depend on \(\underline{x_T}\)).
\end{definition}

If \(a = r\) (the dimension of \(\theta\) is the same as that of \(h(x_t;\theta)\), or \(g(\underline{x_T};\theta)\)), \(\hat{\theta}_T\) is such that:
\[
g(\underline{x_T};\hat{\theta}_T) = 0.
\]
Under regularity and identification conditions:
\[
\hat{\theta}_{T} \overset{p}{\rightarrow} \theta_0,
\]
i.e.~\(\forall \varepsilon>0\), \(\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\theta}_{T} - \theta_0|>\varepsilon) = 0\).

\textbf{Optimal weighting matrix}. The GMM estimator achieving the minimum asymptotic variance is obtained when \(W_T\) is the inverse of the matrix \(S\) defined by:
\[
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
\]
where \(\Gamma_\nu := \mathbb{E}[h(x_t;\theta_0) h(x_{t-\nu};\theta_0)']\).

For \(\nu \ge 0\), let us define \(\hat{\Gamma}_{\nu,T}\) by:
\[
\hat{\Gamma}_{\nu,T} = \frac{1}{T} \sum_{t=\nu + 1}^{T} h(x_t;\hat{\theta}_T)h(x_{t-\nu};\hat{\theta}_T)',
\]
\(S\) can be approximated by:
\begin{eqnarray}
&\hat{\Gamma}_{0,T}& \quad \mbox{if the $h(x_t;\theta_0)$ are serially uncorrelated and} \nonumber \\
&\hat{\Gamma}_{0,T}& + \sum_{\nu=1}^{q}[1-\nu/(q+1)](\hat{\Gamma}_{\nu,T}+\hat{\Gamma}_{\nu,T}') \quad \mbox{otherwise.}    \label{eq:Shat}
\end{eqnarray}

\textbf{Asymptotic distribution of \(\hat\theta_T\)}

We have:
\[
\sqrt{T}(\hat\theta_T - \theta_0) \overset{\mathcal{L}}{\rightarrow} \mathcal{N}(0,V),
\]
where \(V = (DS^{-1}D')^{-1}\).

\(V\) can be approximated by \((\hat{D}_T\hat{S}_T^{-1}\hat{D}_T')^{-1}\),
where \(\hat{S}_T\) is given by Eq. \eqref{eq:Shat} and
\[
\hat{D}'_T := \left.\frac{\partial g(\underline{x_T};\theta)}{\partial \theta'}\right|_{\theta = \hat\theta_T}.
\]

\hypertarget{example-estimation-of-the-stochastic-discount-factor-s.d.f.}{%
\subsection{Example: Estimation of the Stochastic Discount Factor (s.d.f.)}\label{example-estimation-of-the-stochastic-discount-factor-s.d.f.}}

Under the no-arbitrage assumption, there exists a random variable \(\mathcal{M}_{t,t+1}\) such that
\[
\mathbb{E}_t(\mathcal{M}_{t,t+1}R_{t+1})=1
\]
for any (gross) asset return \(R_t\). In the following, \(R_t\) denotes a \(n_r\)-dimensional vector of gross returns.

We consider the following specification of the s.d.f.:
\begin{equation}
\mathcal{M}_{t,t+1} = 1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1})), \label{eq:sdf}
\end{equation}
where \(F_t\) is a vector of factors. Eq. \eqref{eq:sdf} then reads:
\[
\mathbb{E}_t([1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1}))]R_{t+1})=1.
\]

Assume that the date-\(t\) information set is \(\mathcal{I}_t=\{\textbf{z}_t,\mathcal{I}_{t-1}\}\), where \(\textbf{z}_t\) is a vector of variables observed on date \(t\). (We then have \(\mathbb{E}_t(\bullet) \equiv \mathbb{E}(\bullet|\mathcal{I}_t)\).)

We can use \(\textbf{z}_t\) as an instrument. Indeed, we have:
\begin{eqnarray}
&&\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]) \nonumber \\
&=&\mathbb{E}(\mathbb{E}_t\{z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]\})\nonumber\\
&=&\mathbb{E}(z_{i,t} \underbrace{\mathbb{E}_t\{\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1\}}_{=0})=0.\label{eq:momF}
\end{eqnarray}
We have then converted a conditional moment condition into a unconditional one (which we need to implement the theory above). However, at that stage, we cannot still not directly use the GMM formulas because of the conditional expectation \(\mathbb{E}_t(F_{t+1})\) that appears in \(\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1])=0\).

To go further, let us assume that:
\[
\mathbb{E}_t(F_{t+1}) = \textbf{b}_F \textbf{z}_t.
\]
We can then easily estimate matrix \(\textbf{b}_F\) (of dimension \(n_F \times n_z\)) by OLS. Note here that these OLS can be seen as a special GMM case. Indeed, as was done in Eq. \eqref{eq:momF}, we can show that, for the \(j^{th}\) component of \(F_t\), we have:
\[
\mathbb{E}( [F_{j,t+1} - \textbf{b}_{F,j} \textbf{z}_t]\textbf{z}_{t})=0,
\]
where \(\textbf{b}_{F,j}\) denotes the \(j^{th}\) row of \(\textbf{b}_{F}\). This yields the OLS formula.

At that stage, we count on the following moment restrictions to estimate \(\textbf{b}_M\):
\[
\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \textbf{b}_F \textbf{z}_t\}R_{t+1}-R_{t+1}+1])=0.
\]
Specifically, the number of restrictions is \(n_R \times n_z\). Let us implement this approach in the U.S. context, using data extracted from the \href{https://fred.stlouisfed.org}{FRED database}. In factor \(F_t\), we use the changes in the VIX and in the personal consumption expenditures. The returns (\(R_t\)) are based on the Wilshire 5000 Price Index (a stock price index) and on the ICE BofA BBB US Corporate Index Total Return Index (a bond return index).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fredr)}
\FunctionTok{fredr\_set\_key}\NormalTok{(}\StringTok{"df65e14c054697a52b4511e77fcfa1f3"}\NormalTok{)}
\NormalTok{start\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"1990{-}01{-}01"}\NormalTok{); end\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2022{-}01{-}01"}\NormalTok{)}
\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(ticker)\{}
  \FunctionTok{fredr}\NormalTok{(}\AttributeTok{series\_id =}\NormalTok{ ticker,}
        \AttributeTok{observation\_start =}\NormalTok{ start\_date,}\AttributeTok{observation\_end =}\NormalTok{ end\_date,}
        \AttributeTok{frequency =} \StringTok{"m"}\NormalTok{,}\AttributeTok{aggregation\_method =} \StringTok{"avg"}\NormalTok{)}
\NormalTok{\}}
\NormalTok{vix }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(}\StringTok{"VIXCLS"}\NormalTok{) }\CommentTok{\# VIX}
\NormalTok{pce }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(}\StringTok{"PCE"}\NormalTok{) }\CommentTok{\# Personal consumption expenditures}
\NormalTok{sto }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(}\StringTok{"WILL5000PRFC"}\NormalTok{) }\CommentTok{\# Wilshire 5000 Full Cap Price Index}
\NormalTok{bdr }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(}\StringTok{"BAMLCC0A4BBBTRIV"}\NormalTok{) }\CommentTok{\# ICE BofA BBB US Corporate Index Total Return Index}
\NormalTok{T }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(vix)[}\DecValTok{1}\NormalTok{]}
\NormalTok{dvix }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(vix}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{vix}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]) }\CommentTok{\# change in VIX t+1}
\NormalTok{dpce }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(pce}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{pce}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]) }\CommentTok{\# change in PCE t+1}
\NormalTok{dsto }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(sto}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{sto}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]) }\CommentTok{\# return t+1}
\NormalTok{dbdr }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(bdr}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{bdr}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]) }\CommentTok{\# return t+1}
\NormalTok{dvix\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(vix}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]}\SpecialCharTok{/}\NormalTok{vix}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{)]) }\CommentTok{\# change in VIX t}
\NormalTok{dpce\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(pce}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]}\SpecialCharTok{/}\NormalTok{pce}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{)]) }\CommentTok{\# change in PCE t}
\NormalTok{dsto\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(sto}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]}\SpecialCharTok{/}\NormalTok{sto}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{)]) }\CommentTok{\# return t}
\NormalTok{dbdr\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(bdr}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{2}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)]}\SpecialCharTok{/}\NormalTok{bdr}\SpecialCharTok{$}\NormalTok{value[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}2}\NormalTok{)]) }\CommentTok{\# return t}
\end{Highlighting}
\end{Shaded}

Define the matrices containing the \(F_{t+1}\), \(\textbf{z}_t\), and \(R_{t+1}\) vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{F\_tp1 }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(dvix,dpce)}
\NormalTok{Z     }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{,dvix\_1,dpce\_1,dsto\_1,dbdr\_1)}
\NormalTok{b\_F }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ Z) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Z) }\SpecialCharTok{\%*\%}\NormalTok{ F\_tp1)}
\NormalTok{F\_innov }\OtherTok{\textless{}{-}}\NormalTok{ F\_tp1 }\SpecialCharTok{{-}}\NormalTok{ Z }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(b\_F)}
\NormalTok{R\_tp1 }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(dsto,dbdr)}
\NormalTok{n\_F }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(F\_tp1)[}\DecValTok{2}\NormalTok{]; n\_R }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(R\_tp1)[}\DecValTok{2}\NormalTok{]; n\_z }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(Z)[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Function \texttt{f\_aux} compute the \(h(x_t;\theta)\) and the \(g(\underline{x_T};\theta)\); function \texttt{f2beMin} is the function to be minimized.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\_aux }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta)\{}
\NormalTok{  b\_M }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(theta[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_F],}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{)}
\NormalTok{  R\_aux }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(F\_innov }\SpecialCharTok{\%*\%}\NormalTok{ b\_M,T}\DecValTok{{-}2}\NormalTok{,n\_R) }\SpecialCharTok{*}\NormalTok{ R\_tp1 }\SpecialCharTok{{-}}\NormalTok{ R\_tp1 }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{  H }\OtherTok{\textless{}{-}}\NormalTok{ (R\_aux }\SpecialCharTok{\%x\%} \FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,n\_z)) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,n\_R) }\SpecialCharTok{\%x\%}\NormalTok{ Z)}
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{apply}\NormalTok{(H,}\DecValTok{2}\NormalTok{,mean),}\AttributeTok{ncol=}\DecValTok{1}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{g=}\NormalTok{g,}\AttributeTok{H=}\NormalTok{H))}
\NormalTok{\}}
\NormalTok{f2beMin }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta,W)\{}\CommentTok{\# function to be minimized}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{t}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{g) }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{g)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now, let's minimize this function. We consider 5 iterations (where \(W\) is updated).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,n\_F)) }\CommentTok{\# inital value}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}\CommentTok{\# recursion on W}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta)}
\NormalTok{  W }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{T }\SpecialCharTok{*} \FunctionTok{t}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{H) }\SpecialCharTok{\%*\%}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{H)}
\NormalTok{  res.optim }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(theta,f2beMin,}\AttributeTok{W=}\NormalTok{W,}
                     \AttributeTok{method=}\StringTok{"BFGS"}\NormalTok{, }\CommentTok{\# could be "Nelder{-}Mead"}
                     \AttributeTok{control=}\FunctionTok{list}\NormalTok{(}\AttributeTok{trace=}\ConstantTok{FALSE}\NormalTok{,}\AttributeTok{maxit=}\DecValTok{200}\NormalTok{),}\AttributeTok{hessian=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  theta }\OtherTok{\textless{}{-}}\NormalTok{ res.optim}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Finally, let's compute the standard deviation of the parameter estimates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eps }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{0001}
\NormalTok{g0 }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta)}\SpecialCharTok{$}\NormalTok{g}
\NormalTok{D }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(theta))\{}
\NormalTok{  theta.i }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{  theta.i[i] }\OtherTok{\textless{}{-}}\NormalTok{ theta.i[i] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{  gi }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta.i)}\SpecialCharTok{$}\NormalTok{g}
\NormalTok{  D }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(D,(gi}\SpecialCharTok{{-}}\NormalTok{g0)}\SpecialCharTok{/}\NormalTok{eps)}
\NormalTok{\}}
\NormalTok{V }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{T }\SpecialCharTok{*} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(D) }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ D)}
\NormalTok{std.dev }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(V));t.stud }\OtherTok{\textless{}{-}}\NormalTok{ theta}\SpecialCharTok{/}\NormalTok{std.dev}
\FunctionTok{cbind}\NormalTok{(theta,std.dev,t.stud)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           theta    std.dev     t.stud
## [1,] -0.6774355  0.3182614 -2.1285502
## [2,]  4.3612746 13.1163581  0.3325065
\end{verbatim}

The Hansen statistic can be used to test the model. If the model is correct, we have:
\[
T g(\underline{x_T};\theta)'\, S^{-1} \, g(\underline{x_T};\theta) \sim \,i.i.d.\,\chi^2(J - K),
\]
where \(J\) is the number of moment contraints (\(n_z \times n_r\) here) and \(K\) is the number of estimated parameters (\(=n_F\) here).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{f\_aux}\NormalTok{(theta)}\SpecialCharTok{$}\NormalTok{g}
\NormalTok{Hanse\_stat }\OtherTok{\textless{}{-}}\NormalTok{ T }\SpecialCharTok{*} \FunctionTok{t}\NormalTok{(g) }\SpecialCharTok{\%*\%}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ g}
\NormalTok{pvalue }\OtherTok{\textless{}{-}} \FunctionTok{pchisq}\NormalTok{(}\AttributeTok{q =}\NormalTok{ Hanse\_stat,}\AttributeTok{df =}\NormalTok{ n\_R}\SpecialCharTok{*}\NormalTok{n\_z }\SpecialCharTok{{-}}\NormalTok{ n\_F)}
\end{Highlighting}
\end{Shaded}

\hypertarget{maximum-likelihood-estimation}{%
\section{Maximum Likelihood Estimation}\label{maximum-likelihood-estimation}}

Intuition behind the Maximum Likelihood Estimation: Estimator = the value of \(\theta\) that is such that the probability of having observed \(\mathbf{y}\) is the highest possible.

Assume that the time periods between the arrivals of two customers in a shop, denoted by \(y_i\), are i.i.d. and follow an exponential distribution, i.e.~\(y_i \sim \mathcal{E}(\lambda)\).

You have observed these arrivals for some time, thereby constituting a sample \(\{y_1,\dots,y_n\}\). You want to estimate \(\lambda\) (i.e.~in that case, the vector of parameters is simply \(\theta = \lambda\)).

The density of \(Y\) is \(f(y;\lambda) = \dfrac{1}{\lambda}\exp(-y/\lambda)\). Fig. \ref{fig:MLE1} represents that density functions for different values of \(\lambda\).

Your 200 observations are reported at the bottom of Fig. \ref{fig:MLE1} (red).
You build the histogram and report it on the same chart.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{AdvECTS_files/figure-latex/MLE1-1} 

}

\caption{The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations}\label{fig:MLE1}
\end{figure}

What is your estimate of \(\lambda\)?

Now, assume that you have only four observations: \(y_1=1.1\), \(y_2=2.2\), \(y_3=0.7\) and \(y_4=5.0\).
What was the probability of observing, for a small \(\varepsilon\),

\begin{itemize}
\tightlist
\item
  \(1.1-\varepsilon \le Y_1 < 1.1+\varepsilon\),
\item
  \(2.2-\varepsilon \le Y_2 < 2.2+\varepsilon\),
\item
  \(0.7-\varepsilon \le Y_3 < 0.7+\varepsilon\) and
\item
  \(5.0-\varepsilon \le Y_4 < 5.0+\varepsilon\)?
\end{itemize}

Because the \(y_i\)s are i.i.d., this probability is \(\prod_{i=1}^4(2\varepsilon f(y_i,\lambda))\).
The next plot shows the probability (divided by \(16\varepsilon^4\)) as a function of \(\lambda\).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{AdvECTS_files/figure-latex/MLE2-1} 

}

\caption{Proba. that $y_i-\varepsilon \le Y_i < y_i+\varepsilon$, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function.}\label{fig:MLE2}
\end{figure}

Back to the example with 200 observations:

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{AdvECTS_files/figure-latex/MLE3-1} 

}

\caption{Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.}\label{fig:MLE3}
\end{figure}

\hypertarget{notations}{%
\subsection{Notations}\label{notations}}

\(f(y;\boldsymbol\theta)\) denotes the probability density function (p.d.f.) of a random variable \(Y\) which depends on a set of parameters \(\boldsymbol\theta\).

Density of \(n\) independent and identically distributed (i.i.d.) observations of \(Y\):
\[
f(y_1,\dots,y_n;\boldsymbol\theta) = \prod_{i=1}^n f(y_i;\boldsymbol\theta).
\]
\(\mathbf{y}\) denotes the vector of observations; \(\mathbf{y} = \{y_1,\dots,y_n\}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}[Likelihood function]
\protect\hypertarget{def:likelihood}{}\label{def:likelihood}\(\mathcal{L}: \boldsymbol\theta \rightarrow \mathcal{L}(\boldsymbol\theta;\mathbf{y})=f(y_1,\dots,y_n;\boldsymbol\theta)\) is the \textbf{likelihood function}.
\end{definition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

We often work with \(\log \mathcal{L}\), the \textbf{log-likelihood function}.

\begin{example}[Gaussian distribution]
\protect\hypertarget{exm:normal}{}\label{exm:normal}If \(y_i \sim \mathcal{N}(\mu,\sigma^2)\), then
\[
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = - \frac{1}{2}\sum_{i=1}^n\left( \log \sigma^2 + \log 2\pi + \frac{(y_i-\mu)^2}{\sigma^2} \right).
\]
\end{example}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}[Score]
\protect\hypertarget{def:score}{}\label{def:score}The score \(S(y;\boldsymbol\theta)\) is given by \(\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\).
\end{definition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

If \(y_i \sim \mathcal{N}(\mu,\sigma^2)\) (Example \ref{exm:normal}), then
\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} =
\left[\begin{array}{c}
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \mu}\\
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \sigma^2}
\end{array}\right] =
\left[\begin{array}{c}
\dfrac{y-\mu}{\sigma^2}\\
\frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right)
\end{array}\right].
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}[Score expectation]
\protect\hypertarget{prp:score}{}\label{prp:score}The expectation of the score is zero.
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
We have:
\begin{eqnarray*}
\mathbb{E}\left(\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\right) &=&
\int \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta) dy \\
&=& \int \frac{\partial f(y;\boldsymbol\theta)/\partial \boldsymbol\theta}{f(y;\boldsymbol\theta)} f(y;\boldsymbol\theta) dy =
\frac{\partial}{\partial \boldsymbol\theta} \int f(y;\boldsymbol\theta) dy =
\partial 1 /\partial \boldsymbol\theta = 0,
\end{eqnarray*}
which gives the result.
\end{proof}

\begin{definition}[Fisher information matrix]
\protect\hypertarget{def:Fisher}{}\label{def:Fisher}The \textbf{information matrix} is (minus) the the expectation of the second derivatives of the log-likelihood function:
\[
\mathcal{I}_Y(\boldsymbol\theta) = - \mathbb{E} \left( \frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right).
\]
\end{definition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}
\protect\hypertarget{prp:Fisher}{}\label{prp:Fisher}We have \(\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E} \left[ \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right) \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)' \right] = \mathbb{V}ar[S(Y;\boldsymbol\theta)]\).
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
We have \(\frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = \frac{\partial^2 f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\frac{1}{f(Y;\boldsymbol\theta)} - \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta'}\). The expectation of the first right-hand side term is \(\partial^2 1 /(\partial \boldsymbol\theta \partial \boldsymbol\theta') = \mathbf{0}\), which gives the result.
\end{proof}

\begin{example}
If \(y_i \sim\,i.i.d.\, \mathcal{N}(\mu,\sigma^2)\), let \(\boldsymbol\theta = [\mu,\sigma^2]'\) then
\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} = \left[\frac{y-\mu}{\sigma^2} \quad \frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right) \right]'
\]
and
\[
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E}\left( \frac{1}{\sigma^4}
\left[
\begin{array}{cc}
\sigma^2&y-\mu\\
y-\mu & \frac{(y-\mu)^2}{\sigma^2}-\frac{1}{2}
\end{array}\right]
\right)=
\left[
\begin{array}{cc}
1/\sigma^2&0\\
0 & 1/(2\sigma^4)
\end{array}\right].
\]
\end{example}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}[Additive property of the Info. mat.]
\protect\hypertarget{prp:additiv}{}\label{prp:additiv}The information matrix resulting from two independent experiments is the sum of the information matrices:
\[
\mathcal{I}_{X,Y}(\boldsymbol\theta) = \mathcal{I}_X(\boldsymbol\theta) + \mathcal{I}_Y(\boldsymbol\theta).
\]
\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

:::\{.proof\} Immediately obtained from the definition (see Def. @ref\{def:Fisher\}).
:::

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{theorem}[Fr\'echet-Darmois-Cram\'er-Rao bound]
\protect\hypertarget{thm:FDCR}{}\label{thm:FDCR}Consider an unbiased estimator of \(\boldsymbol\theta\) denoted by \(\hat{\boldsymbol\theta}(Y)\).

The variance of the random variable \(\boldsymbol\omega'\hat{\boldsymbol\theta}\) (which is a linear combination of the components of \(\hat{\boldsymbol\theta}\)) is larger than:
\[
(\boldsymbol\omega'\boldsymbol\omega)^2/(\boldsymbol\omega' \mathcal{I}_Y(\boldsymbol\theta) \boldsymbol\omega).
\]
\end{theorem}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
The Cauchy-Schwarz inequality implies that \(\sqrt{\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y))\mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))} \ge |\boldsymbol\omega'\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)]\boldsymbol\omega |\). Now, \(\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)] = \int_y \hat{\boldsymbol\theta}(y) \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta)dy = \frac{\partial}{\partial \boldsymbol\theta}\int_y \hat{\boldsymbol\theta}(y) f(y;\boldsymbol\theta)dy = \mathbf{I}\) because \(\hat{\boldsymbol\theta}\) is unbiased.

Therefore \(\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y)) \ge \mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))^{-1} (\boldsymbol\omega'\boldsymbol\omega)^2\). Prop. \ref{prp:Fisher} leads to the result.
\end{proof}

\begin{definition}
\protect\hypertarget{def:identif}{}\label{def:identif}The vector of parameters \(\boldsymbol\theta\) is identifiable if, for any other vector \(\boldsymbol\theta^*\):
\[
\boldsymbol\theta^* \ne \boldsymbol\theta \Rightarrow \mathcal{L}(\boldsymbol\theta^*;\mathbf{y}) \ne \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
\]
\end{definition}

\begin{definition}[Maximum Likelihood Estimator (MLE)]
\protect\hypertarget{def:MLEest}{}\label{def:MLEest}The maximum likelihood estimator (MLE) is the vector \(\boldsymbol\theta\) that maximizes the likelihood function. Formally:
\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\mathbf{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).\label{eq:MLEestimator}
\end{equation}
\end{definition}

\begin{definition}[Likelihood equation]
\protect\hypertarget{def:likFunction}{}\label{def:likFunction}Necessary condition for maximizing the likelihood function:
\begin{equation}
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta} = \mathbf{0}.
\end{equation}
\end{definition}

\begin{hypothesis}[Regularity assumptions]
\protect\hypertarget{hyp:MLEregularity}{}\label{hyp:MLEregularity}

We have:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  \(\boldsymbol\theta \in \Theta\) where \(\Theta\) is compact.
\item
  \(\boldsymbol\theta_0\) is identified.
\item
  The log-likelihood function is continuous in \(\boldsymbol\theta\).
\item
  \(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\) exists.
\item
  The log-likelihood function is such that \((1/n)\log\mathcal{L}(\boldsymbol\theta;\mathbf{y})\) converges almost surely to \(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\), uniformly in \(\boldsymbol\theta \in \Theta\).
\item
  The log-likelihood function is twice continuously differentiable in an open neighborood of \(\boldsymbol\theta_0\).
\item
  The matrix \(\mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \quad \mbox{(Fisher Information matrix)}\) exists and is nonsingular.
\end{enumerate}

\end{hypothesis}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proposition}[Properties of MLE]
\protect\hypertarget{prp:MLEproperties}{}\label{prp:MLEproperties}

Under regularity conditions (Assumptions \ref{hyp:MLEregularity}), the MLE is:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \textbf{Consistent}: \(\mbox{plim}\quad \boldsymbol\theta_{MLE} = \theta_0\) (\(\theta_0\) is the true vector of parameters).
\item
  \textbf{Asymptotically normal}: \(\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\mathbf{I}(\boldsymbol\theta_0)^{-1})\), where
  \[
  \mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0). \quad \mbox{(Fisher Info. matrix)}
  \]
\item
  \textbf{Asymptotically efficient}: \(\boldsymbol\theta_{MLE}\) is asymptotically efficient and achieves the Fr'echet-Darmois-Cram'er-Rao lower bound for consistent estimators.
\item
  \textbf{Invariant}: The MLE of \(g(\boldsymbol\theta_0)\) is \(g(\boldsymbol\theta_{MLE})\) if \(g\) is a continuous and continuously differentiable function.
\end{enumerate}

\end{proposition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{proof}
See \href{https://www.dropbox.com/s/8xelbghtvnd43yh/Proofs_MLE.pdf?dl=0}{Online additional material}.
\end{proof}

Note that (b) also writes:
\begin{equation}
\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0)^{-1}). \label{eq:normMLE}
\end{equation}

The asymptotic covariance matrix of the MLE is:
\[
[\mathbf{I}(\boldsymbol\theta_0)]^{-1} = \left[- \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \right]^{-1}.
\]
A direct (analytical) evaluation of this expectation is often out of reach.

It can however be estimated by, either:
\begin{eqnarray}
\hat{\mathbf{I}}_1^{-1} &=&  \left( - \frac{\partial^2 \log \mathcal{L}({\boldsymbol\theta_{MLE}};\mathbf{y})}{\partial {\boldsymbol\theta} \partial {\boldsymbol\theta}'}\right)^{-1}, \label{eq:III1}\\
\hat{\mathbf{I}}_2^{-1} &=&  \left( \sum_{i=1}^n \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta}} \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta'}} \right)^{-1}.  \label{eq:I2}
\end{eqnarray}

\hypertarget{to-sum-up-mle-in-practice}{%
\subsection{To sum up -- MLE in practice}\label{to-sum-up-mle-in-practice}}

\begin{itemize}
\item
  A parametric model (depending on the vector of parameters \(\boldsymbol\theta\) whose ``true'' value is \(\boldsymbol\theta_0\)) is specified.
\item
  i.i.d. sources of randomness are identified.
\item
  The density associated to one observation \(y_i\) is computed analytically (as a function of \(\boldsymbol\theta\)): \(f(y;\boldsymbol\theta)\).
\item
  The log-likelihood is \(\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = \sum_i \log f(y_i;\boldsymbol\theta)\).
\item
  The MLE estimator results from the optimization problem (this is Eq. \eqref{eq:MLEestimator}):
  \begin{equation}
  \boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
  \end{equation}
\item
  We have: \(\boldsymbol\theta_{MLE} \sim \mathcal{N}(\theta_0,\mathbf{I}(\boldsymbol\theta_0)^{-1})\), where \(\mathbf{I}(\boldsymbol\theta_0)^{-1}\) is estimated by means of Eq. \eqref{eq:III1} or Eq. \eqref{eq:I2}. Most of the time, this computation is numerical.
\end{itemize}

\hypertarget{example-mle-estimation-of-a-mixture-of-gaussian-distribution}{%
\subsection{Example: MLE estimation of a mixture of Gaussian distribution}\label{example-mle-estimation-of-a-mixture-of-gaussian-distribution}}

Consider the returns of the SMI index. Let's assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. \(f(x;\boldsymbol\theta)\), with \(\boldsymbol\theta = [\mu_1,\sigma_1,\mu_2,\sigma_2,p]'\), is given by:
\[
p \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) + (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right).
\]
(See \href{https://jrenne.shinyapps.io/density/}{p.d.f. of mixtures of Gaussian dist.})

The maximum likelihood estimate is \(\boldsymbol\theta_{MLE}=[0.30,1.40,-1.45,3.61,0.87]'\).

The first two entries of the diagonal of \(\hat{\mathbf{I}}_1^{-1}\) are \(0.00528\) and \(0.00526\). They are the estimates of \(\mathbb{V}ar(\mu_{1,MLE})\) and of \(\mathbb{V}ar(\sigma_{1,MLE})\), respectively.

95\% confidence intervals for \(\mu_1\) and \(\sigma_1\) are, respectively:
\[
0.30 \pm 1.96\underbrace{\sqrt{0.00528}}_{=0.0726} \quad \mbox{ and } \quad 1.40 \pm 1.96\underbrace{\sqrt{0.00526}.}_{=0.0725}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smi }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/jrenne/Data4courses/master/SMI/SMI.csv"}\NormalTok{,}
                \AttributeTok{dec =} \StringTok{"."}\NormalTok{,}\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{na.strings =} \StringTok{"null"}\NormalTok{)}
\NormalTok{smi}\SpecialCharTok{$}\NormalTok{Date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{Date,}\StringTok{"\%m/\%d/\%y"}\NormalTok{)}
\NormalTok{T }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(smi)[}\DecValTok{1}\NormalTok{]}
\NormalTok{h }\OtherTok{\textless{}{-}} \DecValTok{5} \CommentTok{\# holding period (one week)}
\NormalTok{smi}\SpecialCharTok{$}\NormalTok{r }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\ConstantTok{NaN}\NormalTok{,h),}
           \DecValTok{100}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(}\FunctionTok{log}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{Close[(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{h)}\SpecialCharTok{:}\NormalTok{T]}\SpecialCharTok{/}\NormalTok{smi}\SpecialCharTok{$}\NormalTok{Close[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\SpecialCharTok{{-}}\NormalTok{h)])))}
\NormalTok{indic.dates }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,T,}\AttributeTok{by=}\DecValTok{5}\NormalTok{)  }\CommentTok{\# weekly returns}
\NormalTok{smi }\OtherTok{\textless{}{-}}\NormalTok{ smi[indic.dates,]}
\NormalTok{smi }\OtherTok{\textless{}{-}}\NormalTok{ smi[}\FunctionTok{complete.cases}\NormalTok{(smi),]}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{Date,smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{""}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"in percent"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{0}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\FunctionTok{mean}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\FunctionTok{sd}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}\AttributeTok{lty=}\DecValTok{3}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\FunctionTok{mean}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{*}\FunctionTok{sd}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}\AttributeTok{lty=}\DecValTok{3}\NormalTok{,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{AdvECTS_files/figure-latex/smiData-1} 

}

\caption{Time series of SMI weekly returns (source: Yahoo Finance).}\label{fig:smiData}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta,y)\{ }\CommentTok{\# Likelihood function}
\NormalTok{  mu}\FloatTok{.1} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]; mu}\FloatTok{.2} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{2}\NormalTok{]}
\NormalTok{  sigma}\FloatTok{.1} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]; sigma}\FloatTok{.2} \OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{4}\NormalTok{]}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{5}\NormalTok{])}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(theta[}\DecValTok{5}\NormalTok{]))}
\NormalTok{  res }\OtherTok{\textless{}{-}}\NormalTok{ p}\SpecialCharTok{*}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{sigma}\FloatTok{.1}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(y}\SpecialCharTok{{-}}\NormalTok{mu}\FloatTok{.1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{sigma}\FloatTok{.1}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{+} 
\NormalTok{    (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)}\SpecialCharTok{*}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{*}\NormalTok{sigma}\FloatTok{.2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(y}\SpecialCharTok{{-}}\NormalTok{mu}\FloatTok{.2}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{sigma}\FloatTok{.2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}
\NormalTok{log.f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta,y)\{ }\CommentTok{\#log{-}Likelihood function}
  \FunctionTok{return}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\FunctionTok{f}\NormalTok{(theta,y))))}
\NormalTok{\}}
\NormalTok{res.optim }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\FloatTok{1.5}\NormalTok{,.}\DecValTok{5}\NormalTok{),}
\NormalTok{                   log.f,}
                   \AttributeTok{y=}\NormalTok{smi}\SpecialCharTok{$}\NormalTok{r,}
                   \AttributeTok{method=}\StringTok{"BFGS"}\NormalTok{, }\CommentTok{\# could be "Nelder{-}Mead"}
                   \AttributeTok{control=}\FunctionTok{list}\NormalTok{(}\AttributeTok{trace=}\ConstantTok{FALSE}\NormalTok{,}\AttributeTok{maxit=}\DecValTok{100}\NormalTok{),}\AttributeTok{hessian=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{theta }\OtherTok{\textless{}{-}}\NormalTok{ res.optim}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{theta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.3012379 -1.3167476  1.7715072  4.8197596  1.9454889
\end{verbatim}

Now, let us compute estimates of the covariance matrix of the MLE:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Hessian approach:}
\NormalTok{J }\OtherTok{\textless{}{-}}\NormalTok{ res.optim}\SpecialCharTok{$}\NormalTok{hessian}
\NormalTok{I}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(J)}
\CommentTok{\# Outer{-}product of gradient approach:}
\NormalTok{log.f}\FloatTok{.0} \OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{f}\NormalTok{(theta,smi}\SpecialCharTok{$}\NormalTok{r))}
\NormalTok{epsilon }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{00000001}
\NormalTok{d.log.f }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(theta))\{}
\NormalTok{  theta.i }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{  theta.i[i] }\OtherTok{\textless{}{-}}\NormalTok{ theta.i[i] }\SpecialCharTok{+}\NormalTok{ epsilon}
\NormalTok{  log.f.i }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{f}\NormalTok{(theta.i,smi}\SpecialCharTok{$}\NormalTok{r))}
\NormalTok{  d.log.f }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(d.log.f,}
\NormalTok{                   (log.f.i }\SpecialCharTok{{-}}\NormalTok{ log.f}\FloatTok{.0}\NormalTok{)}\SpecialCharTok{/}\NormalTok{epsilon)}
\NormalTok{\}}
\NormalTok{V }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(d.log.f) }\SpecialCharTok{\%*\%}\NormalTok{ d.log.f}
\NormalTok{I}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(d.log.f) }\SpecialCharTok{\%*\%}\NormalTok{ d.log.f)}
\CommentTok{\# Misspecification{-}robust approach (sandwich formula):}
\NormalTok{I}\FloatTok{.3} \OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(J) }\SpecialCharTok{\%*\%}\NormalTok{ V }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(J)}
\FunctionTok{cbind}\NormalTok{(}\FunctionTok{diag}\NormalTok{(I}\FloatTok{.1}\NormalTok{),}\FunctionTok{diag}\NormalTok{(I}\FloatTok{.2}\NormalTok{),}\FunctionTok{diag}\NormalTok{(I}\FloatTok{.3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             [,1]        [,2]       [,3]
## [1,] 0.003683422 0.003199481 0.00586160
## [2,] 0.226892824 0.194283391 0.38653389
## [3,] 0.005764271 0.002769579 0.01712255
## [4,] 0.194081311 0.047466419 0.83130838
## [5,] 0.092114437 0.040366005 0.31347858
\end{verbatim}

According to the first (respectively third) type of estimate for the covariance matrix, a 95\% confidence interval for \(\mu_1\) is {[}0.182, 0.42{]} (resp. {[}0.151, 0.451{]}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\AttributeTok{by=}\NormalTok{.}\DecValTok{01}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x,}\FunctionTok{f}\NormalTok{(theta,x),}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"returns, in percent"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{""}\NormalTok{,}
     \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{1.4}\SpecialCharTok{*}\FunctionTok{max}\NormalTok{(}\FunctionTok{f}\NormalTok{(theta,x))))}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{density}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r),}\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{,}\AttributeTok{lty=}\DecValTok{3}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x,}\FunctionTok{dnorm}\NormalTok{(x,}\AttributeTok{mean=}\FunctionTok{mean}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r),}\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r)),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{lty=}\DecValTok{2}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{rug}\NormalTok{(smi}\SpecialCharTok{$}\NormalTok{r,}\AttributeTok{col=}\StringTok{"blue"}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \FunctionTok{c}\NormalTok{(}\StringTok{"Kernel estimate (non{-}parametric)"}\NormalTok{,}\StringTok{"Estimated mixture of Gaussian distr. (MLE, parametric)"}\NormalTok{,}\StringTok{"Normal distribution"}\NormalTok{),}
       \AttributeTok{lty=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\CommentTok{\# gives the legend appropriate symbols (lines)}
       \AttributeTok{lwd=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{), }\CommentTok{\# line width}
       \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{,}\StringTok{"black"}\NormalTok{,}\StringTok{"red"}\NormalTok{), }\CommentTok{\# gives the legend lines the correct color and width}
       \AttributeTok{pt.bg=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
       \AttributeTok{pt.cex =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
       \AttributeTok{bg=}\StringTok{"white"}\NormalTok{,}
       \AttributeTok{seg.len =} \DecValTok{4}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{AdvECTS_files/figure-latex/smidistri-1} 

}

\caption{Comparison of different estimates of the distribution of returns.}\label{fig:smidistri}
\end{figure}

\hypertarget{microeconometrics}{%
\chapter{Microeconometrics}\label{microeconometrics}}

\hypertarget{time-series}{%
\chapter{Time Series}\label{time-series}}

blabla

\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

\hypertarget{statitical-tables}{%
\section{Statitical Tables}\label{statitical-tables}}

\begin{table}

\caption{\label{tab:Normal}Quantiles of the $\mathcal{N}(0,1)$ distribution. If $a$ and $b$ are respectively the row and column number; then the corresponding cell gives $\mathbb{P}(0<X\le a+b)$, where $X \sim \mathcal{N}(0,1)$.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r|r}
\hline
  & 0 & 0.01 & 0.02 & 0.03 & 0.04 & 0.05 & 0.06 & 0.07 & 0.08 & 0.09\\
\hline
0 & 0.5000 & 0.6179 & 0.7257 & 0.8159 & 0.8849 & 0.9332 & 0.9641 & 0.9821 & 0.9918 & 0.9965\\
\hline
0.1 & 0.5040 & 0.6217 & 0.7291 & 0.8186 & 0.8869 & 0.9345 & 0.9649 & 0.9826 & 0.9920 & 0.9966\\
\hline
0.2 & 0.5080 & 0.6255 & 0.7324 & 0.8212 & 0.8888 & 0.9357 & 0.9656 & 0.9830 & 0.9922 & 0.9967\\
\hline
0.3 & 0.5120 & 0.6293 & 0.7357 & 0.8238 & 0.8907 & 0.9370 & 0.9664 & 0.9834 & 0.9925 & 0.9968\\
\hline
0.4 & 0.5160 & 0.6331 & 0.7389 & 0.8264 & 0.8925 & 0.9382 & 0.9671 & 0.9838 & 0.9927 & 0.9969\\
\hline
0.5 & 0.5199 & 0.6368 & 0.7422 & 0.8289 & 0.8944 & 0.9394 & 0.9678 & 0.9842 & 0.9929 & 0.9970\\
\hline
0.6 & 0.5239 & 0.6406 & 0.7454 & 0.8315 & 0.8962 & 0.9406 & 0.9686 & 0.9846 & 0.9931 & 0.9971\\
\hline
0.7 & 0.5279 & 0.6443 & 0.7486 & 0.8340 & 0.8980 & 0.9418 & 0.9693 & 0.9850 & 0.9932 & 0.9972\\
\hline
0.8 & 0.5319 & 0.6480 & 0.7517 & 0.8365 & 0.8997 & 0.9429 & 0.9699 & 0.9854 & 0.9934 & 0.9973\\
\hline
0.9 & 0.5359 & 0.6517 & 0.7549 & 0.8389 & 0.9015 & 0.9441 & 0.9706 & 0.9857 & 0.9936 & 0.9974\\
\hline
1 & 0.5398 & 0.6554 & 0.7580 & 0.8413 & 0.9032 & 0.9452 & 0.9713 & 0.9861 & 0.9938 & 0.9974\\
\hline
1.1 & 0.5438 & 0.6591 & 0.7611 & 0.8438 & 0.9049 & 0.9463 & 0.9719 & 0.9864 & 0.9940 & 0.9975\\
\hline
1.2 & 0.5478 & 0.6628 & 0.7642 & 0.8461 & 0.9066 & 0.9474 & 0.9726 & 0.9868 & 0.9941 & 0.9976\\
\hline
1.3 & 0.5517 & 0.6664 & 0.7673 & 0.8485 & 0.9082 & 0.9484 & 0.9732 & 0.9871 & 0.9943 & 0.9977\\
\hline
1.4 & 0.5557 & 0.6700 & 0.7704 & 0.8508 & 0.9099 & 0.9495 & 0.9738 & 0.9875 & 0.9945 & 0.9977\\
\hline
1.5 & 0.5596 & 0.6736 & 0.7734 & 0.8531 & 0.9115 & 0.9505 & 0.9744 & 0.9878 & 0.9946 & 0.9978\\
\hline
1.6 & 0.5636 & 0.6772 & 0.7764 & 0.8554 & 0.9131 & 0.9515 & 0.9750 & 0.9881 & 0.9948 & 0.9979\\
\hline
1.7 & 0.5675 & 0.6808 & 0.7794 & 0.8577 & 0.9147 & 0.9525 & 0.9756 & 0.9884 & 0.9949 & 0.9979\\
\hline
1.8 & 0.5714 & 0.6844 & 0.7823 & 0.8599 & 0.9162 & 0.9535 & 0.9761 & 0.9887 & 0.9951 & 0.9980\\
\hline
1.9 & 0.5753 & 0.6879 & 0.7852 & 0.8621 & 0.9177 & 0.9545 & 0.9767 & 0.9890 & 0.9952 & 0.9981\\
\hline
2 & 0.5793 & 0.6915 & 0.7881 & 0.8643 & 0.9192 & 0.9554 & 0.9772 & 0.9893 & 0.9953 & 0.9981\\
\hline
2.1 & 0.5832 & 0.6950 & 0.7910 & 0.8665 & 0.9207 & 0.9564 & 0.9778 & 0.9896 & 0.9955 & 0.9982\\
\hline
2.2 & 0.5871 & 0.6985 & 0.7939 & 0.8686 & 0.9222 & 0.9573 & 0.9783 & 0.9898 & 0.9956 & 0.9982\\
\hline
2.3 & 0.5910 & 0.7019 & 0.7967 & 0.8708 & 0.9236 & 0.9582 & 0.9788 & 0.9901 & 0.9957 & 0.9983\\
\hline
2.4 & 0.5948 & 0.7054 & 0.7995 & 0.8729 & 0.9251 & 0.9591 & 0.9793 & 0.9904 & 0.9959 & 0.9984\\
\hline
2.5 & 0.5987 & 0.7088 & 0.8023 & 0.8749 & 0.9265 & 0.9599 & 0.9798 & 0.9906 & 0.9960 & 0.9984\\
\hline
2.6 & 0.6026 & 0.7123 & 0.8051 & 0.8770 & 0.9279 & 0.9608 & 0.9803 & 0.9909 & 0.9961 & 0.9985\\
\hline
2.7 & 0.6064 & 0.7157 & 0.8078 & 0.8790 & 0.9292 & 0.9616 & 0.9808 & 0.9911 & 0.9962 & 0.9985\\
\hline
2.8 & 0.6103 & 0.7190 & 0.8106 & 0.8810 & 0.9306 & 0.9625 & 0.9812 & 0.9913 & 0.9963 & 0.9986\\
\hline
2.9 & 0.6141 & 0.7224 & 0.8133 & 0.8830 & 0.9319 & 0.9633 & 0.9817 & 0.9916 & 0.9964 & 0.9986\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:Student}Quantiles of the Student-$t$ distribution. The rows correspond to different degrees of freedom ($\nu$, say); the columns correspond to different probabilities ($z$, say). The cell gives $q$ that is s.t. $\mathbb{P}(-q<X<q)=z$, with $X \sim t(\nu)$.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r}
\hline
  & 0.05 & 0.1 & 0.75 & 0.9 & 0.95 & 0.975 & 0.99 & 0.999\\
\hline
1 & 0.079 & 0.158 & 2.414 & 6.314 & 12.706 & 25.452 & 63.657 & 636.619\\
\hline
2 & 0.071 & 0.142 & 1.604 & 2.920 & 4.303 & 6.205 & 9.925 & 31.599\\
\hline
3 & 0.068 & 0.137 & 1.423 & 2.353 & 3.182 & 4.177 & 5.841 & 12.924\\
\hline
4 & 0.067 & 0.134 & 1.344 & 2.132 & 2.776 & 3.495 & 4.604 & 8.610\\
\hline
5 & 0.066 & 0.132 & 1.301 & 2.015 & 2.571 & 3.163 & 4.032 & 6.869\\
\hline
6 & 0.065 & 0.131 & 1.273 & 1.943 & 2.447 & 2.969 & 3.707 & 5.959\\
\hline
7 & 0.065 & 0.130 & 1.254 & 1.895 & 2.365 & 2.841 & 3.499 & 5.408\\
\hline
8 & 0.065 & 0.130 & 1.240 & 1.860 & 2.306 & 2.752 & 3.355 & 5.041\\
\hline
9 & 0.064 & 0.129 & 1.230 & 1.833 & 2.262 & 2.685 & 3.250 & 4.781\\
\hline
10 & 0.064 & 0.129 & 1.221 & 1.812 & 2.228 & 2.634 & 3.169 & 4.587\\
\hline
20 & 0.063 & 0.127 & 1.185 & 1.725 & 2.086 & 2.423 & 2.845 & 3.850\\
\hline
30 & 0.063 & 0.127 & 1.173 & 1.697 & 2.042 & 2.360 & 2.750 & 3.646\\
\hline
40 & 0.063 & 0.126 & 1.167 & 1.684 & 2.021 & 2.329 & 2.704 & 3.551\\
\hline
50 & 0.063 & 0.126 & 1.164 & 1.676 & 2.009 & 2.311 & 2.678 & 3.496\\
\hline
60 & 0.063 & 0.126 & 1.162 & 1.671 & 2.000 & 2.299 & 2.660 & 3.460\\
\hline
70 & 0.063 & 0.126 & 1.160 & 1.667 & 1.994 & 2.291 & 2.648 & 3.435\\
\hline
80 & 0.063 & 0.126 & 1.159 & 1.664 & 1.990 & 2.284 & 2.639 & 3.416\\
\hline
90 & 0.063 & 0.126 & 1.158 & 1.662 & 1.987 & 2.280 & 2.632 & 3.402\\
\hline
100 & 0.063 & 0.126 & 1.157 & 1.660 & 1.984 & 2.276 & 2.626 & 3.390\\
\hline
200 & 0.063 & 0.126 & 1.154 & 1.653 & 1.972 & 2.258 & 2.601 & 3.340\\
\hline
500 & 0.063 & 0.126 & 1.152 & 1.648 & 1.965 & 2.248 & 2.586 & 3.310\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:Chi2}Quantiles of the $\chi^2$ distribution. The rows correspond to different degrees of freedom; the columns correspond to different probabilities.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r}
\hline
  & 0.05 & 0.1 & 0.75 & 0.9 & 0.95 & 0.975 & 0.99 & 0.999\\
\hline
1 & 0.004 & 0.016 & 1.323 & 2.706 & 3.841 & 5.024 & 6.635 & 10.828\\
\hline
2 & 0.103 & 0.211 & 2.773 & 4.605 & 5.991 & 7.378 & 9.210 & 13.816\\
\hline
3 & 0.352 & 0.584 & 4.108 & 6.251 & 7.815 & 9.348 & 11.345 & 16.266\\
\hline
4 & 0.711 & 1.064 & 5.385 & 7.779 & 9.488 & 11.143 & 13.277 & 18.467\\
\hline
5 & 1.145 & 1.610 & 6.626 & 9.236 & 11.070 & 12.833 & 15.086 & 20.515\\
\hline
6 & 1.635 & 2.204 & 7.841 & 10.645 & 12.592 & 14.449 & 16.812 & 22.458\\
\hline
7 & 2.167 & 2.833 & 9.037 & 12.017 & 14.067 & 16.013 & 18.475 & 24.322\\
\hline
8 & 2.733 & 3.490 & 10.219 & 13.362 & 15.507 & 17.535 & 20.090 & 26.124\\
\hline
9 & 3.325 & 4.168 & 11.389 & 14.684 & 16.919 & 19.023 & 21.666 & 27.877\\
\hline
10 & 3.940 & 4.865 & 12.549 & 15.987 & 18.307 & 20.483 & 23.209 & 29.588\\
\hline
20 & 10.851 & 12.443 & 23.828 & 28.412 & 31.410 & 34.170 & 37.566 & 45.315\\
\hline
30 & 18.493 & 20.599 & 34.800 & 40.256 & 43.773 & 46.979 & 50.892 & 59.703\\
\hline
40 & 26.509 & 29.051 & 45.616 & 51.805 & 55.758 & 59.342 & 63.691 & 73.402\\
\hline
50 & 34.764 & 37.689 & 56.334 & 63.167 & 67.505 & 71.420 & 76.154 & 86.661\\
\hline
60 & 43.188 & 46.459 & 66.981 & 74.397 & 79.082 & 83.298 & 88.379 & 99.607\\
\hline
70 & 51.739 & 55.329 & 77.577 & 85.527 & 90.531 & 95.023 & 100.425 & 112.317\\
\hline
80 & 60.391 & 64.278 & 88.130 & 96.578 & 101.879 & 106.629 & 112.329 & 124.839\\
\hline
90 & 69.126 & 73.291 & 98.650 & 107.565 & 113.145 & 118.136 & 124.116 & 137.208\\
\hline
100 & 77.929 & 82.358 & 109.141 & 118.498 & 124.342 & 129.561 & 135.807 & 149.449\\
\hline
200 & 168.279 & 174.835 & 213.102 & 226.021 & 233.994 & 241.058 & 249.445 & 267.541\\
\hline
500 & 449.147 & 459.926 & 520.950 & 540.930 & 553.127 & 563.852 & 576.493 & 603.446\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:Fstat}Quantiles of the $\mathcal{F}$ distribution. The columns and rows correspond to different degrees of freedom (resp. $n_1$ and $n_2$). The different panels correspond to different probabilities ($\alpha$) The corresponding cell gives $z$ that is s.t. $\mathbb{P}(X \le z)=\alpha$, with $X \sim \mathcal{F}(n_1,n_2)$.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r|r}
\hline
  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
\hline
alpha = 0.9 &  &  &  &  &  &  &  &  &  & \\
\hline
5 & 4.060 & 3.780 & 3.619 & 3.520 & 3.453 & 3.405 & 3.368 & 3.339 & 3.316 & 3.297\\
\hline
10 & 3.285 & 2.924 & 2.728 & 2.605 & 2.522 & 2.461 & 2.414 & 2.377 & 2.347 & 2.323\\
\hline
15 & 3.073 & 2.695 & 2.490 & 2.361 & 2.273 & 2.208 & 2.158 & 2.119 & 2.086 & 2.059\\
\hline
20 & 2.975 & 2.589 & 2.380 & 2.249 & 2.158 & 2.091 & 2.040 & 1.999 & 1.965 & 1.937\\
\hline
50 & 2.809 & 2.412 & 2.197 & 2.061 & 1.966 & 1.895 & 1.840 & 1.796 & 1.760 & 1.729\\
\hline
100 & 2.756 & 2.356 & 2.139 & 2.002 & 1.906 & 1.834 & 1.778 & 1.732 & 1.695 & 1.663\\
\hline
500 & 2.716 & 2.313 & 2.095 & 1.956 & 1.859 & 1.786 & 1.729 & 1.683 & 1.644 & 1.612\\
\hline
alpha = 0.95 &  &  &  &  &  &  &  &  &  & \\
\hline
5 & 6.608 & 5.786 & 5.409 & 5.192 & 5.050 & 4.950 & 4.876 & 4.818 & 4.772 & 4.735\\
\hline
10 & 4.965 & 4.103 & 3.708 & 3.478 & 3.326 & 3.217 & 3.135 & 3.072 & 3.020 & 2.978\\
\hline
15 & 4.543 & 3.682 & 3.287 & 3.056 & 2.901 & 2.790 & 2.707 & 2.641 & 2.588 & 2.544\\
\hline
20 & 4.351 & 3.493 & 3.098 & 2.866 & 2.711 & 2.599 & 2.514 & 2.447 & 2.393 & 2.348\\
\hline
50 & 4.034 & 3.183 & 2.790 & 2.557 & 2.400 & 2.286 & 2.199 & 2.130 & 2.073 & 2.026\\
\hline
100 & 3.936 & 3.087 & 2.696 & 2.463 & 2.305 & 2.191 & 2.103 & 2.032 & 1.975 & 1.927\\
\hline
500 & 3.860 & 3.014 & 2.623 & 2.390 & 2.232 & 2.117 & 2.028 & 1.957 & 1.899 & 1.850\\
\hline
alpha = 0.99 &  &  &  &  &  &  &  &  &  & \\
\hline
5 & 16.258 & 13.274 & 12.060 & 11.392 & 10.967 & 10.672 & 10.456 & 10.289 & 10.158 & 10.051\\
\hline
10 & 10.044 & 7.559 & 6.552 & 5.994 & 5.636 & 5.386 & 5.200 & 5.057 & 4.942 & 4.849\\
\hline
15 & 8.683 & 6.359 & 5.417 & 4.893 & 4.556 & 4.318 & 4.142 & 4.004 & 3.895 & 3.805\\
\hline
20 & 8.096 & 5.849 & 4.938 & 4.431 & 4.103 & 3.871 & 3.699 & 3.564 & 3.457 & 3.368\\
\hline
50 & 7.171 & 5.057 & 4.199 & 3.720 & 3.408 & 3.186 & 3.020 & 2.890 & 2.785 & 2.698\\
\hline
100 & 6.895 & 4.824 & 3.984 & 3.513 & 3.206 & 2.988 & 2.823 & 2.694 & 2.590 & 2.503\\
\hline
500 & 6.686 & 4.648 & 3.821 & 3.357 & 3.054 & 2.838 & 2.675 & 2.547 & 2.443 & 2.356\\
\hline
\end{tabular}
\end{table}

  \bibliography{book.bib,packages.bib}

\end{document}

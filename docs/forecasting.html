<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Forecasting | Advanced Econometrics</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="Macroeconomic forecasts are done in many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. These institutions are...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 3 Forecasting | Advanced Econometrics">
<meta property="og:type" content="book">
<meta property="og:description" content="Macroeconomic forecasts are done in many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. These institutions are...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Forecasting | Advanced Econometrics">
<meta name="twitter:description" content="Macroeconomic forecasts are done in many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. These institutions are...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Advanced Econometrics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Prerequisites</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="active" href="forecasting.html"><span class="header-section-number">3</span> Forecasting</a></li>
<li><a class="" href="appendix.html"><span class="header-section-number">4</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="forecasting" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Forecasting<a class="anchor" aria-label="anchor" href="#forecasting"><i class="fas fa-link"></i></a>
</h1>
<p>Macroeconomic forecasts are done in many places: Public Administration (notably Treasuries), Central Banks, International Institutions (e.g. IMF, OECD), banks, big firms. These institutions are interested in the <strong>point estimates</strong> (<span class="math inline">\(\sim\)</span> most likely value) of the variable of interest. They also sometimes need to measure the <strong>uncertainty</strong> (<span class="math inline">\(\sim\)</span> dispersion of likely outcomes) associated to the point estimates.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;In its inflation report, the Bank of England displays charts showing the conditional distribution of future inflation, called fan charts. This fan charts show the uncertainty associated with future inflation. See &lt;a href="https://www.bankofengland.co.uk/quarterly-bulletin/1998/q1/the-inflation-report-projections-understanding-the-fan-chart"&gt;this page&lt;/a&gt;.&lt;/p&gt;'><sup>1</sup></a></p>
<p>Forecasts, produced by professional forecasters, are available on these web pages:</p>
<ul>
<li>
<a href="https://www.philadelphiafed.org/research-and-data/real-time-center/survey-of-professional-forecasters/">Philly Fed Survey of Professional Forecasters</a>.</li>
<li>
<a href="http://www.ecb.europa.eu/stats/prices/indic/forecast/html/index.en.html">ECB Survey of Professional Forecasters</a>.</li>
<li>
<a href="https://www.imf.org/external/pubs/ft/weo/2016/update/01/">IMF World Economic Outlook</a>.</li>
<li>
<a href="http://www.oecd.org/eco/economicoutlook.htm">OECD Global Economic Outlook</a>.</li>
<li>
<a href="http://ec.europa.eu/economy_finance/eu/forecasts/index_en.htm">European Commission Economic Forecasts</a>.</li>
</ul>
<p>How to formalize the forecasting problem? Assume the current date is <span class="math inline">\(t\)</span>. We want to forecast the value that variable <span class="math inline">\(y_t\)</span> will take on date <span class="math inline">\(t+1\)</span> (i.e., <span class="math inline">\(y_{t+1}\)</span>) based on the observation of a set of variables gathered in vector <span class="math inline">\(x_t\)</span> (<span class="math inline">\(x_t\)</span> may contain lagged values of <span class="math inline">\(y_t\)</span>).</p>
<p>The forecaster aims at minimizing (a function of) the forecast error. It is usal to consider the following (quadratic) loss function:
<span class="math display">\[
\underbrace{\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)}_{\mbox{Mean square error (MSE)}}
\]</span>
where <span class="math inline">\(y^*_{t+1}\)</span> is the forecast of <span class="math inline">\(y_{t+1}\)</span> (function of <span class="math inline">\(x_t\)</span>).</p>
<div class="proposition">
<p><span id="prp:smallestMSE" class="proposition"><strong>Proposition 3.1  (Smallest MSE) </strong></span>The smallest MSE is obtained with MSEthe expectation of <span class="math inline">\(y_{t+1}\)</span> conditional on <span class="math inline">\(x_t\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>See Appendix <a href="appendix.html#smallestMSE">4.4.7</a>.</p>
</div>
<div class="proposition" names="Smallest MSE for linear forecasts">
<p><span id="prp:smallestMSElinear" class="proposition"><strong>Proposition 3.2  </strong></span>Among the class of linear forecasts, the smallest MSE is obtained with the linear projection of <span class="math inline">\(y_{t+1}\)</span> on <span class="math inline">\(x_t\)</span>.
This projection, denoted by <span class="math inline">\(\hat{P}(y_{t+1}|x_t):=\boldsymbol\alpha'x_t\)</span>, satisfies:
<span class="math display" id="eq:proj">\[\begin{equation}
\mathbb{E}\left( [y_{t+1} - \boldsymbol\alpha'x_t]x_t \right)=\mathbf{0}.\tag{3.1}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>Consider the function <span class="math inline">\(f:\)</span> <span class="math inline">\(\boldsymbol\alpha \rightarrow \mathbb{E}\left( [y_{t+1} - \boldsymbol\alpha'x_t]^2 \right)\)</span>. We have:
<span class="math display">\[
f(\boldsymbol\alpha) = \mathbb{E}\left( y_{t+1}^2 - 2 y_t x_t'\boldsymbol\alpha + \boldsymbol\alpha'x_t x_t'\boldsymbol\alpha] \right).
\]</span>
We have <span class="math inline">\(\partial f(\boldsymbol\alpha)/\partial \boldsymbol\alpha = \mathbb{E}(-2 y_{t+1} x_t + 2 x_t x_t'\boldsymbol\alpha)\)</span> (see Proposition <a href="appendix.html#prp:partial">4.13</a>). The function is minimised for <span class="math inline">\(\partial f(\boldsymbol\alpha)/\partial \boldsymbol\alpha =0\)</span>.</p>
</div>
<p>Eq. <a href="forecasting.html#eq:proj">(3.1)</a> implies that <span class="math inline">\(\mathbb{E}\left( y_{t+1}x_t \right)=\mathbb{E}\left(x_tx_t' \right)\boldsymbol\alpha\)</span>. (Note that <span class="math inline">\(x_t x_t'\boldsymbol\alpha=x_t (x_t'\boldsymbol\alpha)=(\boldsymbol\alpha'x_t) x_t'\)</span>.)</p>
<p>Hence, if <span class="math inline">\(\mathbb{E}\left(x_tx_t' \right)\)</span> is nonsingular,
<span class="math display" id="eq:linproj">\[\begin{equation}
\boldsymbol\alpha=[\mathbb{E}\left(x_tx_t' \right)]^{-1}\mathbb{E}\left( y_{t+1}x_t \right).\tag{3.2}
\end{equation}\]</span></p>
<p>The MSE then is:
<span class="math display">\[
\mathbb{E}([y_{t+1} - \boldsymbol\alpha'x_t]^2) = \mathbb{E}{(y_{t+1}^2)} - \mathbb{E}\left( y_{t+1}x_t' \right)[\mathbb{E}\left(x_tx_t' \right)]^{-1}\mathbb{E}\left(x_ty_{t+1} \right).
\]</span></p>
<p>Consider the regression <span class="math inline">\(y_{t+1} = \boldsymbol\beta'\mathbf{x}_t + \varepsilon_{t+1}\)</span>.</p>
<p>The OLS estimate is:
<span class="math display">\[
\mathbf{b} = \left[ \underbrace{ \frac{1}{T} \sum_{i=1}^T \mathbf{x}_t\mathbf{x}_t'}_{\mathbf{m}_1} \right]^{-1}\left[  \underbrace{ \frac{1}{T} \sum_{i=1}^T \mathbf{x}_t'y_{t+1}}_{\mathbf{m}_2} \right].
\]</span>
If <span class="math inline">\(\{x_t,y_t\}\)</span> is covariance-stationary and ergodic for the second moments then the sample moments (<span class="math inline">\(\mathbf{m}_1\)</span> and <span class="math inline">\(\mathbf{m}_2\)</span>) converges in probability to the associated population moments and <span class="math inline">\(\mathbf{b} \overset{p}{\rightarrow} \boldsymbol\alpha\)</span> (where <span class="math inline">\(\boldsymbol\alpha\)</span> is defined in Eq. <a href="forecasting.html#eq:linproj">(3.2)</a>).</p>
<p>Consider the MA(q) process:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q},
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise sequence (Def. <a href="#def:whitenoise"><strong>??</strong></a>).</p>
<p>We have:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots) =\\
&amp;&amp;\left\{
\begin{array}{lll}
\mu + \theta_h \varepsilon_{t} + \dots + \theta_q \varepsilon_{t-q+h}  \quad &amp;for&amp; \quad h \in [1,q]\\
\mu \quad &amp;for&amp; \quad h &gt; q
\end{array}
\right.
\end{eqnarray*}\]</span>
and
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{V}ar(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots)= \mathbb{E}\left( [y_{t+h} - \mathbb{E}(y_{t+h}|\varepsilon_{t},\varepsilon_{t-1},\dots)]^2 \right) =\\
&amp;&amp;\left\{
\begin{array}{lll}
\sigma^2(1+\theta_1^2+\dots+\theta_{h-1}^2) \quad &amp;for&amp; \quad h \in [1,q]\\
\sigma^2(1+\theta_1^2+\dots+\theta_q^2) \quad &amp;for&amp; \quad h&gt;q.
\end{array}
\right.
\end{eqnarray*}\]</span></p>
<p>Remark: The previous reasoning relies on the assumption that the <span class="math inline">\(\varepsilon_t\)</span>s are observed. But this is generally not the case in practice. Note that consistent estimates are available if the MA process is invertible (see Eq. <a href="#eq:invertible">(<strong>??</strong>)</a>) .</p>
<p>Consider the AR(p) process:
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t,
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise sequence (Def. <a href="#def:whitenoise"><strong>??</strong></a>).</p>
<p>Using the notation of Eq. <a href="#eq:F">(<strong>??</strong>)</a>, we have:
<span class="math display">\[
\mathbf{y}_t - \boldsymbol\mu = F (\mathbf{y}_{t-1}- \boldsymbol\mu) + \boldsymbol\xi_t,
\]</span>
with <span class="math inline">\(\boldsymbol\mu = [\mu,\dots,\mu]'\)</span> (<span class="math inline">\(\mu\)</span> is defined in Eq. <a href="#eq:EAR">(<strong>??</strong>)</a>). Hence:
<span class="math display">\[
\mathbf{y}_{t+h} - \boldsymbol\mu = \boldsymbol\xi_{t+h} + F \boldsymbol\xi_{t+h-1} + \dots + F^{h-1} \boldsymbol\xi_{t+1} + F^h (\mathbf{y}_{t}- \mu).
\]</span>
Therefore:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\mathbf{y}_{t+h}|y_{t},y_{t-1},\dots) &amp;=&amp; \boldsymbol\mu + F^{h}(\mathbf{y}_t - \boldsymbol\mu)\\
\mathbb{V}ar\left( [\mathbf{y}_{t+h} - \mathbb{E}(\mathbf{y}_{t+h}|y_{t},y_{t-1},\dots)] \right) &amp;=&amp; \Sigma + F\Sigma F' + \dots + F^{h-1}\Sigma (F^{h-1})',
\end{eqnarray*}\]</span>
where:
<span class="math display">\[
\Sigma = \left[
\begin{array}{ccc}
\sigma^2  &amp; 0&amp; \dots\\
0  &amp; 0 &amp; \\
\vdots  &amp; &amp; \ddots \\
\end{array}
\right].
\]</span></p>
<p>XXXX <a href="https://jrenne.shinyapps.io/ARpFcst">Web interface</a> XXXX</p>
<p>Alternative approach: Taking the (conditional) expectations of both sides of
<span class="math display">\[
y_{t+h} - \mu = \phi_1 (y_{t+h-1} - \mu) + \phi_2 (y_{t+h-2} - \mu) + \dots + \phi_p (y_{t-p} - \mu) + \varepsilon_{t+h},
\]</span>
we obtain:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(y_{t+h}|y_{t},y_{t-1},\dots) &amp;=&amp; \mu + \phi_1\left(\mathbb{E}[y_{t+h-1}|y_{t},y_{t-1},\dots] - \mu\right)+\\
&amp;&amp;\phi_2\left(\mathbb{E}[y_{t+h-2}|y_{t},y_{t-1},\dots] - \mu\right) + \dots +\\
&amp;&amp; \phi_p\left(\mathbb{E}[y_{t+h-p}|y_{t},y_{t-1},\dots] - \mu\right),
\end{eqnarray*}\]</span>
which can be exploited recursively.</p>
<p>The recursion begins with <span class="math inline">\(\mathbb{E}(y_{t-k}|y_{t},y_{t-1},\dots)=y_{t-k}\)</span> (for any <span class="math inline">\(k \ge 0\)</span>).</p>
<p>Forecasting an ARMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) process</p>
<p>Consider the process:
<span class="math display" id="eq:armaForecast">\[\begin{equation}
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q},\tag{3.3}
\end{equation}\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise sequence (Def. @ref{def:whitenoise)).</p>
<p>We assume that the MA part of the process is invertible (see Eq. <a href="#eq:invertible">(<strong>??</strong>)</a>), which implies that the information contained in <span class="math inline">\(\{y_{t},y_{t-1},y_{t-2},\dots\}\)</span> is identical to that in <span class="math inline">\(\{\varepsilon_{t},\varepsilon_{t-1},\varepsilon_{t-2},\dots\}\)</span>.</p>
<p>We have:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}(y_{t+h}|y_{t},y_{t-1},\dots) =\\
&amp;&amp;\left\{
\begin{array}{ll}
\mu + \phi_1\left(\mathbb{E}(y_{t+h-1}|y_{t},y_{t-1},\dots) - \mu\right)+\\
\phi_2\left(\mathbb{E}(y_{t+h-2}|y_{t},y_{t-1},\dots) - \mu\right) + \dots +\\
\phi_p\left(\mathbb{E}(y_{t+h-p}|y_{t},y_{t-1},\dots) - \mu\right) +\\
\theta_{h}\varepsilon_{t} + \theta_{h+1}\varepsilon_{t-1} + \dots + \theta_{q}\varepsilon_{t+h-q} &amp;\mbox{for  $h \in[1,q]$},\\
\\
\mu + \phi_1\left(\mathbb{E}(y_{t+h-1}|y_{t},y_{t-1},\dots) - \mu\right)+\\
\phi_2\left(\mathbb{E}(y_{t+h-2}|y_{t},y_{t-1},\dots) - \mu\right) + \dots +\\
\phi_p\left(\mathbb{E}(y_{t+h-p}|y_{t},y_{t-1},\dots) - \mu\right) &amp; \mbox{for $h&gt;q$}.
\end{array}
\right.
\end{eqnarray*}\]</span></p>
<p>To compute the MSE, it is convenient to use the Wold decomposition of the process (see Theorem <a href="#thm:Wold"><strong>??</strong></a>):
<span class="math display">\[
y_t = \mu + \sum_{i=0}^{+\infty} \psi_i \varepsilon_{t-i}.
\]</span></p>
<p>This implies:
<span class="math display">\[\begin{eqnarray*}
y_{t+h} &amp;=&amp; \mu + \sum_{i=0}^{h-1} \psi_i \varepsilon_{t+h-i} + \sum_{i=h}^{+\infty} \psi_i \varepsilon_{t+h-i}\\
&amp;=&amp; \mu + \sum_{i=0}^{h-1} \psi_i \varepsilon_{t+h-i} + \sum_{i=0}^{+\infty} \psi_{i+h} \varepsilon_{t-i}.
\end{eqnarray*}\]</span></p>
<p>Since <span class="math inline">\(\mathbb{E}(y_{t+h}|y_t,y_{t-1},\dots)=\mu+\sum_{i=0}^{+\infty} \psi_{i+h} \varepsilon_{t-i}\)</span>, we get:
<span class="math display">\[
\mathbb{V}ar(y_{t+h}|y_t,y_{t-1},\dots) =\mathbb{V}ar\left(\sum_{i=0}^{h-1} \psi_i \varepsilon_{t+h-i}\right)= \sigma^2 \sum_{i=0}^{h-1} \psi_i^2.
\]</span></p>
<p>How to use the previous formulas in practice?</p>
<p>One has first to select a specification and to estimate the model.
Two methods to determine relevant specifications:</p>
<ol style="list-style-type: lower-alpha">
<li>Information criteria (see Definition @ref(def:info_criteria)).</li>
<li>Box-Jenkins approach.</li>
</ol>
<p><span class="citation">Box and Jenkins (<a href="references.html#ref-boxjen76" role="doc-biblioref">1976</a>)</span> have proposed an approach that is now widely used.</p>
<ol style="list-style-type: decimal">
<li>Data transformation (<span class="math inline">\(\Rightarrow\)</span> to make the data covariance-stationary)</li>
<li>Select <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>.</li>
<li>Estimate the model parameters.</li>
<li>Check that the estimated model is consistent with the data.</li>
</ol>
<p><strong>Data transformation</strong></p>
<p>The data should be transformed to “make them stationary”. To do so, one can e.g. take logarithms, take changes in the considered series, remove (deterministic) trends.</p>
<p><strong>Selection of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span></strong></p>
<p>Based on two elements: (a) sample autocorrelations and (b) partial autocorrelations (see Def. <a href="#def:partialAC"><strong>??</strong></a>).</p>
<p><strong>Sample autocorrelation</strong></p>
<p>Population autocorrelation: <span class="math inline">\(\rho_i = \gamma_i / \gamma_0\)</span>. Natural estimate based on sample moments: <span class="math inline">\(\hat{\rho_i} = \hat\gamma_j / \hat\gamma_0\)</span> where:
<span class="math display">\[
\gamma_j = \frac{1}{T} \sum_{t=j+1}^{T} (y_t - \bar{y})(y_{t-j} - \bar{y}).
\]</span>
For an MA(<span class="math inline">\(q\)</span>) process, <span class="math inline">\(\rho_j = 0\)</span> for <span class="math inline">\(j&gt;q\)</span>.</p>
<p>If the data are generated by a Gaussian MA(<span class="math inline">\(q\)</span>) process, then:
<span class="math display">\[
\mathbb{V}ar(\hat\rho_j) \approx \frac{1}{T}\left\{1 + 2 \sum_{i=1}^q \rho_i^2 \right\}
\]</span>
In particular, if the data correspond to a Gaussian white noise then <span class="math inline">\(\hat\rho_i \in [\pm 2/\sqrt{T}]\)</span> about 95% of the time.</p>
<p><strong>Partial autocorrelation</strong></p>
<p>The <span class="math inline">\(m^{th}\)</span> population partial autocorrelation (see also Def. <a href="#def:partialAC"><strong>??</strong></a>) is the <span class="math inline">\(m^{th}\)</span> coefficient in a linear projection of <span class="math inline">\(y_{t+1}\)</span> on its <span class="math inline">\(m\)</span> most recent lags:
<span class="math display">\[
\hat{y}_{t+1|t} = \mu + \phi_{1,m}(y_t - \mu) + \phi_{2,m}(y_{t-1} - \mu) + \dots +\phi_{m,m}(y_{t-m+1} - \mu).
\]</span>
Using the OLS formula, it can be shown that the (population) vector <span class="math inline">\(\boldsymbol\phi_{.,m}=[\phi_{1,m},\dots,\phi_{m,m}]'\)</span> satisfies:
<span class="math display">\[
\boldsymbol\phi_{.,m} = \left[
\begin{array}{cccc}
\gamma_0 &amp; \gamma_1&amp; \dots &amp; \gamma_{m-1}\\
\gamma_1 &amp; \gamma_0&amp; \dots &amp; \gamma_{m-2}\\
\vdots &amp; &amp; \ddots &amp; \\
\gamma_{m-1} &amp; \gamma_{m-2}&amp; \dots &amp; \gamma_{0}
\end{array}
\right]^{-1}\left[
\begin{array}{c}
\gamma_1\\
\gamma_2\\
\vdots\\
\gamma_{m}
\end{array}
\right].
\]</span>
For an AR(<span class="math inline">\(p\)</span>) process, <span class="math inline">\(\phi_{m,m}=0\)</span> for <span class="math inline">\(m&gt;p\)</span>, which reflects the fact that only the first <span class="math inline">\(p\)</span> lags are useful to forecast <span class="math inline">\(y_{t}\)</span>.</p>
<p>A natural estimate <span class="math inline">\(\hat\phi_{m,m}\)</span> of <span class="math inline">\(\phi_{m,m}\)</span> is obtained by running the regression:
<span class="math display">\[
\hat{y}_{t+1} = \hat{c} + \hat\phi_{1,m}y_t + \hat\phi_{2,m}y_{t-1} + \dots + \hat\phi_{m,m}y_{t-m+1} + \hat\varepsilon_{t+1}.
\]</span>
Under the hypothesis that the data are generated by an AR(<span class="math inline">\(p\)</span>) process:
<span class="math display">\[
\mathbb{V}ar(\hat\phi_{m,m}) \approx \frac{1}{T} \quad \mbox{for} \quad m &gt; p.
\]</span>
Besides, for <span class="math inline">\(i,j&gt;p\)</span>, <span class="math inline">\(\hat\phi_{i,i}\)</span> and <span class="math inline">\(\hat\phi_{j,j}\)</span> are asymptotically independent.</p>
<p><strong>Assessing the performances of a forecasting model</strong></p>
<p>Once one has fitted a model on a given dataset (of length <span class="math inline">\(T\)</span>, say), one compute MSE (mean square errors) to evaluate the performance of the model.</p>
<p>But this MSE is the <strong>in-sample</strong> one.</p>
<p>It is easy to reduce in-sample MSE. Typically, if the model is estimated by OLS, adding covariates mechanically reduces the MSE.</p>
<p><span class="math inline">\(\Rightarrow\)</span> Even if additional data are irrelevant, the <span class="math inline">\(R^2\)</span> of the regression increases.</p>
<p>Adding irrelevant variables increases the (in-sample) <span class="math inline">\(R^2\)</span> but is bound to increase the <strong>out-of-sample</strong> MSE.</p>
<p>One also has to analyse <strong>out-of-sample</strong> performances of the forecasting model.</p>
<ol style="list-style-type: lower-alpha">
<li>Estimate a model on a sample of reduced size (<span class="math inline">\(1,\dots,T^*\)</span>, with <span class="math inline">\(T^*&lt;T\)</span>)</li>
<li>Use the remaining available periods (<span class="math inline">\(T^*+1,\dots,T\)</span>) to compute <strong>out-of-sample</strong> forecasting errors (and compute their MSE).</li>
</ol>
<p>XXX</p>
<p>The previous (out-of-sample) exercise is a form of <strong>backtesting</strong>. Other forms of backtesting aim at testing whether the realisations of the variable are consistent with (previous) confidence intervals. Typically, if realised (ex post) variables tend to be far outside (ex ante) confidence intervals, we may deem the model inadequate.</p>
<p><strong>Diebold-Mariano test</strong></p>
<p>How to compare different forecasting approaches? <span class="citation">Diebold and Mariano (<a href="references.html#ref-Diebold_Mariano_1995" role="doc-biblioref">1995</a>)</span> have proposed a simple test to address this question.</p>
<p>Assume that you want to compare approaches A and B. You have historical data sets and you have implemented both approaches in the past, providing you with two sets of forecasting errors: <span class="math inline">\(\{e^{A}_t\}_{t=1,\dots,T}\)</span> and <span class="math inline">\(\{e^{B}_t\}_{t=1,\dots,T}\)</span>.</p>
<p>It may be the case that your forecasts serve a specific purpose and that, for instance, you dislike positive forecasting errors and you care less about negative errors. We assume you are able to formalise this by means of a <strong>loss function <span class="math inline">\(L(e)\)</span></strong>.</p>
<p>For instance:</p>
<ul>
<li>If you dislike large positive errors, you may set <span class="math inline">\(L(e)=\exp(e)\)</span>.</li>
<li>If you are concerned about both positive and negative errors (indifferently), you may set <span class="math inline">\(L(e)=e^2\)</span>.</li>
</ul>
<p>Let us define the sequence <span class="math inline">\(\{d_t\}_{t=1,\dots,T} \equiv \{L(e^{A}_t)-L(e^{B}_t)\}_{t=1,\dots,T}\)</span> and assume that this sequence is covariance stationary. We consider the following null hypothesis: <span class="math inline">\(H_0:\)</span> <span class="math inline">\(\bar{d}=0\)</span>, where <span class="math inline">\(\bar{d}\)</span> denotes the population mean of the <span class="math inline">\(d_t\)</span>s. Under <span class="math inline">\(H_0\)</span> and under the assumption of covariance-stationarity of <span class="math inline">\(d_t\)</span>, we have (Theorem @ref{(hm:CLTcovstat)):
<span class="math display">\[
\sqrt{T} \bar{d}_T \overset{d}{\rightarrow} \mathcal{N}\left(0,\sum_{j=-\infty}^{+\infty} \gamma_j \right),
\]</span><br>
where the <span class="math inline">\(\gamma_j\)</span>s are the autocovariances of <span class="math inline">\(d_t\)</span>.</p>
<p>Hence, assuming that <span class="math inline">\(\hat{\sigma}^2\)</span> is a consistent estimate of <span class="math inline">\(\sum_{j=-\infty}^{+\infty} \gamma_j\)</span> (for instance the one given by the Newey-West formula, see Def. <a href="#def:NW"><strong>??</strong></a>), we have, under <span class="math inline">\(H_0\)</span>:
<span class="math display">\[
DM_T := \sqrt{T}\frac{\bar{d}_T}{\sqrt{\hat{\sigma}^2}} \overset{d}{\rightarrow}  \mathcal{N}(0,1).
\]</span>
<span class="math inline">\(DM_T\)</span> is the test statistics. For a test of size <span class="math inline">\(\alpha\)</span>, the critical region is
<span class="math display">\[
]-\infty,-\Phi^{-1}(1-\alpha/2)] \cup [\Phi^{-1}(1-\alpha/2),+\infty[.
\]</span></p>
<p>XXX <a href="https://jrenne.shinyapps.io/tests/">ShinyApp on Tests</a> XXX</p>

</div>

  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">2</span> Introduction</a></div>
<div class="next"><a href="appendix.html"><span class="header-section-number">4</span> Appendix</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav"><li><a class="nav-link" href="#forecasting"><span class="header-section-number">3</span> Forecasting</a></li></ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Advanced Econometrics</strong>" was written by Jean-Paul Renne. It was last built on 2022-09-04.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Time Series | Advanced Econometrics</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="3.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 3 Time Series | Advanced Econometrics">
<meta property="og:type" content="book">
<meta property="og:description" content="3.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Time Series | Advanced Econometrics">
<meta name="twitter:description" content="3.1 Introduction to time series A time series is an infinite sequence of random variables indexed by time: \(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\),...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Advanced Econometrics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Prerequisites</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="active" href="time-series.html"><span class="header-section-number">3</span> Time Series</a></li>
<li><a class="" href="appendix.html"><span class="header-section-number">4</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="time-series" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Time Series<a class="anchor" aria-label="anchor" href="#time-series"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-to-time-series" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Introduction to time series<a class="anchor" aria-label="anchor" href="#introduction-to-time-series"><i class="fas fa-link"></i></a>
</h2>
<p>A time series is an infinite sequence of random variables indexed by time: <span class="math inline">\(\{y_t\}_{t=-\infty}^{+\infty}=\{\dots, y_{-2},y_{-1},y_{0},y_{1},\dots,y_t,\dots\}\)</span>, <span class="math inline">\(y_i \in \mathbb{R}^k\)</span>. In practice, we only observe samples, typically: <span class="math inline">\(\{y_{1},\dots,y_T\}\)</span>.</p>
<p>Standard time series models are built using <strong>shocks</strong> that we will often denote by <span class="math inline">\(\varepsilon_t\)</span>. Typically, <span class="math inline">\(\mathbb{E}(\varepsilon_t)=0\)</span>. In many models, the shocks are supposed to be i.i.d., but there exist other (less restrictive) notions of shocks. In particular, the definition of many processes is based on whote noises:</p>
<div class="definition">
<p><span id="def:whitenoise" class="definition"><strong>Definition 3.1  (White noise) </strong></span>The process <span class="math inline">\(\{\varepsilon_t\}_{t \in] -\infty,+\infty[}\)</span> is a white noise if, for all <span class="math inline">\(t\)</span> (a) <span class="math inline">\(\mathbb{E}(\varepsilon_t)=0\)</span>, (b) <span class="math inline">\(\mathbb{E}(\varepsilon_t^2)=\sigma^2&lt;\infty\)</span> and (c) for all <span class="math inline">\(s\ne t\)</span>, <span class="math inline">\(\mathbb{E}(\varepsilon_t \varepsilon_s)=0\)</span>.</p>
</div>
<p>Another shocks that are commonly used are Martingale Difference Sequences:</p>
<div class="definition">
<p><span id="def:MDS" class="definition"><strong>Definition 3.2  (Martingale Difference Sequence) </strong></span>The process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a martingale difference sequence (MDS) if <span class="math inline">\(\mathbb{E}(|\varepsilon_{t}|)&lt;\infty\)</span> and if, for all <span class="math inline">\(t\)</span>,
<span class="math display">\[
\underbrace{\mathbb{E}_{t-1}(\varepsilon_{t})}_{\mbox{Expectation conditional on the past}}=0.
\]</span></p>
</div>
<p>By definition, if <span class="math inline">\(y_t\)</span> is a martingale, then <span class="math inline">\(y_{t}-y_{t-1}\)</span> is a MDS.</p>
<p>The Autoregressive conditional heteroskedasticity (ARCH) process is an example of shock that satisfies the MDS definition but that is not i.i.d.:
<span class="math display">\[
\varepsilon_{t} = \sigma_t \times z_{t},
\]</span>
where <span class="math inline">\(z_t \sim i.i.d.\,\mathcal{N}(0,1)\)</span> and <span class="math inline">\(\sigma_t^2 = w + \alpha \varepsilon_{t-1}^2\)</span>.</p>
<p>A whote noise process is not necessarily a MDS. This is for instance the following process:
<span class="math display">\[
\varepsilon_{t} = z_t + z_{t-1}z_{t-2},
\]</span>
where <span class="math inline">\(z_t \sim i.i.d.\mathcal{N}(0,1)\)</span>.</p>
<p>Let us now introduce the lag operator. The lag operator, denoted by <span class="math inline">\(L\)</span>, is defined on the time series space and is defined by:
<span class="math display" id="eq:lagOp">\[\begin{equation}
L: \{y_t\}_{t=-\infty}^{+\infty} \rightarrow \{w_t\}_{t=-\infty}^{+\infty} \quad \mbox{with} \quad w_t = y_{t-1}.\tag{3.1}
\end{equation}\]</span></p>
<p>We have: <span class="math inline">\(L^2 y_t = y_{t-2}\)</span> and, generally, <span class="math inline">\(L^k y_t = y_{t-k}\)</span>.</p>
<p>Consider a time series <span class="math inline">\(y_t\)</span> defined by <span class="math inline">\(y_t = \mu + \phi y_{t-1} + \varepsilon_t\)</span>, where the <span class="math inline">\(\varepsilon_t\)</span> are i.i.d.,<span class="math inline">\(\mathcal{N}(0,\sigma^2)\)</span>. Using the lag operator, the dynamics of <span class="math inline">\(y_t\)</span> can be expressed as follows:
<span class="math display">\[
(1-\phi L) y_t = \mu + \varepsilon_t.
\]</span></p>
<p>It is easily checked that we have <span class="math inline">\(L^2 y_t = y_{t-2}\)</span> and, generally, <span class="math inline">\(L^k y_t = y_{t-k}\)</span>.</p>
<p>if it exists, the <strong>unconditional (or marginal) mean</strong> of the random variable <span class="math inline">\(y_t\)</span> is given by:
<span class="math display">\[
\mu_t := \mathbb{E}(y_t) = \int_{-\infty}^{\infty} y_t f_{Y_t}(y_t) dy_t,
\]</span>
where <span class="math inline">\(f_{Y_t}\)</span> is the unconditional (or marginal) density of <span class="math inline">\(y_t\)</span>.</p>
<p>Similarly, if it exists, the <strong>unconditional (or marginal) variance</strong> of the random variable <span class="math inline">\(y_t\)</span> is:
<span class="math display">\[
\mathbb{V}ar(y_t) = \int_{-\infty}^{\infty} (y_t - \mathbb{E}(y_t))^2 f_{Y_t}(y_t) dy_t.
\]</span></p>
<div class="definition">
<p><span id="def:autocov" class="definition"><strong>Definition 3.3  (Autocovariance) </strong></span>The <span class="math inline">\(j^{th}\)</span> autocovariance of <span class="math inline">\(y_t\)</span> is given by:
<span class="math display">\[\begin{eqnarray*}
\gamma_{j,t} &amp;:=&amp; \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty} [y_t - \mathbb{E}(y_t)][y_{t-j} - \mathbb{E}(y_{t-j})] \times\\
&amp;&amp; f_{Y_t,Y_{t-1},\dots,Y_{t-j}}(y_t,y_{t-1},\dots,y_{t-j}) dy_t dy_{t-1} \dots dy_{t-j} \\
&amp;=&amp; \mathbb{E}([y_t - \mathbb{E}(y_t)][y_{t-j} - \mathbb{E}(y_{t-j})]),
\end{eqnarray*}\]</span>
where <span class="math inline">\(f_{Y_t,Y_{t-1},\dots,Y_{t-j}}(y_t,y_{t-1},\dots,y_{t-j})\)</span> is the joint distribution of <span class="math inline">\(y_t,y_{t-1},\dots,y_{t-j}\)</span>.</p>
</div>
<p>In particular, <span class="math inline">\(\gamma_{0,t} = \mathbb{V}ar(y_t)\)</span>.</p>
<div class="definition">
<p><span id="def:covstat" class="definition"><strong>Definition 3.4  (Covariance --or weak-- stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is covariance stationary –or weakly stationary– if, for all <span class="math inline">\(t\)</span> and <span class="math inline">\(j\)</span>,
<span class="math display">\[
\mathbb{E}(y_t) = \mu \quad \mbox{and} \quad \mathbb{E}\{(y_t - \mu)(y_{t-j} - \mu)\} = \gamma_j.
\]</span></p>
</div>
<p>Figure <a href="time-series.html#fig:nonstat1">3.1</a> displays the simulation of a process that is not covariance stationary. This process follows <span class="math inline">\(y_t = 0.1t + \varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim\,i.i.d.\, \\mathcal{N}(0,1)\)</span>. Indeed, for such a process, we have: <span class="math inline">\(\mathbb{E}(y_t)=0.1t\)</span>, which depends on <span class="math inline">\(t\)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:nonstat1"></span>
<img src="AdvECTS_files/figure-html/nonstat1-1.png" alt="Example of a process that is not covariance stationary ($y_t = 0.1t + \varepsilon_t$, where $\varepsilon_t \sim \mathcal{N}(0,1)$)." width="90%"><p class="caption">
Figure 3.1: Example of a process that is not covariance stationary (<span class="math inline">\(y_t = 0.1t + \varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,1)\)</span>).
</p>
</div>
<div class="definition">
<p><span id="def:strictstat" class="definition"><strong>Definition 3.5  (Strict stationarity) </strong></span>The process <span class="math inline">\(y_t\)</span> is strictly stationary if, for all <span class="math inline">\(t\)</span> and all sets of integers <span class="math inline">\(J=\{j_1,\dots,j_n\}\)</span>, the distribution of <span class="math inline">\((y_{t},y_{t+j_1},\dots,y_{t+j_n})\)</span> depends on <span class="math inline">\(J\)</span> but not on <span class="math inline">\(t\)</span>.</p>
</div>
<p>The following process is covariance stationary but not strictly stationary:
<span class="math display">\[
y_t = \mathbb{I}_{\{t&lt;1000\}}\varepsilon_{1,t}+\mathbb{I}_{\{t\ge1000\}}\varepsilon_{2,t},
\]</span>
where <span class="math inline">\(\varepsilon_{1,t} \sim \mathcal{N}(0,1)\)</span> and <span class="math inline">\(\varepsilon_{2,t} \sim \sqrt{\frac{\nu - 2}{\nu}} t(\nu)\)</span> and <span class="math inline">\(\nu = 4\)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:nonstat2"></span>
<img src="AdvECTS_files/figure-html/nonstat2-1.png" alt="Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99\% confidence interval of the standard normal distri. ($\pm 2.58$)." width="90%"><p class="caption">
Figure 3.2: Example of a process that is covariance stationary but not strictly stationary. The red lines delineate the 99% confidence interval of the standard normal distri. (<span class="math inline">\(\pm 2.58\)</span>).
</p>
</div>
<hr>
<div class="proposition">
<p><span id="prp:gammaMinus" class="proposition"><strong>Proposition 3.1  </strong></span>If <span class="math inline">\(y_t\)</span> is covariance stationary, then <span class="math inline">\(\gamma_j = \gamma_{-j}\)</span>.</p>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(y_t\)</span> is covariance stationary, the covariance between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-j}\)</span> (i.e <span class="math inline">\(\gamma_j\)</span>) is the same as that between <span class="math inline">\(y_{t+j}\)</span> and <span class="math inline">\(y_{t+j-j}\)</span> (i.e. <span class="math inline">\(\gamma_{-j}\)</span>).</p>
</div>
<div class="definition">
<p><span id="def:autocor" class="definition"><strong>Definition 3.6  (Auto-correlation) </strong></span>The <span class="math inline">\(j^{th}\)</span> auto-correlation of a covariance-stationary process is:
<span class="math display">\[
\rho_j = \frac{\gamma_j}{\gamma_0}.
\]</span></p>
</div>
<p>Consider a long historical time series of the Swiss GDP growth, taken from the <span class="citation">Jordà, Schularick, and Taylor (<a href="references.html#ref-JST_2017" role="doc-biblioref">2017</a>)</span> dataset.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Version 6 of the dataset, available on &lt;a href="https://www.macrohistory.net"&gt;this website&lt;/a&gt;.&lt;/p&gt;'><sup>1</sup></a></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:autocov"></span>
<img src="AdvECTS_files/figure-html/autocov-1.png" alt="Annual growth rate of Swiss GDP, based on the Jord\`a-Schularick-Taylor Macrohistory Database." width="90%"><p class="caption">
Figure 3.3: Annual growth rate of Swiss GDP, based on the Jord`a-Schularick-Taylor Macrohistory Database.
</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:autocov2"></span>
<img src="AdvECTS_files/figure-html/autocov2-1.png" alt="For order $j$, the slope of the blue line is, approximately, $\hat{\gamma}_j/\widehat{\mathbb{V}ar}(y_t)$, where hats indicate sample moments." width="90%"><p class="caption">
Figure 3.4: For order <span class="math inline">\(j\)</span>, the slope of the blue line is, approximately, <span class="math inline">\(\hat{\gamma}_j/\widehat{\mathbb{V}ar}(y_t)\)</span>, where hats indicate sample moments.
</p>
</div>
<div class="definition">
<p><span id="def:ergodicity" class="definition"><strong>Definition 3.7  (Mean ergodicity) </strong></span>The covariance-stationary process <span class="math inline">\(y_t\)</span> is ergodic for the mean if:
<span class="math display">\[
\mbox{plim}_{T \rightarrow +\infty} \frac{1}{T}\sum_{t=1}^T y_t = \mathbb{E}(y_t).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:ergod2nd" class="definition"><strong>Definition 3.8  (Second-moment ergodicity) </strong></span>The covariance-stationary process <span class="math inline">\(y_t\)</span> is ergodic for second moments if, for all <span class="math inline">\(j\)</span>:
<span class="math display">\[
\mbox{plim}_{T \rightarrow +\infty} \frac{1}{T}\sum_{t=1}^T (y_t-\mu) (y_{t-j}-\mu) = \gamma_j.
\]</span></p>
</div>
<p>It should be noted that ergodicity and stationarity are different properties. Typically if the process <span class="math inline">\(\{x_t\}\)</span> is such that, <span class="math inline">\(\forall t\)</span>, <span class="math inline">\(x_t \equiv y\)</span>, where <span class="math inline">\(y \sim\,\mathcal{N}(0,1)\)</span> (say), then <span class="math inline">\(\{x_t\}\)</span> is stationary but not ergodic.</p>
<div class="theorem">
<p><span id="thm:CLTcovstat" class="theorem"><strong>Theorem 3.1  (Central Limit Theorem for covariance-stationary processes) </strong></span>If process <span class="math inline">\(y_t\)</span> is covariance stationary and if the series of autocovariances is absolutely summable (<span class="math inline">\(\sum_{j=-\infty}^{+\infty} |\gamma_j| &lt;\infty\)</span>), then:
<span class="math display" id="eq:TCL4ts">\[\begin{eqnarray}
\bar{y}_T \overset{m.s.}{\rightarrow} \mu &amp;=&amp; \mathbb{E}(y_t) \tag{3.2}\\
\mbox{lim}_{T \rightarrow +\infty} T \mathbb{E}\left[(\bar{y}_T - \mu)^2\right] &amp;=&amp; \sum_{j=-\infty}^{+\infty} \gamma_j \tag{3.3}\\
\sqrt{T}(\bar{y}_T - \mu) &amp;\overset{d}{\rightarrow}&amp; \mathcal{N}\left(0,\sum_{j=-\infty}^{+\infty} \gamma_j \right) \tag{3.4}.
\end{eqnarray}\]</span></p>
<p>[Mean square (m.s.) and distribution (d.) convergences: see Definitions <a href="appendix.html#def:cvgceDistri">4.11</a> and <a href="appendix.html#def:convergenceLr">4.9</a>.]</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>By Proposition <a href="appendix.html#prp:absMs">4.15</a>, Eq. <a href="time-series.html#eq:TCL2">(3.3)</a> implies Eq. <a href="time-series.html#eq:TCL20">(3.2)</a>. For Eq. <a href="time-series.html#eq:TCL2">(3.3)</a>, see Appendix <a href="appendix.html#proofTVTCL">4.4.6</a>. For Eq. <a href="time-series.html#eq:TCL4ts">(3.4)</a>, see <span class="citation">Anderson (<a href="references.html#ref-Anderson_1971" role="doc-biblioref">1971</a>)</span>, p. 429.</p>
</div>
<div class="definition">
<p><span id="def:LRV" class="definition"><strong>Definition 3.9  (Long-run variance) </strong></span>Under the assumptions of Theorem <a href="time-series.html#thm:CLTcovstat">3.1</a>, the limit appearing in Eq. <a href="time-series.html#eq:TCL2">(3.3)</a> exists and is called <strong>long-run variance</strong>. It is denoted by <span class="math inline">\(S\)</span>, i.e.:
<span class="math display">\[
S = \Sigma_{j=-\infty}^{+\infty} \gamma_j  = \mbox{lim}_{T \rightarrow +\infty} T \mathbb{E}[(\bar{y}_T - \mu)^2]
\]</span></p>
</div>
<p>If <span class="math inline">\(y_t\)</span> is ergodic for second moments (see Def. <a href="time-series.html#def:ergod2nd">3.8</a>), a natural estimator of <span class="math inline">\(S\)</span> is:
<span class="math display" id="eq:covSmplMean">\[\begin{equation}
\hat\gamma_0 + 2 \sum_{\nu=1}^{q} \hat\gamma_\nu \tag{3.5}
\end{equation}\]</span>
where <span class="math inline">\(\hat\gamma_\nu = \frac{1}{T}\sum_{\nu+1}^{T} (y_t - \bar{y})(y_{t-\nu} - \bar{y})\)</span>.</p>
<p>However, for small samples, Eq. <a href="time-series.html#eq:covSmplMean">(3.5)</a> does not necessarily result in a positive definite matrix. <span class="citation">Newey and West (<a href="references.html#ref-Newey_West_1987" role="doc-biblioref">1987</a>)</span> have proposed an estimator that does not have this defect. Their estimator is given by:
<span class="math display">\[
S^{NW}=\hat\gamma_0 + 2 \sum_{\nu=1}^{q}\left(1-\frac{\nu}{q+1}\right) \hat\gamma_\nu.
\]</span></p>
<p>Loosely speaking, Theorem <a href="time-series.html#thm:CLTcovstat">3.1</a> says that, for a given sample size, the higher the “persistency” of a proicess, the lower the accuracy of the sample mean as an estimate of the population mean. To illustrate, consider three processes that feature the same marginal variance (one, say), but different autocorrelations: 0, 80%, and 99%. Figure <a href="time-series.html#fig:TVTCL">3.5</a> displays such three processes. It indeed appears that, the larger the autocorrelation of the process, the further the sample mean (dashed red line) from the population mean (red solid line).</p>
<p>Identical simulations can be performed using <a href="https://jrenne.shinyapps.io/MacroEc/">this ShinyApp</a> (use panel “AR(1)”).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:TVTCL"></span>
<img src="AdvECTS_files/figure-html/TVTCL-1.png" alt="The three samples have been simulated using the following data generating process: $x_t = \mu + \rho (x_{t-1}-\mu) + \sqrt{1-\rho^2}\varepsilon_t$, where $\varepsilon_t \sim \mathcal{N}(0,1)$. Case A: $\rho = 0$;  Case B: $\rho = 0.9$;  Case C: $\rho = 0.99$. In the three cases, $\mathbb{E}(x_t)=\mu=2$ and $\mathbb{V}ar(x_t)=1$." width="90%"><p class="caption">
Figure 3.5: The three samples have been simulated using the following data generating process: <span class="math inline">\(x_t = \mu + \rho (x_{t-1}-\mu) + \sqrt{1-\rho^2}\varepsilon_t\)</span>, where <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,1)\)</span>. Case A: <span class="math inline">\(\rho = 0\)</span>; Case B: <span class="math inline">\(\rho = 0.9\)</span>; Case C: <span class="math inline">\(\rho = 0.99\)</span>. In the three cases, <span class="math inline">\(\mathbb{E}(x_t)=\mu=2\)</span> and <span class="math inline">\(\mathbb{V}ar(x_t)=1\)</span>.
</p>
</div>
</div>
<div id="univariate-processes" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Univariate processes<a class="anchor" aria-label="anchor" href="#univariate-processes"><i class="fas fa-link"></i></a>
</h2>
<div id="moving-average-ma-processes" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Moving Average (MA) processes<a class="anchor" aria-label="anchor" href="#moving-average-ma-processes"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 3.10  </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (Def. <a href="time-series.html#def:whitenoise">3.1</a>). Then <span class="math inline">\(y_t\)</span> is a first-order moving average process if, for all <span class="math inline">\(t\)</span>:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}.
\]</span></p>
</div>
<p>If <span class="math inline">\(\mathbb{E}(\varepsilon_t^2)=\sigma^2\)</span>, the unconditional mean and variances of <span class="math inline">\(y_t\)</span> are:
<span class="math display">\[
\mathbb{E}(y_t) = \mu, \quad \mathbb{V}ar(y_t) = (1+\theta^2)\sigma^2.
\]</span></p>
<p>The first auto-covariance is:
<span class="math display">\[
\gamma_1=\mathbb{E}\{(y_t - \mu)(y_{t-1} - \mu)\} = \theta \sigma^2.
\]</span></p>
<p>Higher-order auto-covariances are zero (<span class="math inline">\(\gamma_j=0\)</span> for <span class="math inline">\(j&gt;1\)</span>). Therefore: A MA(1) process is covariance-stationary (Def. <a href="time-series.html#def:covstat">3.4</a>).</p>
<p>Figure <a href="time-series.html#fig:simMA">3.6</a> displays simulated paths of two MA processes (an MA(1) and an MA(4)). Such simulations can be produced by using panel “ARMA(p,q)” of <a href="https://jrenne.shinyapps.io/MacroEc/">this web interface</a>.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">100</span>;<span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">y.0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">c</span> <span class="op">&lt;-</span> <span class="fl">1</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>;<span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span> <span class="co"># MA(1) specification</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,<span class="va">sigma</span>,<span class="cn">T</span>,<span class="va">y.0</span>,<span class="va">nb.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.2</span>,<span class="fl">.9</span>,<span class="fl">.2</span>,<span class="fl">.85</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>,<span class="st">"=1, "</span>,<span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,<span class="st">"=1"</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="va">c</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span> <span class="co"># MA(4) specification</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,<span class="va">sigma</span>,<span class="cn">T</span>,<span class="va">y.0</span>,<span class="va">nb.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">0</span><span class="op">]</span>,<span class="st">"=...="</span>,<span class="va">theta</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span>,<span class="st">"=1"</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="va">c</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:simMA"></span>
<img src="AdvECTS_files/figure-html/simMA-1.png" alt="Simulation of MA processes." width="90%"><p class="caption">
Figure 3.6: Simulation of MA processes.
</p>
</div>
<p>For a MA(1) process, the autocorrelation of order <span class="math inline">\(j\)</span> (see Def. <a href="time-series.html#def:autocor">3.6</a>) is given by:
<span class="math display">\[
\rho_j =
\left\{
\begin{array}{lll}
1 &amp;\mbox{ if }&amp; j=0,\\
\theta / (1 + \theta^2) &amp;\mbox{ if }&amp; j = 1\\
0 &amp;\mbox{ if }&amp; j&gt;1.
\end{array}
\right.
\]</span></p>
<p>Notice that process <span class="math inline">\(y_t\)</span> defined through:
<span class="math display">\[
y_t = \mu + \varepsilon_t +\theta \varepsilon_{t-1},
\]</span>
where <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=\sigma^2\)</span>, has the same mean and autocovariances as
<span class="math display">\[
y_t = \mu + \varepsilon^*_t +\frac{1}{\theta}\varepsilon^*_{t-1}
\]</span>
where <span class="math inline">\(\mathbb{V}ar(\varepsilon^*_t)=\theta^2\sigma^2\)</span>.</p>
<div class="definition">
<p><span id="def:MAq" class="definition"><strong>Definition 3.11  (MA(q) process) </strong></span>A <span class="math inline">\(q^{th}\)</span> order Moving Average process is defined through:
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}.
\]</span>
where <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process (Def. <a href="time-series.html#def:whitenoise">3.1</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:covMAq" class="proposition"><strong>Proposition 3.2  (Covariance-stationarity of an MA(q) process) </strong></span>Finite-order Moving Average processes are covariance-stationary.</p>
<p>Moreover, the autocovariances of an MA(q) process (as defined in Def. <a href="time-series.html#def:MAq">3.11</a>) are given by:
<span class="math display" id="eq:autocovMA">\[\begin{equation}
\gamma_j = \left\{ \begin{array}{ll} \sigma^2(\theta_j\theta_0 + \theta_{j+1}\theta_{1} +  \dots + \theta_{q}\theta_{q-j}) &amp;\mbox{for} \quad j \in \{0,\dots,q\} \\ 0 &amp;\mbox{for} \quad j&gt;q, \end{array} \right.\tag{3.6}
\end{equation}\]</span>
where we use the notation <span class="math inline">\(\theta_0=1\)</span>, and <span class="math inline">\(\mathbb{V}ar(\varepsilon_t)=\sigma^2\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>The unconditional expectation of <span class="math inline">\(y_t\)</span> does not depend on time, since <span class="math inline">\(\mathbb{E}(y_t)=\mu\)</span>. Let’s turn to autocovariances. We can extend the series of the <span class="math inline">\(\theta_j\)</span>s by setting <span class="math inline">\(\theta_j=0\)</span> for <span class="math inline">\(j&gt;q\)</span>. We then have:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}((y_t-\mu)(y_{t-j}-\mu)) &amp;=&amp; \mathbb{E}\left[(\theta_0 \varepsilon_t +\theta_1 \varepsilon_{t-1} + \dots +\theta_j \varepsilon_{t-j}+\theta_{j+1} \varepsilon_{t-j-1} + \dots) \right.\times \\
&amp;&amp;\left. (\theta_0 \varepsilon_{t-j} +\theta_1 \varepsilon_{t-j-1} + \dots)\right].
\end{eqnarray*}\]</span>
Then use the fact that <span class="math inline">\(\mathbb{E}(\varepsilon_t\varepsilon_s)=0\)</span> if <span class="math inline">\(t \ne s\)</span> (because <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process).</p>
</div>
<p>What if <span class="math inline">\(k\)</span> gets infinite? The notion of <strong>infinite-order Moving Average process</strong> exists and is important in time series analysis. The (infinite) sequence of <span class="math inline">\(\theta_j\)</span> has to satisfy some conditions for such a process to be well-defined (see Theorem <a href="time-series.html#thm:infMA">3.2</a> below). These conditions relate to the “summability” of <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span>.</p>
<div class="definition">
<p><span id="def:summability" class="definition"><strong>Definition 3.12  (Absolute and square summability) </strong></span>The sequence <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is <strong>absolutely summable</strong> if <span class="math inline">\(\sum_{i=0}^{\infty}|\theta_i| &lt; + \infty\)</span>, and it is <strong>square summable</strong> if <span class="math inline">\(\sum_{i=0}^{\infty} \theta_i^2 &lt; + \infty\)</span>.</p>
</div>
<p>According to Theorem <a href="#thm:absMs"><strong>??</strong></a>, absolute summability implies square summability.</p>
<div class="theorem">
<p><span id="thm:infMA" class="theorem"><strong>Theorem 3.2  (Existence condition for an infinite MA process) </strong></span>If <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is square summable (see Def. <a href="time-series.html#def:summability">3.12</a> and if <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> is a white noise process (see Def. <a href="time-series.html#def:whitenoise">3.1</a>), then
<span class="math display">\[
\mu + \sum_{i=0}^{+\infty} \theta_{i} \varepsilon_{t-i}
\]</span>
defines a well-behaved [covariance-stationary] process, called <strong>infinite-order MA process</strong> (MA(<span class="math inline">\(\infty\)</span>)).</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>See Appendix 3.A in Hamilton. By “well behaved”, we mean <span class="math inline">\(\Sigma_{i=0}^{T} \theta_{t-i} \varepsilon_{t-i}\)</span> converges in mean square (Def. <a href="appendix.html#def:convergenceLr">4.9</a>) to some random variable <span class="math inline">\(Z_t\)</span>. The proof makes use of the fact that:
<span class="math display">\[
\mathbb{E}\left[\left(\sum_{i=N}^{M}\theta_{i} \varepsilon_{t-i}\right)^2\right] = \sum_{i=N}^{M}|\theta_{i}|^2 \sigma^2,
\]</span>
and that, when <span class="math inline">\(\{\theta_{i}\}\)</span> is square summable, <span class="math inline">\(\forall \eta&gt;0\)</span>, <span class="math inline">\(\exists N\)</span> s.t., the right-hand-side term in the last equation is lower than <span class="math inline">\(\eta\)</span> for all <span class="math inline">\(M \ge N\)</span> (static Cauchy criterion, Theorem <a href="appendix.html#thm:cauchycritstatic">4.1</a>). This implies that <span class="math inline">\(\Sigma_{i=0}^{T} \theta_{i} \varepsilon_{t-i}\)</span> converges in mean square (stochastic Cauchy criterion, see Theorem <a href="appendix.html#thm:cauchycritstochastic">4.2</a>).</p>
</div>
<div class="proposition">
<p><span id="prp:momentsMAinf" class="proposition"><strong>Proposition 3.3  (First two moments of an infinite MA process) </strong></span>If <span class="math inline">\(\{\theta_{i}\}_{i\in\mathbb{N}}\)</span> is absolutely summable, i.e. if <span class="math inline">\(\sum_{i=0}^{\infty}|\theta_i| &lt; + \infty\)</span>, then</p>
<ol style="list-style-type: lower-roman">
<li>
<span class="math inline">\(y_t = \mu + \sum_{i=0}^{+\infty} \theta_{i} \varepsilon_{t-i}\)</span> exists (Theorem <a href="#them:infMA"><strong>??</strong></a>) and is such that:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(y_t) &amp;=&amp; \mu\\
\gamma_0 = \mathbb{E}([y_t-\mu]^2) &amp;=&amp; \sigma^2(\theta_0^2 +\theta_1^2 + \dots)\\
\gamma_j = \mathbb{E}([y_t-\mu][y_{t-j}-\mu]) &amp;=&amp; \sigma^2(\theta_0\theta_j + \theta_{1}\theta_{j+1} + \dots).
\end{eqnarray*}\]</span>
</li>
<li>Process <span class="math inline">\(y_t\)</span> has absolutely summable auto-covariances, which implies that the results of Theorem <a href="time-series.html#thm:CLTcovstat">3.1</a> (Central Limit) apply.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>The absolute summability of <span class="math inline">\(\{\theta_{t}\}\)</span> and the fact that <span class="math inline">\(\mathbb{E}(\varepsilon^2)&lt;\infty\)</span> implies that the order of integration and summation is interchangeable (see Hamilton, 1994, Footnote p. 52), which proves (i). For (ii), see end of Appendix 3.A in Hamilton (1994).</p>
</div>
</div>
<div id="auto-regressive-ar-processes" class="section level3" number="3.2.2">
<h3>
<span class="header-section-number">3.2.2</span> Auto-Regressive (AR) processes<a class="anchor" aria-label="anchor" href="#auto-regressive-ar-processes"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:AR1" class="definition"><strong>Definition 3.13  (First-order AR process) </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (see Def. <a href="time-series.html#def:whitenoise">3.1</a>). Process <span class="math inline">\(y_t\)</span> is an AR(1) process if it is defined by the following difference equation:
<span class="math display">\[
y_t = c + \phi y_{t-1} + \varepsilon_t.
\]</span></p>
</div>
<p>If <span class="math inline">\(|\phi|\ge1\)</span>, <span class="math inline">\(y_t\)</span> is not stationary. Indeed, we have:
<span class="math display">\[
y_{t+k} = c + \varepsilon_{t+k} + \phi  ( c + \varepsilon_{t+k-1})+ \phi^2  ( c + \varepsilon_{t+k-2})+ \dots + \phi^{k-1}  ( c + \varepsilon_{t+1}) + \phi^k y_t.
\]</span>
Therefore:
<span class="math display">\[
\mathbb{V}ar_t(y_{t+k}) = \sigma^2(1 + \phi^2 + \phi^4 + \dots + \phi^{2(k-1)}) \underset{k\rightarrow +\infty}{\rightarrow} + \infty.
\]</span></p>
<p>By contrast, if <span class="math inline">\(|\phi| &lt; 1\)</span>, one can see that:
<span class="math display">\[
y_t = c + \varepsilon_t + \phi  ( c + \varepsilon_{t-1})+ \phi^2  ( c + \varepsilon_{t-2})+ \dots + \phi^k  ( c + \varepsilon_{t-k}) + \dots
\]</span>
Hence, if <span class="math inline">\(|\phi| &lt; 1\)</span>, the unconditional mean and variance of <span class="math inline">\(y_t\)</span> are:
<span class="math display">\[
\mathbb{E}(y_t) = \frac{c}{1-\phi} =: \mu \quad \mbox{and} \quad \mathbb{V}ar(y_t) = \frac{\sigma^2}{1-\phi^2}.
\]</span></p>
<p>Let us compute the <span class="math inline">\(j^{th}\)</span> autocovariance of the AR(1) process:
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;\mathbb{E}([y_{t} - \mu][y_{t-j} - \mu])\\
&amp;=&amp; \mathbb{E}([\varepsilon_t + \phi  \varepsilon_{t-1}+ \phi^2 \varepsilon_{t-2} + \dots + \color{red}{\phi^j \varepsilon_{t-j}} + \color{blue}{\phi^{j+1} \varepsilon_{t-j-1}} \dots]\times \\
&amp;&amp;[\color{red}{\varepsilon_{t-j}} + \color{blue}{\phi \varepsilon_{t-j-1}} + \phi^2 \varepsilon_{t-j-2} + \dots + \phi^k \varepsilon_{t-j-k} + \dots])\\
&amp;=&amp; \mathbb{E}(\color{red}{\phi^j \varepsilon_{t-j}^2}+\color{blue}{\phi^{j+2} \varepsilon_{t-j-1}^2}+\phi^{j+4} \varepsilon_{t-j-2}^2+\dots)\\
&amp;=&amp; \frac{\phi^j \sigma^2}{1 - \phi^2}.
\end{eqnarray*}\]</span></p>
<p>Therefore <span class="math inline">\(\rho_j = \phi^j\)</span>.</p>
<p>By what precedes, we have:</p>
<hr>
<div class="proposition">
<p><span id="prp:statioAR1" class="proposition"><strong>Proposition 3.4  (Covariance-stationarity of an AR(1) process) </strong></span>The AR(1) process, as defined in Def. <a href="#defn:AR1"><strong>??</strong></a>, is cov.-stationary iff <span class="math inline">\(|\phi|&lt;1\)</span>.</p>
</div>
<hr>
<div class="definition">
<p><span id="def:ARp" class="definition"><strong>Definition 3.14  (AR(p) process) </strong></span>Consider a white noise process <span class="math inline">\(\{\varepsilon_t\}_{t = -\infty}^{+\infty}\)</span> (see Def. <a href="time-series.html#def:whitenoise">3.1</a>). Process <span class="math inline">\(y_t\)</span> is a <span class="math inline">\(p^{th}\)</span>-order autoregressive process (AR(p)) if its dynamics is defined by the following difference equation (with <span class="math inline">\(\phi_p \ne 0\)</span>):
<span class="math display" id="eq:AR">\[\begin{equation}
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t.\tag{3.7}
\end{equation}\]</span></p>
</div>
<p>As we will see, the covariance-stationarity of <span class="math inline">\(F\)</span> hinges on matrix <span class="math inline">\(F\)</span> defined as:
<span class="math display" id="eq:F">\[\begin{equation}
F = \left[
\begin{array}{ccccc}
\phi_1 &amp; \phi_2 &amp; \dots&amp; &amp; \phi_p \\
1 &amp; 0 &amp;\dots &amp;&amp; 0 \\
0 &amp; 1 &amp;\dots &amp;&amp; 0 \\
\vdots &amp;  &amp; \ddots &amp;&amp; \vdots \\
0 &amp; 0 &amp;\dots &amp;1&amp; 0 \\
\end{array}
\right].\tag{3.8}
\end{equation}\]</span></p>
<p>Indeed, the fact that process <span class="math inline">\(y_t\)</span> follows Eq. <a href="time-series.html#eq:AR">(3.7)</a> is equivalent to the fact that
<span class="math display">\[
\mathbf{y}_t = F \mathbf{y}_{t-1} + \boldsymbol\xi_t
\]</span>
with
<span class="math display">\[
\boldsymbol\xi_t =
\left[\begin{array}{c}
\varepsilon_t\\
0\\
\vdots\\
0
\end{array}\right],
\quad
\mathbf{y}_t =
\left[\begin{array}{c}
y_t\\
y_{t-1}\\
\vdots\\
y_{t-p+1}
\end{array}\right].
\]</span></p>
<p>and
<span class="math display">\[\begin{equation}
\mathbf{y}_{t+j} = \boldsymbol\xi_{t+j} + F \boldsymbol\xi_{t+j-1} + F^2 \boldsymbol\xi_{t+j-2} + \dots + F^{j-1} \boldsymbol\xi_{t+1} + F^{j} \mathbf{y}_{t}
\end{equation}\]</span>
or
<span class="math display" id="eq:Fyt">\[\begin{equation}
\mathbf{y}_{t+j} = \boldsymbol\xi_{t+j} + F \boldsymbol\xi_{t+j-1} + F^2 \boldsymbol\xi_{t+j-2} + \dots + F^{j} \boldsymbol\xi_{t} + F^{j+1} \mathbf{y}_{t-1}.\tag{3.9}
\end{equation}\]</span></p>
<!-- :::{.definition #dynmult name="Dynamic multiplier"} -->
<!-- The **dynamic multiplier** of $y_{t+j}$ w.r.t. $\varepsilon_{t}$ is $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}}$. -->
<!-- ::: -->
<!-- Eq. \@ref(eq:Fyt) implies that we have: $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = (F^j)_{[1,1]}$ -->
<!-- (for any $M,i,j$, $(M)_{[i,j]}$ denotes the $(i,j)$ element of matrix $M$). -->
<!-- Let us assume that the eigenvalues of $F$ (see Def. \@ref(def:determinant)), denoted by $\lambda_1,\dots,\lambda_p$, are distinct. -->
<!-- Then, there exists a nonsingular matrix $P$ such that: -->
<!-- $$ -->
<!-- F = P -->
<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- \lambda_1 & 0 & \dots& 0 \\ -->
<!-- 0 & \lambda_2 & 0 & \dots \\ -->
<!-- &&\ddots\\ -->
<!-- 0 & \dots & 0& \lambda_p\\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- P^{-1} = P D P^{-1}. -->
<!-- $$ -->
<!-- It can be seen that: -->
<!-- $$ -->
<!-- F^j = P -->
<!-- \left[ -->
<!-- \begin{array}{cccc} -->
<!-- \lambda_1^j & 0 & \dots& 0 \\ -->
<!-- 0 & \lambda_2^j & 0 & \dots \\ -->
<!-- &&\ddots\\ -->
<!-- 0 & \dots & 0& \lambda_p^j\\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- P^{-1}. -->
<!-- $$ -->
<!-- Hence, the dynamic multiplier of $y_{t+j}$ w.r.t. $\varepsilon_{t}$ is given by: -->
<!-- $$ -->
<!-- \dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = \sum_i (P)_{[1,i]}(P^{-1})_{[i,1]}\lambda_i^j. -->
<!-- $$ -->
<!-- Denoting by $c_i$ the scalar $(P)_{[1,i]}(P^{-1})_{[i,1]}$, we have $\dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} = \sum_i c_i\lambda_i^j$. If the eigenvalues of $F$ are distinct and nonzero, then $c_i \ne 0$. Therefore, if $\exists i$ s.t. $|\lambda_i|>1$ then: -->
<!-- $$ -->
<!-- \left| \dfrac{\partial y_{t+j}}{\partial \varepsilon_{t}} \right| \underset{j \rightarrow \infty}{\rightarrow} \infty. -->
<!-- $$ -->
<div class="proposition">
<p><span id="prp:Feigen" class="proposition"><strong>Proposition 3.5  (The eigenvalues of matrix F) </strong></span>The eigenvalues of <span class="math inline">\(F\)</span> (defined by Eq. <a href="time-series.html#eq:F">(3.8)</a>) are the solutions of:
<span class="math display" id="eq:Feigen">\[\begin{equation}
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_{p-1}\lambda - \phi_p = 0.\tag{3.10}
\end{equation}\]</span></p>
</div>
<hr>
<div class="proposition">
<p><span id="prp:stability" class="proposition"><strong>Proposition 3.6  (Covariance-stationarity of an AR(p) process) </strong></span>These four statements are equivalent:</p>
<ol style="list-style-type: lower-roman">
<li>Process <span class="math inline">\(\{y_t\}\)</span>, defined in Def. <a href="time-series.html#def:ARp">3.14</a>, is covariance-stationary.</li>
<li>The eigenvalues of <span class="math inline">\(F\)</span> (as defined Eq. <a href="time-series.html#eq:F">(3.8)</a>) lie within the unit circle.</li>
<li>The roots of Eq. <a href="time-series.html#eq:outside">(3.11)</a> (below) lie strictly outside the unit circle.
<span class="math display" id="eq:outside">\[\begin{equation}
1 - \phi_1 z - \dots - \phi_{p-1}z^{p-1} - \phi_p z^p = 0.\tag{3.11}
\end{equation}\]</span>
</li>
<li>The roots of Eq. <a href="time-series.html#eq:inside">(3.12)</a> (below) lie strictly inside the unit circle.
<span class="math display" id="eq:inside">\[\begin{equation}
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_{p-1}\lambda - \phi_p = 0.\tag{3.12}
\end{equation}\]</span>
</li>
</ol>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>We consider the case where the eigenvalues of <span class="math inline">\(F\)</span> are distinct; Jordan decomposition can be used in the general case. When the eigenvalues of <span class="math inline">\(F\)</span> are distinct, <span class="math inline">\(F\)</span> admits the following spectral decomposition: <span class="math inline">\(F = PDP^{-1}\)</span>, where <span class="math inline">\(D\)</span> is diagonal. Using the notations introduced in Eq. <a href="time-series.html#eq:F">(3.8)</a>, we have:
<span class="math display">\[
\mathbf{y}_{t} = \mathbf{c} + F \mathbf{y}_{t-1} + \boldsymbol\xi_{t}.
\]</span>
Let’s introduce <span class="math inline">\(\mathbf{d} = P^{-1}\mathbf{c}\)</span>, <span class="math inline">\(\mathbf{z}_t = P^{-1}\mathbf{y}_t\)</span> and <span class="math inline">\(\boldsymbol\eta_t = P^{-1}\boldsymbol\xi_t\)</span>. We have:
<span class="math display">\[
\mathbf{z}_{t} = \mathbf{d} + D \mathbf{z}_{t-1} + \boldsymbol\eta_{t}.
\]</span>
Because <span class="math inline">\(D\)</span> is diagonal, the different component of <span class="math inline">\(\mathbf{z}_t\)</span>, denoted by <span class="math inline">\(z_{i,t}\)</span>, follow AR(1) processes. The (scalar) autoregressive parameters of these AR(1) processes are the diagonal entries of <span class="math inline">\(D\)</span> –which also are the eigenvalues of <span class="math inline">\(F\)</span>– that we denote by <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Process <span class="math inline">\(y_t\)</span> is cov.-stationary iff <span class="math inline">\(\mathbf{y}_{t}\)</span> also is cov.-stationary, which is the case iff all <span class="math inline">\(z_{i,t}\)</span>, <span class="math inline">\(i \in [1,p]\)</span>, are cov.-stationary. By Prop. <a href="time-series.html#prp:statioAR1">3.4</a>, process <span class="math inline">\(z_{i,t}\)</span> is cov.-stationary iff <span class="math inline">\(|\lambda_i|&lt;1\)</span>. This proves that (i) is equivalent to (ii).</p>
<p>Prop. <a href="time-series.html#prp:Feigen">3.5</a> further proves that (ii) is equivalent to (iv). Finally, it is easily seen that (iii) is equivalent to (iv) (as long as <span class="math inline">\(\phi_p \ne 0\)</span>).</p>
</div>
<p>Using the lag operator (see Eq <a href="time-series.html#eq:lagOp">(3.1)</a>), if <span class="math inline">\(y_t\)</span> is a cov.-stationary AR(p) process (Def. <a href="time-series.html#def:ARp">3.14</a>), we can write:
<span class="math display">\[
y_t = \mu + \psi(L)\varepsilon_t,
\]</span>
where
<span class="math display" id="eq:EAR">\[\begin{equation}
\psi(L) = (1 - \phi_1 L - \dots - \phi_p L^p)^{-1} \quad and \quad \mu = \mathbb{E}(y_t) = \dfrac{c}{1-\phi_1 -\dots - \phi_p}.\tag{3.13}
\end{equation}\]</span></p>
<p>In the following lines of codes, we compute the eigenvalues of the <span class="math inline">\(F\)</span> matrices associated with the following processes (where <span class="math inline">\(\varepsilon_t\)</span> is a white noise):
<span class="math display">\[\begin{eqnarray*}
x_t &amp;=&amp; 0.9 y_{t-1} -0.2 x_{t-2} + \varepsilon_t\\
y_t &amp;=&amp; 1.1 y_{t-1} -0.3 y_{t-2} + \varepsilon_t\\
w_t &amp;=&amp; 1.4 y_{t-1} -0.7 w_{t-2} + \varepsilon_t\\
z_t &amp;=&amp; 0.9 y_{t-1} +0.2 z_{t-2} + \varepsilon_t
\end{eqnarray*}\]</span></p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">F</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.9</span>,<span class="fl">1</span>,<span class="op">-</span><span class="fl">.2</span>,<span class="fl">0</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">lambda_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.1</span>,<span class="op">-</span><span class="fl">.3</span><span class="op">)</span></span>
<span><span class="va">lambda_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.4</span>,<span class="op">-</span><span class="fl">.7</span><span class="op">)</span></span>
<span><span class="va">lambda_w</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="cn">F</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.9</span>,<span class="fl">.2</span><span class="op">)</span></span>
<span><span class="va">lambda_z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="cn">F</span><span class="op">)</span><span class="op">$</span><span class="va">values</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">lambda_x</span>,<span class="va">lambda_y</span>,<span class="va">lambda_w</span>,<span class="va">lambda_z</span><span class="op">)</span></span></code></pre></div>
<pre><code>##                         [,1]                  [,2]
## lambda_x 0.500000+0.0000000i  0.4000000+0.0000000i
## lambda_y 0.600000+0.0000000i  0.5000000+0.0000000i
## lambda_w 0.700000+0.4582576i  0.7000000-0.4582576i
## lambda_z 1.084429+0.0000000i -0.1844289+0.0000000i</code></pre>
<p>The absolute values of the eigenvalues associated with process <span class="math inline">\(w_t\)</span> are equal to 0.837. Therefore, according to Proposition <a href="time-series.html#prp:stability">3.6</a>, processes <span class="math inline">\(x_t\)</span>, <span class="math inline">\(y_t\)</span>, and <span class="math inline">\(w_t\)</span> are covariance-stationary, but not <span class="math inline">\(z_t\)</span> (because the absolute value of one of the eigenvalues of the <span class="math inline">\(F\)</span> matrix associated with this process is larger than 1).</p>
<p>The computation of the autocovariances of <span class="math inline">\(y_t\)</span> is based on the so-called <strong>Yule-Walker equations</strong> (Eq. <a href="time-series.html#eq:gammas">(3.14)</a>). Let’s rewrite Eq. <a href="time-series.html#eq:AR">(3.7)</a>:
<span class="math display">\[
(y_t-\mu) = \phi_1 (y_{t-1}-\mu) + \phi_2 (y_{t-2}-\mu) + \dots + \phi_p (y_{t-p}-\mu) + \varepsilon_t.
\]</span>
Multiplying both sides by <span class="math inline">\(y_{t-j}-\mu\)</span> and taking expectations leads to the (Yule-Walker) equations:
<span class="math display" id="eq:gammas">\[\begin{equation}
\gamma_j = \left\{
\begin{array}{l}
\phi_1 \gamma_{j-1}+\phi_2 \gamma_{j-2}+ \dots + \phi_p \gamma_{j-p} \quad if \quad j&gt;0\\
\phi_1 \gamma_{1}+\phi_2 \gamma_{2}+ \dots + \phi_p \gamma_{p} + \sigma^2 \quad for \quad j=0.
\end{array}
\right.\tag{3.14}
\end{equation}\]</span>
Using <span class="math inline">\(\gamma_j = \gamma_{-j}\)</span> (Prop. <a href="time-series.html#prp:gammaMinus">3.1</a>), one can solve for <span class="math inline">\((\gamma_0,\gamma_1,\dots,\gamma_{p})\)</span> as functions of <span class="math inline">\((\sigma^2,\phi_1,\dots,\phi_p)\)</span>.</p>
</div>
<div id="ar-ma-processes" class="section level3" number="3.2.3">
<h3>
<span class="header-section-number">3.2.3</span> AR-MA processes<a class="anchor" aria-label="anchor" href="#ar-ma-processes"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:ARMApq" class="definition"><strong>Definition 3.15  (ARMA(p,q) process) </strong></span><span class="math inline">\(\{y_t\}\)</span> is an ARMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) process if its dynamics is described by the following equation:
<span class="math display" id="eq:ARMApq">\[\begin{equation}
y_t = c + \underbrace{\phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR part}} + \underbrace{\varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}}_{\mbox{MA part}},\tag{3.15}
\end{equation}\]</span>
where <span class="math inline">\(\{\varepsilon_t\}_{t \in ] -\infty,+\infty[}\)</span> is a white noise process (see Def. <a href="time-series.html#def:whitenoise">3.1</a>).</p>
</div>
<hr>
<div class="proposition">
<p><span id="prp:statioARMApq" class="proposition"><strong>Proposition 3.7  (Stationarity of an ARMA(p,q) process) </strong></span>The ARMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) process defined in <a href="time-series.html#def:ARMApq">3.15</a> is covariance stationary iff the roots of
<span class="math display">\[
1 - \phi_1 z - \dots - \phi_p z^p=0
\]</span>
lie strictly outside the unit circle or, equivalently, iff those of
<span class="math display">\[
\lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_p=0
\]</span>
lie strictly within the unit circle.</p>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>The proof of Prop. <a href="time-series.html#prp:stability">3.6</a> can be adapted to the present case.</p>
</div>
<p>We can write:
<span class="math display">\[
(1 - \phi_1 L - \dots - \phi_p L^p)y_t = c + (1 + \theta_1 L + \dots + \theta_q L^q)\varepsilon_t.
\]</span></p>
<p>If the roots of <span class="math inline">\(1 - \phi_1 z - \dots - \phi_p z^p=0\)</span> lie outside the unit circle, we have:
<span class="math display">\[\begin{equation}
y_t = \mu + \psi(L)\varepsilon_t,(\#eq:ARMA_wold)
\end{equation}\]</span>
where
<span class="math display">\[
\psi(L) = \frac{1 + \theta_1 L + \dots + \theta_q L^q}{1 - \phi_1 L - \dots - \phi_p L^p} \quad and \quad \mu = \dfrac{c}{1-\phi_1 -\dots - \phi_p}.
\]</span></p>
<p>Eq. @ref(eq:ARMA_wold) is the <strong>Wold representation</strong> of this ARMA process (see Theorem <a href="time-series.html#thm:Wold">3.3</a> below).</p>
<p>The stationarity of the process depends only on the AR specification. If the process is stationary, the weights in <span class="math inline">\(\psi(L)\)</span> decay at a geometric rate.</p>
<p>We have seen that the <span class="math inline">\(k^{th}\)</span>-order auto-correlation of a MA(q) process is null if <span class="math inline">\(k&gt;q\)</span>.</p>
<p>This is exploited, in practice, to determine the order of a MA process. There exists an equivalent tool for the AR processes; it is based on partial auto-correlations (see definition below).</p>
<div class="definition">
<p><span id="def:partialAC" class="definition"><strong>Definition 3.16  (Partial correlation) </strong></span>The partial correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, given <span class="math inline">\(Z\)</span>, is the correlation between:</p>
<ol style="list-style-type: lower-alpha">
<li>the residuals of the linear regression of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span> and</li>
<li>the residuals of the linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(Z\)</span>.</li>
</ol>
</div>
<p>In a time series context, the <strong>partial auto-correlation</strong> (see Def. <a href="time-series.html#def:partialAC">3.16</a>) <span class="math inline">\(\phi_{h,h}\)</span> is defined as the partial correlation of <span class="math inline">\(y_{t+h}\)</span> and <span class="math inline">\(y_t\)</span> given <span class="math inline">\(y_{t+h-1},\dots,y_{t+1}\)</span>.</p>
<p>If <span class="math inline">\(h&gt;p\)</span>, the regression of <span class="math inline">\(y_{t+h}\)</span> on <span class="math inline">\(y_{t+h-1},\dots,y_{t+1}\)</span> is:
<span class="math display">\[
y_{t+h} = c + \phi_1 y_{t+h-1}+\dots+ \phi_p  y_{t+h-p} + \varepsilon_{t+h}.
\]</span>
The residuals of the latter regressions (<span class="math inline">\(\varepsilon_{t+h}\)</span>) are uncorrelated to <span class="math inline">\(y_t\)</span>. Then the partial autocorrelation is zero for <span class="math inline">\(h&gt;p\)</span>.</p>
<p>Besides, it can be shown that <span class="math inline">\(\phi_{p,p}=\phi_p\)</span>. Hence <span class="math inline">\(\phi_{p,p}=\phi_p\)</span> but <span class="math inline">\(\phi_{h,h}=0\)</span> for <span class="math inline">\(h&gt;p\)</span>. This can be used to determine the order of a AR process. By contrast (importantly) if <span class="math inline">\(y_t\)</span> follows an MA(q) process, then <span class="math inline">\(\phi_{k,k}\)</span> asymptotically approaches zero instead of cutting off abruptly.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.2</span>,<span class="fl">.9</span>,<span class="fl">.2</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span><span class="op">=</span><span class="fl">0</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,T<span class="op">=</span><span class="fl">1000</span>,y.0<span class="op">=</span><span class="fl">0</span>,nb.sim<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span><span class="op">=</span><span class="fl">0.9</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,T<span class="op">=</span><span class="fl">1000</span>,y.0<span class="op">=</span><span class="fl">0</span>,nb.sim<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y.sim</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">y.sim</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pacf"></span>
<img src="AdvECTS_files/figure-html/pacf-1.png" alt="ACF/PACF analysis of two processes (MA process on the left, AR on the right)." width="100%"><p class="caption">
Figure 3.7: ACF/PACF analysis of two processes (MA process on the left, AR on the right).
</p>
</div>
<div class="theorem">
<p><span id="thm:Wold" class="theorem"><strong>Theorem 3.3  (Wold decomposition) </strong></span>Any covariance-stationary process admits the following representation:
<span class="math display">\[
y_t = \mu + \sum_{0}^{+\infty} \theta_i \varepsilon_{t-i} + \kappa_t,
\]</span>
where</p>
<ul>
<li>
<span class="math inline">\(\theta_0 = 1\)</span>, <span class="math inline">\(\sum_{i=0}^{\infty} \theta_i^2 &lt; +\infty\)</span> (square summability, see Def. <a href="time-series.html#def:summability">3.12</a>).</li>
<li>
<span class="math inline">\(\{\varepsilon_t\}\)</span> is a white noise (see Def. <a href="time-series.html#def:whitenoise">3.1</a>); <span class="math inline">\(\varepsilon_t\)</span> is the error made when forecasting <span class="math inline">\(y_t\)</span> based on a linear combination of lagged <span class="math inline">\(y_t\)</span>s (<span class="math inline">\(\varepsilon_t = y_t - \hat{\mathbb{E}}[y_t|y_{t-1},y_{t-2},\dots]\)</span>).</li>
<li>For any <span class="math inline">\(j \ge 1\)</span>, <span class="math inline">\(\kappa_t\)</span> is not correlated with <span class="math inline">\(\varepsilon_{t-j}\)</span>; but <span class="math inline">\(\kappa_t\)</span> can be perfectly forecasted based on a linear combination of lagged <span class="math inline">\(y_t\)</span>s (i.e. <span class="math inline">\(\kappa_t = \hat{\mathbb{E}}(\kappa_t|y_{t-1},y_{t-2},\dots)\)</span>). <span class="math inline">\(\kappa_t\)</span> is called the <strong>deterministic component</strong> of <span class="math inline">\(y_t\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>Proof</em>. </span>See <span class="citation">Anderson (<a href="references.html#ref-Anderson_1971" role="doc-biblioref">1971</a>)</span>. Partial proof in <a href="http://faculty.wcas.northwestern.edu/~lchrist/finc520/wold.pdf">L. Christiano</a>.</p>
</div>
<p>For an ARMA process, the Wold representation is given by Eq. @ref(eq:ARMA_wold). It can be computed by recursively replacing the lagged <span class="math inline">\(y_t\)</span>s in Eq. <a href="time-series.html#eq:ARMApq">(3.15)</a>. In this case, the deterministic component (<span class="math inline">\(\kappa\)</span>) is null.</p>
</div>
<div id="impulse-response-functions-irfs-in-arma-models" class="section level3" number="3.2.4">
<h3>
<span class="header-section-number">3.2.4</span> Impulse Response Functions (IRFs) in ARMA models<a class="anchor" aria-label="anchor" href="#impulse-response-functions-irfs-in-arma-models"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the following ARMA(p,q) process, as defined in Def. <a href="time-series.html#def:ARMApq">3.15</a>. Based on its associated sequence of whote noise (<span class="math inline">\(\{\varepsilon_t\}\)</span>), let us construct a novel sequence of shocks <span class="math inline">\(\{\tilde\varepsilon_t^{(s)}\}\)</span>:
<span class="math display">\[
\tilde\varepsilon_t^{(s)} = \left\{
\begin{array}{lcc}
\varepsilon_{t} &amp; if &amp; t \ne s,\\
\varepsilon_{t} + \delta &amp;if&amp; t=s.
\end{array}
\right.
\]</span>
We denote by <span class="math inline">\(\{\tilde{y}_t^{(s)}\}\)</span> the process following Eq. <a href="time-series.html#eq:ARMApq">(3.15)</a> where <span class="math inline">\(\{\varepsilon_t\}\)</span> is replaced by <span class="math inline">\(\{\tilde\varepsilon_t^{(s)}\}\)</span>. The time series <span class="math inline">\(\{\tilde{y}_t^{(s)}\}\)</span> is the (counterfactual) series <span class="math inline">\(\{y_t\}\)</span> that would have prevailed if <span class="math inline">\(\varepsilon_t\)</span> had been higher than its realised value at date <span class="math inline">\(s\)</span> (only).</p>
<p>What is the relationship between <span class="math inline">\(\{y_t\}\)</span> and <span class="math inline">\(\{\tilde{y}_t^{(s)}\}\)</span>?</p>
<p>By definition, we have:
<span class="math display">\[
\boxed{\tilde{y}_t^{(s)} = y_t + \frac{\partial y_t}{\partial \varepsilon_{s}}\delta.}
\]</span></p>
<p>The object <span class="math inline">\(\frac{\partial y_t}{\partial \varepsilon_{s}}\)</span> is called the dynamic multiplier of <span class="math inline">\(y_t\)</span> w.r.t. <span class="math inline">\(\varepsilon_{t}\)</span> (of order <span class="math inline">\(t-s\)</span>).</p>
<p>Let us consider the <strong>Wold decomposition</strong> of <span class="math inline">\(\{y_t\}\)</span>, i.e. (see Theorem <a href="time-series.html#thm:Wold">3.3</a> and Eq. @ref(eq:ARMA_wold)) for its application to ARMA processes):
<span class="math display">\[
y_t = \mu + \sum_{i=0}^{+\infty} \psi_i \varepsilon_{t-i}.
\]</span>
For <span class="math inline">\(t&lt;s\)</span>, we have <span class="math inline">\(y_t = \tilde{y}_t^{(s)}\)</span> (because <span class="math inline">\(\tilde{\varepsilon}_{t-i}= \varepsilon_{t-i}\)</span> for all <span class="math inline">\(i \ge 0\)</span> if <span class="math inline">\(t&lt;s\)</span>).</p>
<p>For <span class="math inline">\(t \ge s\)</span>:
<span class="math display">\[
\tilde{y}_t^{(s)} = \mu + \left( \sum_{i=0}^{t-s-1} \psi_i \varepsilon_{t-i} \right) + \psi_{t-s}(\varepsilon_{s}+\delta) + \left( \sum_{i=t-s+1}^{+\infty} \psi_i \varepsilon_{t-i} \right)=y_t + \frac{\partial y_t}{\partial \varepsilon_{s}}\delta.
\]</span>
Therefore, for <span class="math inline">\(t \ge s\)</span>, we have <span class="math inline">\(\boxed{\dfrac{\partial y_t}{\partial \varepsilon_{s}}=\psi_{t-s}.}\)</span></p>
<p>The sequence <span class="math inline">\(\left\{\dfrac{\partial y_{t+h}}{\partial \varepsilon_{t}}\right\}_{h \ge 0} \equiv \left\{\psi_h\right\}_{h \ge 0}\)</span> defines the <strong>impulse response function (IRF)</strong> of <span class="math inline">\(y_t\)</span> to the shock <span class="math inline">\(\varepsilon_t\)</span>.</p>
<p>For ARMA processes, the computation of the IRFs is easy:</p>
<hr>
<div class="proposition">
<p><span id="prp:computPsi" class="proposition"><strong>Proposition 3.8  (IRF of an ARMA(p,q) process) </strong></span>The coefficients <span class="math inline">\(\psi_h\)</span>, that define the IRF of process <span class="math inline">\(y_t\)</span> to <span class="math inline">\(\varepsilon_t\)</span> can be computed recursively as follows:</p>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\psi_{-1}=\dots=\psi_{-p}=0\)</span>.</li>
<li>For <span class="math inline">\(h \ge 0\)</span>, (recursively) apply Eq. <a href="time-series.html#eq:ARMApq">(3.15)</a>:
<span class="math display">\[
\psi_h = \phi_1 \psi_{h-1} + \dots + \phi_p \psi_{h-p} + \theta_h,
\]</span>
where <span class="math inline">\(\theta_h = 0\)</span> for <span class="math inline">\(h&gt;q\)</span>.</li>
</ol>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span>This is obtained by apllying <span class="math inline">\(\partial/\partial \varepsilon_{t}\)</span> on both sides of (Eq. <a href="time-series.html#eq:ARMApq">(3.15)</a>):
<span class="math display">\[
y_{t+h} = c + \phi_1 y_{t+h-1} + \dots + \phi_p y_{t+h-p} + \varepsilon_{t+h} + \theta_1 \varepsilon_{t+h-1} + \dots + \theta_q \varepsilon_{t+h-q}.
\]</span></p>
</div>
<p>Note that Proposition <a href="time-series.html#prp:computPsi">3.8</a> constitutes a simple way to compute the MA(<span class="math inline">\(\infty\)</span>) representation of an ARMA process.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">21</span> <span class="co"># number of periods for IRF</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>;<span class="va">c</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.25</span>,<span class="fl">.95</span>,<span class="fl">.2</span>,<span class="fl">.85</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(a) Process 1"</span>,xlab<span class="op">=</span><span class="st">"Time after shock on epsilon"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">"Dynamic multiplier (shock on epsilon at t=0)"</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">.5</span><span class="op">)</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.6</span><span class="op">)</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(b) Process 2"</span>,xlab<span class="op">=</span><span class="st">"Time after shock on epsilon"</span>,ylab<span class="op">=</span><span class="st">""</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>;<span class="va">phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">.5</span>,<span class="fl">.4</span><span class="op">)</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="va">c</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>     main<span class="op">=</span><span class="st">"(c) Process 3"</span>,xlab<span class="op">=</span><span class="st">"Time after shock on epsilon"</span>,ylab<span class="op">=</span><span class="st">""</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:IRFarma"></span>
<img src="AdvECTS_files/figure-html/IRFarma-1.png" alt="IRFs of three processes. Process 1 (MA(2)): $y_t = \varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2}$. Process 2 (ARMA(1,2)): $y_{t}=0.6y_{t-1} + \varepsilon_t + 0.5\varepsilon_{t-1}$. . Process 3 (ARMA(4,2)): $y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2}$." width="100%"><p class="caption">
Figure 3.8: IRFs of three processes. Process 1 (MA(2)): <span class="math inline">\(y_t = \varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2}\)</span>. Process 2 (ARMA(1,2)): <span class="math inline">\(y_{t}=0.6y_{t-1} + \varepsilon_t + 0.5\varepsilon_{t-1}\)</span>. . Process 3 (ARMA(4,2)): <span class="math inline">\(y_{t}=0.5y_{t-3} + 0.4y_{t-4} + \varepsilon_t + \varepsilon_{t-1} + \varepsilon_{t-2}\)</span>.
</p>
</div>
<p>Consider the annual Swiss GDP growth from the JST macro-history database. Let us first determine relevant orders for AR and MA processes using the (P)ACF approach.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html">subset</a></span><span class="op">(</span><span class="va">JST</span>,<span class="va">iso</span><span class="op">==</span><span class="st">"CHE"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.1</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">growth</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">data</span><span class="op">$</span><span class="va">gdp</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.95</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">data</span><span class="op">$</span><span class="va">year</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span>,<span class="va">growth</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,lty<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html">acf</a></span><span class="op">(</span><span class="va">growth</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/acf.html">pacf</a></span><span class="op">(</span><span class="va">growth</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:IRFgdp1"></span>
<img src="AdvECTS_files/figure-html/IRFgdp1-1.png" alt="(P)ACF analysis of Swiss GDP growth." width="100%"><p class="caption">
Figure 3.9: (P)ACF analysis of Swiss GDP growth.
</p>
</div>
<p>Figure <a href="time-series.html#fig:IRFgdp2">3.10</a> shows the IRFs based on the two respective specifications (MA and AR).</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit an AR process:</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">growth</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">phi</span> <span class="op">&lt;-</span> <span class="va">res</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">11</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,theta<span class="op">=</span><span class="fl">1</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">3</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">"Time after shock on epsilon"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">"Dynamic multiplier (shock on epsilon at t=0)"</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="co"># Fit a MA process:</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/arima.html">arima</a></span><span class="op">(</span><span class="va">growth</span>,order<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">phi</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">res</span><span class="op">$</span><span class="va">coef</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">y.sim</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,<span class="va">phi</span>,<span class="va">theta</span>,sigma<span class="op">=</span><span class="fl">1</span>,<span class="cn">T</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>,<span class="va">y.sim</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,lwd<span class="op">=</span><span class="fl">3</span>,col<span class="op">=</span><span class="st">"red"</span>,lty<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:IRFgdp2"></span>
<img src="AdvECTS_files/figure-html/IRFgdp2-1.png" alt="Dynamic response of Swiss annual growth to a shock on the innovation $\varepsilon_t$ at date $t=0$. The solid line corresponds to an AR(1) specification; the dashed line corresponds to a MA(2) specification." width="100%"><p class="caption">
Figure 3.10: Dynamic response of Swiss annual growth to a shock on the innovation <span class="math inline">\(\varepsilon_t\)</span> at date <span class="math inline">\(t=0\)</span>. The solid line corresponds to an AR(1) specification; the dashed line corresponds to a MA(2) specification.
</p>
</div>
<p>The same kind of algorithm can be used to compute the impact of an increase in an exogenous variable <span class="math inline">\(x_t\)</span> within an ARMAX(p,q,r) model (see next section).</p>
</div>
<div id="ARMAIRF" class="section level3" number="3.2.5">
<h3>
<span class="header-section-number">3.2.5</span> ARMA processes with exogenous variables (ARMA-X)<a class="anchor" aria-label="anchor" href="#ARMAIRF"><i class="fas fa-link"></i></a>
</h3>
<!-- Recall that $\{y_t\}$ follows an ARMAX(p,q,r) model if its dynamics is of the form (see Def. \@ref(def:ARMAX)): -->
<!-- \begin{eqnarray} -->
<!-- y_t &=& \underbrace{c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR(p) part}} + \underbrace{\beta_0 x_t + \dots + \beta_{r} x_{t-r}}_{\mbox{X(r) part}} + \nonumber \\ -->
<!-- &&\underbrace{\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q}.}_{\mbox{MA(q) part}}(\#eq:armaxirf) -->
<!-- \end{eqnarray} -->
<!-- where $\{\varepsilon_t\}$ is an i.i.d. white noise sequence and $\{x_t\}$ is an exogenous variable. -->
<!-- This algorithm was presented in Prop. \@ref(prop:computPsiARMAX). -->
<p>ARMA processes do not allow to investigate the influence of an exogenous variable (say <span class="math inline">\(x_t\)</span>) on the variable of interest (say <span class="math inline">\(y_t\)</span>). When <span class="math inline">\(x_t\)</span> and <span class="math inline">\(y_t\)</span> have reciprocal influences, the Vector Autoregressive (VAR) model may be used (this tools will be studied later, in Section <a href="#VAR"><strong>??</strong></a>). However, when one suspects that <span class="math inline">\(x_t\)</span> has an “exogenous” influence on <span class="math inline">\(y_t\)</span>, then a simple extension of the ARMA processes may be considered. Loosely speaking, <span class="math inline">\(x_t\)</span> has an “exogenous” influence on <span class="math inline">\(y_t\)</span> if <span class="math inline">\(y_t\)</span> does not affect <span class="math inline">\(x_t\)</span>. This extension is called ARMAX(p,q,r).</p>
<p>To begin with, let us formalize this notion of exogeneity. Consider a white noise sequence <span class="math inline">\(\{\varepsilon_t\}\)</span> (Def. <a href="time-series.html#def:whitenoise">3.1</a>).</p>
<div class="definition">
<p><span id="def:exogeneity" class="definition"><strong>Definition 3.17  (Exogeneity) </strong></span>We say that <span class="math inline">\(x_t\)</span> is (strictly) exogenous to <span class="math inline">\(\{\varepsilon_t\}\)</span> if
<span class="math display">\[
\mathbb{E}(\varepsilon_t|\underbrace{\dots,x_{t+1}}_{\mbox{future}},\underbrace{x_t,x_{t-1},\dots}_{\mbox{present \&amp; past}}) = 0.
\]</span></p>
</div>
<p>If <span class="math inline">\(x_t\)</span> is exogenous to <span class="math inline">\(\{\varepsilon_t\}\)</span> then, in particular:
<span class="math display" id="eq:expepsx">\[\begin{equation}
\mathbb{E}(\varepsilon_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0,\tag{3.16}
\end{equation}\]</span>
which implies that:
<span class="math display">\[\begin{equation}
\mathbb{E}(\varepsilon_tx_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0,
\end{equation}\]</span>
or
<span class="math display">\[\begin{equation}
\mathbb{C}ov(\varepsilon_t,x_t|x_{t-k},x_{t-k-1},\dots)=0 \mbox{ for any } k\ge0,
\end{equation}\]</span></p>
<p>Past values of the exogenous variable do not allow to predict present and future values of <span class="math inline">\(\varepsilon_t\)</span> (Eq. <a href="time-series.html#eq:expepsx">(3.16)</a>).</p>
<p>For strictly exogenous proceses <span class="math inline">\(\{x_t\}\)</span>, past, present and future values of the exogenous variable do not allow to predict any of the <span class="math inline">\(\varepsilon_t\)</span>s.</p>
<p>In the following, we assume that <span class="math inline">\(\{x_t\}\)</span> is a covariance stationary process.</p>
<div class="definition">
<p><span id="def:ARMAX" class="definition"><strong>Definition 3.18  (ARMAX(p,q,r) model) </strong></span>The process <span class="math inline">\(\{y_t\}\)</span> is an ARMAX(p,q,r) if it follows a difference equation:
<span class="math display" id="eq:DLM">\[\begin{eqnarray}
y_t &amp;=&amp; \underbrace{c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p}}_{\mbox{AR(p) part}} + \underbrace{\beta_0 x_t + \dots + \beta_{r} x_{t-r}}_{\mbox{X(r) part}} + \nonumber \\
&amp;&amp;\underbrace{\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q}.}_{\mbox{MA(q) part}} \tag{3.17}
\end{eqnarray}\]</span>
where <span class="math inline">\(\{\varepsilon_t\}\)</span> is an i.i.d. white noise sequence and <span class="math inline">\(\{x_t\}\)</span> is exogenous to <span class="math inline">\(y_t\)</span>.</p>
</div>
<p>The Autoregressive Distributed Lag (ADL) Model ADL(p,r) is an ARMAX(p,0,r) model (see <span class="citation">Stock and Watson (<a href="references.html#ref-Stock_Watson_2003" role="doc-biblioref">2003</a>)</span>, Chapter 16).</p>
<p>What is the effect of a one-unit increase in <span class="math inline">\(x_t\)</span> on <span class="math inline">\(y_t\)</span>? To address this question, this has to be formalized appropriately. Let us introduce two related sequences of values for <span class="math inline">\(\{x\}\)</span>. Denote the first by <span class="math inline">\(\{a\}^t\)</span> and the second by <span class="math inline">\(\{\tilde{a}\}^t\)</span>. Further, assume that <span class="math inline">\(a_s = \tilde{a}_s\)</span> for all <span class="math inline">\(s \ne t\)</span>, and that <span class="math inline">\(\tilde{a}_t = a_t+1\)</span>.</p>
<p>With these notations, we define <span class="math inline">\(\frac{\partial y_{t+h}}{\partial x_t}\)</span> as follows:
<span class="math display" id="eq:dynmultX">\[\begin{equation}
\frac{\partial y_{t+h}}{\partial x_t} := \mathbb{E}(y_{t+h}|\{x\} = \{\tilde{a}^t\}) - \mathbb{E}(y_{t+h}|\{x\} = \{a\}^t).\tag{3.18}
\end{equation}\]</span>
Under the exogeneity assumption, it is easily seen that
<!-- \begin{eqnarray*} -->
<!-- y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \\ -->
<!-- &&\beta_0 x_t + \dots + \beta_{r} x_{t-r} +\\ -->
<!-- &&\varepsilon_t + \theta_1\varepsilon_{t-1}+\dots +\theta_{q}\varepsilon_{t-q}. -->
<!-- \end{eqnarray*} --></p>
<p><span class="math display">\[
\frac{\partial y_t}{\partial x_t} = \beta_0.
\]</span>
Since
<span class="math display">\[\begin{eqnarray*}
y_{t+1} = c + \phi_1 y_{t} + \dots + \phi_p y_{t+1-p} + \beta_0 x_{t+1} + \dots + \beta_{r} x_{t+1-r} +\\
&amp;&amp;\varepsilon_{t+1} + \theta_1\varepsilon_{t}+\dots +\theta_{q}\varepsilon_{t+1-q},
\end{eqnarray*}\]</span>
and using the exogeneity assumption, we obtain:
<span class="math display">\[
\frac{\partial y_{t+1}}{\partial x_t} := \phi_1 \frac{\partial y_{t}}{\partial x_t} + \beta_1 = \phi_1\beta_0 + \beta_1.
\]</span>
This can be applied recursively to give <span class="math inline">\(\dfrac{\partial y_{t+h}}{\partial x_t}\)</span> for any <span class="math inline">\(h \ge 0\)</span>:</p>
<hr>
<div class="proposition">
<p><span id="prp:computPsiARMAX" class="proposition"><strong>Proposition 3.9  (Dynamic multipliers in ARMAX models) </strong></span>One can recursively compute the dynamic multipliers <span class="math inline">\(\frac{\partial y_{t+h}}{\partial x_t}\)</span> as follows:</p>
<ol style="list-style-type: lower-roman">
<li>Initialization: <span class="math inline">\(\dfrac{\partial y_{t+h}}{\partial x_t}=0\)</span> for <span class="math inline">\(h&lt;0\)</span>.</li>
<li>For <span class="math inline">\(h \ge 0\)</span> and assuming that the first <span class="math inline">\(h-1\)</span> multipliers have been computed, we have:
<span class="math display" id="eq:dynmultX">\[\begin{eqnarray}
\dfrac{\partial y_{t+h}}{\partial x_t} &amp;=&amp; \phi_1 \dfrac{\partial y_{t+h-1}}{\partial x_t} + \dots + \phi_p \dfrac{\partial y_{t+h-p}}{\partial x_t} + \beta_h,\tag{3.18}
\end{eqnarray}\]</span>
where we use the notation <span class="math inline">\(\beta_h=0\)</span> if <span class="math inline">\(h&gt;r\)</span>.</li>
</ol>
</div>
<hr>
<p>Remark that the resulting dynamic multipliers are the same as those obtained for an ARMA(p,r) model where the <span class="math inline">\(\theta_i\)</span>s are replaced with <span class="math inline">\(\beta_i\)</span>’s (see Proposition <a href="time-series.html#prp:computPsi">3.8</a> in Section <a href="time-series.html#ARMAIRF">3.2.5</a>).</p>
<p>Moreover, it has to be stressed that the definition of the dynamic multipliers (Eq. <a href="time-series.html#eq:dynmultX">(3.18)</a>) does not allow for a potential persistency, in the <span class="math inline">\(\{x\}\)</span> process of the shock occuring on date <span class="math inline">\(t\)</span>. Going in this direction would necessitate to model the dynamics of <span class="math inline">\(x_t\)</span>.</p>
<p>The next example is based on data used in <span class="citation">Stock and Watson (<a href="references.html#ref-Stock_Watson_2003" role="doc-biblioref">2003</a>)</span> (Chapter 16). The objective is to study the influence of the number of freezing days on the price of orange juice.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AER</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"FrozenJuice"</span><span class="op">)</span></span>
<span><span class="va">FJ</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">FrozenJuice</span><span class="op">)</span></span>
<span><span class="va">date</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/time.html">time</a></span><span class="op">(</span><span class="va">FrozenJuice</span><span class="op">)</span></span>
<span><span class="va">price</span> <span class="op">&lt;-</span> <span class="va">FJ</span><span class="op">$</span><span class="va">price</span><span class="op">/</span><span class="va">FJ</span><span class="op">$</span><span class="va">ppi</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">price</span><span class="op">)</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">dprice</span> <span class="op">&lt;-</span> <span class="fl">100</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">price</span><span class="op">[</span><span class="op">(</span><span class="va">k</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">price</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">k</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">fdd</span> <span class="op">&lt;-</span> <span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">k</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.75</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">date</span>,<span class="va">price</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="st">"(a) Price of orange Juice"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">date</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="va">dprice</span><span class="op">)</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="st">"(b) Monthly pct Change (Y[t])"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">date</span>,<span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span>,type<span class="op">=</span><span class="st">"l"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,main<span class="op">=</span><span class="st">"(c) Number of freezing days (X[t])"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="AdvECTS_files/figure-html/freez-1.png" width="672"></div>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nb.lags</span> <span class="op">&lt;-</span> <span class="fl">12</span></span>
<span><span class="va">FDD</span> <span class="op">&lt;-</span> <span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">nb.lags</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span></span>
<span><span class="va">names.FDD</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nb.lags</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">FDD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">FDD</span>,<span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">nb.lags</span><span class="op">+</span><span class="fl">1</span><span class="op">-</span><span class="va">i</span><span class="op">)</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">i</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">names.FDD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">names.FDD</span>,<span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">" Lag "</span>,<span class="fu"><a href="https://rdrr.io/r/base/toString.html">toString</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">FDD</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">" Lag 0"</span>,<span class="va">names.FDD</span><span class="op">)</span></span>
<span><span class="va">dprice</span> <span class="op">&lt;-</span> <span class="va">dprice</span><span class="op">[</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">)</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">FDD</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">eq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">~</span><span class="va">FDD</span><span class="op">)</span></span>
<span><span class="co"># Compute the Newey-West std errors:</span></span>
<span><span class="va">var.cov.mat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/NeweyWest.html">NeweyWest</a></span><span class="op">(</span><span class="va">eq</span>,lag <span class="op">=</span> <span class="fl">7</span>, prewhite <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">robust_se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">var.cov.mat</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Stargazer output (with and without Robust SE)</span></span>
<span><span class="fu">stargazer</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/stargazer/man/stargazer.html">stargazer</a></span><span class="op">(</span><span class="va">eq</span>, <span class="va">eq</span>, type <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>                     column.labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"(no HAC)"</span>, <span class="st">"(HAC)"</span><span class="op">)</span>,keep.stat <span class="op">=</span> <span class="st">"n"</span>,</span>
<span>                     se <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="cn">NULL</span>,<span class="va">robust_se</span><span class="op">)</span>,no.space <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## =========================================
##                  Dependent variable:     
##              ----------------------------
##                         dprice           
##                 (no HAC)        (HAC)    
##                   (1)            (2)     
## -----------------------------------------
## FDD Lag 0       0.496***      0.496***   
##                 (0.058)        (0.139)   
## FDD Lag 1       0.150***       0.150*    
##                 (0.058)        (0.087)   
## FDD Lag 2        0.046          0.046    
##                 (0.057)        (0.056)   
## FDD Lag 3        0.062          0.062    
##                 (0.057)        (0.046)   
## FDD Lag 4        0.024          0.024    
##                 (0.057)        (0.030)   
## FDD Lag 5        0.036          0.036    
##                 (0.057)        (0.030)   
## FDD Lag 6        0.037          0.037    
##                 (0.057)        (0.046)   
## FDD Lag 7        0.019          0.019    
##                 (0.057)        (0.015)   
## FDD Lag 8        -0.038        -0.038    
##                 (0.057)        (0.034)   
## FDD Lag 9        -0.006        -0.006    
##                 (0.057)        (0.050)   
## FDD Lag 10      -0.112*        -0.112    
##                 (0.057)        (0.069)   
## FDD Lag 11       -0.063        -0.063    
##                 (0.058)        (0.052)   
## FDD Lag 12      -0.140**       -0.140*   
##                 (0.058)        (0.078)   
## Constant        -0.426*        -0.426*   
##                 (0.238)        (0.243)   
## -----------------------------------------
## Observations      600            600     
## =========================================
## Note:         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>Let us now use function <code>estim.armax</code>, from package <code>AEC</code>to estimate an ARMA-X(2,0,1) model:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nb.lags</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">FDD</span> <span class="op">&lt;-</span> <span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">nb.lags</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span></span>
<span><span class="va">names.FDD</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nb.lags</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">FDD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">FDD</span>,<span class="va">FJ</span><span class="op">$</span><span class="va">fdd</span><span class="op">[</span><span class="op">(</span><span class="va">nb.lags</span><span class="op">+</span><span class="fl">1</span><span class="op">-</span><span class="va">i</span><span class="op">)</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">i</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">names.FDD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">names.FDD</span>,<span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">" Lag "</span>,<span class="fu"><a href="https://rdrr.io/r/base/toString.html">toString</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">FDD</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">" Lag 0"</span>,<span class="va">names.FDD</span><span class="op">)</span></span>
<span><span class="va">dprice</span> <span class="op">&lt;-</span> <span class="fl">100</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">price</span><span class="op">[</span><span class="op">(</span><span class="va">k</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">price</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">k</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">dprice</span> <span class="op">&lt;-</span> <span class="va">dprice</span><span class="op">[</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">)</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">FDD</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">dprice</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">res.armax</span> <span class="op">&lt;-</span> <span class="fu">estim.armax</span><span class="op">(</span>Y <span class="op">=</span> <span class="va">dprice</span>,p<span class="op">=</span><span class="fl">3</span>,q<span class="op">=</span><span class="fl">0</span>,X<span class="op">=</span><span class="va">FDD</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] "=================================================="
## [1] "  ESTIMATING"
## [1] "=================================================="
## [1] "  END OF ESTIMATION"
## [1] "=================================================="
## [1] ""
## [1] "  RESULTS:"
## [1] "  -----------------------"
##                 THETA     st.dev   t.ratio
## c         -0.46556249 0.19554352 -2.380864
## phi   t-1  0.09788977 0.04025907  2.431496
## phi   t-2  0.05049849 0.03827488  1.319364
## phi   t-3  0.07155170 0.03764750  1.900570
## sigma      4.64917949 0.13300769 34.954215
## beta  t-0  0.47015552 0.05665344  8.298800
## beta  t-1  0.10015862 0.05972526  1.676989
## [1] "=================================================="</code></pre>
<p>Figure <a href="time-series.html#fig:freez4">3.11</a> shows the IRF associated with each of the two models.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">nb.periods</span> <span class="op">&lt;-</span> <span class="fl">20</span></span>
<span><span class="va">IRF1</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,phi<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>,theta<span class="op">=</span><span class="va">eq</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">13</span><span class="op">]</span>,sigma<span class="op">=</span><span class="fl">1</span>,</span>
<span>                 T<span class="op">=</span><span class="va">nb.periods</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">IRF2</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span>c<span class="op">=</span><span class="fl">0</span>,phi<span class="op">=</span><span class="va">res.armax</span><span class="op">$</span><span class="va">phi</span>,theta<span class="op">=</span><span class="va">res.armax</span><span class="op">$</span><span class="va">beta</span>,sigma<span class="op">=</span><span class="fl">1</span>,</span>
<span>                 T<span class="op">=</span><span class="va">nb.periods</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">res.armax</span><span class="op">$</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">IRF1</span>,type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">2</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">IRF2</span>,lwd<span class="op">=</span><span class="fl">2</span>,col<span class="op">=</span><span class="st">"red"</span>,lty<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"grey"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:freez4"></span>
<img src="AdvECTS_files/figure-html/freez4-1.png" alt="The solid (respectively dashed) line corresponds to the ARMAX(0,0,12) (resp. ARMAX(3,0,1)) model. The first model is esitmated by OLS, the second by MLE." width="90%"><p class="caption">
Figure 3.11: The solid (respectively dashed) line corresponds to the ARMAX(0,0,12) (resp. ARMAX(3,0,1)) model. The first model is esitmated by OLS, the second by MLE.
</p>
</div>
<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \begin{exmpl}[Effect of monetary policy shocks] -->
<!-- \begin{itemize} -->
<!-- \item \href{https://www.aeaweb.org/articles?id=10.1257/mac.20130329}{Gertler and Karadi (2015)}'s type of shocks (high-frequency change in Euro-dollar futures). -->
<!-- \item Effect on 12-month growth rate of Industrial Production (IP)? -->
<!-- \item Data from \href{http://econweb.ucsd.edu/~vramey/research.html\#data}{Ramey's website}. -->
<!-- \end{itemize} -->
<!-- \begin{figure} -->
<!-- \caption{IP and MP shocks} -->
<!-- \begin{center} -->
<!-- \includegraphics[width=.8\linewidth]{../../figures/Figure_Ramey.pdf} -->
<!-- \end{center} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[Effect of monetary policy shocks (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{IRF of IP growth rate to a 1 std-deviation MP shock} -->
<!-- \begin{center} -->
<!-- \includegraphics[width=.8\linewidth]{../../figures/Figure_Ramey2.pdf} -->
<!-- \end{center} -->
<!-- \begin{tiny} -->
<!-- ARMAX(1,1,1). estimated by MLE (Section \ref{section:MLE_ARMA}). The initial shock corresponds to one standard deviation of the series of monetary-policy shocks. The blue lines delineate the 95\% confidence interval. -->
<!-- \end{tiny} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
<p>In the following example, we make use of monetary shocks identified through high-frequency data (see <span class="citation">Gertler and Karadi (<a href="references.html#ref-Gertler_Karadi_2015" role="doc-biblioref">2015</a>)</span>). This dataset comes from <a href="https://econweb.ucsd.edu/~vramey/research.html">Valerie Ramey’s website</a> (see <span class="citation">Ramey (<a href="references.html#ref-Ramey_2016_NBER" role="doc-biblioref">2016</a>)</span>).</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">AEC</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Ramey</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="co"># Construct growth series:</span></span>
<span><span class="va">Ramey</span><span class="op">$</span><span class="va">growth</span> <span class="op">&lt;-</span> <span class="va">Ramey</span><span class="op">$</span><span class="va">LIP</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fl">12</span><span class="op">)</span>,<span class="va">Ramey</span><span class="op">$</span><span class="va">LIP</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Ramey</span><span class="op">$</span><span class="va">LIP</span><span class="op">)</span><span class="op">-</span><span class="fl">12</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co"># Prepare matrix of exogenous variables:</span></span>
<span><span class="va">vec.lags</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">9</span>,<span class="fl">12</span>,<span class="fl">18</span><span class="op">)</span></span>
<span><span class="va">Matrix.of.Exog</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="va">shocks</span> <span class="op">&lt;-</span> <span class="va">Ramey</span><span class="op">$</span><span class="va">ED2_TC</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">vec.lags</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">Matrix.of.Exog</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">Matrix.of.Exog</span>,</span>
<span>                          <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="va">vec.lags</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span>,<span class="va">shocks</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">vec.lags</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co"># Look for dates where data are available:</span></span>
<span><span class="va">indic.good.dates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span><span class="op">(</span><span class="va">Matrix.of.Exog</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:Ramey1fig"></span>
<img src="AdvECTS_files/figure-html/Ramey1fig-1.png" alt="The blue line corresponds to monetary-policy shocks identified by means of the Gertler and Karadi (2015)'s approach (high-frequency change in Euro-dollar futures). The black slid line is the year-on-year growth rate of industrial production." width="90%"><p class="caption">
Figure 3.12: The blue line corresponds to monetary-policy shocks identified by means of the Gertler and Karadi (2015)’s approach (high-frequency change in Euro-dollar futures). The black slid line is the year-on-year growth rate of industrial production.
</p>
</div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Estimate ARMAX:</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">1</span>; <span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">estim.armax</span><span class="op">(</span><span class="va">Ramey</span><span class="op">$</span><span class="va">growth</span><span class="op">[</span><span class="va">indic.good.dates</span><span class="op">]</span>,<span class="va">p</span>,<span class="va">q</span>,</span>
<span>                 X<span class="op">=</span><span class="va">Matrix.of.Exog</span><span class="op">[</span><span class="va">indic.good.dates</span>,<span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] "=================================================="
## [1] "  ESTIMATING"
## [1] "=================================================="
## [1] "  END OF ESTIMATION"
## [1] "=================================================="
## [1] ""
## [1] "  RESULTS:"
## [1] "  -----------------------"
##                   THETA       st.dev    t.ratio
## c         -0.0001716198 0.0005845907 -0.2935726
## phi   t-1  0.9825608412 0.0120458531 81.5683897
## sigma      0.0087948724 0.0003211748 27.3834438
## beta  t-0 -0.0193570616 0.0087331529 -2.2165032
## beta  t-1 -0.0225707935 0.0086750938 -2.6017925
## beta  t-2 -0.0070131593 0.0086387440 -0.8118263
## [1] "=================================================="</code></pre>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute IRF:</span></span>
<span><span class="va">irf</span> <span class="op">&lt;-</span> <span class="fu">sim.arma</span><span class="op">(</span><span class="fl">0</span>,<span class="va">x</span><span class="op">$</span><span class="va">phi</span>,<span class="va">x</span><span class="op">$</span><span class="va">beta</span>,<span class="va">x</span><span class="op">$</span><span class="va">sigma</span>,T<span class="op">=</span><span class="fl">60</span>,y.0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">phi</span><span class="op">)</span><span class="op">)</span>,nb.sim<span class="op">=</span><span class="fl">1</span>,make.IRF<span class="op">=</span><span class="fl">1</span>,</span>
<span>                X<span class="op">=</span><span class="cn">NaN</span>,beta<span class="op">=</span><span class="cn">NaN</span><span class="op">)</span></span></code></pre></div>
<p>Figure @(fig:Ramey3) show the result IRF, with a 95% confidence band. The code used to produce the confidence bands (i.e., to compute the standard deviation of the dynamic multipliers for the different horizons) is based on the Delta method. The codes arae available in Appendix <a href="appendix.html#IRFDELTA">4.5.2</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:Ramey3"></span>
<img src="AdvECTS_files/figure-html/Ramey3-1.png" alt="Response of industrial-production growth to monetary-policy shocks. Dashed lines correpsond to the $\pm$  2-standard-deviation bands." width="90%"><p class="caption">
Figure 3.13: Response of industrial-production growth to monetary-policy shocks. Dashed lines correpsond to the <span class="math inline">\(\pm\)</span> 2-standard-deviation bands.
</p>
</div>
</div>
<div id="maximum-likelihood-estimation-of-arma-processes" class="section level3" number="3.2.6">
<h3>
<span class="header-section-number">3.2.6</span> Maximum Likelihood Estimation of ARMA processes<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation-of-arma-processes"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the general case (of any time series); assume we observe a sample <span class="math inline">\(\mathbf{y}=[y_1,\dots,y_T]'\)</span>. In order to implement ML techniques (see Section <a href="#MLE"><strong>??</strong></a>), we need to evaluate the joint p.d.f. (then called likelihood) of <span class="math inline">\(\mathbf{y}\)</span>, i.e., <span class="math inline">\(\mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span>, where <span class="math inline">\(\boldsymbol\theta\)</span> is a vector of parameters that characterizes the dynamics of <span class="math inline">\(y_t\)</span>. The Maximum likelihood estimate of <span class="math inline">\(\boldsymbol\theta\)</span> will then be given by:
<span class="math display">\[
\boxed{\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\mathbf{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).}
\]</span></p>
<p>In the time series context, if process <span class="math inline">\(y_t\)</span> is Markovian, then there exists a useful way to rewrite the likelihood <span class="math inline">\(\mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span>. Let us first recall the definition of a Markovian process:</p>
<div class="definition">
<p><span id="def:Markov" class="definition"><strong>Definition 3.19  (Markovian process) </strong></span>Process <span class="math inline">\(y_t\)</span> is Markovian if <span class="math inline">\(f_{Y_t|Y_{t-1},Y_{t-2},\dots} = f_{Y_t|Y_{t-1}}\)</span>.</p>
</div>
<p>Now, remember Bayes’ formula:
<span class="math display">\[
\mathbb{P}(X_2=x,X_1=y) = \mathbb{P}(X_2=x|X_1=y)\mathbb{P}(X_1=y).
\]</span>
This leads to the following decomposition of our likelihood function:
<span class="math display">\[\begin{eqnarray*}
f_{Y_T,\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) &amp;=&amp;f_{Y_T|Y_{T-1},\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) \times \\
&amp;&amp; f_{Y_{T-1},\dots,Y_1}(y_{T-1},\dots,y_1;\boldsymbol\theta).
\end{eqnarray*}\]</span>
Using the previous expression recursively, one obtains:
<span class="math display" id="eq:recursMLE">\[\begin{equation}
f_{Y_T,\dots,Y_1}(y_T,\dots,y_1;\boldsymbol\theta) = f_{Y_1}(y_1;\boldsymbol\theta) \prod_{t=2}^{T} f_{Y_t|Y_{t-1},\dots,Y_1}(y_t,\dots,y_1;\boldsymbol\theta).\tag{3.19}
\end{equation}\]</span></p>
<p>Let us start with the Gaussian AR(1) process (which is Markovian):
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim\,i.i.d.\, \mathcal{N}(0,\sigma^2)
\]</span>
For <span class="math inline">\(t&gt;1\)</span>:
<span class="math display">\[
f_{Y_t|Y_{t-1},\dots,Y_1}(y_t,\dots,y_1;\boldsymbol\theta) = f_{Y_t|Y_{t-1}}(y_t,y_{t-1};\boldsymbol\theta).
\]</span>
and
<span class="math display">\[
f_{Y_t|Y_{t-1}}(y_t,y_{t-1};\boldsymbol\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2}\right).
\]</span></p>
<p>These expressins can be plugged in Eq. <a href="time-series.html#eq:recursMLE">(3.19)</a>. But what about <span class="math inline">\(f_{Y_1}(y_1;\boldsymbol\theta)\)</span>? There exist two possibilities:</p>
<ol style="list-style-type: decimal">
<li>We use the marginal distribution: <span class="math inline">\(y_1 \sim \mathcal{N}\left(\dfrac{c}{1-\phi_1},\dfrac{\sigma^2}{1-\phi_1^2}\right)\)</span>.</li>
<li>
<span class="math inline">\(y_1\)</span> is considered to be deterministic. In a way, that means observation is “sacrificed”.</li>
</ol>
<p>In the Gaussian AR(1) process, we have:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Case 1</strong>: The (exact) log-likelihood is:
<span class="math display">\[\begin{eqnarray}
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y})  &amp;=&amp; - \frac{T}{2} \log(2\pi) - T\log(\sigma) + \frac{1}{2}\log(1-\phi_1^2)\nonumber \\
&amp;&amp; - \frac{(y_1 - c/(1-\phi_1))^2}{2\sigma^2/(1-\phi_1^2)}\\
&amp;&amp; - \sum_{t=2}^T \left[\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2} \right].
\end{eqnarray}\]</span>
The Maximum Likelihood Estimator of <span class="math inline">\(\boldsymbol\theta= [c,\phi_1,\sigma^2]\)</span> is obtained by numerical optimization.</p></li>
<li><p><strong>Case 2</strong>: The (conditional) log-likelihood is:
<span class="math display" id="eq:Lstar">\[\begin{eqnarray}
\log \mathcal{L}^*(\boldsymbol\theta;\mathbf{y})  &amp;=&amp; - \frac{T-1}{2} \log(2\pi) - (T-1)\log(\sigma)\nonumber\\
&amp;&amp; - \sum_{t=2}^T \left[\frac{(y_t - c - \phi_1 y_{t-1})^2}{2\sigma^2} \right].\tag{3.20}
\end{eqnarray}\]</span></p></li>
</ol>
<p>Exact MLE and conditional MLE have the same asymptotic (i.e. large-sample) distribution. Indeed, when the process is stationary, <span class="math inline">\(f_{Y_1}(y_1;\boldsymbol\theta)\)</span> makes a relatively negligible contribution to <span class="math inline">\(\log \mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span>.</p>
<p>The conditional MLE has a substantial advantage: in the Gaussian case, the conditional MLE is simply obtained by OLS. Indeed, let us introduce the notations:
<span class="math display">\[
Y = \left[\begin{array}{c}
y_2\\
\vdots\\
y_T
\end{array}\right] \quad and \quad
X = \left[\begin{array}{cc}
1 &amp;y_1\\
\vdots&amp;\vdots\\
1&amp;y_{T-1}
\end{array}\right],
\]</span>
Eq. <a href="time-series.html#eq:Lstar">(3.20)</a> becomes:
<span class="math display">\[\begin{eqnarray}
\log \mathcal{L}^*(\boldsymbol\theta;\mathbf{y})  &amp;=&amp; - \frac{T-1}{2} \log(2\pi) - (T-1)\log(\sigma) \nonumber \\
&amp;&amp; - \frac{1}{2\sigma^2} (Y-X[c,\phi_1]')'(Y-X[c,\phi_1]'),
\end{eqnarray}\]</span>
which is maximised for:
<span class="math display" id="eq:AROLSsigma">\[\begin{eqnarray}
[\hat{c},\hat\phi_1]' &amp;=&amp; (X'X)^{-1}X'Y \tag{3.21} \\
\hat{\sigma^2} &amp;=&amp; \frac{1}{T-1} \sum_{t=2}^T (y_t - \hat{c} - \hat{\phi_1}y_{t-1})^2 \nonumber \\
&amp;=&amp; \frac{1}{T-1} Y'(I - X(X'X)^{-1}X')Y. \tag{3.22}
\end{eqnarray}\]</span></p>
<p>Let us turn to the case of an AR(p) process. We use:
<span class="math display">\[\begin{eqnarray*}
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) &amp;=&amp; \log f_{Y_p,\dots,Y_1}(y_p,\dots,y_1;\boldsymbol\theta) +\\
&amp;&amp; \underbrace{\sum_{t=p+1}^{T} \log f_{Y_t|Y_{t-1},\dots,Y_{t-p}}(y_t,\dots,y_{t-p};\boldsymbol\theta)}_{\log \mathcal{L}^*(\boldsymbol\theta;\mathbf{y})}.
\end{eqnarray*}\]</span>
where <span class="math inline">\(f_{Y_p,\dots,Y_{1}}(y_p,\dots,y_{1};\boldsymbol\theta)\)</span>is the marginal distribution of <span class="math inline">\(\mathbf{y}_{1:p} := [y_p,\dots,y_1]\)</span>. The marginal distribution of <span class="math inline">\(\mathbf{y}_{1:p}\)</span> is Gaussian; it is therefore perfectly characterised by its mean and covariance matrix:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\mathbf{y}_{1:p})&amp;=&amp;\frac{c}{1-\phi_1-\dots-\phi_p} \mathbf{1}_{p\times 1} \\
\mathbb{V}ar(\mathbf{y}_{1:p}) &amp;=&amp; \left[\begin{array}{cccc}
\gamma_0 &amp; \gamma_1 &amp; \dots &amp; \gamma_{p-1} \\
\gamma_1 &amp; \gamma_0 &amp; \dots &amp; \gamma_{p-2} \\
\vdots &amp;  &amp; \ddots &amp; \vdots \\
\gamma_{p-1} &amp; \gamma_{p-2} &amp; \dots &amp; \gamma_{0} \\
\end{array}\right],
\end{eqnarray*}\]</span>
where the <span class="math inline">\(\gamma_i\)</span>s are computed using the Yule-Walker equations (Eq. <a href="time-series.html#eq:gammas">(3.14)</a>). Note that they depend, in a non-linear way, on the model parameters. Hence, the maximization of the exact log-likelihood necessitates numerical oprimization procedures. By contrast, the maximization of the conditional log-likelihood <span class="math inline">\(\log \mathcal{L}^*(\boldsymbol\theta;\mathbf{y})\)</span> only requires OLS, using Eqs. <a href="time-series.html#eq:AROLSmean">(3.21)</a> and <a href="time-series.html#eq:AROLSsigma">(3.22)</a>, with:
<span class="math display">\[
Y = \left[\begin{array}{c}
y_{p+1}\\
\vdots\\
y_T
\end{array}\right] \quad and \quad
X = \left[\begin{array}{cccc}
1 &amp; y_p &amp; \dots &amp; y_1\\
\vdots&amp;\vdots&amp;&amp;\vdots\\
1&amp;y_{T-1}&amp;\dots&amp;y_{T-p}
\end{array}\right].
\]</span></p>
<p>Again, for stationary processes, conditional and exact MLE have the same asymptotic (large-sample) distribution.</p>
<p>Consider the regression (where <span class="math inline">\(y_t\)</span> follows Eq. <a href="#eq:armaMLE">(<strong>??</strong>)</a>):
<span class="math display" id="eq:OLSregARp">\[\begin{equation}
y_t = \boldsymbol\beta'\mathbf{x}_t + \varepsilon_t,\tag{3.23}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{x}_t = [1,y_{t-1},\dots,y_{t-p}]'\)</span> and <span class="math inline">\(\boldsymbol\beta = [c,\phi_1,\dots,\phi_p]'\)</span>.</p>
<p>The fact that <span class="math inline">\(\mathbf{x}_t\)</span> correlates to the <span class="math inline">\(\varepsilon_s\)</span> for <span class="math inline">\(s&lt;t\)</span> implies that the OLS estimator <span class="math inline">\(\mathbf{b}\)</span> of <span class="math inline">\(\boldsymbol\beta\)</span> is biased in small sample. Indeed, we have:
<span class="math display" id="eq:olsar1">\[\begin{equation}
\mathbf{b} = \boldsymbol{\beta} + (X'X)^{-1}X'\boldsymbol\varepsilon,\tag{3.24}
\end{equation}\]</span>
and because of the specific form of <span class="math inline">\(X\)</span>, we have non-zero correlation between <span class="math inline">\(\mathbf{x}_t\)</span> and <span class="math inline">\(\varepsilon_s\)</span> for <span class="math inline">\(s&lt;t\)</span>, therefore <span class="math inline">\(\mathbb{E}[(X'X)^{-1}X'\boldsymbol\varepsilon] \ne 0\)</span>.</p>
<p>However, asymptotically, the previous correlations vanish and we have:</p>
<hr>
<div class="proposition">
<p><span id="prp:cgceOLSARp" class="proposition"><strong>Proposition 3.10  (Large-sample porperties of the OLS estimator of AR(p) models) </strong></span>Assume <span class="math inline">\(\{y_t\}\)</span> follows the AR(p) process:
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \varepsilon_t
\]</span>
where <span class="math inline">\(\{\varepsilon_{t}\}\)</span> is an i.i.d. white noise process. If <span class="math inline">\(\mathbf{b}\)</span> is the OLS estimator of <span class="math inline">\(\boldsymbol\beta\)</span> (Eq. <a href="time-series.html#eq:OLSregARp">(3.23)</a>), we have:
<span class="math display">\[
\sqrt{T}(\mathbf{b}-\boldsymbol{\beta}) =  \underbrace{\left[\frac{1}{T}\sum_{t=p}^T \mathbf{x}_t\mathbf{x}_t' \right]^{-1}}_{\overset{p}{\rightarrow} \mathbf{Q}^{-1}}
\underbrace{\sqrt{T} \left[\frac{1}{T}\sum_{t=1}^T \mathbf{x}_t\varepsilon_t \right]}_{\overset{d}{\rightarrow} \mathcal{N}(0,\sigma^2\mathbf{Q})},
\]</span>
where <span class="math inline">\(\mathbf{Q} = \mbox{plim }\frac{1}{T}\sum_{t=p}^T \mathbf{x}_t\mathbf{x}_t'= \mbox{plim }\frac{1}{T}\sum_{t=1}^T \mathbf{x}_t\mathbf{x}_t'\)</span> is given by:
<span class="math display" id="eq:Qols">\[\begin{equation}
\mathbf{Q} = \left[
\begin{array}{ccccc}
1 &amp; \mu &amp;\mu &amp; \dots &amp; \mu \\
\mu &amp; \gamma_0 + \mu^2 &amp; \gamma_1 + \mu^2 &amp; \dots &amp; \gamma_{p-1} + \mu^2\\
\mu &amp; \gamma_1 + \mu^2 &amp; \gamma_0 + \mu^2 &amp; \dots &amp; \gamma_{p-2} + \mu^2\\
\vdots &amp;\vdots &amp;\vdots &amp;\dots &amp;\vdots \\
\mu &amp; \gamma_{p-1} + \mu^2 &amp; \gamma_{p-2} + \mu^2 &amp; \dots &amp; \gamma_{0} + \mu^2
\end{array}
\right].\tag{3.25}
\end{equation}\]</span></p>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span>Rearranging Eq. @ref{eq:ols_ar1), we have:
<span class="math display">\[
\sqrt{T}(\mathbf{b}-\boldsymbol{\beta}) =  (X'X/T)^{-1}\sqrt{T}(X'\boldsymbol\varepsilon/T).
\]</span>
Let us consider the autocovariances of <span class="math inline">\(\mathbf{v}_t = \mathbf{x}_t \varepsilon_t\)</span>, denoted by <span class="math inline">\(\gamma^v_j\)</span>. Using the fact that <span class="math inline">\(\mathbf{x}_t\)</span> is a linear combination of past <span class="math inline">\(\varepsilon_t\)</span>s and that <span class="math inline">\(\varepsilon_t\)</span> is a white noise, we get that <span class="math inline">\(\mathbb{E}(\varepsilon_t\mathbf{x}_t)=0\)</span>. Therefore
<span class="math display">\[
\gamma^v_j = \mathbb{E}(\varepsilon_t\varepsilon_{t-j}\mathbf{x}_t\mathbf{x}_{t-j}').
\]</span>
If <span class="math inline">\(j&gt;0\)</span>, we have <span class="math inline">\(\mathbb{E}(\varepsilon_t\varepsilon_{t-j}\mathbf{x}_t\mathbf{x}_{t-j}')=\mathbb{E}(\mathbb{E}[\varepsilon_t\varepsilon_{t-j}\mathbf{x}_t\mathbf{x}_{t-j}'|\varepsilon_{t-j},\mathbf{x}_t,\mathbf{x}_{t-j}])=\)</span> <span class="math inline">\(\mathbb{E}(\varepsilon_{t-j}\mathbf{x}_t\mathbf{x}_{t-j}'\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},\mathbf{x}_t,\mathbf{x}_{t-j}])=0\)</span>. Note that we have <span class="math inline">\(\mathbb{E}[\varepsilon_t|\varepsilon_{t-j},\mathbf{x}_t,\mathbf{x}_{t-j}]=0\)</span> because <span class="math inline">\(\{\varepsilon_t\}\)</span> is an i.i.d. white noise sequence. If <span class="math inline">\(j=0\)</span>, we have:
<span class="math display">\[
\gamma^v_0 = \mathbb{E}(\varepsilon_t^2\mathbf{x}_t\mathbf{x}_{t}')= \mathbb{E}(\varepsilon_t^2) \mathbb{E}(\mathbf{x}_t\mathbf{x}_{t}')=\sigma^2\mathbf{Q}.
\]</span>
The convergence in distri. of <span class="math inline">\(\sqrt{T}(X'\boldsymbol\varepsilon/T)=\sqrt{T}\frac{1}{T}\sum_{t=1}^Tv_t\)</span> results from Theorem <a href="time-series.html#thm:CLTcovstat">3.1</a> (applied on <span class="math inline">\(\mathbf{v}_t=\mathbf{x}_t\varepsilon_t\)</span>), using the <span class="math inline">\(\gamma_j^v\)</span> computed above.</p>
</div>
<p>Let us now turn to Moving-Average processes. Start with the MA(1):
<span class="math display">\[
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1},\quad \varepsilon_t \sim i.i.d.\mathcal{N}(0,\sigma^2).
\]</span>
The <span class="math inline">\(\varepsilon_t\)</span>s are easily computed recursively, starting with <span class="math inline">\(\varepsilon_t = y_t - \mu - \theta_1 \varepsilon_{t-1}\)</span>. We obtain:
<span class="math display">\[
\varepsilon_t = y_t - \theta_1 y_{t-1} + \theta_1^2 y_{t-2}^2 + \dots + (-1)^{t-1} \theta_1^{t-1} y_{1} + (-1)^t\theta_1^{t}\varepsilon_{0}.
\]</span>
Assume that one wants to recover the sequence of <span class="math inline">\(\{\varepsilon_t\}\)</span>’s based on observed values of <span class="math inline">\(y_t\)</span> (from date 1 to date <span class="math inline">\(t\)</span>). One can use the previous expression, but value should be used for <span class="math inline">\(\varepsilon_0\)</span>? If one does not use the true value of <span class="math inline">\(\varepsilon_0\)</span> but 0 (say), one does not obtain <span class="math inline">\(\varepsilon_t\)</span> but only an estimate of it (<span class="math inline">\(\hat\varepsilon_t\)</span>, say), with:
<span class="math display">\[
\hat\varepsilon_t = \varepsilon_t - (-1)^t\theta_1^{t}\varepsilon_{0}.
\]</span>
Clearly, if <span class="math inline">\(|\theta_1|&lt;1\)</span>, then the error becomes small for large <span class="math inline">\(t\)</span>. Formally, when <span class="math inline">\(|\theta_1|&lt;1\)</span>, we have:
<span class="math display">\[
\hat\varepsilon_t \overset{p}{\rightarrow} \varepsilon_t.
\]</span>
Hence, when <span class="math inline">\(|\theta_1|&lt;1\)</span>, a consistent estimate of the conditional log-likelihood is given by:
<span class="math display" id="eq:MALstar">\[\begin{equation}
\log \hat{\mathcal{L}}^*(\boldsymbol\theta;\mathbf{y}) = -\frac{T}{2}\log(2\pi) - \frac{T}{2}\log(\sigma^2) - \sum_{t=1}^T \frac{\hat\varepsilon_t^2}{2\sigma^2}.\tag{3.26}
\end{equation}\]</span>
Loosely speaking, if <span class="math inline">\(|\theta_1|&lt;1\)</span> and if <span class="math inline">\(T\)</span> is sufficiently large:
<span class="math display">\[
\mbox{approximate conditional MLE $\approx$ exact MLE.}
\]</span></p>
<p>Note that <span class="math inline">\(\hat{\mathcal{L}}^*(\boldsymbol\theta;\mathbf{y})\)</span> is a complicated nonlinear function of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\theta\)</span>. Its maximization involves numerical optimization procedures.</p>
<p>Let us not consider the case of a Gaussian MA(<span class="math inline">\(q\)</span>) process:
<span class="math display" id="eq:estimMAq">\[\begin{equation}
y_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q} , \quad \varepsilon_t \sim i.i.d.\mathcal{N}(0,\sigma^2). \tag{3.27}
\end{equation}\]</span></p>
<p>Let us assume that this process is an <strong>invertible MA process</strong>. That is, we assume that the roots of:
<span class="math display">\[
\lambda^q + \theta_1 \lambda^{q-1} + \dots + \theta_{q-1} \lambda + \theta_q = 0
\]</span>
lie strictly inside of the unit circle. In this case, the polynomial form <span class="math inline">\(\Theta(L)=1 + \theta_1 L + \dots + \theta_q L^q\)</span> is <em>invertible</em> and Eq. <a href="time-series.html#eq:estimMAq">(3.27)</a> writes:
<span class="math display">\[
\varepsilon_t = \Theta(L)^{-1}(y_t - \mu),
\]</span>
which implies that, if we knew all past values of <span class="math inline">\(y_t\)</span>, we also know <span class="math inline">\(\varepsilon_t\)</span>. In this case, we can consistently estimate the <span class="math inline">\(\varepsilon_t\)</span> by recursively computing <span class="math inline">\(\hat\varepsilon_t\)</span> as (for <span class="math inline">\(t&gt;0\)</span>):
<span class="math display">\[
\hat\varepsilon_t = y_t - \mu - \theta_1 \hat\varepsilon_{t-1} - \dots  - \theta_q \hat\varepsilon_{t-q}.
\]</span>
with
<span class="math display" id="eq:condiVarepsiMA">\[\begin{equation}
\hat\varepsilon_{0}=\dots=\hat\varepsilon_{-q+1}=0,\tag{3.28}
\end{equation}\]</span></p>
<p>A consistent estimate of the conditional log-likelihood is still given by Eq. <a href="time-series.html#eq:MALstar">(3.26)</a>.</p>
<p>Note that we could determine the exact likelihood of an MA process. Indeed, vector <span class="math inline">\(Y = [y_1,\dots,y_T]'\)</span> is a Gaussian-distributed vector of mean <span class="math inline">\(\boldsymbol\mu = [\mu,\dots,\mu]'\)</span>. Its p.d.f. is given by (see Def. <a href="#def:multivarG"><strong>??</strong></a>):
<span class="math display">\[
(2\pi)^{-T/2}|\boldsymbol\Omega|^{-1/2}\exp\left( -\frac{1}{2} (Y-\boldsymbol\mu)' \boldsymbol\Omega^{-1} (Y-\boldsymbol\mu)\right),
\]</span>
where <span class="math inline">\(\boldsymbol\Omega\)</span> is of the form:
<span class="math display">\[
\boldsymbol\Omega = \left[\begin{array}{ccccccc}
\gamma_0 &amp; \gamma_1&amp;\dots&amp;\gamma_q&amp;{\color{red}0}&amp;{\color{red}\dots}&amp;{\color{red}0}\\
\gamma_1 &amp; \gamma_0&amp;\gamma_1&amp;&amp;\ddots&amp;{\color{red}\ddots}&amp;{\color{red}\vdots}\\
\vdots &amp; \gamma_1&amp;\ddots&amp;\ddots&amp;&amp;\ddots&amp;{\color{red}0}\\
\gamma_q &amp;&amp;\ddots&amp;&amp;&amp;&amp;\gamma_q\\
{\color{red}0} &amp;&amp;&amp;\ddots&amp;\ddots&amp;\ddots&amp;\vdots\\
{\color{red}\vdots}&amp;{\color{red}\ddots}&amp;\ddots&amp;&amp;\gamma_1&amp;\gamma_0&amp;\gamma_1\\
{\color{red}0}&amp;{\color{red}\dots}&amp;{\color{red}0}&amp;\gamma_q&amp;\dots&amp;\gamma_1&amp;\gamma_0
\end{array}\right],
\]</span>
where the <span class="math inline">\(\gamma_j\)</span>s are given by Eq. <a href="time-series.html#eq:autocovMA">(3.6)</a>. However, for large samples, the computation of the likelihood can be numerically demanding.</p>
<p>Finally, let us consider the MLE of an ARMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>) processes:
<span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} +
\dots + \theta_q \varepsilon_{t-q} , \; \varepsilon_t \sim i.i.d.\,\mathcal{N}(0,\sigma^2).
\]</span>
If the MA part of this process is invertible, the log-likelihood function can be consistently approximated by its conditional counterpart (of the form of Eq. (eq:MALstar)), using consistent estimates <span class="math inline">\(\hat\varepsilon_t\)</span> of the <span class="math inline">\(\varepsilon_t\)</span>. The <span class="math inline">\(\hat\varepsilon_t\)</span>s are computed recursively as:
<span class="math display" id="eq:recvareps">\[\begin{equation}
\hat\varepsilon_t = y_t - c - \phi_1 y_{t-1} - \dots - \phi_p y_{t-p} - \theta_1 \hat\varepsilon_{t-1} - \dots - \theta_q \hat\varepsilon_{t-q}\tag{3.29}
\end{equation}\]</span>
given some initial conditions, for instance:</p>
<ol style="list-style-type: lower-alpha">
<li>
<span class="math inline">\(\hat\varepsilon_0=\dots=\hat\varepsilon_{-q+1}=0\)</span> and <span class="math inline">\(y_{0}=\dots=y_{-p+1}=\mathbb{E}(y_i)=\mu\)</span>. (Recursions in Eq. <a href="time-series.html#eq:recvareps">(3.29)</a> then start for <span class="math inline">\(t=1\)</span>.)</li>
<li>
<span class="math inline">\(\hat\varepsilon_p=\dots=\hat\varepsilon_{p-q+1}=0\)</span> and actual values of the <span class="math inline">\(y_{i}\)</span>s for <span class="math inline">\(i \in [1,p]\)</span>. In that case, the first <span class="math inline">\(p\)</span> observations of <span class="math inline">\(y_t\)</span> will not be used. Recursions in Eq. <a href="time-series.html#eq:recvareps">(3.29)</a> then start for <span class="math inline">\(t=p+1\)</span>.</li>
</ol>
</div>
<div id="specification-choice" class="section level3" number="3.2.7">
<h3>
<span class="header-section-number">3.2.7</span> Specification choice<a class="anchor" aria-label="anchor" href="#specification-choice"><i class="fas fa-link"></i></a>
</h3>
<p>The previouss section explains how to fit a given ARMA specification. But how to choose an appropriate specification? A possibility is to employ the (P)ACF approach (see Figure <a href="time-series.html#fig:pacf">3.7</a>). However, the previous approach leads to either an AR or a MA process (bit not an ARMA process). If one wants to consider various ARMA(p,q) specifications, for <span class="math inline">\(p \in \{1,\dots,P\}\)</span> and <span class="math inline">\(q \in \{1,7dots,Q\}\)</span>, say, then one can resort to <strong>information criteria</strong>.</p>
<p>In general, when choosing a specification, one is faced with the following dilemma:</p>
<ol style="list-style-type: lower-alpha">
<li>Too rich a specification may lead to “overfitting”/misspecification, implying additional estimation errors (in forecasts).</li>
<li>Too simple a specification may lead to potential omission of valuable information (e.g., contained in older lags).</li>
</ol>
<p>The lag selection approach based on the so-called <strong>information criteria</strong> consists in maximizing the fit of the data, but adding a penalty for the “richness” of the model. More precisely, using this approach amounts to minimizing a loss function that (a) negatively depends on the fitting errors and (b) positively depends on the number of parameters in the model.</p>
<div class="definition">
<p><span id="def:infocriteria" class="definition"><strong>Definition 3.20  (Information Criteria) </strong></span>The Akaike (AIC), Hannan-Quinn (HQ) and Schwarz information (BIC) criteria are of the form
<span class="math display">\[
c^{(i)}(k) = \underbrace{\frac{- 2 \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})}{T}}_{\mbox{decreases w.r.t. $k$}} \quad +
\underbrace{
\frac{k\phi^{(i)}(T)}{T}}_{\mbox{increases w.r.t. $k$}}
\]</span>
with <span class="math inline">\((i) \in\{AIC,HQ,BIC\}\)</span> and where <span class="math inline">\(\hat{\boldsymbol\theta}_T(k)\)</span> denotes the ML estimate of <span class="math inline">\(\boldsymbol\theta_0(k)\)</span> – a vector of parameters of length <span class="math inline">\(k\)</span>. For <span class="math inline">\(q \ge 0\)</span>, the model specified by <span class="math inline">\(k\)</span> parameters is a particular case of a model of <span class="math inline">\(k+q\)</span> parameters.</p>
<p>The lag suggested by criterion <span class="math inline">\((i)\)</span> is given by:
<span class="math display">\[
\boxed{\hat{k}^{(i)} = \underset{k}{\mbox{argmin}} \quad c^{(i)}(k).}
\]</span></p>
</div>
<p>In the case of an ARMA(p,q) process, <span class="math inline">\(k=2+p+q\)</span></p>
<div class="proposition">
<p><span id="prp:infocriteria" class="proposition"><strong>Proposition 3.11  (Consistency of the criteria-based lag selection) </strong></span>The lag selection procedure is consistent (see Def. <a href="#def:asmyptconsisttes"><strong>??</strong></a>) if
<span class="math display">\[
\lim_{T \rightarrow \infty} \phi(T) = \infty \quad and \quad \lim_{T \rightarrow \infty} \phi(T)/T = 0.
\]</span>
This is notably the case of the HQ and the BIC criteria.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-12" class="proof"><em>Proof</em>. </span>The true number of lags is denoted by <span class="math inline">\(k_0\)</span>. We will show that <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}_T \ne k_0)=0\)</span>.</p>
<ul>
<li>Case <span class="math inline">\(k &lt; k_0\)</span>: The model with <span class="math inline">\(k\)</span> parameter is misspecified, therefore:
<span class="math display">\[
\mbox{plim}_{T \rightarrow \infty}  \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})/T &lt; \mbox{plim}_{T \rightarrow \infty}  \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\mathbf{y})/T.
\]</span>
Hence, if <span class="math inline">\(\lim_{T \rightarrow \infty} \phi(T)/T = 0\)</span>, we have: <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(c(k_0) \ge c(k)) \rightarrow 0\)</span> and
<span class="math display">\[
\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}&lt;k_0) \le \lim_{T \rightarrow \infty} \mathbb{P}\left\{c(k_0) \ge c(k) \mbox{ for some $k &lt; k_0$}\right\} = 0.
\]</span>
</li>
<li>Case <span class="math inline">\(k &gt; k_0\)</span>: The likelihood ratio (LR) test results imply that: <span class="math inline">\(2 \left(\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})-\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\mathbf{y})\right) \sim \chi^2(k-k_0)\)</span>.
If <span class="math inline">\(\lim_{T \rightarrow \infty} \phi(T) = \infty\)</span>, we have: <span class="math inline">\(\mbox{plim}_{T \rightarrow \infty} -2 \left(\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})-\log \mathcal{L}(\hat{\boldsymbol\theta}_T(k_0);\mathbf{y})\right)/\phi(T) = 0\)</span>. Hence <span class="math inline">\(\mbox{plim}_{T \rightarrow \infty} T[c(k_0) - c(k)]/\phi(T) \le -1\)</span> and <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(c(k_0) \ge c(k)) \rightarrow 0\)</span>, which implies, in the same spirit as before, that <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}&gt;k_0) = 0\)</span>.</li>
</ul>
<p>Therefore, <span class="math inline">\(\lim_{T \rightarrow \infty} \mathbb{P}(\hat{k}=k_0) = 1\)</span>.</p>
</div>
<p>To illustrate, consider a linear regression with normal disturbances:
<span class="math display">\[
y_t = \mathbf{x}_t \boldsymbol\beta + \varepsilon_t, \quad \varepsilon_t \sim i.i.d. \mathcal{N}(0,\sigma^2).
\]</span>
The associated log-likelihood is of the form of Eq. <a href="time-series.html#eq:MALstar">(3.26)</a>. In that case, we have:
<span class="math display">\[\begin{eqnarray*}
c^{(i)}(k) &amp;=&amp; \frac{- 2 \log \mathcal{L}(\hat{\boldsymbol\theta}_T(k);\mathbf{y})}{T} + \frac{k\phi^{(i)}(T)}{T}\\
&amp;\approx&amp; \log(2\pi) + \log(\widehat{\sigma^2}) + \frac{1}{T}\sum_{t=1}^T \frac{\varepsilon_t^2}{\widehat{\sigma^2}} + \frac{k\phi^{(i)}(T)}{T}.
\end{eqnarray*}\]</span>
For a large <span class="math inline">\(T\)</span>, for all consistent estimation scheme, we have:
<span class="math display">\[
\widehat{\sigma^2} \approx \frac{1}{T}\sum_{t=1}^T \varepsilon_t^2 = SSR/T.
\]</span>
Hence <span class="math inline">\(\hat{k}^{(i)} \approx \underset{k}{\mbox{argmin}} \quad \log(SSR/T) + \dfrac{k\phi^{(i)}(T)}{T}\)</span>.</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">2</span> Introduction</a></div>
<div class="next"><a href="appendix.html"><span class="header-section-number">4</span> Appendix</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#time-series"><span class="header-section-number">3</span> Time Series</a></li>
<li><a class="nav-link" href="#introduction-to-time-series"><span class="header-section-number">3.1</span> Introduction to time series</a></li>
<li>
<a class="nav-link" href="#univariate-processes"><span class="header-section-number">3.2</span> Univariate processes</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#moving-average-ma-processes"><span class="header-section-number">3.2.1</span> Moving Average (MA) processes</a></li>
<li><a class="nav-link" href="#auto-regressive-ar-processes"><span class="header-section-number">3.2.2</span> Auto-Regressive (AR) processes</a></li>
<li><a class="nav-link" href="#ar-ma-processes"><span class="header-section-number">3.2.3</span> AR-MA processes</a></li>
<li><a class="nav-link" href="#impulse-response-functions-irfs-in-arma-models"><span class="header-section-number">3.2.4</span> Impulse Response Functions (IRFs) in ARMA models</a></li>
<li><a class="nav-link" href="#ARMAIRF"><span class="header-section-number">3.2.5</span> ARMA processes with exogenous variables (ARMA-X)</a></li>
<li><a class="nav-link" href="#maximum-likelihood-estimation-of-arma-processes"><span class="header-section-number">3.2.6</span> Maximum Likelihood Estimation of ARMA processes</a></li>
<li><a class="nav-link" href="#specification-choice"><span class="header-section-number">3.2.7</span> Specification choice</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Advanced Econometrics</strong>" was written by Jean-Paul Renne. It was last built on 2022-09-03.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

[{"path":"index.html","id":"prerequisites","chapter":"1 Prerequisites","heading":"1 Prerequisites","text":"sample book written Markdown. can use anything Pandoc’s Markdown supports, e.g., math equation \\(^2 + b^2 = c^2\\).bookdown package can installed CRAN Github:Remember Rmd file contains one one chapter, chapter defined first-level heading #.compile example PDF, need XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.name/tinytex/.","code":"\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"can label chapter section titles using {#label} , e.g., can reference Chapter 2. manually label , automatic labels anyway, e.g., Chapter ??.Figures tables captions placed figure table environments, respectively.\nFigure 2.1: nice figure!\nReference figure code chunk label fig: prefix, e.g., see Figure 2.1. Similarly, can reference tables generated knitr::kable(), e.g., see Table 2.1.Table 2.1: nice table!can write citations, . example, using bookdown package (Xie 2022) sample book, built top R Markdown knitr (Xie 2015).example borrowed Petersen.XXXXSargan-Hansen () test. Sargan (1958) Hansen (1982)Durbin-Wu-Hausman test: Durbin (1954) / Wu (1973) / Hausman (1978)Use R! excellent tutorial.\n(notably plm Arellano-Bond example, 140 UK firms)Program evaluation (good survey): Abadie Cattaneo (2018)\nMostly harmless: Angrist Pischke (2008)Diff--Diff: Card Krueger (1994)Diff--Diff:\nMeyer, Viscusi, Durbin (1995)\ndata wooldridge package. See page","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(iris, 20), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)\nlibrary(sandwich)\n## Petersen's data\ndata(\"PetersenCL\", package = \"sandwich\")\nm <- lm(y ~ x, data = PetersenCL)\n\n## clustered covariances\n## one-way\nvcovCL(m, cluster = ~ firm)##               (Intercept)             x\n## (Intercept)  4.490702e-03 -6.473517e-05\n## x           -6.473517e-05  2.559927e-03\nvcovCL(m, cluster = PetersenCL$firm) ## same##               (Intercept)             x\n## (Intercept)  4.490702e-03 -6.473517e-05\n## x           -6.473517e-05  2.559927e-03\n## one-way with HC2\nvcovCL(m, cluster = ~ firm, type = \"HC2\")##               (Intercept)             x\n## (Intercept)  4.494487e-03 -6.592912e-05\n## x           -6.592912e-05  2.568236e-03\n## two-way\nvcovCL(m, cluster = ~ firm + year)##               (Intercept)             x\n## (Intercept)  4.233313e-03 -2.845344e-05\n## x           -2.845344e-05  2.868462e-03\nvcovCL(m, cluster = PetersenCL[, c(\"firm\", \"year\")]) ## same##               (Intercept)             x\n## (Intercept)  4.233313e-03 -2.845344e-05\n## x           -2.845344e-05  2.868462e-03"},{"path":"microeconometrics.html","id":"microeconometrics","chapter":"3 Microeconometrics","heading":"3 Microeconometrics","text":"","code":""},{"path":"microeconometrics.html","id":"binary-choice-models","chapter":"3 Microeconometrics","heading":"3.1 Binary-choice models","text":"many instances, variables explained (\\(y_i\\)s) two possible values (\\(0\\) \\(1\\), say).Assume suspect variable \\(\\mathbf{x}_i\\) (\\(K \\times 1\\)) able account probability \\(y_i=1\\).model reads:\n\\[\\begin{equation}\\label{eq:binaryBenroulli}\ny_i | \\mathbf{X} \\sim \\mathcal{B}(g(\\mathbf{x}_i;\\boldsymbol\\theta)),\n\\end{equation}\\]\n\\(g(\\mathbf{x}_i;\\boldsymbol\\theta)\\) parameter Bernoulli distribution. words, conditionally \\(\\mathbf{X}\\):\n\\[\ny_i = \\left\\{\n\\begin{array}{cl}\n1 & \\mbox{ probability } g(\\mathbf{x}_i;\\boldsymbol\\theta)\\\\\n0 & \\mbox{ probability } 1-g(\\mathbf{x}_i;\\boldsymbol\\theta),\n\\end{array}\n\\right.\n\\]\n\\(\\boldsymbol\\theta\\) vector parameters estimated.objective estimate vector population parameters \\(\\boldsymbol\\theta\\).Binary-choice models can used account …binary decisions (e.g. referendums, owner renter, living city countryside, /labour force,…),contamination (disease default),success/failure (exams).possibility run linear regression (situation called Linear Probability Model, LPM):\n\\[\ny_i = \\boldsymbol\\theta'\\mathbf{x}_i + \\varepsilon_i.\n\\]regression consistent conditional-mean-zero assumption (Hypothesis ??) assumption non-correlated residuals (Hypothesis ??), difficultly homoskedasticity assumption (Hypothesis ??). Moreover, \\(\\varepsilon_i\\)s Gaussian (\\(y_i \\\\{0,1\\}\\)). Therefore, using linear regression study relationship \\(\\mathbf{x}_i\\) \\(y_i\\) can consistent inefficient.\nFigure 3.1: Fitting binary variable linear model (Linear Probability Model, LPM). model \\(\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)\\), \\(\\Phi\\) c.d.f. normal distribution \\(x_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\).\nTable 3.1:  table provides examples function \\(g\\), s.t. \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol heta) = g(\\boldsymbol\\theta'\\mathbf{x}_i)\\). “linear” case given comparison, note satisfy \\(g(\\boldsymbol\\theta'\\mathbf{x}_i)\\) value \\(\\boldsymbol\\theta'\\mathbf{x}_i\\).Two prominent models tackle situation. models, :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta)=g(\\boldsymbol\\theta'\\mathbf{x}_i).\n\\]Probit model, \n\\[\\begin{equation}\ng(z) = \\Phi(z),\\tag{3.1}\n\\end{equation}\\]\n\\(\\Phi\\) c.d.f. normal distribution.logit model:\n\\[\\begin{equation}\ng(z) = \\frac{1}{1+\\exp(-z)}.\\tag{3.2}\n\\end{equation}\\]Figure 3.2 displays functions \\(g\\) appearing Table 3.1.\nFigure 3.2: Probit, Logit, Log-log functions.\n\nFigure 3.3: model \\(\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)\\), \\(\\Phi\\) c.d.f. normal distribution \\(x_i \\sim \\,..d.\\,\\mathcal{N}(0,1)\\). Crosses give model-implied probabilities \\(y_i=1\\).\n","code":""},{"path":"microeconometrics.html","id":"interpretation-in-terms-of-latent-variable-and-utility-based-models","chapter":"3 Microeconometrics","heading":"3.1.1 Interpretation in terms of latent variable, and utility-based models","text":"probit model interpretation terms latent variables. model, indeed :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = \\Phi(\\boldsymbol\\theta'\\mathbf{x}_i) = \\mathbb{P}(-\\varepsilon_{}<\\boldsymbol\\theta'\\mathbf{x}_i),\n\\]\n\\(\\varepsilon_{} \\sim \\mathcal{N}(0,1)\\). :\n\\[\n\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) = \\mathbb{P}(0< y_i^*),\n\\]\n\\(y_i^* = \\boldsymbol\\theta'\\mathbf{x}_i + \\varepsilon_i\\), \\(\\varepsilon_{} \\sim \\mathcal{N}(0,1)\\). Variable \\(y_i^*\\) can interpreted (latent) variable determines \\(y_i\\) since \\(y_i = \\mathbb{}_{\\{y_i^*>0\\}}\\).\nFigure 3.4: Distribution \\(y_i^*\\) conditional \\(\\mathbf{x}_i\\).\nRandom Utility Models (RUM) based view probit models. Assume agent (\\(\\)) chooses \\(y_i=1\\) utility associated choice (\\(U_{,1}\\)) higher one associated \\(y_i=0\\) (\\(U_{,0}\\)). Assume utility agent \\(\\), chooses outcome \\(j\\) (\\(\\\\{0,1\\}\\)), given \n\\[\nU_{,j} = V_{,j} + \\varepsilon_{,j},\n\\]\n\\(V_{,j}\\) deterministic component utility associated choice \\(\\varepsilon_{,j}\\) random (agent-specific) component.Moreover, posit \\(V_{,j} = \\boldsymbol\\theta_j'\\mathbf{x}_i\\). :\n\\[\\begin{eqnarray}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta) &=& \\mathbb{P}(\\boldsymbol\\theta_1'\\mathbf{x}_i+\\varepsilon_{,1}>\\boldsymbol\\theta_0'\\mathbf{x}_i+\\varepsilon_{,0}) \\nonumber\\\\\n&=& F(\\boldsymbol\\theta_1'\\mathbf{x}_i-\\boldsymbol\\theta_0'\\mathbf{x}_i) = F([\\boldsymbol\\theta_1-\\boldsymbol\\theta_0]'\\mathbf{x}_i),\\tag{3.3}\n\\end{eqnarray}\\]\n\\(F\\) c.d.f. \\(\\varepsilon_{,0}-\\varepsilon_{,1}\\).Note difference \\(\\boldsymbol\\theta_1-\\boldsymbol\\theta_0\\) identifiable (opposed \\(\\boldsymbol\\theta_1\\) \\(\\boldsymbol\\theta_0\\)). Indeed, replacing \\(U\\) \\(aU\\) (\\(>0\\)) gives model \\(\\Leftrightarrow\\) scaling issue, solved fixing variance \\(\\varepsilon_{,0}-\\varepsilon_{,1}\\).types structural models –based comparison marginal costs benefits– give rise existence latent variable probit models. example Nakosteen Zimmer (1980). main ingredients approach follows:Wage can earned present location: \\(y_p^* = \\boldsymbol\\theta_p'\\mathbf{x}_p + \\varepsilon_p\\).Wage can earned present location: \\(y_p^* = \\boldsymbol\\theta_p'\\mathbf{x}_p + \\varepsilon_p\\).Migration cost: \\(C^*= \\boldsymbol\\theta_c'\\mathbf{x}_c + \\varepsilon_c\\).Migration cost: \\(C^*= \\boldsymbol\\theta_c'\\mathbf{x}_c + \\varepsilon_c\\).Wage earned elsewhere: \\(y_m^* = \\boldsymbol\\theta_m'\\mathbf{x}_m + \\varepsilon_m\\).Wage earned elsewhere: \\(y_m^* = \\boldsymbol\\theta_m'\\mathbf{x}_m + \\varepsilon_m\\).context, agents decision migrate \\(y_m^* > y_p^* + C^*\\), .e. \n\\[\ny^* = y_m^* -  y_p^* - C^* =  \\boldsymbol\\theta'\\mathbf{x} + \\underbrace{\\varepsilon}_{=\\varepsilon_m - \\varepsilon_c - \\varepsilon_p}>0,\n\\]\n\\(\\mathbf{x}\\) union \\(\\mathbf{x}_i\\)s, \\(\\\\{p,m,c\\}\\).","code":""},{"path":"microeconometrics.html","id":"Avregressors","chapter":"3 Microeconometrics","heading":"3.1.2 Alternative-Varying Regressors","text":"cases, regressors may depend considered alternative (\\(0\\) \\(1\\)). instance:modeling decision participate labour force (), wage depends alternative. included among regressors given observed considered agent decided work.modeling decision participate labour force (), wage depends alternative. included among regressors given observed considered agent decided work.context choice transportation mode, time cost depends considered transportation mode.context choice transportation mode, time cost depends considered transportation mode.terms utility, :\n\\[\nV_{,j} = {\\theta^{(u)}_{j}}'\\mathbf{u}_{,j} + {\\theta^{(v)}_{j}}'\\mathbf{v}_{},\n\\]\n\\(\\mathbf{u}_{,j}\\)s regressors associated agent \\(\\), taking different values different choices (\\(j=0\\) \\(j=1\\)).case, Eq. (3.3) becomes:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta)  = F\\left({\\theta^{(u)}_{1}}'\\mathbf{u}_{,1}-{\\theta^{(u)}_{0}}'\\mathbf{u}_{,0}+[\\boldsymbol\\theta_1^{(v)}-\\boldsymbol\\theta_0^{(v)}]'\\mathbf{v}_i\\right),\\tag{3.4}\n\\end{equation}\\]\n, \\(\\theta^{(u)}_{1}=\\theta^{(u)}_{0}=\\theta^{(u)}\\) –customary– get:\n\\[\\begin{equation}\n\\mathbb{P}(y_i = 1|\\mathbf{x}_i;\\boldsymbol\\theta)  = F\\left({\\theta^{(u)}_{1}}'(\\mathbf{u}_{,1}-\\mathbf{u}_{,0})+[\\boldsymbol\\theta_1^{(v)}-\\boldsymbol\\theta_0^{(v)}]'\\mathbf{v}_i\\right).\\tag{3.5}\n\\end{equation}\\]fishing-mode dataset used Cameron Trivedi (2005) (Chapter 14 15) contains alternative-specific variables. Specifically, individual, price catch rate depend fishing model. table reported , lines price catch correspond prices catch rates associated chosen alternative.","code":"\nlibrary(Ecdat)\ndata(Fishing)\nstargazer::stargazer(Fishing,type=\"text\")## \n## ======================================================\n## Statistic   N     Mean    St. Dev.    Min      Max    \n## ------------------------------------------------------\n## price     1,182  52.082    53.830    1.290   666.110  \n## catch     1,182   0.389     0.561   0.0002    2.310   \n## pbeach    1,182  103.422   103.641   1.290   843.186  \n## ppier     1,182  103.422   103.641   1.290   843.186  \n## pboat     1,182  55.257    62.713    2.290   666.110  \n## pcharter  1,182  84.379    63.545   27.290   691.110  \n## cbeach    1,182   0.241     0.191    0.068    0.533   \n## cpier     1,182   0.162     0.160    0.001    0.452   \n## cboat     1,182   0.171     0.210   0.0002    0.737   \n## ccharter  1,182   0.629     0.706    0.002    2.310   \n## income    1,182 4,099.337 2,461.964 416.667 12,500.000\n## ------------------------------------------------------"},{"path":"microeconometrics.html","id":"estimation","chapter":"3 Microeconometrics","heading":"3.1.3 Estimation","text":"models can estimated Maximum Likelihood approaches (see Section ??).simplify exposition, consider \\(\\mathbf{x}_i\\) deterministic. Also, assume r.v. independent across entities \\(\\). write likelihood ? can seen :\n\\[\\begin{eqnarray}\nf(y_i|\\mathbf{x}_i;\\boldsymbol\\theta) &=& y_i g(\\boldsymbol\\theta'\\mathbf{x}_i) + (1-y_i) (1-g(\\boldsymbol\\theta'\\mathbf{x}_i)) \\nonumber \\\\\n&=&  g(\\boldsymbol\\theta'\\mathbf{x}_i)^{y_i}(1-g(\\boldsymbol\\theta'\\mathbf{x}_i))^{1-y_i}.\n\\end{eqnarray}\\]Therefore, observations \\((\\mathbf{x}_i,y_i)\\) independent across entities \\(\\), :\n\\[\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X}) = \\sum_{=1}^{n}y_i \\log[g(\\boldsymbol\\theta'\\mathbf{x}_i)] + (1-y_i)\\log[1-g(\\boldsymbol\\theta'\\mathbf{x}_i)].\n\\]likelihood equation reads (FOC optimization program, see Def. ??):\n\\[\n\\dfrac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta} = \\mathbf{0},\n\\]\n:\n\\[\n\\sum_{=1}^{n} y_i \\mathbf{x}_i\\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}{g(\\boldsymbol\\theta'\\mathbf{x}_i)} - (1-y_i) \\mathbf{x}_i \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}{1-g(\\boldsymbol\\theta'\\mathbf{x}_i)} = \\mathbf{0}.\n\\]nonlinear equation generally numerically solved. regularity conditions (Hypotheses ??), (Prop. ??):\n\\[\n\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}(\\boldsymbol\\theta_0,\\mathbf{}(\\boldsymbol\\theta_0)^{-1}),\n\\]\n\n\\[\n\\mathbf{}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right) = n \\mathcal{}_Y(\\boldsymbol\\theta_0).\n\\]finite samples, can e.g. approximate \\(\\mathbf{}(\\boldsymbol\\theta_0)^{-1}\\) (Eq. (??)):\n\\[\n\\mathbf{}(\\boldsymbol\\theta_0)^{-1} \\approx -\\left(\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_{MLE};\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}\\right)^{-1}.\n\\]Probit case (see Table 3.1), can shown :\n\\[\\begin{eqnarray*}\n&&\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = - \\sum_{=1}^{n} g'(\\boldsymbol\\theta'\\mathbf{x}_i) [\\mathbf{x}_i \\mathbf{x}_i'] \\times \\\\\n&&\\left[y_i \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i) + \\boldsymbol\\theta'\\mathbf{x}_ig(\\boldsymbol\\theta'\\mathbf{x}_i)}{g(\\boldsymbol\\theta'\\mathbf{x}_i)^2} + (1-y_i) \\frac{g'(\\boldsymbol\\theta'\\mathbf{x}_i) - \\boldsymbol\\theta'\\mathbf{x}_i (1 - g(\\boldsymbol\\theta'\\mathbf{x}_i))}{(1-g(\\boldsymbol\\theta'\\mathbf{x}_i))^2}\\right].\n\\end{eqnarray*}\\]Logit case (see Table 3.1), can shown :\n\\[\n\\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} = - \\sum_{=1}^{n} g'(\\boldsymbol\\theta'\\mathbf{x}_i) \\mathbf{x}_i\\mathbf{x}_i',\n\\]\n\\(g'(x)=\\dfrac{\\exp(-x)}{(1 + \\exp(-x))^2}\\).Since \\(g'(x)>0\\), can checked \\(-\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y},\\mathbf{X})/\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'\\) positive definite.","code":""},{"path":"microeconometrics.html","id":"marginal-effectss","chapter":"3 Microeconometrics","heading":"3.1.4 Marginal effectss","text":"measure marginal effects, .e. effect probability \\(y_i=1\\) marginal increase \\(x_{,k}\\)? object given :\n\\[\n\\frac{\\partial \\mathbb{P}(y_i=1|\\mathbf{x_i};\\boldsymbol\\theta)}{\\partial x_{,k}} = \\underbrace{g'(\\boldsymbol\\theta'\\mathbf{x}_i)}_{>0}\\theta_k,\n\\]\nsign \\(\\theta_k\\).can estimated \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\). important see marginal effect depends \\(\\mathbf{x}_i\\): increases 1 unit \\(x_{,k}\\) (entity \\(\\)) \\(x_{j,k}\\) (entity \\(j\\)) necessarily effects \\(\\mathbb{P}(y_i=1|\\mathbf{x_i};\\boldsymbol\\theta)\\) \\(\\mathbb{P}(y_j=1|\\mathbf{x_j};\\boldsymbol\\theta)\\), respectively.address issue, one can compute measures “average” marginal effect. two main solutions. explanatory variable \\(k\\):Denoting \\(\\hat{\\mathbf{x}}\\) sample average \\(\\mathbf{x}_i\\)s, compute \\(g'(\\boldsymbol\\theta_{MLE}'\\hat{\\mathbf{x}})\\theta_{MLE,k}\\).Denoting \\(\\hat{\\mathbf{x}}\\) sample average \\(\\mathbf{x}_i\\)s, compute \\(g'(\\boldsymbol\\theta_{MLE}'\\hat{\\mathbf{x}})\\theta_{MLE,k}\\).Compute average (across \\(\\)) \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\).Compute average (across \\(\\)) \\(g'(\\boldsymbol\\theta_{MLE}'\\mathbf{x}_i)\\theta_{MLE,k}\\).","code":""},{"path":"microeconometrics.html","id":"goodness-of-fit","chapter":"3 Microeconometrics","heading":"3.1.5 Goodness of fit","text":"obvious version “\\(R^2\\)” binary-choice models. Existing measures called pseudo-\\(R^2\\)} measures.Denoting \\(\\log \\mathcal{L}_0(\\mathbf{y})\\) (maximum) log-likelihood obtained model containing constant term (.e. \\(\\mathbf{x}_i = 1\\) \\(\\)), McFadden’s pseudo-\\(R^2\\) given :\n\\[\nR^2_{MF} = 1 - \\frac{\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\log \\mathcal{L}_0(\\mathbf{y})}.\n\\]\nIntuitively, \\(R^2_{MF}=0\\) explanatory variables allow predict decision (\\(y\\)).","code":""},{"path":"microeconometrics.html","id":"example-credit-data","chapter":"3 Microeconometrics","heading":"3.1.6 Example: Credit data","text":"example makes use credit data package AEC. objective model default probabilities lenders.consider three specifications. first one, explanatory variables, trivial. just used compute pseudo-\\(R^2\\).Let us now compute marginal effects., however, issue annual_inc variable. Indeed, previous computation realize variable appears twice among explanatory variables. address , one can proceed follows.average effect pretty low. compare, let us compute average effect associated unit increase number delinquencies:can employ likelihood ratio test (see Def. ??) see two variables associated annual income jointly statistically significant (context eq1):computation gives p-value 0.0436.","code":"\nlibrary(AEC)\ncredit$Default <- 0\ncredit$Default[credit$loan_status == \"Charged Off\"] <- 1\ncredit$Default[credit$loan_status == \"Does not meet the credit policy. Status:Charged Off\"] <- 1\ncredit$amt2income <- credit$loan_amnt/credit$annual_inc\nplot(as.factor(credit$Default)~log(credit$annual_inc),\n     ylevels=2:1,ylab=\"Default status\",xlab=\"log(annual income)\")\neq0 <- glm(Default ~ 1,data=credit,family=binomial(link=\"probit\"))\neq1 <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs + log(annual_inc)+ I(log(annual_inc)^2),\n                 data=credit,family=binomial(link=\"probit\"))\neq2 <- glm(Default ~ grade + log(loan_amnt) + amt2income + delinq_2yrs + log(annual_inc)+ I(log(annual_inc)^2),\n                 data=credit,family=binomial(link=\"probit\"))\nlogL0 <- logLik(eq0)\nlogL1 <- logLik(eq1)\nlogL2 <- logLik(eq2)\n1 - logL1/logL0 # pseudo R2## 'log Lik.' 0.01173993 (df=6)\n1 - logL2/logL0 # pseudo R2## 'log Lik.' 0.0558487 (df=12)\nstargazer::stargazer(eq0,eq1,eq2,type=\"text\")## \n## ====================================================\n##                           Dependent variable:       \n##                     --------------------------------\n##                                 Default             \n##                        (1)        (2)        (3)    \n## ----------------------------------------------------\n## gradeB                                     0.400*** \n##                                            (0.055)  \n##                                                     \n## gradeC                                     0.587*** \n##                                            (0.057)  \n##                                                     \n## gradeD                                     0.820*** \n##                                            (0.061)  \n##                                                     \n## gradeE                                     0.874*** \n##                                            (0.091)  \n##                                                     \n## gradeF                                     1.230*** \n##                                            (0.147)  \n##                                                     \n## gradeG                                     1.439*** \n##                                            (0.227)  \n##                                                     \n## log(loan_amnt)                  -0.149**  -0.194*** \n##                                 (0.060)    (0.061)  \n##                                                     \n## amt2income                      1.266***   1.222*** \n##                                 (0.383)    (0.393)  \n##                                                     \n## delinq_2yrs                     0.096***    0.009   \n##                                 (0.034)    (0.035)  \n##                                                     \n## log(annual_inc)                 -1.444**    -0.874  \n##                                 (0.569)    (0.586)  \n##                                                     \n## I(log(annual_inc)2)             0.064**     0.038   \n##                                 (0.025)    (0.026)  \n##                                                     \n## Constant            -1.231***   7.937***    4.749   \n##                      (0.017)    (3.060)    (3.154)  \n##                                                     \n## ----------------------------------------------------\n## Observations          9,156      9,156      9,156   \n## Log Likelihood      -3,157.696 -3,120.625 -2,981.343\n## Akaike Inf. Crit.   6,317.392  6,253.250  5,986.686 \n## ====================================================\n## Note:                    *p<0.1; **p<0.05; ***p<0.01\nmean(dnorm(predict(eq2)),na.rm=TRUE)*eq2$coefficients##          (Intercept)               gradeB               gradeC \n##          0.840731198          0.070747353          0.103944305 \n##               gradeD               gradeE               gradeF \n##          0.145089219          0.154773742          0.217702041 \n##               gradeG       log(loan_amnt)           amt2income \n##          0.254722161         -0.034289921          0.216251992 \n##          delinq_2yrs      log(annual_inc) I(log(annual_inc)^2) \n##          0.001574178         -0.154701321          0.006813694\nnew_credit <- credit\nnew_credit$annual_inc <- 1.01 * new_credit$annual_inc # increase of income by 1%\nbas_predict_eq2  <- predict(eq2, newdata = credit, type = \"response\")\n# This is equivalent to pnorm(predict(eq2, newdata = credit))\nnew_predict_eq2  <- predict(eq2, newdata = new_credit, type = \"response\")\nmean(new_predict_eq2 - bas_predict_eq2)## [1] -6.562126e-05\nnew_credit <- credit\nnew_credit$delinq_2yrs <- credit$delinq_2yrs + 1\nnew_predict_eq2  <- predict(eq2, newdata = new_credit, type = \"response\")\nmean(new_predict_eq2 - bas_predict_eq2)## [1] 0.001582332\neq1restr <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs,\n                 data=credit,family=binomial(link=\"probit\"))\nLRstat <- 2*(logL1 - logLik(eq1restr))\npvalue <- 1 - c(pchisq(LRstat,df=2))"},{"path":"microeconometrics.html","id":"replicating-table-14.2-of-cameron-and-trivedi-2005","chapter":"3 Microeconometrics","heading":"3.1.7 Replicating Table 14.2 of Cameron and Trivedi (2005)","text":"following lines codes replicate Table 14.2 Cameron Trivedi (2005).","code":"\ndata.reduced <- subset(Fishing,mode %in% c(\"charter\",\"pier\"))\ndata.reduced$lnrelp <- log(data.reduced$pcharter/data.reduced$ppier)\ndata.reduced$y <- 1*(data.reduced$mode==\"charter\")\n# check first line of Table 14.1:\nprice.charter.y0 <- mean(data.reduced$pcharter[data.reduced$y==0])\nprice.charter.y1 <- mean(data.reduced$pcharter[data.reduced$y==1])\nprice.charter    <- mean(data.reduced$pcharter)\n# Run probit regression:\nreg.probit <- glm(y ~ lnrelp,\n                  data=data.reduced,\n                  family=binomial(link=\"probit\"))\n# Run Logit regression:\nreg.logit <- glm(y ~ lnrelp,\n                 data=data.reduced,\n                 family=binomial(link=\"logit\"))\n# Run OLS regression:\nreg.OLS <- lm(y ~ lnrelp,\n              data=data.reduced)\n# Replicates Table 14.2 of Cameron and Trivedi:\nstargazer::stargazer(reg.logit, reg.probit, reg.OLS,\n                     type=\"text\")## \n## ================================================================\n##                                 Dependent variable:             \n##                     --------------------------------------------\n##                                          y                      \n##                     logistic   probit             OLS           \n##                        (1)       (2)              (3)           \n## ----------------------------------------------------------------\n## lnrelp              -1.823*** -1.056***        -0.243***        \n##                      (0.145)   (0.075)          (0.010)         \n##                                                                 \n## Constant            2.053***  1.194***          0.784***        \n##                      (0.169)   (0.088)          (0.013)         \n##                                                                 \n## ----------------------------------------------------------------\n## Observations           630       630              630           \n## R2                                               0.463          \n## Adjusted R2                                      0.462          \n## Log Likelihood      -206.827  -204.411                          \n## Akaike Inf. Crit.    417.654   412.822                          \n## Residual Std. Error                         0.330 (df = 628)    \n## F Statistic                             542.123*** (df = 1; 628)\n## ================================================================\n## Note:                                *p<0.1; **p<0.05; ***p<0.01"},{"path":"microeconometrics.html","id":"predictions","chapter":"3 Microeconometrics","heading":"3.1.8 Predictions","text":"define predicted outcomes? case \\(y_i\\), predicted outcomes \\(\\hat{y}_i\\) need valued \\(\\{0,1\\}\\). natural choice consists considering \\(\\hat{y}_i=1\\) \\(\\mathbb{P}(y_i=1|\\mathbf{x}_i;\\boldsymbol\\theta) > 0.5\\), .e., taking cutoff \\(c=0.5\\).However, may models predicted probabilities small, less others. context, model-implied probability 10% (say) characterize “high-risk” entity. However, using cutoff 50% identify level riskiness.receiver operating characteristics (ROC) curve consitutes general approach. works follows:potential cutoff \\(c \\[0,1]\\), compute (plot):fraction \\(y = 1\\) values correctly classified (True Positive Rate) againstThe fraction \\(y = 0\\) values incorrectly specified (False Positive Rate).curve mechanically starts (0,0) [situation \\(c=1\\)] terminates (1,1) [situation \\(c=0\\)].case predictive ability (worst situation), ROC curve straight line (0,0) (1,1).","code":"\nlibrary(pROC)\npredict_model <- predict.glm(reg.probit,type = \"response\")\nroc(data.reduced$y, predict_model, percent=T,\n    boot.n=1000, ci.alpha=0.9, stratified=T, plot=TRUE, grid=TRUE,\n    show.thres=TRUE, legacy.axes = TRUE, reuse.auc = TRUE,\n    print.auc = TRUE, print.thres.col = \"blue\", ci=TRUE,\n    ci.type=\"bars\", print.thres.cex = 0.7, col = 'red',\n    main = paste(\"ROC curve using\",\"(N = \",nrow(data.reduced),\")\") )## \n## Call:\n## roc.default(response = data.reduced$y, predictor = predict_model,     percent = T, ci = TRUE, plot = TRUE, boot.n = 1000, ci.alpha = 0.9,     stratified = T, grid = TRUE, show.thres = TRUE, legacy.axes = TRUE,     reuse.auc = TRUE, print.auc = TRUE, print.thres.col = \"blue\",     ci.type = \"bars\", print.thres.cex = 0.7, col = \"red\", main = paste(\"ROC curve using\",         \"(N = \", nrow(data.reduced), \")\"))\n## \n## Data: predict_model in 178 controls (data.reduced$y 0) < 452 cases (data.reduced$y 1).\n## Area under the curve: 91.69%\n## 95% CI: 89.5%-93.87% (DeLong)"},{"path":"appendix.html","id":"appendix","chapter":"4 Appendix","heading":"4 Appendix","text":"","code":""},{"path":"appendix.html","id":"statistical-tables","chapter":"4 Appendix","heading":"4.1 Statistical Tables","text":"Table 4.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 4.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 4.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 4.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"appendix.html","id":"statistics-defintitions-and-results","chapter":"4 Appendix","heading":"4.2 Statistics: defintitions and results","text":"Definition 4.1  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).{skewness} \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]{kurtosis} \\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Definition 4.2  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 4.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 4.3  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 4.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 4.4  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 4.4 quantiles.)Definition 4.5  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 4.2 quantiles.)Definition 4.6  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 4.3 quantiles.)Definition 4.7  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 4.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 4.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 4.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 4.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 4.6  (Chebychev's inequality) \\(\\mathbb{E}(|X|^r)\\) finite \\(r>0\\) :\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}.\n\\]\nparticular, \\(r=2\\):\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}.\n\\]Proof. Remark \\(\\varepsilon^r \\mathbb{}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) take expectation sides.Definition 4.8  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 4.9  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 4.10  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 4.11  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Proposition 4.7  (Rules limiting distributions (Slutsky)) :Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Proposition 4.8  (Implications stochastic convergences) :\n\\[\\begin{align*}\n&\\boxed{\\overset{L^s}{\\rightarrow}}& &\\underset{1 \\le r \\le s}{\\Rightarrow}& &\\boxed{\\overset{L^r}{\\rightarrow}}&\\\\\n&& && &\\Downarrow&\\\\\n&\\boxed{\\overset{.s.}{\\rightarrow}}& &\\Rightarrow& &\\boxed{\\overset{p}{\\rightarrow}}& \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}.\n\\end{align*}\\]Proof. (fact \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting \\(F\\) \\(F_n\\) c.d.f. \\(X\\) \\(X_n\\), respectively:\n\\[\\begin{equation}\nF_n(x) = \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X > x+\\varepsilon) \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\\tag{4.1}\n\\end{equation}\\]\nBesides,\n\\[\nF(x-\\varepsilon) = \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n > x) \\le F_n(x) + \\mathbb{P}(|X_n - X|>\\varepsilon),\n\\]\nimplies:\n\\[\\begin{equation}\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x).\\tag{4.2}\n\\end{equation}\\]\nEqs. (4.1) (4.2) imply:\n\\[\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x)  \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\n\\]\nTaking limits \\(n \\rightarrow \\infty\\) yields\n\\[\nF(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x)  \\le F(x+\\varepsilon).\n\\]\nresult obtained taking limits \\(\\varepsilon \\rightarrow 0\\) (\\(F\\) continuous \\(x\\)).Proposition 4.9  (Convergence distribution constant) \\(X_n\\) converges distribution constant \\(c\\), \\(X_n\\) converges probability \\(c\\).Proof. \\(\\varepsilon>0\\), \\(\\mathbb{P}(X_n < c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) .e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) \\(\\mathbb{P}(X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\),\ngives result.Example \\(plim\\) \\(L^r\\) convergence: Let \\(\\{x_n\\}_{n \\\\mathbb{N}}\\) series random variables defined :\n\\[\nx_n = n u_n,\n\\]\n\\(u_n\\) independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\).\\(x_n \\overset{p}{\\rightarrow} 0\\) \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\).Theorem 4.1  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 4.2  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]Definition 4.12  (Characteristic function) real-valued random variable \\(X\\), characteristic function defined :\n\\[\n\\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)].\n\\]Theorem 4.3  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. 4.12) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 4.4  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n &=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).Proposition 4.10  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Proposition 4.11  \\(\\mathbf{}\\) idempotent \\(\\mathbf{x}\\) Gaussian, \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}\\) independent \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\).Proof. \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\), two Gaussian vectors \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{}\\mathbf{x}\\) independent. implies independence function \\(\\mathbf{L}\\mathbf{x}\\) function \\(\\mathbf{}\\mathbf{x}\\). results follows observation \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}=(\\mathbf{}\\mathbf{x})'(\\mathbf{}\\mathbf{x})\\), function \\(\\mathbf{}\\mathbf{x}\\).Proposition 4.12  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.Theorem 4.5  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 4.13  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 4.13  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Definition 4.14  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) {asymptotic level} equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha,\n\\]\n\\(S_n\\) test statistic \\(\\Theta\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\\\Theta\\).Definition 4.15  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1,\n\\]\n\\(S_n\\) test statistic \\(\\Theta^c\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\notin \\Theta^c\\).Definition 4.16  (Kullback discrepancy) Given two p.d.f. \\(f\\) \\(f^*\\), Kullback discrepancy defined :\n\\[\n(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy.\n\\]Proposition 4.14  (Properties Kullback discrepancy) :\\((f,f^*) \\ge 0\\)\\((f,f^*) \\ge 0\\)\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).Proof. \\(x \\rightarrow -\\log(x)\\) convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves ()). Since \\(x \\rightarrow -\\log(x)\\) strictly convex, equality () holds \\(f(Y)/f^*(Y)\\) constant (proves (ii)).","code":""},{"path":"appendix.html","id":"proofs","chapter":"4 Appendix","heading":"4.3 Proofs","text":"","code":""},{"path":"appendix.html","id":"MLEproperties","chapter":"4 Appendix","heading":"4.3.1 Proof of Proposition ??","text":"Proof. Assumptions () (ii) (set Assumptions ??) imply \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\)).\\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) can interpreted sample mean r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) ..d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – exists (Assumption iv).latter convergence uniform (Assumption v), solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges solution limit problem:\n\\[\n\\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy.\n\\]Properties Kullback information measure (see Prop. 4.14), together identifiability assumption (ii) implies solution limit problem unique equal \\(\\boldsymbol\\theta_0\\).Consider r.v. sequence \\(\\boldsymbol\\theta\\) converges \\(\\boldsymbol\\theta_0\\). Taylor expansion score neighborood \\(\\boldsymbol\\theta_0\\) yields :\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\]\\(\\boldsymbol\\theta_{MLE}\\) converges \\(\\boldsymbol\\theta_0\\) satisfies likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}\\). Therefore:\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]\nequivalently:\n\\[\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]law large numbers, : \\(\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\mathbf{}(\\boldsymbol\\theta_0) = \\mathcal{}_Y(\\boldsymbol\\theta_0)\\).Besides, :\n\\[\\begin{eqnarray*}\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} &=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\\n&=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right)\n\\end{eqnarray*}\\]\nconverges \\(\\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0))\\) CLT.Collecting preceding results leads (b). fact \\(\\boldsymbol\\theta_{MLE}\\) achieves FDCR bound proves (c).","code":""},{"path":"appendix.html","id":"Walddistri","chapter":"4 Appendix","heading":"4.3.2 Proof of Proposition ??","text":"Proof. \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (eq:normMLE)). Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields :\n\\[\\begin{equation}\n\\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{4.3}\n\\end{equation}\\]\n\\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore:\n\\[\\begin{equation}\n\\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{4.4}\n\\end{equation}\\]\nHence\n\\[\n\\sqrt{n} \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right).\n\\]\nTaking quadratic form, obtain:\n\\[\nn h(\\hat{\\boldsymbol\\theta}_{n})'  \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]fact test asymptotic level \\(\\alpha\\) directly stems precedes. Consistency test: Consider \\(\\theta_0 \\\\Theta\\). MLE consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (4.3) still valid. implies \\(\\xi^W_n\\) converges \\(+\\infty\\) therefore \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\).","code":""},{"path":"appendix.html","id":"LMdistri","chapter":"4 Appendix","heading":"4.3.3 Proof of Proposition ??","text":"Proof. Notations: “\\(\\approx\\)” means “equal term converges 0 probability”. \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes unconstrained one.combine two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (definition) get:\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{4.5}\n\\end{equation}\\]\nBesides, (using definition information matrix):\n\\[\\begin{equation}\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{4.6}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{4.7}\n\\end{equation}\\]\nTaking difference multiplying \\(\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\):\n\\[\\begin{equation}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\mathcal{}(\\boldsymbol\\theta_0).\\tag{4.8}\n\\end{equation}\\]\nEqs. (4.5) (4.8) yield :\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}.\\tag{4.9}\n\\end{equation}\\]Recall \\(\\hat{\\boldsymbol\\theta}^0_n\\) MLE \\(\\boldsymbol\\theta_0\\) constraint \\(h(\\boldsymbol\\theta)=0\\). vector Lagrange multipliers \\(\\hat\\lambda_n\\) associated program satisfies:\n\\[\\begin{equation}\n\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{4.10}\n\\end{equation}\\]\nSubstituting latter equation Eq. (4.9) gives:\n\\[\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}}\n\\]\nyields:\n\\[\\begin{equation}\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{4.11}\n\\end{equation}\\]\nfollows, Eq. (4.4), :\n\\[\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\\right).\n\\]\nTaking quadratic form last equation gives:\n\\[\n\\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]\nUsing Eq. (4.10), appears left-hand side term last equation \\(\\xi^{LM}\\) defined Eq. (??). Consistency: see Remark 17.3 Gouriéroux Monfort (1995).","code":""},{"path":"appendix.html","id":"equivWaldLM","chapter":"4 Appendix","heading":"4.3.4 Proof of Proposition ??","text":"Proof. (using Eq. (eq:multiplier)):\n\\[\n\\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n.\n\\]\nSince, \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (4.11) therefore implies :\n\\[\n\\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)' \\left(\n\\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\nh(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W},\n\\]\ngives result.","code":""},{"path":"appendix.html","id":"equivLRLM","chapter":"4 Appendix","heading":"4.3.5 Proof of Proposition ??","text":"Proof. second-order taylor expansions \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y})\\) \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y})\\) :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nTaking difference, obtain:\n\\[\n\\xi_n^{LR} \\approx 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nUsing \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (4.7)), :\n\\[\n\\xi_n^{LR} \\approx\n2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)'\\mathcal{}(\\boldsymbol\\theta_0)\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n)\n+ n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nsecond three terms sum, replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) develop associated product. leads :\n\\[\\begin{equation}\n\\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)' \\mathcal{}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{4.12}\n\\end{equation}\\]\ndifference Eqs. (4.6) (4.7) implies:\n\\[\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n),\n\\]\n, associated Eq. @(eq:lr10), gives:\n\\[\n\\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}.\n\\]\nHence \\(\\xi_n^{LR}\\) asymptotic distribution \\(\\xi_n^{LM}\\).Let’s show test consistent. , note :\n\\[\n\\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\mathbf{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\mathbf{y})}{n} = \\frac{1}{n} \\sum_{=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)] \\rightarrow \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)],\n\\]\n\\(\\boldsymbol\\theta_\\infty\\), pseudo true value, \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (definition \\(H_1\\)). Kullback inequality asymptotic identifiability \\(\\boldsymbol\\theta_0\\), follows \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] >0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) \\(H_1\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]

[{"path":"index.html","id":"intro","chapter":"1 Before starting","heading":"1 Before starting","text":"course covers various econometric topics, including linear regression models, discrete-choice models, time series analysis. provides examples simulations based R codes.R codes use various packages can obtained CRAN. Several pieces code also involve procedures data AEC package. latter available GitHub. install , one need employ devtools library:Useful (R) links:Download R:\nR software: https://cran.r-project.org (basic R software)\nRStudio: https://www.rstudio.com (convenient R editor)\nDownload R:R software: https://cran.r-project.org (basic R software)RStudio: https://www.rstudio.com (convenient R editor)Tutorials:\nRstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)\nR: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)\ntutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/\nTutorials:Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/","code":"\nlibrary(devtools)\ninstall_github(\"jrenne/AEC\")\nlibrary(AEC)"},{"path":"TS.html","id":"TS","chapter":"2 Time Series","heading":"2 Time Series","text":"","code":""},{"path":"TS.html","id":"VAR","chapter":"2 Time Series","heading":"2.1 Multivariate models","text":"section presents Vector Auto-Regressive Moving-Average (SVARMA) models. models widely used macroeconomic analysis. simple easy estimate, make possible conveniently capture dynamics complex multivariate systems. VAR popularity notably due Sims (1980)’s influential work. nice survey proposed J. H. Stock Watson (2016).economics, VAR models often employed order identify structural shocks, independent primitive exogenous forces drive economic variables (Ramey (2016)). often given specific economic meaning (e.g., demand supply shocks).Working models (VAR VARMA models) often often based two steps: first step, reduced-form version model estimated; second step, structural shocks identified IRFs produced.Kilian (1998) See pageSign restrictions: package, Danne (2015).Pfaff (2008)\nKorobilis (2013): R package estimate Bayesian VAR models.","code":"\nlibrary(VAR.etp)\nlibrary(vars) #standard VAR models\ndata(dat) # part of VAR.etp package\na <- VAR.Boot(dat,p=2,nb=200,type=\"const\")\nb <- VAR(dat,p=2)\nrbind(a$coef[1,],(a$coef+a$Bias)[1,],b$varresult$inv$coefficients)##         inv(-1)   inc(-1)   con(-1)    inv(-2)   inc(-2)   con(-2)       const\n## [1,] -0.3296108 0.1805057 0.9498654 -0.1352914 0.1080949 0.9324146 -0.01705139\n## [2,] -0.3196310 0.1459888 0.9612190 -0.1605511 0.1146050 0.9343938 -0.01672199\n## [3,] -0.3196310 0.1459888 0.9612190 -0.1605511 0.1146050 0.9343938 -0.01672199"},{"path":"TS.html","id":"definition-of-vars-and-svarma-models","chapter":"2 Time Series","heading":"2.1.1 Definition of VARs (and SVARMA) models","text":"Definition 2.1  ((S)VAR model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector random variables. Process \\(y_{t}\\) follows \\(p^{th}\\)-order VAR , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t,\\\\\nSVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B \\eta_t,\n\\end{array}\\tag{2.1}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B\\eta_t\\). assume \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.first line Eq. (2.1) corresponds reduced-form VAR model (structural form second line).structural shocks (components \\(\\eta_t\\)) mutually uncorrelated, case innovations, components \\(\\varepsilon_t\\). However, boths cases, vectors \\(\\eta_t\\) \\(\\varepsilon_t\\) serially correlated (time).case univariate models, VARs can extended MA terms \\(\\eta_t\\):Definition 2.2  ((S)VARMA model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector random variables. Process \\(y_{t}\\) follows VARMA model order (p,q) , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t + \\Theta_1\\varepsilon_{t-1} + \\dots + \\Theta_q ,\\\\\nSVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B_0 \\eta_t+ B_1 \\eta_{t-1} + \\dots +  B_q \\eta_{t-q},\n\\end{array}\\tag{2.2}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B_0\\eta_t\\) (\\(B_j = \\Theta_j B_0\\), \\(j \\ge 0\\)). assume \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.","code":""},{"path":"TS.html","id":"IRFSVARMA","chapter":"2 Time Series","heading":"2.1.2 IRFs SVARMA","text":"One main objectives macro-econometrics derive IRFs, represent dynamic effects structural shocks (components \\(\\eta_t\\)) though system variables \\(y_t\\).Formally, IRF difference conditional expectations:\n\\[\n\\boxed{\\color{blue}{\\Psi_{,j,h}} = \\mathbb{E}(y_{,t+h}|\\color{blue}{\\eta_{j,t}=1}) - \\mathbb{E}(y_{,t+h})}\n\\]\n(effect \\(y_{,t+h}\\) one-unit shock \\(\\eta_{j,t}\\)).dynamics process \\(y_t\\) can described VARMA model, \\(y_t\\) covariance stationary (see Def. ??), \\(y_t\\) admits following infinite MA representation (MA(\\(\\infty\\))):\n\\[\\begin{equation}\ny_t = \\mu + \\sum_{h=0}^\\infty \\color{blue}{\\Psi_{h}} \\eta_{t-h}.\\tag{2.3}\n\\end{equation}\\]\nalso Wold decomposition process \\(\\{y_t\\}\\) (see Theorem ??).Estimating IRFs amounts estimating \\(\\Psi_{h}\\)’s. general, exist three main approaches :Calibrate solve (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model first order (linearization). solution takes form Eq. (2.3).Directly estimate \\(\\Psi_{h}\\) based projection approaches (see Section ??).Approximate infinite MA representation estimating parsimonious type model, e.g. VAR(MA) models (see Section 2.1.4). (Structural) VARMA representation obtained, Eq. (2.3) easily deduced. , one can use recursive algorithm univariate processes (see Prop. ??).Typically, consider AR(2) case. first steps algorithm follows:\n\\[\\begin{eqnarray*}\ny_t &=& \\Phi_1 {\\color{blue}y_{t-1}} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& \\Phi_1 \\color{blue}{(\\Phi_1 y_{t-2} + \\Phi_2 y_{t-3} + B \\eta_{t-1})} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{y_{t-2}} + \\Phi_1\\Phi_2 y_{t-3}  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{(\\Phi_1 y_{t-3} + \\Phi_2 y_{t-4} + B \\eta_{t-2})} + \\Phi_1\\Phi_2 y_{t-3} \\\\\n&=& \\underbrace{B}_{=\\Psi_0} \\eta_t + \\underbrace{\\Phi_1 B}_{=\\Psi_1} \\eta_{t-1} + \\underbrace{(\\Phi_2 + \\Phi_1^2)B}_{=\\Psi_2} \\eta_{t-2} + f(y_{t-3},y_{t-4}).\n\\end{eqnarray*}\\]particular, \\(B = \\Psi_0\\). Matrix \\(B\\) indeed captures contemporaneous impact \\(\\eta_t\\) \\(y_t\\).Let us consider following VARMA(1,1) model:\n\\[\\begin{eqnarray}\n\\quad y_t &=&\n\\underbrace{\\left[\\begin{array}{cc}\n0.5 & 0.3 \\\\\n-0.4 & 0.7\n\\end{array}\\right]}_{\\Phi_1}\ny_{t-1} +  \n\\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_t + \\underbrace{\\left[\\begin{array}{cc}\n2 & 0 \\\\\n1 & 0.5\n\\end{array}\\right]}_{\\Theta_1} \\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_{t-1}.\\tag{2.4}\n\\end{eqnarray}\\]can use function simul.VARMA package AEC produce IRFs (using indic.IRF=1 list arguments):\nFigure 2.1: Impulse response functions\n","code":"\nlibrary(AEC)\ndistri <- list(type=c(\"gaussian\",\"gaussian\"),df=c(4,4))\nn <- length(distri$type) # dimension of y_t\nnb.sim <- 30\neps <- simul.distri(distri,nb.sim)\nPhi <- array(NaN,c(n,n,1))\nPhi[,,1] <- matrix(c(.5,-.4,.3,.7),2,2)\np <- dim(Phi)[3]\nTheta <- array(NaN,c(n,n,1))\nTheta[,,1] <- -matrix(c(2,1,0,.5),2,2)\nq <- dim(Theta)[3]\nMu <- rep(0,n)\nC <- matrix(c(1,-1,2,1),2,2)\nModel <- list(\n  Mu = Mu,Phi = Phi,Theta = Theta,C = C,distri = distri\n)\nY0 <- rep(0,n)\neta0 <- c(1,0)\nres.sim.1 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)\neta0 <- c(0,1)\nres.sim.2 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)\n\npar(plt=c(.1,.95,.25,.8))\npar(mfrow=c(2,2))\nplot(res.sim.1$Y[1,],las=1,\n     type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n     main=expression(paste(\"Response of \",y[1,\"*,*\",t],\" to a one-unit increase in \",eta[1],sep=\"\")))\nabline(h=0,col=\"grey\",lty=3)\nplot(res.sim.2$Y[1,],las=1,\n     type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n     main=expression(paste(\"Response of \",y[1,\"*,*\",t],\" to a one-unit increase in \",eta[2],sep=\"\")))\nabline(h=0,col=\"grey\",lty=3)\nplot(res.sim.1$Y[2,],las=1,\n     type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n     main=expression(paste(\"Response of \",y[2,\"*,*\",t],\" to a one-unit increase in \",eta[1],sep=\"\")))\nabline(h=0,col=\"grey\",lty=3)\nplot(res.sim.2$Y[2,],las=1,\n     type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n     main=expression(paste(\"Response of \",y[2,\"*,*\",t],\" to a one-unit increase in \",eta[2],sep=\"\")))\nabline(h=0,col=\"grey\",lty=3)"},{"path":"TS.html","id":"covariance-stationary-varma-models","chapter":"2 Time Series","heading":"2.1.3 Covariance-stationary VARMA models","text":"Let’s come back infinite MA case (Eq. (2.3)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\color{blue}{\\Psi_{h}} \\eta_{t-h}.\n\\]\n\\(y_t\\) covariance-stationary (ergodic mean), case \n\\[\\begin{equation}\n\\sum_{=0}^\\infty \\|\\Psi_i\\| < \\infty,\\tag{2.5}\n\\end{equation}\\]\n\\(\\|\\|\\) denotes norm matrix \\(\\) (e.g. \\(\\|\\|=\\sqrt{tr(AA')}\\)).notably implies \\(y_t\\) stationary (ergodic mean), \\(\\|\\Psi_h\\|\\rightarrow 0\\) \\(h\\) gets large.satisfied \\(\\Phi_k\\)’s \\(\\Theta_k\\)’s VARMA-based process (Eq. @ref{eq:VARMAstd)) stationary? conditions similar univariate case (see Prop. ??). Let us introduce following notations:\n\\[\\begin{eqnarray}\ny_t &=& c + \\underbrace{\\Phi_1 y_{t-1} + \\dots +\\Phi_p y_{t-p}}_{\\color{blue}{\\mbox{AR component}}} +  \\tag{2.6}\\\\\n&&\\underbrace{B \\eta_t+ \\Theta_1 B \\eta_{t-1}+ \\dots+ \\Theta_q B \\eta_{t-q}}_{\\color{red}{\\mbox{MA component}}} \\nonumber\\\\\n&\\Leftrightarrow& \\underbrace{(- \\Phi_1 L - \\dots - \\Phi_p L^p)}_{= \\color{blue}{\\Phi(L)}}y_t = c +  \\underbrace{ \\color{red}{(- \\Theta_1 L - \\ldots - \\Theta_q L^q)}}_{=\\color{red}{\\Theta(L)}} B \\eta_{t}. \\nonumber\n\\end{eqnarray}\\]Process \\(y_t\\) stationary iff roots \\(\\det(\\Phi(z))=0\\) strictly outside unit circle , equivalently, iff eigenvalues \n\\[\\begin{equation}\n\\Phi = \\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]\\tag{2.7}\n\\end{equation}\\]\nlie strictly within unit circle.Hence, case univariate processes, covariance-stationarity VARMA model depends specification AR part.Let’s derive first two unconditional moments (covariance-stationary) VARMA process.Based Eq. (2.6), \\(\\mathbb{E}(\\Phi(L)y_t)=c\\), gives \\(\\Phi(1)\\mathbb{E}(y_t)=c\\), ::\n\\[\n\\mathbb{E}(y_t) = (- \\Phi_1 - \\dots - \\Phi_p)^{-1}c.\n\\]\nautocovariances \\(y_t\\) can deduced infinite MA representation (Eq. (2.3)). :\n\\[\n\\gamma_j \\equiv \\mathbb{C}ov(y_t,y_{t-j}) = \\sum_{=j}^\\infty \\Psi_i \\Psi_{-j}'.\n\\]\n(Note infinite sum exists soon Eq. (2.5) satisfied.)Conditional means autocovariances can also deduced Eq. (2.3). \\(0 \\le h\\) \\(0 \\le h_1 \\le h_2\\):\n\\[\\begin{eqnarray*}\n\\mathbb{E}_t(y_{t+h}) &=& \\mu + \\sum_{k=0}^\\infty \\Psi_{k+h} \\eta_{t-k} \\\\\n\\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &=& \\sum_{k=0}^{h_1} \\Psi_{k}\\Psi_{k+h_2-h_1}'.\n\\end{eqnarray*}\\]previous formula implies particular forecasting error \\(y_{t+h} - \\mathbb{E}_t(y_{t+h})\\) variance equal :\n\\[\n\\mathbb{V}ar_t(y_{t+h}) = \\sum_{k=1}^{h} \\Psi_{k}\\Psi_{k}'.\n\\]\n\\(\\eta_t\\) mutually serially independent (therefore uncorrelated), :\n\\[\n\\mathbb{V}ar(\\Psi_k \\eta_{t-k}) = \\mathbb{V}ar\\left(\\sum_{=1}^n \\psi_{k,} \\eta_{,t-k}\\right)  = \\sum_{=1}^n \\psi_{k,}\\psi_{k,}',\n\\]\n\\(\\psi_{k,}\\) denotes \\(^{th}\\) column \\(\\Psi_k\\).suggests following decomposition variance forecast error (called variance decomposition):\n\\[\n\\mathbb{V}ar_t(y_{t+h}) = \\sum_{=1}^n \\underbrace{\\sum_{k=1}^{h}  \\psi_{k,}\\psi_{k,}'}_{\\mbox{Contribution $\\eta_{,t}$}}.\n\\]Let us now turn estimation VAR(MA) models.MA component, OLS regressions yield biased estimates (even asymptotically large samples).Assume \\(y_t\\) follows VARMA(1,1) model. :\n\\[\ny_{,t} = \\phi_i y_{t-1} + \\varepsilon_{,t},\n\\]\n\\(\\phi_i\\) \\(^{th}\\) row \\(\\Phi_1\\), \\(\\varepsilon_{,t}\\) linear combination \\(\\eta_t\\) \\(\\eta_{t-1}\\).Since \\(y_{t-1}\\) (regressor) correlated \\(\\eta_{t-1}\\), also correlated \\(\\varepsilon_{,t}\\).OLS regression \\(y_{,t}\\) \\(y_{t-1}\\) yields biased estimator \\(\\phi_i\\).Estimation methods VARMA models presented Section ??.","code":""},{"path":"TS.html","id":"estimVAR","chapter":"2 Time Series","heading":"2.1.4 VAR estimation","text":"section discusses estimation VAR models. (estimation SVARMA models challenging, see, e.g., Gouriéroux, Monfort, Renne (2020).) Eq. (2.1) can written:\n\\[\ny_{t}=c+\\Phi(L)y_{t-1}+\\varepsilon_{t},\n\\]\n\\(\\Phi(L) = \\Phi_1 + \\Phi_2 L + \\dots + \\Phi_p L^{p-1}\\).Consequently:\n\\[\ny_{t}\\mid y_{t-1},y_{t-2},\\ldots,y_{-p+1}\\sim \\mathcal{N}(c+\\Phi_{1}y_{t-1}+\\ldots\\Phi_{p}y_{t-p},\\Omega).\n\\]Using Hamilton (1994)’s notations, denote \\(\\Pi\\) matrix \\(\\left[\\begin{array}{ccccc} c & \\Phi_{1} & \\Phi_{2} & \\ldots & \\Phi_{p}\\end{array}\\right]'\\) \\(x_{t}\\) vector \\(\\left[\\begin{array}{ccccc} 1 & y'_{t-1} & y'_{t-2} & \\ldots & y'_{t-p}\\end{array}\\right]'\\), :\n\\[\\begin{equation}\ny_{t}= \\Pi'x_{t} + \\varepsilon_{t}. \\tag{2.8}\n\\end{equation}\\]\nprevious representation convenient discuss estimation VAR model, parameters gathered two matrices : \\(\\Pi\\) \\(\\Omega\\).Let us start case shocks Gaussian.Proposition 2.1  (MLE Gaussian VAR) \\(y_t\\) follows VAR(p) (see Definition 2.1), \\(\\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,\\Omega)\\), ML estimate \\(\\Pi\\), denoted \\(\\hat{\\Pi}\\) (see Eq. (2.8)), given \n\\[\\begin{equation}\n\\hat{\\Pi}=\\left[\\sum_{t=1}^{T}x_{t}x'_{t}\\right]^{-1}\\left[\\sum_{t=1}^{T}y_{t}'x_{t}\\right]= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y},\\tag{2.9}\n\\end{equation}\\]\n\\(\\mathbf{X}\\) \\(T \\times (np)\\) matrix whose \\(t^{th}\\) row \\(x_t\\) \\(\\mathbf{y}\\) \\(T \\times n\\) matrix whose \\(t^{th}\\) row \\(y_{t}'\\)., \\(^{th}\\) column \\(\\hat{\\Pi}\\) (\\(b_i\\), say) OLS estimate \\(\\beta_i\\), :\n\\[\\begin{equation}\ny_{,t} = \\beta_i'x_t + \\varepsilon_{,t},\\tag{2.10}\n\\end{equation}\\]\n(.e., \\(\\beta_i' = [c_i,\\phi_{,1}',\\dots,\\phi_{,p}']'\\)).ML estimate \\(\\Omega\\), denoted \\(\\hat{\\Omega}\\), coincides sample covariance matrix \\(n\\) series OLS residuals Eq. (2.10), .e.:\n\\[\\begin{equation}\n\\hat{\\Omega} = \\frac{1}{T} \\sum_{=1}^T \\hat{\\varepsilon}_t\\hat{\\varepsilon}_t',\\quad\\mbox{} \\hat{\\varepsilon}_t= y_t - \\hat{\\Pi}'x_t.\n\\end{equation}\\]asymptotic distributions estimators ones resulting standard OLS formula.Proof. See Appendix 3.4.7.stated Proposition 3.4.8, shocks Gaussian, OLS regressions still provide consistent estimates model parameters. However, since \\(x_t\\) correlates \\(\\varepsilon_s\\) \\(s<t\\), OLS estimator \\(\\mathbf{b}_i\\) \\(\\boldsymbol\\beta_i\\) biased small sample. (also case ML estimator.)Indeed, denoting \\(\\boldsymbol\\varepsilon_i\\) \\(T \\times 1\\) vector \\(\\varepsilon_{,t}\\)’s, using notations \\(b_i\\) \\(\\beta_i\\) introduced Proposition 2.1, :\n\\[\\begin{equation}\n\\mathbf{b}_i = \\beta_i + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i.\\tag{2.11}\n\\end{equation}\\]\nnon-zero correlation \\(x_t\\) \\(\\varepsilon_{,s}\\) \\(s<t\\) , therefore, \\(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i] \\ne 0\\).However, \\(y_t\\) covariance stationary, \\(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\) converges positive definite matrix \\(\\mathbf{Q}\\), \\(\\frac{1}{n}X'\\boldsymbol\\varepsilon_i\\) converges 0. Hence \\(\\mathbf{b}_i \\overset{p}{\\rightarrow} \\beta_i\\). precisely:Proposition 2.2  (Asymptotic distribution OLS estimate $\\beta_i$) \\(y_t\\) follows VAR model, defined Definition 2.1, :\n\\[\n\\sqrt{T}(\\mathbf{b}_i-\\beta_i) =  \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T x_t x_t' \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}}\n\\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T x_t\\varepsilon_{,t} \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma_i^2\\mathbf{Q})},\n\\]\n\\(\\sigma_i = \\mathbb{V}ar(\\varepsilon_{,t})\\) \\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) given :\n\\[\\begin{equation}\n\\mathbf{Q} = \\left[\n\\begin{array}{ccccc}\n1 & \\mu' &\\mu' & \\dots & \\mu' \\\\\n\\mu & \\gamma_0 + \\mu\\mu' & \\gamma_1 + \\mu\\mu' & \\dots & \\gamma_{p-1} + \\mu\\mu'\\\\\n\\mu & \\gamma_1 + \\mu\\mu' & \\gamma_0 + \\mu\\mu' & \\dots & \\gamma_{p-2} + \\mu\\mu'\\\\\n\\vdots &\\vdots &\\vdots &\\dots &\\vdots \\\\\n\\mu & \\gamma_{p-1} + \\mu\\mu' & \\gamma_{p-2} + \\mu\\mu' & \\dots & \\gamma_{0} + \\mu\\mu'\n\\end{array}\n\\right].\\tag{2.12}\n\\end{equation}\\]Proof. See Appendix 3.4.8.following proposition extends previous proposition includes covariances different \\(\\beta_i\\)’s well asymptotic distribution ML estimates \\(\\Omega\\).Proposition 2.3  (Asymptotic distribution OLS estimates) \\(y_t\\) follows VAR model, defined Definition 2.1, :\n\\[\\begin{equation}\n\\sqrt{T}\\left[\n\\begin{array}{c}\nvec(\\hat\\Pi - \\Pi)\\\\\nvec(\\hat\\Omega - \\Omega)\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\\left(0,\n\\left[\n\\begin{array}{cc}\n\\Omega \\otimes \\mathbf{Q}^{-1} & 0\\\\\n0 & \\Sigma_{22}\n\\end{array}\n\\right]\\right),\\tag{2.13}\n\\end{equation}\\]\ncomponent \\(\\Sigma_{22}\\) corresponding covariance \\(\\hat\\sigma_{,j}\\) \\(\\hat\\sigma_{k,l}\\) (\\(,j,l,m \\\\{1,\\dots,n\\}^4\\)) equal \\(\\sigma_{,l}\\sigma_{j,m}+\\sigma_{,m}\\sigma_{j,l}\\).Proof. See Hamilton (1994), Appendix Chapter 11.Naturally, practice, \\(\\Omega\\) replaced \\(\\hat{\\Omega}\\), \\(Q\\) replaced \\(\\hat{\\mathbf{Q}} = \\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) \\(\\Sigma\\) matrix whose components form \\(\\hat\\sigma_{,l}\\hat\\sigma_{j,m}+\\hat\\sigma_{,m}\\hat\\sigma_{j,l}\\), \\(\\hat\\sigma_{,l}\\)’s components \\(\\hat\\Omega\\).simplicity VAR framework tractability MLE open way convenient econometric testing. Let’s illustrate likelihood ratio test. maximum value achieved MLE \n\\[\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega}) = -\\frac{Tn}{2}\\log(2\\pi)+\\frac{T}{2}\\log\\left|\\hat{\\Omega}^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\]\nlast term :\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t} &=& \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right] = \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right]\\\\\n&=&\\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right] = \\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\left(T\\hat{\\Omega}\\right)\\right]=Tn.\n\\end{eqnarray*}\\]\nTherefore, optimized log-likelihood simply obtained :\n\\[\\begin{equation}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega})=-(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\hat{\\Omega}^{-1}\\right|-Tn/2.\\tag{2.14}\n\\end{equation}\\]Assume want test null hypothesis set variables follows VAR(\\(p_{0}\\)) alternative\nspecification \\(p_{1}\\) (\\(>p_{0}\\)).Let us denote \\(\\hat{L}_{0}\\) \\(\\hat{L}_{1}\\) maximum log-likelihoods obtained \\(p_{0}\\) \\(p_{1}\\) lags, respectively.null hypothesis (\\(H_0\\): \\(p=p_0\\)), :\n\\[\\begin{eqnarray*}\n2\\left(\\hat{L}_{1}-\\hat{L}_{0}\\right)&=&T\\left(\\log\\left|\\hat{\\Omega}_{1}^{-1}\\right|-\\log\\left|\\hat{\\Omega}_{0}^{-1}\\right|\\right)  \\sim \\chi^2(n^{2}(p_{1}-p_{0})).\n\\end{eqnarray*}\\]precedes can used help determine appropriate number lags use specification. VAR, using many lags consumes numerous degrees freedom: \\(p\\) lags, \\(n\\) equations VAR contains \\(n\\times p\\) coefficients plus intercept term. Adding lags improve -sample fit, likely result -parameterization affect --sample prediction performance.select appropriate lag length, selection criteria can used (see Definition ??). context VAR models, using Eq. (2.14), :\n\\[\\begin{eqnarray*}\nAIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{2}{T}N\\\\\nBIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{\\log T}{T}N,\n\\end{eqnarray*}\\]\n\\(N=p \\times n^{2}\\).","code":""},{"path":"TS.html","id":"BlockGranger","chapter":"2 Time Series","heading":"2.1.5 Block exogeneity and Granger causality","text":"Block exogeneityLet’s decompose \\(y_t\\) two subvectors \\(y^{(1)}_{t}\\) (\\(n_1 \\times 1\\)) \\(y^{(2)}_{t}\\) (\\(n_2 \\times 1\\)), \\(y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']\\) (therefore \\(n=n_1 +n_2\\)), :\n\\[\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t}\\\\\ny^{(2)}_{t}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\n\\Phi^{(1,1)} & \\Phi^{(1,2)}\\\\\n\\Phi^{(2,1)} & \\Phi^{(2,2)}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t-1}\\\\\ny^{(2)}_{t-1}\n\\end{array}\n\\right] + \\varepsilon_t.\n\\]\nUsing, e.g., likelihood ratio test (see Def. ??), one can easily test block exogeneity \\(y_t^{(2)}\\) (say). null assumption can expressed \\(\\Phi^{(2,1)}=0\\) \\(\\Sigma^{(2,1)}=0\\).Granger CausalityGranger (1969) developed method explore causal relationships among variables. approach consists determining whether past values \\(y_{1,t}\\) can help explain current \\(y_{2,t}\\) (beyond information already included past values \\(y_{2,t}\\)).Formally, let us denote three information sets:\n\\[\\begin{eqnarray*}\n\\mathcal{}_{1,t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{2,t} & = & \\left\\{ y_{2,t},y_{2,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots y_{2,t},y_{2,t-1},\\ldots\\right\\}.\n\\end{eqnarray*}\\]\nsay \\(y_{1,t}\\) Granger-causes \\(y_{2,t}\\) \n\\[\n\\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{2,t-1}\\right]\\neq \\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{t-1}\\right].\n\\]get intuition behind testing procedure, consider following\nbivariate VAR(\\(p\\)) process:\n\\[\\begin{eqnarray*}\ny_{1,t} & = & c_1+\\Sigma_{=1}^{p}\\Phi_i^{(11)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(12)}y_{2,t-}+\\varepsilon_{1,t}\\\\\ny_{2,t} & = & c_2+\\Sigma_{=1}^{p}\\Phi_i^{(21)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(22)}y_{2,t-}+\\varepsilon_{2,t},\n\\end{eqnarray*}\\]\n\\(\\Phi_k^{(ij)}\\) denotes element \\((,j)\\) \\(\\Phi_k\\)., \\(y_{1,t}\\) said Granger-cause \\(y_{2,t}\\) \n\\[\n\\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0.\n\\]\nTherefore hypothesis testing \n\\[\n\\begin{cases}\nH_{0}: & \\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0\\\\\nH_{1}: & \\Phi_1^{(21)}\\neq0\\mbox{ }\\Phi_2^{(21)}\\neq0\\mbox{ }\\ldots\\Phi_p^{(21)}\\neq0.\\end{cases}\n\\]\nLoosely speaking, reject \\(H_{0}\\) coefficients lagged \\(y_{1,t}\\)’s statistically significant. Formally, can tested using \\(F\\)-test asymptotic chi-square test. \\(F\\)-statistic \n\\[\nF=\\frac{(RSS-USS)/p}{USS/(T-2p-1)},\n\\]\nRSS Restricted sum squared residuals USS Unrestricted sum squared residuals. \\(H_{0}\\), \\(F\\)-statistic distributed \\(\\mathcal{F}(p,T-2p-1)\\).Note \\(pF\\underset{T \\rightarrow \\infty}{\\rightarrow}\\chi^{2}(p)\\). Therefore, large samples \\(H_0\\):\n\\[\nF \\sim \\chi^{2}(p)/p.\n\\]","code":""},{"path":"TS.html","id":"identification-problem-and-standard-identification-techniques","chapter":"2 Time Series","heading":"2.1.6 Identification problem and standard identification techniques","text":"Identification IssueIn Section 2.1.4, seen estimate \\(\\mathbb{V}ar(\\varepsilon_t) =\\Omega\\) \\(\\Phi_k\\) matrices context VAR model. IRFs functions \\(B\\) \\(\\Phi_k\\)’s, \\(\\Omega\\) \\(\\Phi_k\\)’s (see Section 2.1.2). \\(\\Omega = BB'\\), sufficient recover \\(B\\).Indeed, seen system equations whose unknowns \\(b_{,j}\\)’s (components \\(B\\)), system \\(\\Omega = BB'\\) contains \\(n(n+1)/2\\) linearly independent equations. instance, \\(n=2\\):\n\\[\\begin{eqnarray*}\n&&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{array}\n\\right]\\left[\n\\begin{array}{cc}\nb_{11} & b_{21} \\\\\nb_{12} & b_{22}\n\\end{array}\n\\right]\\\\\n&\\Leftrightarrow&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11}^2+b_{12}^2 & \\color{red}{b_{11}b_{21}+b_{12}b_{22}} \\\\\n\\color{red}{b_{11}b_{21}+b_{12}b_{22}} & b_{22}^2 + b_{21}^2\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]3 linearly independent equations 4 unknowns. Therefore, \\(B\\) identified based second-order moments. Additional restrictions required identify \\(B\\).section covers two standard identification schemes: short-run long-run restrictions:short-run restriction (SRR) prevents structural shock affecting endogenous variable contemporaneously.Easy implement: appropriate entries \\(B\\) set 0.Particular case: Cholesky, recursive approach.Examples: Bernanke (1986), Sims (1986), Galí (1992), Ruibio-Ramírez, Waggoner, Zha (2010).long-run restriction (LRR) prevents structural shock cumulative impact one endogenous variables.Additional computations required implement . One needs compute cumulative effect one structural shocks \\(u_{t}\\) one endogenous variable.Examples: Blanchard Quah (1989), Faust Leeper (1997), Galí (1999), Erceg, Guerrieri, Gust (2005), Christiano, Eichenbaum, Vigfusson (2007).two approaches can combined (see, e.g., Gerlach Smets (1995)).Let us consider simple exmaple motivate short-run restrictions. Consider following stylized macro model:\n\\[\\begin{equation}\n\\begin{array}{clll}\ng_{t}&=& \\bar{g}-\\lambda(i_{t-1}-\\mathbb{E}_{t-1}\\pi_{t})+ \\underbrace{{\\color{blue}\\sigma_d \\eta_{d,t}}}_{\\mbox{demand shock}}& (\\mbox{curve})\\\\\n\\Delta \\pi_{t} & = & \\beta (g_{t} - \\bar{g})+ \\underbrace{{\\color{blue}\\sigma_{\\pi} \\eta_{\\pi,t}}}_{\\mbox{cost push shock}} & (\\mbox{Phillips curve})\\\\\ni_{t} & = & \\rho i_{t-1} + \\left[ \\gamma_\\pi \\mathbb{E}_{t}\\pi_{t+1}  + \\gamma_g (g_{t} - \\bar{g}) \\right]\\\\\n&& \\qquad \\qquad+\\underbrace{{\\color{blue}\\sigma_{mp} \\eta_{mp,t}}}_{\\mbox{Mon. Pol. shock}} & (\\mbox{Taylor rule}),\n\\end{array}\\tag{2.15}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\eta_t =\n\\left[\n\\begin{array}{c}\n\\eta_{\\pi,t}\\\\\n\\eta_{d,t}\\\\\n\\eta_{mp,t}\n\\end{array}\n\\right]\n\\sim ..d.\\,\\mathcal{N}(0,).\\tag{2.16}\n\\end{equation}\\]Vector \\(\\eta_t\\) assumed vector structural shocks, mutually serially independent. date \\(t\\):\\(g_t\\) contemporaneously affected \\(\\eta_{d,t}\\) ;\\(\\pi_t\\) contemporaneously affected \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\);\\(i_t\\) contemporaneously affected \\(\\eta_{mp,t}\\), \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\).System (2.15) rewritten form:\n\\[\\begin{equation}\n\\left[\\begin{array}{c}\nd_t\\\\\n\\pi_t\\\\\ni_t\n\\end{array}\\right]\n= \\Phi(L)\n\\left[\\begin{array}{c}\nd_{t-1}\\\\\n\\pi_{t-1}\\\\\ni_{t-1} +\n\\end{array}\\right] +\\underbrace{\\underbrace{\n\\left[\n\\begin{array}{ccc}\n0 & \\bullet & 0 \\\\\n\\bullet & \\bullet & 0 \\\\\n\\bullet & \\bullet & \\bullet\n\\end{array}\n\\right]}_{=B} \\eta_t}_{=\\varepsilon_t}\\tag{2.17}\n\\end{equation}\\]reduced-form model. representation suggests three additional restrictions entries \\(B\\); latter matrix therefore identified (signs columns) soon \\(\\Omega = BB'\\) known.particular cases well-known matrix decomposition \\(\\Omega=\\mathbb{V}ar(\\varepsilon_t)\\) can used easily estimate specific SVAR.Consider following context:first shock (say, \\(\\eta_{n_1,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) one endogenous variable (say, \\(y_{n_1,t}\\));second shock (say, \\(\\eta_{n_2,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) first two endogenous variables (say, \\(y_{n_1,t}\\)\n\\(y_{n_2,t}\\));…implies (1) column \\(n_1\\) \\(B\\) 1 non-zero entry (\\(n_1^{th}\\) entry), (2) column \\(n_2\\) \\(B\\) 2 non-zero entries (\\(n_1^{th}\\) \\(n_2^{th}\\) ones), etc. Without loss generality, can set \\(n_1=n\\), \\(n_2=n-1\\), etc. context, matrix \\(B\\) lower triangular.Cholesky decomposition \\(\\Omega_{\\varepsilon}\\) provides appropriate estimate \\(B\\), lower triangular matrix \\(B\\) :\n\\[\n\\Omega_\\varepsilon = BB'.\n\\]instance, Dedola Lippi (2005) estimate 5 structural VAR models US, UK, Germany, France Italy analyse monetary-policy transmission mechanisms. estimate SVAR(5) models period 1975-1997. shock-identification scheme based Cholesky decompositions, ordering endogenous variables : industrial production, consumer price index, commodity price index, short-term rate, monetary aggregate effective exchange rate (except US). ordering implies monetary policy reacts shocks affecting first three variables latter react monetary policy shocks one-period lag .Importantly, Cholesky approach can useful one interested one specific structural shock. case, e.g., Christiano, Eichenbaum, Evans (1996). identification based following relationship \\(\\varepsilon_t\\) \\(\\eta_t\\):\n\\[\n\\left[\\begin{array}{c}\n\\boldsymbol\\varepsilon_{S,t}\\\\\n\\varepsilon_{r,t}\\\\\n\\boldsymbol\\varepsilon_{F,t}\n\\end{array}\\right] =\n\\left[\\begin{array}{ccc}\nB_{SS} & 0 & 0 \\\\\nB_{rS} & B_{rr} & 0 \\\\\nB_{FS} & B_{Fr} & B_{FF}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n\\boldsymbol\\eta_{S,t}\\\\\n\\eta_{r,t}\\\\\n\\boldsymbol\\eta_{F,t}\n\\end{array}\\right],\n\\]\n\\(S\\), \\(r\\) \\(F\\) respectively correspond slow-moving variables, policy variable (short-term rate) fast-moving variables. \\(\\eta_{r,t}\\) scalar, \\(\\boldsymbol\\eta_{S,t}\\) \\(\\boldsymbol\\eta_{F,t}\\) may vectors. space spanned \\(\\boldsymbol\\varepsilon_{S,t}\\) spanned \\(\\boldsymbol\\eta_{S,t}\\). result, \\(\\varepsilon_{r,t}\\) linear combination \\(\\eta_{r,t}\\) \\(\\boldsymbol\\eta_{S,t}\\) (\\(\\perp\\)), comes \\(B_{rr}\\eta_{r,t}\\)’s (population) residuals regression \\(\\varepsilon_{r,t}\\) \\(\\boldsymbol\\varepsilon_{S,t}\\). \\(\\mathbb{V}ar(\\eta_{r,t})=1\\), \\(B_{rr}\\) given square root variance \\(B_{rr}\\eta_{r,t}\\). \\(B_{F,r}\\) finally obtained regressing components \\(\\boldsymbol\\varepsilon_{F,t}\\) \\(\\eta_{r,t}\\).equivalent approach consists computing Cholesky decomposition \\(BB'\\) contemporaneous impacts monetary policy shock (\\(n\\) endogenous variables) components column \\(B\\) corresponding policy variable.\nFigure 2.2: Response monetary-policy shock. Identification approach Christiano, Eichenbaum Evans (1996).\nLet us now turn Long-run restrictions. restriction concerns long-run influence shock endogenous variable. Let us consider instance structural shock assumed “long-run influence” GDP. express ? long-run change GDP can expressed \\(GDP_{t+h} - GDP_t\\), \\(h\\) large. Note :\n\\[\nGDP_{t+h} - GDP_t = \\Delta GDP_{t+h} +\\Delta GDP_{t+h-1} + \\dots + \\Delta GDP_{t+1}.\n\\]\nHence, fact given structural shock (\\(\\eta_{,t}\\), say) long-run influence GDP means \n\\[\n\\lim_{h\\rightarrow\\infty}\\frac{\\partial GDP_{t+h}}{\\partial \\eta_{,t}} = \\lim_{h\\rightarrow\\infty} \\frac{\\partial}{\\partial \\eta_{,t}}\\left(\\sum_{k=1}^h \\Delta  GDP_{t+k}\\right)= 0.\n\\]can easily formulated function \\(B\\) matrices \\(\\Phi_i\\) \\(y_t\\) (including \\(\\Delta GDP_t\\)) follows VAR process.Without loss generality, consider VAR(1) case. Indeed, one can always write VAR(\\(p\\)) VAR(1). see , stack last \\(p\\) values vector \\(y_t\\) vector \\(y_{t}^{*}=[y_t',\\dots,y_{t-p+1}']'\\); Eq. (2.1) can rewritten companion form:\n\\[\\begin{equation}\ny_{t}^{*} =\n\\underbrace{\\left[\\begin{array}{c}\nc\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{=c^*}+\n\\underbrace{\\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]}_{=\\Phi}\ny_{t-1}^{*}+\n\\underbrace{\\left[\\begin{array}{c}\n\\varepsilon_{t}\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{\\varepsilon_t^*},\\tag{2.18}\n\\end{equation}\\]\nmatrices \\(\\Phi\\) \\(\\Omega^* = \\mathbb{V}ar(\\varepsilon_t^*)\\) dimension \\(np \\times np\\); \\(\\Omega^*\\) filled zeros, except \\(n\\times n\\) upper-left block equal \\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\). (Matrix \\(\\Phi\\) introduced Eq. (2.7).)Focusing VAR(1) case:\n\\[\\begin{eqnarray}\ny_{t} &=& c+\\Phi y_{t-1}+\\varepsilon_{t}\\\\\n& = & c+\\varepsilon_{t}+\\Phi(c+\\varepsilon_{t-1})+\\ldots+\\Phi^{k}(c+\\varepsilon_{t-k})+\\ldots \\nonumber \\\\\n& = & \\mu +\\varepsilon_{t}+\\Phi\\varepsilon_{t-1}+\\ldots+\\Phi^{k}\\varepsilon_{t-k}+\\ldots \\\\\n& = & \\mu +B\\eta_{t}+\\Phi B\\eta_{t-1}+\\ldots+\\Phi^{k}B\\eta_{t-k}+\\ldots,\n\\end{eqnarray}\\]sequence shocks \\(\\{\\eta_t\\}\\) determines sequence \\(\\{y_t\\}\\). \\(\\{\\eta_t\\}\\) replaced \\(\\{\\tilde{\\eta}_t\\}\\), \\(\\tilde{\\eta}_t=\\eta_t\\) \\(t \\ne s\\) \\(\\tilde{\\eta}_s=\\eta_s + \\gamma\\)?Assume \\(\\{\\tilde{y}_t\\}\\) associated “perturbated” sequence. \\(\\tilde{y}_t = y_t\\) \\(t<s\\). \\(t \\ge s\\), Wold decomposition \\(\\{\\tilde{y}_t\\}\\) implies:\n\\[\n\\tilde{y}_t = y_t + \\Phi^{t-s} B \\gamma.\n\\]\nTherefore, cumulative impact \\(\\gamma\\) \\(\\tilde{y}_t\\) (\\(t \\ge s\\)):\n\\[\\begin{eqnarray}\n(\\tilde{y}_t - y_t) +  (\\tilde{y}_{t-1} - y_{t-1}) + \\dots +  (\\tilde{y}_s - y_s) &=& \\nonumber \\\\\n(Id + \\Phi + \\Phi^2 + \\dots + \\Phi^{t-s}) B \\gamma.&& \\tag{2.19}\n\\end{eqnarray}\\]Consider shock \\(\\eta_{1,t}\\), magnitude \\(1\\). shock corresponds \\(\\gamma = [1,0,\\dots,0]'\\). Given Eq. (2.19), long-run cumulative effect shock endogenous variables given :\n\\[\n\\underbrace{(Id+\\Phi+\\ldots+\\Phi^{k}+\\ldots)}_{=(Id - \\Phi)^{-1}}B\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right],\n\\]\nfirst column \\(\\Theta \\equiv (Id - \\Phi)^{-1}B\\).context, consider following long-run restriction: “\\(j^{th}\\) structural shock cumulative impact \\(^{th}\\) endogenous variable”. equivalent \n\\[\n\\Theta_{ij}=0,\n\\]\n\\(\\Theta_{ij}\\) element \\((,j)\\) \\(\\Theta\\).Blanchard Quah (1989) implement long-run restrictions small-scale VAR. Two variables considered: GDP unemployment. Consequently, VAR affected two types shocks. authors want identify supply shocks (can permanent effect output) demand shocks (permanent effect output).1Blanchard Quah (1989)’s dataset quarterly, spanning period 1950:2 1987:4. VAR features 8 lags. data use:Estimate reduced-form VAR(8) model:Now, let us define loss function (loss) equal zero () \\(BB'=\\Omega\\) (b) element (1,1) \\(\\Theta B\\) equal zero:(Note: one can use type approach, based loss function, mix short- long-run restrictions.)Figure 2.3 displays resulting IRFs. Note , GDP, cumulate GDP growth IRF, response GDP level.\nFigure 2.3: IRF GDP unemployment demand supply shocks.\n","code":"\nlibrary(AEC)\ndata(\"USmonthly\")\n# Select sample period:\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables <- c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\nres.svar.ordering <- svar.ordering(y,p=3,\n                                   posit.of.shock = 5,\n                                   nb.periods.IRF = 20,\n                                   nb.bootstrap.replications = 100, # This is used in the parametric bootstrap only\n                                   confidence.interval = 0.90, # expressed in pp.\n                                   indic.plot = 1 # Plots are displayed if = 1.\n)\nlibrary(AEC)\npar(mfrow=c(1,2))\nplot(BQ$Date,BQ$Dgdp,type=\"l\",main=\"GDP quarterly growth rate\",xlab=\"\",ylab=\"\",lwd=2)\nplot(BQ$Date,BQ$unemp,type=\"l\",ylim=c(-3,6),main=\"Unemployment rate (gap)\",xlab=\"\",ylab=\"\",lwd=2)\nlibrary(vars)\ny <- BQ[,2:3]\nest.VAR <- VAR(y,p=8)\nOmega <- var(residuals(est.VAR))\n# Compute (Id - Phi)^{-1}:\nPhi <- Acoef(est.VAR)\nPHI <- make.PHI(Phi)\nsum.PHI.k <- solve(diag(dim(PHI)[1]) - PHI)[1:2,1:2]\nloss <- function(param){\n  B <- matrix(param,2,2)\n  X <- Omega - B %*% t(B)\n  Theta <- sum.PHI.k[1:2,1:2] %*% B\n  loss <- 10000 * ( X[1,1]^2 + X[2,1]^2 + X[2,2]^2 + Theta[1,1]^2 )\n  return(loss)\n}\nres.opt <- optim(c(1,0,0,1),loss,method=\"BFGS\",hessian=FALSE)\nprint(res.opt$par)## [1]  0.8570358 -0.2396345  0.1541395  0.1921221\nB.hat <- matrix(res.opt$par,2,2)\nprint(cbind(Omega,B.hat %*% t(B.hat)))##             Dgdp       unemp                       \n## Dgdp   0.7582704 -0.17576173  0.7582694 -0.17576173\n## unemp -0.1757617  0.09433658 -0.1757617  0.09433558\nnb.sim <- 40\npar(mfrow=c(2,2))\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),indic.IRF = 1,u.shock = c(1,0))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on UNEMP\")\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),indic.IRF = 1,u.shock = c(0,1))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on UNEMP\")"},{"path":"TS.html","id":"Signs","chapter":"2 Time Series","heading":"2.1.7 Sign restrictions","text":"identifiy structural shocks, need find matrix \\(B\\) satisfies \\(\\Omega_{\\varepsilon} = BB'\\) (\\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\)) restrictions. Indeed, \\(\\Omega_{\\varepsilon} = BB'\\) sufficent identify \\(B\\) since, take orthogonal matrix \\(Q\\) (see Def. 2.3), \\(\\mathcal{P}=BQ\\) also satisfies \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\).Definition 2.3  (Orthogonal matrix) orthogonal matri \\(Q\\) matrix \\(QQ' = ,\\) .e., columns (rows) \\(Q\\) \northogonal unit vectors:\n\\[q_i'q_j=0\\text{ }\\neq j\\text{ }q_i'q_j=1\\text{ }= j,\\]\n\\(q_i\\) \\(^{th}\\) column \\(Q\\).idea behind sign-restriction approach “draw” random matrices \\(\\mathcal{P}\\) satisfy \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\), constitute set admissible matrices, keeping set simulated \\(\\mathcal{P}\\) matrices satisfy predefined sign-based restriction. example restriction “one year, contractionary monetary-policy shocks negative impact inflation”.suggested , \\(B\\) matrix satisfies \\(\\Omega = BB'\\) (instance, \\(B\\) can based Cholesky decomposition \\(\\Omega\\)), also \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\) soon \\(\\mathcal{P}=BQ\\), \\(Q\\) orthogonal matrix. Therefore, draw \\(\\mathcal{P}\\) matrices, suffices draw set orthogonal matrices.fix ideas, consider dimension 2. case, orthogonal matrices rotation matrices, set orthogonal matrices can parameterized angle \\(x\\), :\n\\[\nQ_x=\\begin{pmatrix}\\cos(x)&\\cos\\left(x+\\frac{\\pi}{2}\\right)\\\\\n\\sin(x)&\\sin\\left(x+\\frac{\\pi}{2}\\right)\\end{pmatrix}=\\begin{pmatrix}\\cos(x)&-\\sin(x)\\\\\n\\sin(x)&\\cos(x)\\end{pmatrix}.\n\\]\n(angle-\\(x\\) counter-clockwise rotation.) Hence, case, drawing \\(x\\) randomly \\([0,2\\pi]\\), draw randomly set \\(2\\times2\\) rotation matrices. high-dimensional VAR, lose simple geometrical representation, though. always possible parametrize rotation matrix (high-dimentional VARs).proceed, ? Arias, Rubio-Ramírez, Waggoner (2018) provide procedure. approach based -called \\(QR\\) decomposition: square matrix \\(X\\) may decomposed \\(X=QR\\) \\(Q\\) orthogonal matrix \\(R\\) upper diagonal matrix. mind, propose two-step approach:Draw random matrix \\(X\\) drawing element independent standard normal distribution.Let \\(X = QR\\) \\(QR\\) decomposition \\(X\\) diagonal \\(R\\) normalized \npositive. random matrix \\(Q\\) orthogonal draw uniform distribution set orthogonal matrices.Equipped procedure, sign-restriction based following algorithm:Draw random orthogonal matrix \\(Q\\) (using step . ii. described ).Compute \\(B = PQ\\) \\(P\\) Cholesky decomposition reduced form residuals \\(\\Omega_{\\varepsilon}\\).Compute impulse response associated \\(B\\) \\(y_{t,t+k}=\\Phi^kB\\) cumulated response \\(\\bar y_{t,t+k}=\\sum_{j=0}^{k}\\Phi^jB\\).sign restrictions satisfied?\nYes. Store impulse response set admissible response.\n. Discard impulse response.\nYes. Store impulse response set admissible response.. Discard impulse response.Perform \\(N\\) replications report median impulse response (“confidence” intervals).Note: take account uncertainty \\(B\\) \\(\\Phi\\), can draw \\(B\\) \\(\\Phi\\) Steps 2 3 using inference method.method advantage relatively agnostic. Moreover, fairly flexible, one can impose sign restrictions variable, horizon.prominent example Uhlig (2005). Using US monthly data 1965.2003.XII, employs sign restrictions estimate effect monetary policy shocks.According conventional wisdom, monetary contractions :2Raise federal funds rate,Lower prices,Decrease non-borrowed reserves,Reduce real output.restricitons considered Uhlig (2005) follows: expansionary monetary policy shock leads :Increases pricesIncrease nonborrowed reservesDecreases federal funds rateWhat output? Since response interest, leave un-restricted.\nFigure 2.4: IRF associated monetary policy shock; sign-restriction approach.\nremarkable characteristic sign restrictions lead unique IRF. sense, say set-identified, point-identified. drawing rotation matrices (\\(Q\\)) distribution taking median, adopt agnostic approach: “pure sign restriction approach”.alternative approach -called penalty-function approach (PFA, Uhlig (2005), present Danne (2015)’s package). approach relies penalty function:\n\\[\n\\begin{array}{llll}f(x)&=&x&\\text{ }x\\le0\\\\\n&&100.x&\\text{ }x>0\\end{array}\n\\]\npenalizes positive responses rewards negative responses.Let \\(\\psi_k^j(q)\\) impulse response variable \\(j\\). \\(\\psi_k^j(q)\\)’s elements \\(\\psi_k(q)=\\Psi_kq\\).Let \\(\\sigma_j\\) standard deviation variable \\(j\\). Let \\(\\iota_{j,k}=1\\) restrict response variable \\(j\\) \\(k^th\\) horizon negative, \\(\\iota_{j,k}=-1\\) restrict positive, \\(\\iota_{j,k}=0\\) restriction. total penalty given \\[\n\\mathbf{P}(q)=\\sum_{j=1}^m\\sum_{k=0}^Kf\\left(\\iota_{j,k}\\frac{\\psi_k^j(q)}{\\sigma_j}\\right).\n\\]looking solution \n\\[\\begin{array}{ll}&\\min_q \\mathbf{P}(q)\\\\\n&\\\\\n\\text{s.t. }&q'q=1\\end{array}\\]problem solved numerically.","code":"\nlibrary(AEC);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables <- c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\nn <- length(considered.variables)\ny <- as.matrix(USmonthly[considered.variables])\nsign.restrictions <- list()\nhorizon <- list()\n#Define sign restrictions and horizon for restrictions\nfor(i in 1:n){\n  sign.restrictions[[i]] <- matrix(0,n,n)\n  horizon[[i]] <- 1\n}\nsign.restrictions[[1]][1,3] <- 1\nsign.restrictions[[1]][2,5] <- -1\nsign.restrictions[[1]][3,6] <- 1\nhorizon[[1]] <- 1:5\nres.svar.signs <- svar.signs(y,p=3,\n                             nb.shocks = 1, #number of identified shocks\n                             nb.periods.IRF = 20,\n                             bootstrap.replications = 1, # = 0 if no bootstrap\n                             confidence.interval = 0.80, # expressed in pp.\n                             indic.plot = 1, # Plots are displayed if = 1.\n                             nb.draws = 10000, # number of draws\n                             sign.restrictions,\n                             horizon,\n                             recursive =1 #  =0 <- draw Q directly, =1 <- draw q recursively\n)\nnb.rotations <- res.svar.signs$xx"},{"path":"TS.html","id":"forecast-error-variance-maximization","chapter":"2 Time Series","heading":"2.1.8 Forecast error variance maximization","text":"approach presented section exploits derivations Uhlig (2004). Barsky Sims (2011) exploit approach identify TFP news shock, define shock () orthogonal innovation current utilization-adjusted TFP (b) best explains variation future TFP.Consider process \\(\\{y_t\\}\\) admits infinite MA representation Eq. (2.3). Let \\(Q\\) orthogonal matrix, alternative decomposition :\n\\[\\begin{eqnarray}\ny_t&=&\\sum_{h=0}^{+\\infty}\\Psi_h\\underbrace{\\eta_{t-h}}_{Q\\tilde \\eta_{t-h}} = \\sum_{h=0}^{+\\infty}\\underbrace{\\Psi_hQ}_{\\tilde\\Psi_h}\\tilde\n\\eta_{t-h} = \\sum_{h=0}^{+\\infty}\\tilde\\Psi_h\\tilde \\eta_{t-h},\n\\end{eqnarray}\\]\n\\(\\tilde \\eta_{t-h}=Q'\\eta_{t-h}\\) white-noise shocks associated new MA representation. (also satisfy \\(\\mathbb{V}ar(\\tilde\\eta_t)=Id\\).)\\(h\\)-step ahead prediction error \\(y_{t+h}\\), given data including \\(t-1\\) given \n\\[\ne_{t+h}(h)=y_{t+h}-\\mathbb{E}_{t-1}(y_{t+h})=\\sum_{j=0}^h\\tilde \\Psi_h\\tilde \\eta_{t+h-j}.\n\\]variance-covariance matrix \\(e_{t+h}(h)\\) \n\\[\n\\Omega(h)=\\sum_{j=0}^h\\tilde \\Psi_j\\tilde \\Psi_j'=\\sum_{j=0}^h \\Psi_j \\Psi_j'.\n\\]can decompose \\(\\Omega(h)\\) contribution shock \\(l\\) (\\(l^{th}\\) component \\(\\tilde{\\eta}_t\\)):\n\\[\n\\Omega^{(h)}=\\sum_{l=1}^n\\Omega_l^{(h)}(Q)\n\\]\n\n\\[\n\\Omega_l^{(h)}(Q) =\\sum_{j=0}^h(\\Psi_jq_l)(\\Psi_jq_l)',\n\\]\n\\(q_l\\) \\(l^{th}\\) column \\(Q\\).decomposition can used objective finding impulse vector \\(b\\) s.t. explains much possible sum \\(h\\)-step ahead prediction error variance variable \\(\\), say, prediction horizons \\(h \\[\\underline{h} , \\overline{h}]\\).Formally, task explain much possible variance\n\\[\n\\sigma^2(\\underline{h},\\overline{h},q_1)=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)'\\right]_{,}\n\\]\nsingle impulse vector \\(q_1\\).Denote \\(E_{ii}\\) matrix filled zeros, except (\\(,\\)) entry, set 1. :\n\\[\\begin{eqnarray*}\n\\sigma^2(\\underline{h},\\overline{h},q_1)&=&\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)'\\right]_{,}=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[E_{ii}(\\Psi_jq_1)(\\Psi_jq_1)'\\right]\\\\\n&=&\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[q_1'\\Psi_j'E_{ii}\\Psi_j q_1\\right]\\\\\n&=& q_1'Sq_1,\n\\end{eqnarray*}\\]\n\n\\[\\begin{eqnarray*}\n\\begin{array}{lll}S&=&\\sum_{h=\\underline{h}}^{\\overline{h}}\\sum_{j=0}^{h}\\Psi_j'E_{ii}\\Psi_j\\\\\n&=&\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_j'E_{ii}\\Psi_j\\\\\n&=&\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_{j,}'\\Psi_{j,}\\\\\n\\end{array}\n\\end{eqnarray*}\\]\n\\(\\Psi_{j,}\\) denotes row \\(\\) \\(\\Psi_{j}\\), .e., response variable \\(\\) horizon \\(j\\) (\\(Q=Id\\)).maximization problem subject side constraint \\(q_1'q_1=1\\) can written Lagrangian: \\[\nL=q_1'Sq_1-\\lambda(q_1'q_1-1),\n\\]\nfirst-order condition \\(Sq_1=\\lambda q_1\\) (side constraint \\(q_1'q_1=1\\)). equation, see solution \\(q_1\\) eigenvector \\(S\\), one associated eigenvalue \\(\\lambda\\). also see \\(\\sigma^2(\\underline{h},\\overline{h},q_1)=\\lambda\\). Thus, maximize variance, need find eigenvector \\(S\\) associated maximal eigenvalue \\(\\lambda\\). defines first principal component (see XXXXXXX). , \\(S\\) admits following spectral decomposition:\n\\[\nS = \\mathcal{P}D\\mathcal{P}',\n\\]\n\\(D\\) diagonal matrix whose entries (ordered) eigenvalues: \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\), \\(\\sigma^2(\\underline{h},\\overline{h},q_1)\\) maximized \\(q_1 = p_1\\), \\(p_1\\) first column \\(\\mathcal{P}\\).","code":""},{"path":"TS.html","id":"NonGaussian","chapter":"2 Time Series","heading":"2.1.9 Identificagtion based on non-normality of the shocks","text":"section, show non-identification structural shocks (\\(\\eta_t\\)) specific Gaussian case. propose consistent estimation approaches context non-Gaussian shocks. first part, focus non-Gaussian SVAR models; second part, discuss case non-Gaussian SVARMA models.Non-Gaussian SVAR modelsThe estimation (S)VARs common (S)VARMAs. (Simpler estimation \\(\\Phi_k\\)’s). seen precedes , Gaussian case, identify \\(B\\) Gaussian case. , even observe infinite number ..d. \\(B \\eta_t\\), recover \\(B\\) \\(\\eta_t\\)’s Gaussian. Indeed, \\(\\eta_t \\sim \\mathcal{N}(0,Id)\\), distribution \\(\\varepsilon_t \\equiv B \\eta_t\\) \\(\\mathcal{N}(0,BB')\\).Hence \\(\\Omega = B B'\\) observed (population), orthogonal matrix \\(Q\\) (.e. \\(QQ'=Id\\)), also \\(BQ \\eta_t \\sim \\mathcal{N}(0,\\Omega)\\).Example: Bivariate Gaussian case (Eq. (2.4), \\(\\Theta_1=0\\))\\(\\left[\\begin{array}{c}\\eta_{1,t}\\\\ \\eta_{2,t}\\end{array}\\right]\\sim \\mathcal{N}(0,Id)\\),\n\\(B = \\left[\\begin{array}{cc} 1 & 2 \\\\ -1 & 1 \\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) & -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) & \\cos(\\pi/3) \\end{array}\\right]\\) (rotation).\\(\\Rightarrow\\) Distribution \\(B \\eta_t\\) versus \\(BQ\\eta_t\\)?\nFigure 2.5: XXXX.\n\nFigure 2.6: XXXX.\nExternal restrictions (economic hypotheses) needed identify \\(B\\) (see previous sections). restrictions may necessary structural shocks Gaussian., identification problem specific normally-distributed \\(\\eta_t\\)’s.Rigobon (2003)\nNormandin Phaneuf (2004)\nLanne Lutkepohl (2008)Example: Bivariate Gaussian + Student (5) case:\\(\\eta_{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_{2,t} \\sim t(5)\\),\n\\(B = \\left[\\begin{array}{cc} 1 & 2 \\\\ -1 & 1 \\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) & -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) & \\cos(\\pi/3) \\end{array}\\right]\\).Distribution \\(B \\eta_t\\) versus \\(BQ\\eta_t\\)?\nFigure 2.7: XXXX.\nNB: cases, \\(\\mathbb{V}ar(\\varepsilon_t)=BB'\\).Example: Bivariate Student (5) case\\(\\eta_{1,t} \\sim t(5)\\), \\(\\eta_{2,t} \\sim t(5)\\),\n\\(B = \\left[\\begin{array}{cc} 1 & 2 \\\\ -1 & 1 \\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) & -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) & \\cos(\\pi/3) \\end{array}\\right]\\).\\(\\Rightarrow\\) Distribution \\(B \\eta_t\\) versus \\(BQ\\eta_t\\)?NB: cases, \\(\\mathbb{V}ar(\\varepsilon_t)=BB'\\).\nFigure 2.8: XXXX.\nRelationship Signal Processing LiteratureTask performed:observe \\(n\\) linear combinations \\(n\\) independent signals; want estimate linear combinations (\\(\\Leftrightarrow\\) recover independent signals).Without loss generality, can assume \\(BB' = Id\\) (.e. \\(B\\) orthogonal)(case, .e. \\(\\mathbb{V}ar(\\varepsilon_t)=\\Omega \\ne Id\\), pre-multiply data \\(\\Omega^{-1/2}\\)).Classical signal processing problem: Independent Component Analysis (ICA)Find \\(B\\) \\(\\varepsilon_t = B \\eta_t\\) ($_t= B’ _t $) given \n. observe \\(\\varepsilon_t\\)’s,\nii. components \\(\\eta_t\\) independent,\niii. \\(BB'=Id\\) (\\(B\\) orthogonal).\nFigure 2.9: XXXX.\ncases, \\(\\mathbb{V}ar(\\varepsilon_t)=\\mathbb{V}ar(\\eta_t)=Id\\). Assume observe distribution \\(\\varepsilon_t\\) (). rotate \\(\\varepsilon_t\\) get obtain independent signals (\\(\\eta_t = B'\\varepsilon_t\\))?NB: \\(\\varepsilon_{1,t}\\) \\(\\varepsilon_{2,t}\\) independent.instance: \\(\\mathbb{E}(\\varepsilon_{2,t}|\\varepsilon_{1,t}>4)<0\\) (whereas \\(\\mathbb{E}(\\eta_{2,t}|\\eta_{1,t}>4)=0\\)).Hypothesis 2.1  :shocks \\(\\eta_t\\) ..d. \\(\\mathbb{E}(\\eta_t) = 0\\) \\(\\mathbb{V}ar(\\eta_t) = Id.\\)components \\(\\eta_{1,t}, \\ldots, \\eta_{n,t}\\) mutually independent.\niii \n\\[\n\\boxed{Y_t = B_0 \\eta_t,}\n\\]\n\\(\\mathbb{E}(Y_t) = Id\\) (.e. \\(B_0\\) orthogonal).Theorem 2.1  (Eriksson, Koivunen (2004)) Hypothesis 2.1 satisfied one components \\(\\eta\\) Gaussian, matrix \\(B_0\\) identifiable post multiplication \\(DP\\), \\(P\\) permutation matrix \\(D\\) diagonal matrix whose diagonal entries 1 \\(-1\\).}PML approachWe introduce set p.d.f. \\(g_i (\\eta_i), =1,\\ldots,n,\\) consider pseudo log-likelihood function:\n\\[\\begin{equation}\n\\log \\mathcal{L}_T (B) = \\sum^T_{t=1} \\sum^n_{=1} \\log g_i (b'_i Y_t),\\tag{2.20}\n\\end{equation}\\]\n\\(b_i\\) \\(^{th}\\) column matrix \\(B\\) (\\(b'_i\\) \\(^{th}\\) row \\(B^{-1}\\) since \\(B^{-1}=B'\\)).log-likelihood function (2.20) computed errors \\(\\eta_{,t}\\) p.d.f. \\(g_i (\\eta_i)\\).restrictions \\(B'B = Id\\) can eliminated parameterizing \\(B\\).Cayley’s representation:orthogonal matrix eigenvalue equal \\(-1\\) can written \n\\[\\begin{equation}\nB() = (Id+) (Id-)^{-1},\n\\end{equation}\\]\n\\(\\) skew symmetric (antisymmetric) matrix, \\('=-\\).one--one relationship \\(\\), since:\n\\[\\begin{equation}\n= (B()+Id)^{-1} (B()-Id).\n\\end{equation}\\]PML estimator matrix \\(B\\): \\(\\widehat{B_T} = B(\\hat{}_T),\\) :\n\\[\\begin{equation}\n\\hat{}_T = \\arg \\max_{a_{,j}, >j} \\sum^T_{t=1} \\sum^n_{=1} \\log g_i [b_i ()' Y_t].\\tag{2.22}\n\\end{equation}\\]Asymptotic properties PML approachHypothesis 2.2  :functions \\(\\log g_i\\), \\(=1,\\ldots,n\\), twice continuously differentiable.\\(sup_{B: B'B = Id} \\left|\\sum^n_{=1} \\log g_i (b'_i y)\\right| \\leq h(y),\\) \\(\\mathbb{E}_0 [h (Y)] < \\infty\\).Hypothesis 2.3  (Identification asymptotic FOC) solutions system equations:\n\\[\n\\left\\{\n\\begin{array}{l} \\mathbb{E}_0 \\left[b'_j Y_t \\frac{d\\log g_i}{d\\eta} (b'_i Y_t)\\right] = 0,\\;  \\neq j, \\\\\nB' B = Id,\n\\end{array}\n\\right.\n\\]\nelements \\(\\mathcal{P}_0 \\equiv \\mathcal{P}(B_0)\\), set matrices obtained permutation sign change columns \\(B_0\\).Hypothesis 2.4  (Local concavity) asymptotic objective function locally concave neighbourhood matrix \\(B\\) \\(\\mathcal{P}(B_0)\\), case \n\\[\n\\mathbb{E}_0 \\left[ \\frac{d^2 \\log g_i (\\eta_{,t})}{d\\eta^2} + \\frac{d^2 \\log g_j (\\eta_{j,t})}{d\\eta^2} - \\eta_{j,t} \\frac{d\\log g_j (\\eta_{j,t})}{d\\eta}- \\eta_{,t} \\frac{d\\log g_i (\\eta_{,t})}{d\\eta} \\right] < 0, \\forall <j,\n\\]\n\\(\\eta_{,t}\\) \\(^{th}\\) component \\(\\eta_t\\) associated particular element \\(B\\) \\(\\mathcal{P}(B_0)\\).condition particular satisfied following set conditions: derived Hyvarinen (1997) XXX\n\\[\\begin{equation}\n\\mathbb{E}_0 \\left[\\frac{d^2 \\log g_i(\\eta_{,t})}{d\\eta^2} - \\eta_{,t} \\frac{d\\log g_i(\\eta_{,t})}{d\\eta}\\right] <0,\\quad  =1,\\ldots, n. \\tag{2.23}\n\\end{equation}\\]Hyperbolic secant subgaussian distributions (see table next slide): either one, satisfy inequality (2.23) [Hyvarinen, Karhunen, Oja, 2001 XXXX].XXXXXXXXXXXXXXX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Except Gaussian distribution, \\(E[d^2 \\log g(X)/d \\varepsilon^2 - X d\\log g(X)/d \\varepsilon] < 0\\) (.e. Assumption,4 satisfied) pseudo distributions coincide distribution \\(X\\). subGaussian distribution mixture Gaussian distributions: \\(X\\) drawn distribution equal \\(- (1-B)Y\\), \\(B\\) drawn Bernoulli distribution parameter \\(1/2\\) \\(Y \\sim \\mathcal{N}(\\sqrt{(\\pi-2)/\\pi},2/\\pi)\\).Hypotheses ??-??, PML estimator \\(\\widehat{B_T}\\) \\(B_0\\) consistent (\\(\\mathcal{P}_0\\)) asymptotically normal, speed convergence \\(1/\\sqrt{T}\\).asymptotic variance-covariance matrix \\(vec \\sqrt{T} (\\widehat{B_T} - B_0)\\) \\(^{-1} \\left[\\begin{array}{cc} \\Gamma & 0 \\\\ 0 & 0 \\end{array} \\right] (')^{-1}\\), matrices \\(\\) \\(\\Gamma\\) detailed Gouri'eroux, Monfort Renne (2017) XXX.potential misspecification pseudo-distributions \\(g_i\\) effect consistency specific PML estimators.previous proposition can exploited build test whose null hypothesis :\\(H_0\\): \\(B\\) belongs \\(\\mathcal{P}_0\\), \\(\\mathcal{P}_0\\) set orthogonal matrices obtained permuting changing signs columns given orthogonal matrix \\(B_0\\).Application: Structural VAR modelThree dependent variables: inflation (\\(\\pi_t\\)), economic activity (\\(z_t\\)) nominal short-term interest rate (\\(r_t\\)).Structural shocks posited monetary-policy, demand supply shocks.\\(W_t\\): set information made past values \\(y_t= [\\pi_t,z_t,r_t]\\), \\(\\{y_{t-1},y_{t-2},\\dots\\}\\), exogenous variables \\(\\{x_{t},x_{t-1},\\dots\\}\\).reduced-form VAR model reads:\n\\[\ny_t  = \\underbrace{\\mu + \\sum_{=1}^{p} \\Phi_i y_{t-} + \\Theta x_t}_{(W_t;\\theta)} + u_t\n\\]\n\\(u_t\\)’s assumed serially independent, zero mean variance-covariance matrix \\(\\Sigma\\).U.S. data. 1959:IV 2015:quarterly frequency (\\(T=224\\)). Source: Federal Reserve Economic Database (FRED).Two different measures economic activity considered: output gap unemployment gap. Change log oil prices added exogenous variable (\\(x_t\\)).\\(\\mu\\), \\(\\Phi_i\\), \\(\\Theta\\) \\(\\Sigma\\) consistently estimated OLS.Jarque-Bera tests support hypothesis non-normality residuals.want estimate orthogonal matrix \\(B\\) \\(u_t=SB \\eta_t\\), \\(S\\) results Cholesky decomposition \\(\\Sigma\\) andthe components \\(\\eta_t\\) independent, zero-mean unit variance.PML approach applied standardized VAR residuals given :\n\\[\n\\hat{S}_T^{-1}\\underbrace{[y_t - (W_t;\\hat\\theta_T)]}_{\\mbox{VAR residuals}}.\n\\]\nconstruction \\(\\hat{S}_T^{-1}\\), comes covariance matrix residuals \\(Id\\).Pseudo density functions: Distinct asymmetric mixtures Gaussian distributions.\\(B\\) estimated, remains associate structural shocks (monetary-policy, supply demand) different components \\(\\eta_{t}\\).Contractionary monetary-policy shocks: negative impact real activity inflation.Supply shock: influences opposite signs economic activity inflation.Demand shock: influences signs economic activity inflation.Important: method rely assumption specific impacts (e.g. contemporaneous long-run) null.Comparison previous IRFs stemming “recursive” identification approaches based specific short-run restrictions (SRRs).SRRs approach (Section,\\(\\ref{Section:Standard}\\)) based assumptions \\(Cov(\\eta_t)=Id\\),\\(k^{th}\\) structural shock contemporaneously affects first \\(k-1\\) endogenous variables andthe contemporaneous effect \\(k^{th}\\) structural shock \\(k^{th}\\) dependent variable positive.assumptions, structural shocks given \\(S^{-1}u_t\\).SRR approaches assume –potentially wrongly– contemporaneous impacts structural shocks given variables null.null hypothesis tests \\(H_0= (P \\\\mathcal{P}(Id))\\).\nnull hypothesis \\(H_0\\) stating true value \\(B\\) belongs \\(\\mathcal{P}_0\\) standard since finite union simple hypotheses \\(H_{0,j} = (B = B_{j,0})\\).First testing procedure:Define Wald statistics \\(\\hat\\xi_{j,T}\\), \\(j \\J\\):\n\\[\\begin{equation}\n\\hat\\xi_{j,T} = T [vec\\hat{B}_T-vec B_{j,0}]'\\hat{}_T'\n\\left[\n\\begin{array}{cc}\n\\hat{\\Omega}^{-1}_T & 0\\\\\n0&0\n\\end{array}\n\\right]\\hat{}_T\n[vec\\hat{B}_T-vec B_{j,0}],\n\\end{equation}\\]\n\\(\\hat{}_T\\) \\(\\hat{\\Omega}_T\\) consistent estimators matrices \\(\\) \\(\\Omega\\).Since dimension asymptotic distribution \\(\\sqrt{T}[vec\\hat{B}_T-vec B_{j,0}]\\) \\(\\frac{1}{2}n(n-1)\\), asymptotic distribution \\(\\hat\\xi_{j,T}\\) \\(H_{0,j}\\) \\(\\chi^2\\left(\\frac{1}{2}n(n-1)\\right)\\).Define \\(\\hat\\xi_T = \\underset{j \\J}{\\min} \\hat\\xi_{j,T}\\) test statistic \\(H_0\\).\\(H_0\\), \\(\\hat{B}_T\\) converges \\(B_{j_0,0}\\) (say).asymptotic properties Wald statistics simple hypotheses:\n\\[\\begin{equation}\n\\hat\\xi_{j_0,T} \\overset{D}{\\rightarrow} \\chi^2\\left(\\frac{n(n-1)}{2}\\right) \\quad \\mbox{}\\quad  \\hat\\xi_{j,T} \\rightarrow \\infty, \\mbox{ } j \\ne j_0.\n\\end{equation}\\]null hypothesis, \\(\\hat\\xi_T = \\underset{j}{\\min}\\) \\(\\hat\\xi_{j,T}\\) asymptotically equal \\(\\hat\\xi_{j_0,T}\\) asymptotic distribution, \\(\\chi^2\\left(\\frac{1}{2}n(n-1)\\right)\\), depend \\(j_0\\). Therefore \\(\\hat\\xi_T\\) asymptotically pivotal statistic null hypothesis \\(H_0\\) test critical region \\(\\hat\\xi_T \\ge \\chi^2_{1-\\alpha}\\left(\\frac{1}{2}n(n-1)\\right)\\) asymptotic level \\(\\alpha\\) consistent.Second testing procedureDefine \\(B_{0,T} = \\underset{B \\\\mathcal{P}_0}{\\mbox{Argmin }} d(\\hat{B}_T,B)\\) \\(d\\) distance, instance Euclidean one.null hypothesis \\(H_0\\): \\((B \\\\mathcal{P}_0)\\), \\(\\hat{B}_T\\) converges almost surely element \\(\\mathcal{P}_0\\) denoted \\(B_{j_0,0}\\) also case \\(B_{0,T}\\) since, asymptotically, \\(B_{0,T}=B_{j_0,0}\\).Moreover:\n\\[\n\\sqrt{T}(\\hat{B}_T - B_{0,T})=\\sqrt{T}(\\hat{B}_T - B_{j_0,0}) + \\sqrt{T}(B_{j_0,0} - B_{0,T}),\n\\]\n, since \\(B_{0,T}\\) almost surely asymptotically equal \\(B_{j_0,0}\\), asymptotic distribution \\(\\sqrt{T}(\\hat{B}_T - B_{0,T})\\) \\(H_0\\) \\(\\sqrt{T}(\\hat{B}_T - B_{j_0,0})\\).implies \n\\[\n\\tilde\\xi_{T} = T [vec\\hat{B}_T-vec B_{0,T}]'\\hat{}_T'\n\\left[\n\\begin{array}{cc}\n\\hat{\\Omega}^{-1}_T & 0\\\\\n0&0\n\\end{array}\n\\right]\\hat{}_T\n[vec\\hat{B}_T-vec B_{0,T}]\n\\]\nasymptotically distributed \\(\\chi^2\\left(\\frac{1}{2}n(n-1)\\right)\\) \\(H_0\\).advantage second method necessitates computation one Wald test statistic.consider two specific SRR schemes:\n* , assumed monetary-policy shock contemporaneous impact \\(\\pi_t\\) \\(y_t\\).\n* SRR Scheme 1: Inflation contemporaneously impacted one structural shock .\n* SRR Scheme 2: Economic activity contemporaneously impacted one structural shock .\n\\end{itemize}\n* economic activity measured means output gap, SRRs rejected 5% level.Relation Heteroskedasticity IdentificationIn cases, \\(\\varepsilon_t\\)’s heteroskedastic, \\(B\\) matrix can identifiedRigobon (2003),Lanne, Lutkepohl Maciejowska (2010)Consider case still \\(\\varepsilon_t = B \\eta_t\\) \\(\\eta_t\\)’s variance conditionally depends regime \\(s_t \\\\{1,\\dots,M\\}\\). :\n\\[\n\\mathbb{V}ar(\\eta_{k,t}|s_t) = \\lambda_{s_t,k} \\quad \\mbox{} k \\\\{1,\\dots,n\\}\n\\]Denoting \\(\\Lambda_i\\) diagonal matrix whose diagonal entries \\(\\lambda_{,k}\\)’s, comes :\n\\[\n\\mathbb{V}ar(\\eta_{t}|s_t) = \\Lambda_{s_t},\\quad \\mbox{}\\quad \\mathbb{V}ar(\\varepsilon_{t}|s_t) = B\\Lambda_{s_t}B'.\n\\]Without loss generality, can assumed \\(\\Lambda_1=Id\\).context, \\(B\\) identified, apart sign reversal columns \\(k \\ne j \\\\{1,\\dots,n\\}\\), regime \\(\\) s.t. \\(\\lambda_{,k} \\ne \\lambda_{,j}\\). Prop.1 Lanne, L\"utkepohl Maciejowska (2010).Bivariate regime case (\\(M=2\\)): \\(B\\) identified \\(\\lambda_{2,k}\\)’s different. , identification ensured “sufficient heterogeneity volatility changes” [L\"utkepohl Netsunajev (2017)(https://www.sciencedirect.com/science/article/pii/S2452306216300223).regimes \\(s_t\\) exogenous serially independent, situation consistent “non-Gaussian” situation described section.","code":"\ntheta.angle <- pi/3\nQ <- matrix(c(cos(theta.angle),sin(theta.angle),-sin(theta.angle),cos(theta.angle)),2,2)\n#nb.sim <- 10^4\nnb.sim <- 10^2\ndistri.1 <- list(type=c(\"gaussian\"),name=\"Panel (a) Gaussian\",name.4.table=\"Gaussian\")\ndistri.2 <- list(type=c(\"mixt.gaussian\"),mu=0,sigma=5,p=.03,name=\"Panel (b) Mixture of Gaussian\",name.4.table=\"Mixture of Gaussian\")\ndistri.3 <- list(type=c(\"student\"),df=c(5),name=\"Panel (c) Student (df: 5)\",name.4.table=\"Student (df: 5)\")\ndistri.4 <- list(type=c(\"student\"),df=c(10),name=\"Panel (d) Student (df: 10)\",name.4.table=\"Student (df: 10)\")\nx.lim <- c(-7,7)\ny.lim <- c(-5,5)\nnb.points <- 100\nx.points <- seq(x.lim[1],x.lim[2],length.out=nb.points)\ny.points <- x.points\nall.x <- c(matrix(x.points,nb.points,nb.points))\nall.y <- c(t(matrix(x.points,nb.points,nb.points)))\neps <- cbind(all.x,all.y)\npar(plt=c(.25,.9,.25,.8))\neta.1 <- simul.distri(distri.1,nb.sim)\neta.2 <- simul.distri(distri.1,nb.sim)\nepsilon.C <- cbind(eta.1,eta.2) %*% t(C)\nepsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)\nModel$distri <- list(type=c(\"gaussian\",\"gaussian\"),df=c(NaN,NaN))\npar(mfrow=c(1,2))\nplot(epsilon.C[,1],epsilon.C[,2],pch=19,\n     xlim=x.lim,ylim=y.lim,col=\"#00000044\",\n     xlab=expression(epsilon[1]),\n     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,\n     main=expression(paste(\"Distribution of \",epsilon[t],\" = \",B,eta[t],sep=\"\")))\nz <- matrix(exp(g(eps,Model)),nb.points,nb.points)\npar(new=TRUE)\nmax.z <- max(z)\nlevels <- c(.01,.1,.3,.6,.9)*max.z\ncontour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col=\"red\",lwd=2)\nplot(epsilon.CQ[,1],epsilon.CQ[,2],pch=19,\n     xlim=x.lim,ylim=y.lim,col=\"#00000044\",\n     xlab=expression(epsilon[1]),\n     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,\n     main=expression(paste(\"Distribution of \",epsilon[t],\" = \",BQ,eta[t],sep=\"\")))\nz <- matrix(exp(g(eps,Model)),nb.points,nb.points)\npar(new=TRUE)\nmax.z <- max(z)\nlevels <- c(.01,.1,.3,.6,.9)*max.z\ncontour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col=\"red\",lwd=2)\ntheta.angle <- pi/3\nQ <- matrix(c(cos(theta.angle),sin(theta.angle),-sin(theta.angle),cos(theta.angle)),2,2)\n#nb.sim <- 10^4\nnb.sim <- 10^2\ndistri.1 <- list(type=c(\"gaussian\"),name=\"Panel (a) Gaussian\",name.4.table=\"Gaussian\")\ndistri.2 <- list(type=c(\"mixt.gaussian\"),mu=0,sigma=5,p=.03,name=\"Panel (b) Mixture of Gaussian\",name.4.table=\"Mixture of Gaussian\")\ndistri.3 <- list(type=c(\"student\"),df=c(5),name=\"Panel (c) Student (df: 5)\",name.4.table=\"Student (df: 5)\")\ndistri.4 <- list(type=c(\"student\"),df=c(10),name=\"Panel (d) Student (df: 10)\",name.4.table=\"Student (df: 10)\")\nx.lim <- c(-7,7)\ny.lim <- c(-5,5)\nnb.points <- 100\nx.points <- seq(x.lim[1],x.lim[2],length.out=nb.points)\ny.points <- x.points\nall.x <- c(matrix(x.points,nb.points,nb.points))\nall.y <- c(t(matrix(x.points,nb.points,nb.points)))\neps <- cbind(all.x,all.y)\npar(plt=c(.25,.9,.25,.8))\neta.1 <- simul.distri(distri.1,nb.sim)\neta.2 <- simul.distri(distri.3,nb.sim)\nepsilon.C <- cbind(eta.1,eta.2) %*% t(C)\nepsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)\nModel$distri <- list(type=c(\"gaussian\",\"student\"),df=c(NaN,5))\npar(mfrow=c(1,2))\nplot(epsilon.C[,1],epsilon.C[,2],pch=19,\n     xlim=x.lim,ylim=y.lim,col=\"#00000044\",\n     xlab=expression(epsilon[1]),\n     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,\n     main=expression(paste(\"Distribution of \",epsilon[t],\" = \",B,eta[t],sep=\"\")))\nz <- matrix(exp(g(eps,Model)),nb.points,nb.points)\npar(new=TRUE)\nmax.z <- max(z)\nlevels <- c(.01,.1,.3,.6,.9)*max.z\ncontour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col=\"red\",lwd=2)\nplot(epsilon.CQ[,1],epsilon.CQ[,2],pch=19,\n     xlim=x.lim,ylim=y.lim,col=\"#00000044\",\n     xlab=expression(epsilon[1]),\n     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,\n     main=expression(paste(\"Distribution of \",epsilon[t],\" = \",BQ,eta[t],sep=\"\")))\nz <- matrix(exp(g(eps,Model)),nb.points,nb.points)\npar(new=TRUE)\nmax.z <- max(z)\nlevels <- c(.01,.1,.3,.6,.9)*max.z\ncontour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col=\"red\",lwd=2)\ntheta.angle <- pi/3\nQ <- matrix(c(cos(theta.angle),sin(theta.angle),-sin(theta.angle),cos(theta.angle)),2,2)\n#nb.sim <- 10^4\nnb.sim <- 10^2\ndistri.1 <- list(type=c(\"gaussian\"),name=\"Panel (a) Gaussian\",name.4.table=\"Gaussian\")\ndistri.2 <- list(type=c(\"mixt.gaussian\"),mu=0,sigma=5,p=.03,name=\"Panel (b) Mixture of Gaussian\",name.4.table=\"Mixture of Gaussian\")\ndistri.3 <- list(type=c(\"student\"),df=c(5),name=\"Panel (c) Student (df: 5)\",name.4.table=\"Student (df: 5)\")\ndistri.4 <- list(type=c(\"student\"),df=c(10),name=\"Panel (d) Student (df: 10)\",name.4.table=\"Student (df: 10)\")\nx.lim <- c(-7,7)\ny.lim <- c(-5,5)\nnb.points <- 100\nx.points <- seq(x.lim[1],x.lim[2],length.out=nb.points)\ny.points <- x.points\nall.x <- c(matrix(x.points,nb.points,nb.points))\nall.y <- c(t(matrix(x.points,nb.points,nb.points)))\neps <- cbind(all.x,all.y)\npar(plt=c(.25,.9,.25,.8))\neta.1 <- simul.distri(distri.3,nb.sim)\neta.2 <- simul.distri(distri.3,nb.sim)\nepsilon.C <- cbind(eta.1,eta.2) %*% t(C)\nepsilon.CQ <- cbind(eta.1,eta.2) %*% t(C %*% Q)\nModel$distri <- list(type=c(\"student\",\"student\"),df=c(5,5))\npar(mfrow=c(1,2))\nplot(epsilon.C[,1],epsilon.C[,2],pch=19,\n     xlim=x.lim,ylim=y.lim,col=\"#00000044\",\n     xlab=expression(epsilon[1]),\n     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,\n     main=expression(paste(\"Distribution of \",epsilon[t],\" = \",B,eta[t],sep=\"\")))\nz <- matrix(exp(g(eps,Model)),nb.points,nb.points)\npar(new=TRUE)\nmax.z <- max(z)\nlevels <- c(.01,.1,.3,.6,.9)*max.z\ncontour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col=\"red\",lwd=2)\nplot(epsilon.CQ[,1],epsilon.CQ[,2],pch=19,\n     xlim=x.lim,ylim=y.lim,col=\"#00000044\",\n     xlab=expression(epsilon[1]),\n     ylab=expression(epsilon[2]),cex.lab=1.6,cex.main=1.6,\n     main=expression(paste(\"Distribution of \",epsilon[t],\" = \",BQ,eta[t],sep=\"\")))\nz <- matrix(exp(g(eps,Model)),nb.points,nb.points)\npar(new=TRUE)\nmax.z <- max(z)\nlevels <- c(.01,.1,.3,.6,.9)*max.z\ncontour(x.points,y.points,z,levels=levels,xlim=x.lim,ylim=y.lim,col=\"red\",lwd=2)"},{"path":"TS.html","id":"factor-augmented-var-favar","chapter":"2 Time Series","heading":"2.1.10 Factor-Augmented VAR (FAVAR)","text":"VAR models subject curse dimensionality: \\(n\\), large, number parameters (\\(n^2\\)) explodes.case one suspects \\(y_{,t}\\)’s mainly driven small number random sources, factor structure may imposed, principal component analysis can employed estimate relevant factors (Bernanke, Boivin, Eliasz (2005)).Let us denote \\(F_t\\) \\(k\\)-dimensional vector latent factors accounting important shares variances \\(y_{,t}\\)’s (\\(K \\ll n\\)) \\(x_t\\) small \\(M\\)-dimensional subset \\(y_t\\) (\\(M \\ll n\\)). following factor structure posited:\n\\[\ny_t = \\Lambda^f F_t + \\Lambda^x x_t + e_t,\n\\]\n\\(e_t\\) “small” serially mutually ..d. error terms. \\(F_t\\) \\(x_t\\) supposed drive fluctuations \\(y_t\\)’s components.model complemented positing VAR dynamics \\([F_t',x_t']'\\):\n\\[\\begin{equation}\n\\left[\\begin{array}{c}F_t\\\\x_t\\end{array}\\right] = \\Phi(L)\\left[\\begin{array}{c}F_{t-1}\\\\ x_{t-1}\\end{array}\\right] + v_t.\\tag{2.24}\n\\end{equation}\\]Standard identification techniques structural shocks can employed Eq. (2.24): Cholesky approach can used instance last component \\(x_t\\) short-term interest rate assumed MP shock contemporaneous impact macro-variables (\\(x_t\\)).identification procedure, Bernanke, Boivin, Eliasz (2005) exploit fact macro-finance variables can decomposed two sets —fast-moving slow-moving variables— former reacts contemporaneously monetary-policy shocks. Now, estimate (unobserved) factors \\(F_t\\)? Bernanke, Boivin, Eliasz (2005) note first \\(K+M\\) PCA whole dataset (\\(y_t\\)), denote \\(\\hat{C}(F_t,x_t)\\) span space \\(F_t\\) \\(x_t)\\). get estimate \\(F_t\\), dependence \\(\\hat{C}(F_t,x_t)\\) \\(x_t)\\) removed. done regressing, OLS, \\(\\hat{C}(F_t,x_t)\\) \\(x_t)\\) \\(\\hat{C}^*(F_t)\\), latter estimate common components \\(x_t\\). proxy \\(\\hat{C}^*(F_t)\\), Bernanke, Boivin, Eliasz (2005) take principal components set slow-moving variables, comtemporaneously correlated \\(x_t\\). Vector \\(\\hat{F}_t\\) computed \\(\\hat{C}(F_t,x_t) - b_x x_t\\), \\(b_x\\) coefficients coming previous OLS regressions.Note approach implies vectorial space spanned \\((\\hat{F}_t,x_t)\\) spanned \\(\\hat{C}(F_t,x_t)\\)., employ method dataset built McCracken Ng (2016) —FRED:MD database— includes 119 time series.\nFigure 2.10: Responses monetary-policy shock. FAVAR approach Bernanke, Boivin, Eliasz (2005). FRED-MD dataset.\n","code":""},{"path":"TS.html","id":"Projections","chapter":"2 Time Series","heading":"2.1.11 Projections Methods","text":"Consider infinite MA representation \\(y_t\\) (Eq. (2.3)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\color{blue}{\\Psi_{h}} \\eta_{t-h}.\n\\]\nseen Section 2.1.2, entries \\((,j)\\) sequence \\(\\Psi_h\\) matrices define IRF \\(\\eta_{j,t}\\) \\(y_{,t}\\).Assume observe \\(\\eta_{j,t}\\), consistent estimate \\(\\Psi_{,j,h}\\) simply obtained OLS regression \\(y_{,t+h}\\) \\(\\eta_{j,t}\\):\n\\[\\begin{equation}\ny_{,t+h} = \\mu_i + \\Psi_{,j,h}\\eta_{j,t} + u_{,j,t+h}.\\tag{2.25}\n\\end{equation}\\]\nresiduals \\(u_{,j,t+h}\\) autocorrelated (\\(h>0\\)), estimates covariance OLS estimators \\(\\Psi_{,j,h}\\) based robust estimators (e.g. Newey-West, see Eq. (??)). core idea local projection approach proposed Jordà (2005).Now, proceed (usual) case \\(\\eta_{j,t}\\) observed? consider two situations.Situation : Without IVThis corresponds original Jordà (2005)’s approach.Assume structural shock interest (\\(\\eta_{1,t}\\), say) can consistently obtained residual regression variable \\(x_t\\) set control variables \\(w_t\\) independent \\(\\eta_{1,t}\\):\n\\[\\begin{equation}\n\\eta_{1,t} = x_t - \\mathbb{E}(x_t|w_t),\\tag{2.26}\n\\end{equation}\\]\n\\(\\mathbb{E}(x_t|w_t)\\) affine \\(w_t\\) \\(w_t\\) affine transformation \\(\\eta_{2:n,t}\\) past shocks \\(\\eta_{t-1},\\eta_{t-2},\\dots\\).Eq. (2.26) implies , conditional \\(w_t\\), additional knowledge \\(x_t\\) useful comes forecast something depends \\(\\eta_{1,t}\\). Hence, given \\(u_{,1,t+h}\\) (see Eq. (2.25)) independent \\(\\eta_{1,t}\\) (depends \\(\\eta_{t+h},\\dots,\\eta_{t+1},{\\color{blue}\\eta_{2:n,t}},\\eta_{t-1},\\eta_{t-2},\\dots\\)), comes \n\\[\n\\mathbb{E}(u_{,1,t+h}|x_t,w_t)= \\mathbb{E}(u_{,1,t+h}|w_t).\n\\]\nconditional mean independence case.Let’s rewrite Eq. (2.25) follows:\n\\[\\begin{eqnarray*}\ny_{,t+h} &=& \\mu_i + \\Psi_{,1,h}\\eta_{1,t} + u_{,1,t+h}\\\\\n&=&  \\mu_i + \\Psi_{,1,h}x_t  \\color{blue}{-\\Psi_{,1,h}\\mathbb{E}(x_t|w_t) + u_{,1,t+h}},\n\\end{eqnarray*}\\]precedes implies expectation blue term, conditional \\(x_t\\) \\(w_t\\), linear \\(w_t\\). Standard results conditional mean independence case imply regression \\(y_{,t+h}\\) \\(x_t\\), controlling \\(w_t\\), provides consistent estimate \\(\\Psi_{,1,h}\\):\n\\[\\begin{equation}\ny_{,t+h} = \\alpha_i + \\Psi_{,1,h}x_t + \\beta'w_t + v_{,t+h}.\n\\end{equation}\\]instance consistent case \\([\\Delta GDP_t, \\pi_t,i_t]'\\) follows VAR(1) monetary-policy shock contemporaneously affect \\(\\Delta GDP_t\\) \\(\\pi_t\\).IRFs can estimated LP, taking \\(x_t = i_t\\) \\(w_t = [\\Delta GDP_t,\\pi_t,\\Delta GDP_{t-1}, \\pi_{t-1},i_{t-1}]'\\).approach closely relates SVAR Cholesky-based identification approach. Specifically, \\(w_t = [{\\color{blue}y_{1,t},\\dots,y_{k-1,t}}, y_{t-1}',\\dots,y_{t-p}']'\\), \\(k\\le n\\), \\(x_t = y_{k,t}\\), approach corresponds, \\(h=0\\), SVAR(\\(p\\)) Cholesky-based IRF (focusing responses \\(k^{th}\\) structural shock). However, two approaches differ \\(h>0\\), LP methodology assumes VAR dynamics \\(y_t\\).3Situation B: IV approachConsider now valid instrument \\(z_t\\) \\(\\eta_{1,t}\\) (\\(\\mathbb{E}(z_t)=0\\)). :\n\\[\\begin{equation}\n\\left\\{\n\\begin{array}{llll}\n(IV.) & \\mathbb{E}(z_t \\eta_{1,t}) &\\ne 0 & \\mbox{(relevance condition)} \\\\\n(IV.ii) & \\mathbb{E}(z_t \\eta_{j,t}) &= 0 \\quad \\mbox{} j>1 & \\mbox{(exogeneity condition)}\n\\end{array}\\right.\\tag{2.27}\n\\end{equation}\\]\ninstrument \\(z_t\\) can used identify structural shock. Eq. (2.27) implies exist \\(\\rho \\ne 0\\) mean-zero variable \\(\\xi_t\\) :\n\\[\n\\eta_{1,t} = \\rho z_t + \\xi_t,\n\\]\n\\(\\xi_t\\) correlated neither \\(z_t\\), \\(\\eta_{j,t}\\), \\(j\\ge2\\).Proof. Define \\(\\rho = \\frac{\\mathbb{E}(\\eta_{1,t}z_t)}{\\mathbb{V}ar(z_t)}\\) \\(\\xi_t = \\eta_{1,t} - \\rho z_t\\). easily seen \\(\\xi_t\\) satisfies moment restrictions given .Ramey (2016) reviews different approaches employed construct monetary policy-shocks (two main approaches presented 2.1 ?? ). also collected time series shocks, see website.Example 2.1  (Identification Monetary-Policy Shocks Based High-Frequency Data) Instruments monetary-policy shocks can extracted high-frequency market data associated interest-rate products.quotes interest-rate-related financial products sensitive monetary-policy announcements. quotes mainly depends investors’ expectations regarding future short-term rates: \\(\\mathbb{E}_t(i_{t+s})\\). Typically, agents risk-neutral, maturity-\\(h\\) interest rate approximatively given :\n\\[\ni_{t,h} \\approx \\mathbb{E}_t\\left(\\frac{1}{h}\\int_{0}^{h} i_{t+s} ds\\right) = \\frac{1}{h}\\int_{0}^{h} \\mathbb{E}_t\\left(i_{t+s}\\right) ds.\n\\]\ngeneral, changes \\(\\mathbb{E}_t(i_{t+s})\\), \\(s>0\\), can affected types shocks may trigger reaction central bank.However, MP announcement takes place \\(t\\) \\(t+\\epsilon\\), \\(\\mathbb{E}_{t+\\epsilon}(i_{t+s})-\\mathbb{E}_t(i_{t+s})\\) attributed MP shock (see Figure 2.11, Gürkaynak, Sack, Swanson (2005)). Hence, monthly time series MP shocks can obtained summing, month, changes \\(i_{t+ \\epsilon,h} - i_{t,h}\\) associated given interest rate (T-bills, futures, swaps) given maturity \\(h\\).See among others: Kuttner (2001), Cochrane Piazzesi (2002),Gürkaynak, Sack, Swanson (2005), Piazzesi Swanson (2008), Gertler Karadi (2015).\nFigure 2.11: Source: Gurkaynak, Sack Swanson (2005). Transaction rates Federal funds futures June 25, 2003, day regularly scheduled FOMC meeting scheduled. 2:15 p.m., FOMC announced lowering target federal funds rate 1.25% 1%, many market participants expecting 50 bp cut. shows () financial markets seem fully adjust policy action within just minutes (ii) federal funds rate surprise necessarily direction federal funds rate action .\nExample 2.2  (Identification Monetary-Policy Shocks Based Narrative Approach) Romer Romer (2004) propose two-step approach:derive series Federal Reserve intentions federal funds rate (explicit target Fed) around FOMC meetings,control Federal Reserve forecasts.gives measure intended monetary policy actions driven information future economic developments.\n. “intentions” measured combination narrative quantitative evidence. Sources: (among others) Minutes FOMC “Blue Books”.\nb. Controls = variables spanning information Federal Reserve future developments. Data: Federal Reserve’s internal forecasts (inflation, real output unemployment), “Greenbook’s forecasts” – usually issued 6 days FOMC meeting.shock measure residual series linear regression () (b).two main IV approaches estimate IRFs see James H. Stock Watson (2018):LP-IV approach, \\(y_t\\)’s DGP left unspecified,SVAR-IV approach.LP-IV approach based set IV regressions (variable interest, one forecast horizon). SVAR-IV approach based IV regressions VAR innovations (one series VAR innovations).VAR adequately captures DGP, IV-SVAR optimal horizons. However, VAR misspecified, specification errors compounded horizon local projection method lead better results.Situation B.1: SVAR-IV approachAssume consistent estimates \\(\\varepsilon_t = B\\eta_t\\), estimates (\\(\\hat\\varepsilon_{t}\\)) coming estimation VAR model. , \\(\\\\{1,\\dots,n\\}\\):\n\\[\\begin{eqnarray}\n\\varepsilon_{,t} &=& b_{,1} \\eta_{1,t} + u_{,t} (\\#eq:eps_rho)\\\\\n&=& b_{,1} \\rho z_t + \\underbrace{b_{,1}\\xi_t + u_{,t}}_{\\perp z_t}. \\nonumber\n\\end{eqnarray}\\]\n(\\(u_{,t}\\) linear combination \\(\\eta_{j,t}\\)’s, \\(j\\ge2\\)).Hence, multiplicative factor (\\(\\rho\\)), (OLS) regressions \\(\\hat\\varepsilon_{,t}\\)’s \\(z_t\\) provide consistent estimates \\(b_{,1}\\)’s.Combined estimated VAR (\\(\\Phi_k\\) matrices), provides consistent estimates IRFs \\(\\eta_{1,t}\\) \\(y_t\\), though multiplicative factor. scale ambiguity can solved rescaling structural shock (“unit-effect normalisation”, see James H. Stock Watson (2018)). Let us consider \\(\\tilde\\eta_{1,t}=b_{1,1}\\eta_{1,t}\\); construction, \\(\\tilde\\eta_{1,t}\\) one-unit contemporaneous effect \\(y_{1,t}\\). Denoting \\(\\tilde{B}_{,1}\\) contemporaneous impact \\(\\tilde\\eta_{1,t}\\), get:\n\\[\n\\tilde{B}_{1} = \\frac{1}{b_{1,1}} {B}_{1},\n\\]\n\\(B_{1}\\) denotes \\(1^{st}\\) column \\(B\\) \\(\\tilde{B}_{1}=[1,\\tilde{B}_{2,1},\\dots,\\tilde{B}_{n,1}]'\\).Eq. @ref(eq:eps_rho) gives:\n\\[\\begin{eqnarray*}\n\\varepsilon_{1,t} &=& \\tilde\\eta_{1,t} + u_{1,t}\\\\\n\\varepsilon_{,t} &=& \\tilde{B}_{,1} \\tilde\\eta_{1,t} + u_{,t}.\n\\end{eqnarray*}\\]\nsuggests \\(\\tilde{B}_{,1}\\) can estimated regressing \\(\\varepsilon_{,t}\\) \\(\\varepsilon_{1,t}\\), using \\(z_t\\) instrument.inference? use usual TSLS standard deviations \\(\\varepsilon_{,t}\\)’s directly observed. Bootstrap procedures can resorted . James H. Stock Watson (2018) propose, particular, Gaussian parametric bootstrap:Assume estimated \\(\\{\\widehat{\\Phi}_1,\\dots,\\widehat{\\Phi}_p,\\widehat{B}_1\\}\\) using SVAR-IV approach based size-\\(T\\) sample. Generate \\(N\\) (\\(N\\) large) size-\\(T\\) samples following VAR:\n\\[\n\\left[\n\\begin{array}{cc}\n\\widehat{\\Phi}(L) & 0 \\\\\n0 & \\widehat{\\rho}(L)\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny_t \\\\\nz_t\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n\\varepsilon_t \\\\\ne_t\n\\end{array}\n\\right],\n\\]\n\\[\n\\mbox{} \\quad \\left[\n\\begin{array}{c}\n\\varepsilon_t \\\\\ne_t\n\\end{array}\n\\right]\\sim \\, ..d.\\,\\mathcal{N}\\left(\\left[\\begin{array}{c}0\\\\0\\end{array}\\right],\n\\left[\\begin{array}{cc}\n\\Omega & S'_{\\varepsilon,e}\\\\\nS_{\\varepsilon,e}& \\sigma^2_{e}\n\\end{array}\\right]\n\\right),\n\\]\n\\(\\widehat{\\rho}(L)\\) \\(\\sigma^2_{e}\\) result estimation AR process \\(z_t\\), \\(\\Omega\\) \\(S_{\\varepsilon,e}\\) sample covariances VAR/AR residuals.simulated sample (\\(\\tilde{y}_t\\) \\(\\tilde{z}_t\\), say), estimate \\(\\{\\widetilde{\\widehat{\\Phi}}_1,\\dots,\\widetilde{\\widehat{\\Phi}}_p,\\widetilde{\\widehat{B}}_1\\}\\) associated \\(\\widetilde{\\Psi}_{,1,h}\\). provides e.g. sequence \\(N\\) estimates \\(\\Psi_{,1,h}\\), quantiles conf. intervals can deduced.\nFigure 2.12: Gertler-Karadi monthly shocks, fed funds futures 3 months.\n\nFigure 2.13: Reponses monetary-policy shock, SVAR-IV approach.\nSituation B.2: LP-IVIf want posit VAR-type dynamics \\(y_t\\) –e.g. suspect true generating model may non-invertible VARMA model– can directly proceed IV-projection methods obtain \\(\\tilde\\Psi_{,1,h}\\equiv \\Psi_{,1,h}/b_{1,1}\\) (IRFs \\(\\tilde\\eta_{1,t}\\) \\(y_{,t}\\)).However, Assumptions (IV.) (IV.ii) (Eq. (2.27)) complemented (IV.iii):\n\\[\\begin{equation*}\n\\begin{array}{llll}\n(IV.iii) & \\mathbb{E}(z_t \\eta_{j,t+h}) &= 0 \\, \\mbox{ } h \\ne 0 & \\mbox{(lead-lag exogeneity)}\n\\end{array}\n\\end{equation*}\\](IV.), (IV.ii) (IV.iii) satisfied, \\(\\tilde\\Psi_{,1,h}\\) can estimated regressing \\(y_{,t+h}\\) \\(y_{1,t}\\), using \\(z_t\\) instrument, .e. considering TSLS estimation :\n\\[\\begin{equation}\ny_{,t+h} = \\alpha_i + \\tilde\\Psi_{,1,h}y_{1,t} + \\nu_{,t+h},\\tag{2.28}\n\\end{equation}\\]\n\\(\\nu_{,t+h}\\) correlated \\(y_{1,t}\\), \\(z_t\\).indeed:\n\\[\\begin{eqnarray*}\ny_{1,t} &=& \\alpha_1 + \\tilde\\eta_{1,t} + v_{1,t}\\\\\ny_{,t+h} &=& \\alpha_i + \\tilde\\Psi_{,1,h}\\tilde\\eta_{1,t} + v_{,t+h},\n\\end{eqnarray*}\\]\n\\(v_{,t+h}\\)’s uncorrelated \\(z_t\\) (IV.), (IV.ii) (IV.iii).Note , \\(h>0\\), \\(v_{,t+h}\\) (\\(\\nu_{,t+h}\\)) auto-correlated. Newey-West corrections therefore used compute std errors \\(\\tilde\\Psi_{,1,h}\\)’s estimates.Consider linear regression:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon,\n\\]\n\\(\\mathbb{E}(\\boldsymbol\\varepsilon)=0\\), explicative variables \\(\\mathbf{X}\\) supposed correlated residuals \\(\\boldsymbol\\varepsilon\\).Moreover, \\(\\boldsymbol\\varepsilon\\) supposed possibly heteroskedastic auto-correlated.consider instruments \\(\\mathbf{Z}\\), \\(\\mathbb{E}(\\mathbf{X}'\\mathbf{Z}) \\ne 0\\) \\(\\mathbb{E}(\\boldsymbol\\varepsilon'\\mathbf{Z}) = 0\\).IV estimator \\(\\boldsymbol\\beta\\) obtained regressing \\(\\hat{\\mathbf{Y}}\\) \\(\\hat{\\mathbf{X}}\\), \\(\\hat{\\mathbf{Y}}\\) \\(\\hat{\\mathbf{X}}\\) respective residuals regressions \\(\\mathbf{Y}\\) \\(\\mathbf{X}\\) \\(\\mathbf{Z}\\).\n\\[\\begin{eqnarray*}\n\\mathbf{b}_{iv} &=& [\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{Y}\\\\\n\\mathbf{b}_{iv} &=& \\boldsymbol\\beta + \\frac{1}{\\sqrt{T}}\\underbrace{T[\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}}_{=Q(\\mathbf{X},\\mathbf{Z}) \\overset{p}{\\rightarrow} \\mathbf{Q}_{xz}}\\underbrace{\\sqrt{T}\\left(\\frac{1}{T}\\mathbf{Z}'\\boldsymbol\\varepsilon\\right)}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,S)},\n\\end{eqnarray*}\\]\n\\(\\mathbf{S}\\) long-run variance \\(\\mathbf{z}_t\\varepsilon_t\\) (see next slide).asymptotic covariance matrix \\(\\sqrt{T}\\mathbf{b}_{iv}\\) \\(\\mathbf{Q}_{xz} \\mathbf{S} \\mathbf{Q}_{xz}'\\).covariance matrix \\(\\mathbf{b}_{iv}\\) can approximated \\(\\frac{1}{T}Q(\\mathbf{X},\\mathbf{Z})\\hat{\\mathbf{S}}Q(\\mathbf{X},\\mathbf{Z})'\\) \\(\\hat{\\mathbf{S}}\\) Newey-West estimator \\(\\mathbf{S}\\) (see Eq. (??))(IV.iii) usually restrictive \\(h>0\\) (\\(z_t\\) usually affected future shocks). contrast, may restrictive \\(h<0\\). can solved adding controls Regression (2.28). controls span space \\(\\{\\eta_{t-1},\\eta_{t-2},\\dots\\}\\).\\(z_t\\) suspected correlated past values \\(\\eta_{1,t}\\) \\(\\eta_{j,t}\\)’s, \\(j>1\\), one can add lags \\(z_t\\) controls (method e.g. advocated Ramey, 2016, p.108, considering instrument Gertler Karadi (2015)).general case, one can use lags \\(y_t\\) controls. Note , even (IV.iii) holds, adding controls may reduce variance regression error.noted James H. Stock Watson (2018), relevant variance long-run variance instrument-times-error term. also recommend (p.926) using leads lags \\(z_t\\) improve efficiency.\nFigure 2.14: Reponses monetary-policy shock, LP-IV approach.\n","code":"\n# Load vars package:\nlibrary(vars)\nlibrary(AEC)\ndata(\"USmonthly\")\nFirst.date <- \"1990-05-01\"\nLast.date <- \"2012-6-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nshock.name <- \"FF4_TC\" #\"FF4_TC\", \"ED2_TC\", \"ff1_vr\", \"rrshock83b\"\nindic.shock.name <- which(names(USmonthly)==shock.name)\nZ <- matrix(USmonthly[,indic.shock.name],ncol=1)\nplot(USmonthly$DATES,Z,type=\"l\",xlab=\"\",ylab=\"\")\nconsidered.variables <- c(\"GS1\",\"LIP\",\"LCPI\",\"EBP\")\ny <- as.matrix(USmonthly[,considered.variables])\nn <- length(considered.variables)\ncolnames(y) <- considered.variables\nres.svar.iv <- svar.iv(y,Z,p = 4,\n                       names.of.variables=considered.variables,\n                       nb.periods.IRF = 20,\n                       z.AR.order=1, # This is used in the parametric bootstrap only\n                       nb.bootstrap.replications = 100, # This is used in the parametric bootstrap only\n                       confidence.interval = 0.90, # This is used in the parametric bootstrap only\n                       indic.plot=1)\nres.LP.IV <- make.LPIV.irf(y,Z,\n                           nb.periods.IRF = 20,\n                           nb.lags.Y.4.control=4,\n                           nb.lags.Z.4.control=4,\n                           indic.plot = 1, # Plots are displayed if = 1.\n                           confidence.interval = 0.90)## [1] \"LP-IV approach, Currently working on horizon h=0\"\n## [1] \"LP-IV approach, Currently working on horizon h=1\"\n## [1] \"LP-IV approach, Currently working on horizon h=2\"\n## [1] \"LP-IV approach, Currently working on horizon h=3\"\n## [1] \"LP-IV approach, Currently working on horizon h=4\"\n## [1] \"LP-IV approach, Currently working on horizon h=5\"\n## [1] \"LP-IV approach, Currently working on horizon h=6\"\n## [1] \"LP-IV approach, Currently working on horizon h=7\"\n## [1] \"LP-IV approach, Currently working on horizon h=8\"\n## [1] \"LP-IV approach, Currently working on horizon h=9\"\n## [1] \"LP-IV approach, Currently working on horizon h=10\"\n## [1] \"LP-IV approach, Currently working on horizon h=11\"\n## [1] \"LP-IV approach, Currently working on horizon h=12\"\n## [1] \"LP-IV approach, Currently working on horizon h=13\"\n## [1] \"LP-IV approach, Currently working on horizon h=14\"\n## [1] \"LP-IV approach, Currently working on horizon h=15\"\n## [1] \"LP-IV approach, Currently working on horizon h=16\"\n## [1] \"LP-IV approach, Currently working on horizon h=17\"\n## [1] \"LP-IV approach, Currently working on horizon h=18\"\n## [1] \"LP-IV approach, Currently working on horizon h=19\"\n## [1] \"LP-IV approach, Currently working on horizon h=20\""},{"path":"append.html","id":"append","chapter":"3 Appendix","heading":"3 Appendix","text":"","code":""},{"path":"append.html","id":"statistical-tables","chapter":"3 Appendix","heading":"3.1 Statistical Tables","text":"Table 3.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 3.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 3.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 3.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"append.html","id":"variousResults","chapter":"3 Appendix","heading":"3.2 Statistics: definitions and results","text":"Definition 3.1  (Partial correlation) partial correlation \\(y\\) \\(z\\), controlling variables \\(\\mathbf{X}\\) sample correlation \\(y^*\\) \\(z^*\\), latter two variables residuals regressions \\(y\\) \\(\\mathbf{X}\\) \\(z\\) \\(\\mathbf{X}\\), respectively.correlation denoted \\(r_{yz}^\\mathbf{X}\\). definition, :\n\\[\\begin{equation}\nr_{yz}^\\mathbf{X} = \\frac{\\mathbf{z^*}'\\mathbf{y^*}}{\\sqrt{(\\mathbf{z^*}'\\mathbf{z^*})(\\mathbf{y^*}'\\mathbf{y^*})}}.\\tag{3.1}\n\\end{equation}\\]Definition 3.2  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).\\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]\\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Definition 3.3  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 3.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 3.4  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 3.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 3.5  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 3.4 quantiles.)Definition 3.6  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 3.2 quantiles.)Definition 3.7  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 3.3 quantiles.)Definition 3.8  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 3.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 3.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 3.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 3.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 3.6  (Chebychev's inequality) \\(\\mathbb{E}(|X|^r)\\) finite \\(r>0\\) :\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}.\n\\]\nparticular, \\(r=2\\):\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}.\n\\]Proof. Remark \\(\\varepsilon^r \\mathbb{}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) take expectation sides.Definition 3.9  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 3.10  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 3.11  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 3.12  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Proposition 3.7  (Rules limiting distributions (Slutsky)) :Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Proposition 3.8  (Implications stochastic convergences) :\n\\[\\begin{align*}\n&\\boxed{\\overset{L^s}{\\rightarrow}}& &\\underset{1 \\le r \\le s}{\\Rightarrow}& &\\boxed{\\overset{L^r}{\\rightarrow}}&\\\\\n&& && &\\Downarrow&\\\\\n&\\boxed{\\overset{.s.}{\\rightarrow}}& &\\Rightarrow& &\\boxed{\\overset{p}{\\rightarrow}}& \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}.\n\\end{align*}\\]Proof. (fact \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting \\(F\\) \\(F_n\\) c.d.f. \\(X\\) \\(X_n\\), respectively:\n\\[\\begin{equation}\nF_n(x) = \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X > x+\\varepsilon) \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\\tag{3.2}\n\\end{equation}\\]\nBesides,\n\\[\nF(x-\\varepsilon) = \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n > x) \\le F_n(x) + \\mathbb{P}(|X_n - X|>\\varepsilon),\n\\]\nimplies:\n\\[\\begin{equation}\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x).\\tag{3.3}\n\\end{equation}\\]\nEqs. (3.2) (3.3) imply:\n\\[\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x)  \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\n\\]\nTaking limits \\(n \\rightarrow \\infty\\) yields\n\\[\nF(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x)  \\le F(x+\\varepsilon).\n\\]\nresult obtained taking limits \\(\\varepsilon \\rightarrow 0\\) (\\(F\\) continuous \\(x\\)).Proposition 3.9  (Convergence distribution constant) \\(X_n\\) converges distribution constant \\(c\\), \\(X_n\\) converges probability \\(c\\).Proof. \\(\\varepsilon>0\\), \\(\\mathbb{P}(X_n < c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) .e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) \\(\\mathbb{P}(X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\),\ngives result.Example \\(plim\\) \\(L^r\\) convergence: Let \\(\\{x_n\\}_{n \\\\mathbb{N}}\\) series random variables defined :\n\\[\nx_n = n u_n,\n\\]\n\\(u_n\\) independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\).\\(x_n \\overset{p}{\\rightarrow} 0\\) \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\).Theorem 3.1  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 3.2  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]Definition 3.13  (Characteristic function) real-valued random variable \\(X\\), characteristic function defined :\n\\[\n\\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)].\n\\]Theorem 3.3  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. 3.13) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 3.4  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n &=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).Proposition 3.10  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Proposition 3.11  \\(\\mathbf{}\\) idempotent \\(\\mathbf{x}\\) Gaussian, \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}\\) independent \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\).Proof. \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\), two Gaussian vectors \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{}\\mathbf{x}\\) independent. implies independence function \\(\\mathbf{L}\\mathbf{x}\\) function \\(\\mathbf{}\\mathbf{x}\\). results follows observation \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}=(\\mathbf{}\\mathbf{x})'(\\mathbf{}\\mathbf{x})\\), function \\(\\mathbf{}\\mathbf{x}\\).Proposition 3.12  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.Theorem 3.5  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 3.14  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 3.13  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Definition 3.15  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha,\n\\]\n\\(S_n\\) test statistic \\(\\Theta\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\\\Theta\\).Definition 3.16  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1,\n\\]\n\\(S_n\\) test statistic \\(\\Theta^c\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\notin \\Theta^c\\).Definition 3.17  (Kullback discrepancy) Given two p.d.f. \\(f\\) \\(f^*\\), Kullback discrepancy defined :\n\\[\n(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy.\n\\]Proposition 3.14  (Properties Kullback discrepancy) :\\((f,f^*) \\ge 0\\)\\((f,f^*) \\ge 0\\)\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).Proof. \\(x \\rightarrow -\\log(x)\\) convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves ()). Since \\(x \\rightarrow -\\log(x)\\) strictly convex, equality () holds \\(f(Y)/f^*(Y)\\) constant (proves (ii)).Proposition 3.15  (Square absolute summability) :\n\\[\n\\underbrace{\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty}_{\\mbox{Square summability}}.\n\\]Proof. See Appendix 3.Hamilton. Idea: Absolute summability implies exist \\(N\\) , \\(j>N\\), \\(|\\theta_j| < 1\\) (deduced Cauchy criterion, Theorem 3.1 therefore \\(\\theta_j^2 < |\\theta_j|\\).","code":""},{"path":"append.html","id":"GaussianVar","chapter":"3 Appendix","heading":"3.3 Some properties of Gaussian variables","text":"Proposition 3.16  (Bayesian update vector Gaussian variables) \n\\[\n\\left[\n\\begin{array}{c}\nY_1\\\\\nY_2\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\n\\left(0,\n\\left[\\begin{array}{cc}\n\\Omega_{11} & \\Omega_{12}\\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right]\n\\right),\n\\]\n\n\\[\nY_{2}|Y_{1} \\sim \\mathcal{N}\n\\left(\n\\Omega_{21}\\Omega_{11}^{-1}Y_{1},\\Omega_{22}-\\Omega_{21}\\Omega_{11}^{-1}\\Omega_{12}\n\\right).\n\\]\n\\[\nY_{1}|Y_{2} \\sim \\mathcal{N}\n\\left(\n\\Omega_{12}\\Omega_{22}^{-1}Y_{2},\\Omega_{11}-\\Omega_{12}\\Omega_{22}^{-1}\\Omega_{21}\n\\right).\n\\]Proposition 3.17  (Truncated distributions) \\(X\\) random variable distributed according p.d.f. \\(f\\), c.d.f. \\(F\\), infinite support. p.d.f. \\(X|\\le X < b\\) \n\\[\ng(x) = \\frac{f(x)}{F(b)-F()}\\mathbb{}_{\\{\\le x < b\\}},\n\\]\n\\(<b\\).partiucular, Gaussian variable \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\nf(X=x|\\le X<b) = \\dfrac{\\dfrac{1}{\\sigma}\\phi\\left(\\dfrac{x - \\mu}{\\sigma}\\right)}{Z}.\n\\]\n\\(Z = \\Phi(\\beta)-\\Phi(\\alpha)\\), \\(\\alpha = \\dfrac{- \\mu}{\\sigma}\\) \\(\\beta = \\dfrac{b - \\mu}{\\sigma}\\).Moreover:\n\\[\\begin{eqnarray}\n\\mathbb{E}(X|\\le X<b) &=& \\mu - \\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\sigma. \\tag{3.4}\n\\end{eqnarray}\\]also :\n\\[\\begin{eqnarray}\n&& \\mathbb{V}ar(X|\\le X<b) \\nonumber\\\\\n&=& \\sigma^2\\left[\n1 -  \\frac{\\beta\\phi\\left(\\beta\\right)-\\alpha\\phi\\left(\\alpha\\right)}{Z} -  \\left(\\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\right)^2 \\right] \\tag{3.5}\n\\end{eqnarray}\\]particular, \\(b \\rightarrow \\infty\\), get:\n\\[\\begin{equation}\n\\mathbb{V}ar(X|< X) = \\sigma^2\\left[1 + \\alpha\\lambda(-\\alpha) - \\lambda(-\\alpha)^2 \\right], \\tag{3.6}\n\\end{equation}\\]\n\\(\\lambda(x)=\\dfrac{\\phi(x)}{\\Phi(x)}\\) called inverse Mills ratio.Consider case \\(\\rightarrow - \\infty\\) (.e. conditioning set \\(X<b\\)) \\(\\mu=0\\), \\(\\sigma=1\\). Eq. (3.4) gives \\(\\mathbb{E}(X|X<b) = - \\lambda(b) = - \\dfrac{\\phi(b)}{\\Phi(b)}\\), \\(\\lambda\\) function computing inverse Mills ratio.\nFigure 3.1: \\(\\mathbb{E}(X|X<b)\\) function \\(b\\) \\(X\\sim \\mathcal{N}(0,1)\\) (black).\nProposition 3.18  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"proofs","chapter":"3 Appendix","heading":"3.4 Proofs","text":"","code":""},{"path":"append.html","id":"MLEproperties","chapter":"3 Appendix","heading":"3.4.1 Proof of Proposition ??","text":"Proof. Assumptions () (ii) (set Assumptions ??) imply \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\)).\\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) can interpreted sample mean r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) ..d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – exists (Assumption iv).latter convergence uniform (Assumption v), solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges solution limit problem:\n\\[\n\\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy.\n\\]Properties Kullback information measure (see Prop. 3.14), together identifiability assumption (ii) implies solution limit problem unique equal \\(\\boldsymbol\\theta_0\\).Consider r.v. sequence \\(\\boldsymbol\\theta\\) converges \\(\\boldsymbol\\theta_0\\). Taylor expansion score neighborood \\(\\boldsymbol\\theta_0\\) yields :\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\]\\(\\boldsymbol\\theta_{MLE}\\) converges \\(\\boldsymbol\\theta_0\\) satisfies likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}\\). Therefore:\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]\nequivalently:\n\\[\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]law large numbers, : \\(\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\mathbf{}(\\boldsymbol\\theta_0) = \\mathcal{}_Y(\\boldsymbol\\theta_0)\\).Besides, :\n\\[\\begin{eqnarray*}\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} &=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\\n&=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right)\n\\end{eqnarray*}\\]\nconverges \\(\\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0))\\) CLT.Collecting preceding results leads (b). fact \\(\\boldsymbol\\theta_{MLE}\\) achieves FDCR bound proves (c).","code":""},{"path":"append.html","id":"Walddistri","chapter":"3 Appendix","heading":"3.4.2 Proof of Proposition ??","text":"Proof. \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (eq:normMLE)). Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields :\n\\[\\begin{equation}\n\\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{3.7}\n\\end{equation}\\]\n\\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore:\n\\[\\begin{equation}\n\\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{3.8}\n\\end{equation}\\]\nHence\n\\[\n\\sqrt{n} \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right).\n\\]\nTaking quadratic form, obtain:\n\\[\nn h(\\hat{\\boldsymbol\\theta}_{n})'  \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]fact test asymptotic level \\(\\alpha\\) directly stems precedes. Consistency test: Consider \\(\\theta_0 \\\\Theta\\). MLE consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (3.7) still valid. implies \\(\\xi^W_n\\) converges \\(+\\infty\\) therefore \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\).","code":""},{"path":"append.html","id":"LMdistri","chapter":"3 Appendix","heading":"3.4.3 Proof of Proposition ??","text":"Proof. Notations: “\\(\\approx\\)” means “equal term converges 0 probability”. \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes unconstrained one.combine two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (definition) get:\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{3.9}\n\\end{equation}\\]\nBesides, (using definition information matrix):\n\\[\\begin{equation}\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{3.10}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{3.11}\n\\end{equation}\\]\nTaking difference multiplying \\(\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\):\n\\[\\begin{equation}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\mathcal{}(\\boldsymbol\\theta_0).\\tag{3.12}\n\\end{equation}\\]\nEqs. (3.9) (3.12) yield :\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}.\\tag{3.13}\n\\end{equation}\\]Recall \\(\\hat{\\boldsymbol\\theta}^0_n\\) MLE \\(\\boldsymbol\\theta_0\\) constraint \\(h(\\boldsymbol\\theta)=0\\). vector Lagrange multipliers \\(\\hat\\lambda_n\\) associated program satisfies:\n\\[\\begin{equation}\n\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{3.14}\n\\end{equation}\\]\nSubstituting latter equation Eq. (3.13) gives:\n\\[\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}}\n\\]\nyields:\n\\[\\begin{equation}\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{3.15}\n\\end{equation}\\]\nfollows, Eq. (3.8), :\n\\[\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\\right).\n\\]\nTaking quadratic form last equation gives:\n\\[\n\\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]\nUsing Eq. (3.14), appears left-hand side term last equation \\(\\xi^{LM}\\) defined Eq. (??). Consistency: see Remark 17.3 Gouriéroux Monfort (1995).","code":""},{"path":"append.html","id":"equivLRLMW","chapter":"3 Appendix","heading":"3.4.4 Proof of Proposition ??","text":"Proof. Let us first demonstrate asymptotic equivalence \\(\\xi^{LM}\\) \\(\\xi^{LR}\\).second-order taylor expansions \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y})\\) \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y})\\) :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nTaking difference, obtain:\n\\[\n\\xi_n^{LR} \\approx 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nUsing \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (3.11)), :\n\\[\n\\xi_n^{LR} \\approx\n2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)'\\mathcal{}(\\boldsymbol\\theta_0)\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n)\n+ n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nsecond three terms sum, replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) develop associated product. leads :\n\\[\\begin{equation}\n\\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)' \\mathcal{}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{3.16}\n\\end{equation}\\]\ndifference Eqs. (3.10) (3.11) implies:\n\\[\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n),\n\\]\n, associated Eq. @(eq:lr10), gives:\n\\[\n\\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}.\n\\]\nHence \\(\\xi_n^{LR}\\) asymptotic distribution \\(\\xi_n^{LM}\\).Let’s show LR test consistent. , note :\n\\[\n\\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\mathbf{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\mathbf{y})}{n} = \\frac{1}{n} \\sum_{=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)] \\rightarrow \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)],\n\\]\n\\(\\boldsymbol\\theta_\\infty\\), pseudo true value, \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (definition \\(H_1\\)). Kullback inequality asymptotic identifiability \\(\\boldsymbol\\theta_0\\), follows \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] >0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) \\(H_1\\).Let us now demonstrate equivalence \\(\\xi^{LM} \\xi^{W}\\).(using Eq. (eq:multiplier)):\n\\[\n\\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n.\n\\]\nSince, \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (3.15) therefore implies :\n\\[\n\\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)' \\left(\n\\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\nh(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W},\n\\]\ngives result.","code":""},{"path":"append.html","id":"proofTVTCL","chapter":"3 Appendix","heading":"3.4.5 Proof of Eq. (??)","text":"Proof. :\n\\[\\begin{eqnarray*}\n&&T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\\n&=& T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s<t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\\n&=& \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\\n&&+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\\n&=&  \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} .\n\\end{eqnarray*}\\]\nTherefore:\n\\[\\begin{eqnarray*}\nT\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j &=& - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots\n\\end{eqnarray*}\\]\n:\n\\[\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| \\le 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\]\\(q \\le T\\), :\n\\[\\begin{eqnarray*}\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\\n&&2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots  + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\\n&\\le& \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\\n&&2|\\gamma_{q+1}| + \\dots  + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]Consider \\(\\varepsilon > 0\\). fact autocovariances absolutely summable implies exists \\(q_0\\) (Cauchy criterion, Theorem 3.1):\n\\[\n2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots < \\varepsilon/2.\n\\]\n, \\(T > q_0\\), comes :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2.\n\\]\n\\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) \n\\[\n\\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2.\n\\]\n, \\(T>f(q_0)\\) \\(T>q_0\\), .e. \\(T>\\max(f(q_0),q_0)\\), :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon.\n\\]","code":""},{"path":"append.html","id":"smallestMSE","chapter":"3 Appendix","heading":"3.4.6 Proof of Proposition ??","text":"Proof. :\n\\[\\begin{eqnarray}\n\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\\n&=&  \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)\\nonumber\\\\\n&& + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right). \\tag{3.17}\n\\end{eqnarray}\\]\nLet us focus last term. :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right)\\\\\n&=& \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\\mbox{function $x_t$}}}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.\n\\end{eqnarray*}\\]Therefore, Eq. (3.17) becomes:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\\n&=&  \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ depend $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ depends $y^*_{t+1}$}}.\n\\end{eqnarray*}\\]\nimplies \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) always larger \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), therefore minimized second term equal zero, \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\).","code":""},{"path":"append.html","id":"estimVARGaussian","chapter":"3 Appendix","heading":"3.4.7 Proof of Proposition 2.1","text":"Proof. Using Proposition ?? (Appendix ??), obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.","code":""},{"path":"append.html","id":"OLSVAR","chapter":"3 Appendix","heading":"3.4.8 Proof of Proposition 2.2","text":"Proof. Let us drop \\(\\) subscript. Rearranging Eq. (2.11), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"append.html","id":"additional-codes","chapter":"3 Appendix","heading":"3.5 Additional codes","text":"","code":""},{"path":"append.html","id":"App:GEV","chapter":"3 Appendix","heading":"3.5.1 Simulating GEV distributions","text":"following lines code used generate Figure ??.","code":"\nn.sim <- 4000\npar(mfrow=c(1,3),\n    plt=c(.2,.95,.2,.85))\nall.rhos <- c(.3,.6,.95)\nfor(j in 1:length(all.rhos)){\n  theta <- 1/all.rhos[j]\n  v1 <- runif(n.sim)\n  v2 <- runif(n.sim)\n  w <- rep(.000001,n.sim)\n  # solve for f(w) = w*(1 - log(w)/theta) - v2 = 0\n  for(i in 1:20){\n    f.i <- w * (1 - log(w)/theta) - v2\n    f.prime <- 1 - log(w)/theta - 1/theta\n    w <- w - f.i/f.prime\n  }\n  u1 <- exp(v1^(1/theta) * log(w))\n  u2 <- exp((1-v1)^(1/theta) * log(w))\n\n  # Get eps1 and eps2 using the inverse of the Gumbel distribution's cdf:\n  eps1 <- -log(-log(u1))\n  eps2 <- -log(-log(u2))\n  cbind(cor(eps1,eps2),1-all.rhos[j]^2)\n  plot(eps1,eps2,pch=19,col=\"#FF000044\",\n       main=paste(\"rho = \",toString(all.rhos[j]),sep=\"\"),\n       xlab=expression(epsilon[1]),\n       ylab=expression(epsilon[2]),\n       cex.lab=2,cex.main=1.5)\n}"},{"path":"append.html","id":"IRFDELTA","chapter":"3 Appendix","heading":"3.5.2 Computing the covariance matrix of IRF using the delta method","text":"","code":"\nirf.function <- function(THETA){\n  c <- THETA[1]\n  phi <- THETA[2:(p+1)]\n  if(q>0){\n    theta <- c(1,THETA[(1+p+1):(1+p+q)])\n  }else{\n    theta <- 1\n  }\n  sigma <- THETA[1+p+q+1]\n  r <- dim(Matrix.of.Exog)[2] - 1\n  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]\n  \n  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60,y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,\n                  X=NaN,beta=NaN)\n  return(irf)\n}\n\nIRF.0 <- 100*irf.function(x$THETA)\neps <- .00000001\nd.IRF <- NULL\nfor(i in 1:length(x$THETA)){\n  THETA.i <- x$THETA\n  THETA.i[i] <- THETA.i[i] + eps\n  IRF.i <- 100*irf.function(THETA.i)\n  d.IRF <- cbind(d.IRF,\n                 (IRF.i - IRF.0)/eps\n                 )\n}\nmat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]

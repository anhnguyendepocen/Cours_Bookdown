[{"path":"index.html","id":"prerequisites","chapter":"1 Prerequisites","heading":"1 Prerequisites","text":"sample book written Markdown. can use anything Pandoc’s Markdown supports, e.g., math equation \\(^2 + b^2 = c^2\\).bookdown package can installed CRAN Github:Remember Rmd file contains one one chapter, chapter defined first-level heading #.compile example PDF, need XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.name/tinytex/.","code":"\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"can label chapter section titles using {#label} , e.g., can reference Chapter 2. manually label , automatic labels anyway, e.g., Chapter ??.Figures tables captions placed figure table environments, respectively.\nFigure 2.1: nice figure!\nReference figure code chunk label fig: prefix, e.g., see Figure 2.1. Similarly, can reference tables generated knitr::kable(), e.g., see Table 2.1.Table 2.1: nice table!can write citations, . example, using bookdown package (Xie 2022) sample book, built top R Markdown knitr (Xie 2015).example borrowed Petersen.XXXXSargan-Hansen () test. Sargan (1958) Hansen (1982)Durbin-Wu-Hausman test: Durbin (1954) / Wu (1973) / Hausman (1978)Use R! excellent tutorial.\n(notably plm Arellano-Bond example, 140 UK firms)Program evaluation (good survey): Abadie Cattaneo (2018)\nMostly harmless: Angrist Pischke (2008)Diff--Diff: Card Krueger (1994)XXXX","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(iris, 20), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)\nlibrary(sandwich)\n## Petersen's data\ndata(\"PetersenCL\", package = \"sandwich\")\nm <- lm(y ~ x, data = PetersenCL)\n\n## clustered covariances\n## one-way\nvcovCL(m, cluster = ~ firm)##               (Intercept)             x\n## (Intercept)  4.490702e-03 -6.473517e-05\n## x           -6.473517e-05  2.559927e-03\nvcovCL(m, cluster = PetersenCL$firm) ## same##               (Intercept)             x\n## (Intercept)  4.490702e-03 -6.473517e-05\n## x           -6.473517e-05  2.559927e-03\n## one-way with HC2\nvcovCL(m, cluster = ~ firm, type = \"HC2\")##               (Intercept)             x\n## (Intercept)  4.494487e-03 -6.592912e-05\n## x           -6.592912e-05  2.568236e-03\n## two-way\nvcovCL(m, cluster = ~ firm + year)##               (Intercept)             x\n## (Intercept)  4.233313e-03 -2.845344e-05\n## x           -2.845344e-05  2.868462e-03\nvcovCL(m, cluster = PetersenCL[, c(\"firm\", \"year\")]) ## same##               (Intercept)             x\n## (Intercept)  4.233313e-03 -2.845344e-05\n## x           -2.845344e-05  2.868462e-03"},{"path":"linear-regressions.html","id":"linear-regressions","chapter":"3 Linear Regressions","heading":"3 Linear Regressions","text":"","code":""},{"path":"linear-regressions.html","id":"specification","chapter":"3 Linear Regressions","heading":"3.1 Specification","text":"Definition 3.1  linear regression model defined :\n\\[\ny_i = \\boldsymbol\\beta'{\\bf x}_{} + \\varepsilon_i,\n\\]\n\\({\\bf x}_{}=[x_{,1},\\dots,x_{,K}]'\\) vector dimension \\(K \\times 1\\).entity \\(\\), \\(x_{,k}\\)’s, \\(k \\\\{1,\\dots,K\\}\\), explanatory variables. one wants intercept specification, set \\(x_{,1}=1\\) \\(\\), \\(\\beta_1\\) corresponds intercept.Hypothesis 3.1  (Full rank) exact linear relationship among independent variables (\\(x_{,k}\\)s, given \\(\\\\{1,\\dots,n\\}\\)).Hypothesis 3.2  (Conditional mean-zero assumption) \\[\\begin{equation}\n\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X}) = 0.\n\\end{equation}\\]Note , Hypothesis 3.2, \\(\\boldsymbol\\varepsilon\\) \\(n\\)-dimensional vector (\\(n\\) sample size), \\(\\mathbf{X}\\) matrix containing explanatory variables, dimension \\(n \\times K\\).Proposition 3.1  Hypothesis 3.2:\\(\\mathbb{E}(\\varepsilon_{})=0\\);\\(\\mathbb{E}(\\varepsilon_{})=0\\);\\(x_{ij}\\)s \\(\\varepsilon_{}\\)s uncorrelated, .e. \\(\\forall ,\\,j \\quad \\mathbb{C}orr(x_{ij},\\varepsilon_{})=0\\).\\(x_{ij}\\)s \\(\\varepsilon_{}\\)s uncorrelated, .e. \\(\\forall ,\\,j \\quad \\mathbb{C}orr(x_{ij},\\varepsilon_{})=0\\).Proof. Let us prove () (ii):law iterated expectations:\n\\[\n\\mathbb{E}(\\boldsymbol\\varepsilon)=\\mathbb{E}(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X}))=\\mathbb{E}(0)=0.\n\\]\\(\\mathbb{E}(x_{ij}\\varepsilon_i)=\\mathbb{E}(\\mathbb{E}(x_{ij}\\varepsilon_i|\\mathbf{X}))=\\mathbb{E}(x_{ij}\\underbrace{\\mathbb{E}(\\varepsilon_i|\\mathbf{X})}_{=0})=0\\).\\(\\square\\)Hypothesis 3.3  (Homoskedasticity) \\[\n\\forall , \\quad \\mathbb{V}ar(\\varepsilon_i|\\mathbf{X}) = \\sigma^2.\n\\]\nFigure 3.1: caption\nPanel (b) Figure 3.1 corresponds situation heteroskedasticity. Let us specific. two plots, \\(X_i \\sim \\mathcal{N}(0,1)\\) \\(\\varepsilon^*_i \\sim \\mathcal{N}(0,1)\\). Panel () (homoskedasticity): \\(Y_i = 2 + 2X_i + \\varepsilon^*_i\\). Panel (b) (heteroskedasticity): \\(Y_i = 2 + 2X_i + \\left(2\\mathbb{1}_{\\{X_i<0\\}}+0.2\\mathbb{1}_{\\{X_i\\ge0\\}}\\right)\\varepsilon^*_i\\).\nFigure 3.2: Salary versus years PhD\nHypothesis 3.4  (Uncorrelated residuals) \\[\n\\forall \\ne j, \\quad \\mathbb{C}ov(\\varepsilon_i,\\varepsilon_j|\\mathbf{X})=0.\n\\]Proposition 3.2  3.3 3.4 hold, :\n\\[\n\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})= \\sigma^2 Id,\n\\]\n\\(Id\\) \\(n \\times n\\) identity matrix.Hypothesis 3.5  (Normal distribution) \\[\n\\forall , \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2).\n\\]","code":"\n# load data into R\ndata(Salaries, package = \"carData\")\n# first six rows of the data\nhead(Salaries)##        rank discipline yrs.since.phd yrs.service  sex salary\n## 1      Prof          B            19          18 Male 139750\n## 2      Prof          B            20          16 Male 173200\n## 3  AsstProf          B             4           3 Male  79750\n## 4      Prof          B            45          39 Male 115000\n## 5      Prof          B            40          41 Male 141500\n## 6 AssocProf          B             6           6 Male  97000\n# Regression:\neq <- lm(salary~.,data=Salaries)\nsummary(eq)## \n## Call:\n## lm(formula = salary ~ ., data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -65248 -13211  -1775  10384  99592 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    65955.2     4588.6  14.374  < 2e-16 ***\n## rankAssocProf  12907.6     4145.3   3.114  0.00198 ** \n## rankProf       45066.0     4237.5  10.635  < 2e-16 ***\n## disciplineB    14417.6     2342.9   6.154 1.88e-09 ***\n## yrs.since.phd    535.1      241.0   2.220  0.02698 *  \n## yrs.service     -489.5      211.9  -2.310  0.02143 *  \n## sexMale         4783.5     3858.7   1.240  0.21584    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 22540 on 390 degrees of freedom\n## Multiple R-squared:  0.4547, Adjusted R-squared:  0.4463 \n## F-statistic:  54.2 on 6 and 390 DF,  p-value: < 2.2e-16\npar(mfrow=c(1,1))\npar(plt=c(.2,.95,.2,.95))\nplot(salary/1000~yrs.since.phd,pch=19,xlab=\"years since PhD\",ylab=\"Salary\",data=Salaries,las=1)\nabline(lm(salary/1000~yrs.since.phd,data=Salaries),col=\"red\",lwd=2)"},{"path":"linear-regressions.html","id":"least-square-estimation","chapter":"3 Linear Regressions","heading":"3.2 Least square estimation","text":"given vector coefficients \\(\\mathbf{b}=[b_1,\\dots,b_K]'\\), sum square residuals :\n\\[\nf(\\mathbf{b}) =\\sum_{=1}^n \\left(y_i - \\sum_{j=1}^K x_{,j} b_j \\right)^2 = \\sum_{=1}^n (y_i - \\mathbf{x}_i' \\mathbf{b})^2.\n\\]\nMinimizing sum squared residuals amounts minimizing:\n\\[\nf(\\mathbf{b}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{b})'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}).\n\\]:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = - 2 \\mathbf{X}'\\mathbf{y} + 2 \\mathbf{X}'\\mathbf{X}\\mathbf{b}.\n\\]\nNecessary first-order condition (FOC):\n\\[\\begin{equation}\n\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}.\\tag{3.1}\n\\end{equation}\\]\nAssumption 3.1, \\(\\mathbf{X}'\\mathbf{X}\\) invertible. Hence:\n\\[\n\\boxed{\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y}.}\n\\]\nVector \\(\\mathbf{b}\\) minimises sum squared residuals. (\\(f\\) non-negative quadratic function, admits minimum.)estimated residuals :\n\\[\\begin{equation}\n\\mathbf{e} = \\mathbf{y} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y} = \\mathbf{M} \\mathbf{y}\\tag{3.2}\n\\end{equation}\\]\n\\(\\mathbf{M} := \\mathbf{} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\) called residual maker matrix. Let us define projection matrix \\(\\mathbf{P}=\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\). matrices \\(\\mathbf{M}\\) \\(\\mathbf{P}\\) :\\(\\mathbf{M} \\mathbf{X} = \\mathbf{0}\\): one regresses one explanatory variables \\(\\mathbf{X}\\), residuals null.\\(\\mathbf{M}\\mathbf{y}=\\mathbf{M}\\boldsymbol\\varepsilon\\) (\\(\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon\\) \\(\\mathbf{M} \\mathbf{X} = \\mathbf{0}\\)).fitted values :\n\\[\\begin{equation}\n\\hat{\\mathbf{y}}=\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y} = \\mathbf{P} \\mathbf{y},\\tag{3.3}\n\\end{equation}\\]\n.e., \\(\\hat{\\mathbf{y}}\\) projection vector \\(\\mathbf{y}\\) onto vectorial space spanned columns \\(\\mathbf{X}\\).can shown column \\(\\tilde{\\mathbf{x}}_k\\) \\(\\mathbf{X}\\) orthogonal \\(\\mathbf{e}\\). \\(\\Rightarrow\\) intercepts included regression (\\(x_{,1} \\equiv 1\\)), average residuals null.properties \\(\\mathbf{M}\\) \\(\\mathbf{P}\\):\\(\\mathbf{M}\\) symmetric (\\(\\mathbf{M} = \\mathbf{M}'\\)) idempotent (\\(\\mathbf{M} = \\mathbf{M}^2 = \\mathbf{M}^k\\) \\(k>0\\)).\\(\\mathbf{P}\\) symmetric idempotent.\\(\\mathbf{P}\\mathbf{X} = \\mathbf{X}\\).\\(\\mathbf{P} \\mathbf{M} = \\mathbf{M} \\mathbf{P} = 0\\).\\(\\mathbf{y} = \\mathbf{P}\\mathbf{y} + \\mathbf{M}\\mathbf{y}\\) (decomposition \\(\\mathbf{y}\\) two orthogonal parts).Proposition 3.3  (Properties OLS estimator) :Assumptions 3.1 3.2, OLS estimator linear unbiased.Assumptions 3.1 3.2, OLS estimator linear unbiased.Hypotheses 3.1 3.4, conditional covariance matrix \\(\\mathbf{b}\\) : \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\).Hypotheses 3.1 3.4, conditional covariance matrix \\(\\mathbf{b}\\) : \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\).Proof. Hypothesis 3.1, \\(\\mathbf{X}'\\mathbf{X}\\) can inverted. :\n\\[\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}.\n\\]Let us consider expectation last term, .e. \\(\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon})\\). Using law iterated expectations, obtain:\n\\[\n\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}) = \\mathbb{E}(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}|\\mathbf{X}]) = \\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbb{E}[\\mathbf{\\varepsilon}|\\mathbf{X}]).\n\\]\nHypothesis 3.2, \\(\\mathbb{E}[\\mathbf{\\varepsilon}|\\mathbf{X}]=0\\). Hence \\(\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}) =0\\) result () follows.\\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbb{E}(\\boldsymbol\\varepsilon\\boldsymbol\\varepsilon'|\\mathbf{X}) \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1}\\).\nProp. 3.2, 3.3 3.4 hold, \\(\\mathbb{E}(\\boldsymbol\\varepsilon\\boldsymbol\\varepsilon'|\\mathbf{X})=\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})=\\sigma^2 Id\\). result follows. \\(\\square\\)","code":""},{"path":"linear-regressions.html","id":"bivariate-case","chapter":"3 Linear Regressions","heading":"3.2.1 Bivariate case","text":"Consider bivariate situation, regress\\(y_i\\) constant explanatory variable \\(w_i\\). \\(K=2\\), \\(\\mathbf{X}\\) \\(n \\times 2\\) matrix whose \\(^{th}\\) row \\([x_{,1},x_{,2}]\\), \\(x_{,1}=1\\) (account intercept) \\(w_i = x_{,2}\\) (say).:\n\\[\\begin{eqnarray*}\n\\mathbf{X}'\\mathbf{X} &=&\n\\left[\\begin{array}{cc}\nn & \\sum_i w_i \\\\\n\\sum_i w_i & \\sum_i w_i^2\n\\end{array}\n\\right],\\\\\n(\\mathbf{X}'\\mathbf{X})^{-1} &=&\n\\frac{1}{n\\sum_i w_i^2-(\\sum_i w_i)^2}\n\\left[\\begin{array}{cc}\n\\sum_i w_i^2 & -\\sum_i w_i \\\\\n-\\sum_i w_i & n\n\\end{array}\n\\right],\\\\\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} &=&\n\\frac{1}{n\\sum_i w_i^2-(\\sum_i w_i)^2}\n\\left[\\begin{array}{c}\n\\sum_i w_i^2\\sum_i y_i -\\sum_i w_i \\sum_i w_iy_i \\\\\n-\\sum_i w_i \\sum_i y_i + n \\sum_i w_i y_i\n\\end{array}\n\\right]\\\\\n&=& \\frac{1}{\\frac{1}{n}\\sum_i(w_i - \\bar{w})^2}\n\\left[\\begin{array}{c}\n\\frac{\\bar{y}}{n}\\sum_i w_i^2 -\\frac{\\bar{w}}{n}\\sum_i w_iy_i \\\\\n\\frac{1}{n}\\sum_i (w_i-\\bar{w})(y_i-\\bar{y})\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]can seen second element \\(\\mathbf{b}=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\) :\n\\[\nb_2 = \\frac{\\overline{\\mathbb{C}ov(W,Y)}}{\\overline{\\mathbb{V}ar(W)}},\n\\]\n\\(\\overline{\\mathbb{C}ov(W,Y)}\\) \\(\\overline{\\mathbb{V}ar(W)}\\) sample estimates.Since constant regression, \\(b_1 = \\bar{y} - b_2 \\bar{w}\\).","code":""},{"path":"linear-regressions.html","id":"gauss-markow-theorem","chapter":"3 Linear Regressions","heading":"3.2.2 Gauss Markow Theorem","text":"Theorem 3.1  (Gauss-Markov Theorem) Assumptions 3.1 3.4, vector \\(w\\), minimum-variance linear unbiased estimator \\(w' \\boldsymbol\\beta\\) \\(w' \\mathbf{b}\\), \\(\\mathbf{b}\\) least squares estimator. (BLUE: Best Linear Unbiased Estimator.)Proof. Consider \\(\\mathbf{b}^* = C \\mathbf{y}\\), another linear unbiased estimator \\(\\boldsymbol\\beta\\). Since unbiased, must \\(\\mathbb{E}(C\\mathbf{y}|\\mathbf{X}) = \\mathbb{E}(C\\mathbf{X}\\boldsymbol\\beta + C\\boldsymbol\\varepsilon|\\mathbf{X}) = \\boldsymbol\\beta\\). \\(\\mathbb{E}(C\\boldsymbol\\varepsilon|\\mathbf{X})=C\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X})=0\\) (3.2).Therefore \\(\\mathbf{b}^*\\) unbiased \\(\\mathbb{E}(C\\mathbf{X})\\boldsymbol\\beta=\\boldsymbol\\beta\\). case \\(\\boldsymbol\\beta\\), implies must \\(C\\mathbf{X}=\\mathbf{}\\).\\Let us compute \\(\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X})\\). , introduce \\(D = C - (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\), \\(D\\mathbf{y}=\\mathbf{b}^*-\\mathbf{b}\\). fact \\(C\\mathbf{X}=\\mathbf{}\\) implies \\(D\\mathbf{X} = \\mathbf{0}\\).\\(\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X}) = \\mathbb{V}ar(C \\mathbf{y}|\\mathbf{X}) =\\mathbb{V}ar(C \\boldsymbol\\varepsilon|\\mathbf{X}) = \\sigma^2CC'\\) (Assumptions 3.3 3.4, see Prop. 3.2). Using \\(C=D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) exploiting fact \\(D\\mathbf{X} = \\mathbf{0}\\) leads :\n\\[\n\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X}) =\\sigma^2\\left[(D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')(D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')'\\right] = \\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) + \\sigma^2 \\mathbf{D}\\mathbf{D}'.\n\\]\nTherefore, \\(\\mathbb{V}ar(w'\\mathbf{b^*}|\\mathbf{X})=w'\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})w + \\sigma^2 w'\\mathbf{D}\\mathbf{D}'w\\ge w'\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})w=\\mathbb{V}ar(w'\\mathbf{b}|\\mathbf{X})\\). \\(\\square\\)","code":""},{"path":"linear-regressions.html","id":"frish-waugh","chapter":"3 Linear Regressions","heading":"3.2.3 Frish-Waugh","text":"Consider linear least square regression \\(\\mathbf{y}\\) \\(\\mathbf{X}\\). introduce notations:\\(\\mathbf{b}^{\\mathbf{y}/\\mathbf{X}}\\): OLS estimates \\(\\boldsymbol\\beta\\),\\(\\mathbf{M}^{\\mathbf{X}}\\): residual-maker matrix regression \\(\\mathbf{X}\\),\\(\\mathbf{P}^{\\mathbf{X}}\\): projection matrix regression \\(\\mathbf{X}\\).Consider case two sets explanatory variables: \\(\\mathbf{X} = [\\mathbf{X}_1,\\mathbf{X}_2]\\). obvious notations: \\(\\mathbf{b}^{\\mathbf{y}/\\mathbf{X}}=[\\mathbf{b}_1',\\mathbf{b}_2']'\\).Theorem 3.2  (Frisch-Waugh Theorem) :\n\\[\n\\mathbf{b}_2 = \\mathbf{b}^{\\mathbf{M^{\\mathbf{X}_1}y}/\\mathbf{M^{\\mathbf{X}_1}\\mathbf{X}_2}}.\n\\]Proof. minimization least squares leads (first-order conditions, see Eq. (3.1)):\n\\[\n\\left[ \\begin{array}{cc} \\mathbf{X}_1'\\mathbf{X}_1 & \\mathbf{X}_1'\\mathbf{X}_2 \\\\ \\mathbf{X}_2'\\mathbf{X}_1 & \\mathbf{X}_2'\\mathbf{X}_2\\end{array}\\right]\n\\left[ \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2\\end{array}\\right] =\n\\left[ \\begin{array}{c} \\mathbf{X}_1' \\mathbf{y} \\\\ \\mathbf{X}_2' \\mathbf{y} \\end{array}\\right].\n\\]\nUse first-row block equations solve \\(\\mathbf{b}_1\\) first; comes function \\(\\mathbf{b}_2\\). use second set equations solve \\(\\mathbf{b}_2\\), leads :\n\\[\n\\mathbf{b}_2 = [\\mathbf{X}_2'\\mathbf{X}_2 - \\mathbf{X}_2'\\mathbf{X}_1(\\mathbf{X}_1'\\mathbf{X}_1)\\mathbf{X}_1'\\mathbf{X}_2]^{-1}\\mathbf{X}_2'(Id - \\mathbf{X}_1(\\mathbf{X}_1'\\mathbf{X}_1)\\mathbf{X}_1')\\mathbf{y}=[\\mathbf{X}_2' \\mathbf{M}^{\\mathbf{X}_1}\\mathbf{X}_2]^{-1}\\mathbf{X}_2'\\mathbf{M}^{\\mathbf{X}_1}\\mathbf{y}.\n\\]\nUsing fact \\(\\mathbf{M}^{\\mathbf{X}_1}\\) idempotent symmetric leads result.suggests second way estimating \\(\\mathbf{b}_2\\):Regress \\(Y\\) \\(X_1\\), regress \\(X_2\\) \\(X_1\\).Regress former residuals latter.\\(b_2\\) scalar (\\(\\mathbf{X}_2\\) dimension \\(n \\times 1\\)), Theorem 3.2 leads :\n\\[\nb_2 = \\frac{\\mathbf{X}_2'M^{\\mathbf{X}_1}\\mathbf{y}}{\\mathbf{X}_2'M^{\\mathbf{X}_1}\\mathbf{X}_2} \\quad \\text{(partial regression coefficient)}.\n\\]","code":"\nData <- read.csv(\"https://raw.githubusercontent.com/jrenne/Data4courses/master/parapluie/data4parapluie.csv\")\ndummies <- as.matrix(Data[,4:14])\neq_all <- lm(parapluie~precip+dummies,data=Data)\nsummary(eq_all)$coefficients##                Estimate Std. Error    t value     Pr(>|t|)\n## (Intercept)  -2.9534478 3.40601605 -0.8671268 0.3893855798\n## precip        0.1300055 0.03594492  3.6167985 0.0006192876\n## dummiesX1    -8.5990682 3.30046623 -2.6054101 0.0115966151\n## dummiesX2   -13.3290386 3.30657128 -4.0310755 0.0001613897\n## dummiesX3    -7.9829582 3.25949898 -2.4491366 0.0173091333\n## dummiesX4    -3.3923533 3.27605582 -1.0354992 0.3046614769\n## dummiesX5    -3.7038158 3.25710094 -1.1371511 0.2600724511\n## dummiesX6    -3.3606412 3.27334327 -1.0266694 0.3087670632\n## dummiesX7    -7.3158812 3.28491529 -2.2271141 0.0297682836\n## dummiesX8    -7.7172773 3.26128164 -2.3663327 0.0212677987\n## dummiesX9    -4.6491997 3.26005024 -1.4261129 0.1591057652\n## dummiesX10   -5.1091987 3.25143961 -1.5713651 0.1214457733\n## dummiesX11    1.9807700 3.25942144  0.6077060 0.5457145714\ndeseas_parapluie <- lm(parapluie~dummies,data=Data)$residuals\ndeseas_precip    <- lm(precip~dummies,data=Data)$residuals\neq_frac <- lm(deseas_parapluie~deseas_precip)\nsummary(eq_frac)$coefficients##                    Estimate Std. Error       t value     Pr(>|t|)\n## (Intercept)   -3.265931e-16 0.60898084 -5.362946e-16 1.0000000000\n## deseas_precip  1.300055e-01 0.03300004  3.939557e+00 0.0001907741"},{"path":"linear-regressions.html","id":"goodness-of-fit","chapter":"3 Linear Regressions","heading":"3.2.4 Goodness of fit","text":"Define total variation \\(y\\) sum squared deviations:\n\\[\nTSS = \\sum_{=1}^{n} (y_i - \\bar{y})^2.\n\\]\n:\n\\[\n\\mathbf{y} = \\mathbf{X}\\mathbf{b} + \\mathbf{e} = \\hat{\\mathbf{y}} + \\mathbf{e}\n\\]\nfollowing, assume regression includes constant (.e. \\(\\), \\(x_{,1}=1\\)). Denote \\(\\mathbf{M}^0\\) matrix transforms observations deviations sample means. Using \\(\\mathbf{M}^0 \\mathbf{e} = \\mathbf{e}\\) \\(\\mathbf{X}' \\mathbf{e}=0\\), :\n\\[\\begin{eqnarray*}\n\\underbrace{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}}_{\\mbox{Total sum sq.}} &=& (\\mathbf{X}\\mathbf{b} + \\mathbf{e})' \\mathbf{M}^0 (\\mathbf{X}\\mathbf{b} + \\mathbf{e})\\\\\n&=& \\underbrace{\\mathbf{b}' \\mathbf{X}' \\mathbf{M}^0 \\mathbf{X}\\mathbf{b}}_{\\mbox{\"Explained\" sum sq.}} + \\underbrace{\\mathbf{e}'\\mathbf{e}}_{\\mbox{Sum sq. residuals}}\\\\\nTSS &=& Expl.SS + SSR.\n\\end{eqnarray*}\\]\\[\\begin{equation}\n\\boxed{\\mbox{Coefficient determination} = \\frac{Expl.SS}{TSS} = 1 - \\frac{SSR}{TSS} = 1 - \\frac{\\mathbf{e}'\\mathbf{e}}{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}}.}\\tag{3.4}\n\\end{equation}\\]can shown [Greene, 2012, Section 3.5] :\n\\[\n\\mbox{Coefficient determination} = \\frac{[\\sum_{=1}^n(y_i - \\bar{y})(\\hat{y_i} - \\bar{y})]^2}{\\sum_{=1}^n(y_i - \\bar{y})^2 \\sum_{=1}^n(\\hat{y_i} - \\bar{y})^2}.\n\\]\n\\(\\Rightarrow\\) \\(R^2\\) sample squared correlation \\(y\\) (regression-implied) \\(y\\)’s predictions.partial correlation \\(y\\) \\(z\\), controlling variables \\(\\mathbf{X}\\) sample correlation \\(y^*\\) \\(z^*\\), latter two variables residuals regressions \\(y\\) \\(\\mathbf{X}\\) \\(z\\) \\(\\mathbf{X}\\), respectively.correlation denoted \\(r_{yz}^\\mathbf{X}\\). definition, :\n\\[\\begin{equation}\nr_{yz}^\\mathbf{X} = \\frac{\\mathbf{z^*}'\\mathbf{y^*}}{\\sqrt{(\\mathbf{z^*}'\\mathbf{z^*})(\\mathbf{y^*}'\\mathbf{y^*})}}.\\tag{3.5}\n\\end{equation}\\]Proposition 3.4  (Change SSR variable added) :\n\\[\\begin{equation}\n\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e} - c^2(\\mathbf{z^*}'\\mathbf{z^*}) \\qquad (\\le \\mathbf{e}'\\mathbf{e}) \\tag{3.6}\n\\end{equation}\\]\n() \\(\\mathbf{u}\\) \\(\\mathbf{e}\\) residuals regressions \\(\\mathbf{y}\\) \\([\\mathbf{X},\\mathbf{z}]\\) \\(\\mathbf{y}\\) \\(\\mathbf{X}\\), respectively, (ii) \\(c\\) regression coefficient \\(\\mathbf{z}\\) former regression \\(\\mathbf{z}^*\\) residuals regression \\(\\mathbf{z}\\) \\(\\mathbf{X}\\).Proof. OLS estimates \\([\\mathbf{d}',\\mathbf{c}]'\\) regression \\(\\mathbf{y}\\) \\([\\mathbf{X},\\mathbf{z}]\\) satisfies (first-order cond., Eq. (3.1))\n\\[\n\\left[ \\begin{array}{cc} \\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{z} \\\\ \\mathbf{z}'\\mathbf{X} & \\mathbf{z}'\\mathbf{z}\\end{array}\\right]\n\\left[ \\begin{array}{c} \\mathbf{d} \\\\ \\mathbf{c}\\end{array}\\right] =\n\\left[ \\begin{array}{c} \\mathbf{X}' \\mathbf{y} \\\\ \\mathbf{z}' \\mathbf{y} \\end{array}\\right].\n\\]\nHence, particular \\(\\mathbf{d} = \\mathbf{b} - (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{z}\\mathbf{c}\\), \\(\\mathbf{b}\\) OLS \\(\\mathbf{y}\\) \\(\\mathbf{X}\\). Substituting \\(\\mathbf{u} = \\mathbf{y} - \\mathbf{X}\\mathbf{d} - \\mathbf{z}c\\), get \\(\\mathbf{u} = \\mathbf{e} - \\mathbf{z}^*c\\). therefore :\n\\[\\begin{equation}\n\\mathbf{u}'\\mathbf{u} = (\\mathbf{e} - \\mathbf{z}^*c)(\\mathbf{e} - \\mathbf{z}^*c)= \\mathbf{e}'\\mathbf{e} + c^2(\\mathbf{z^*}'\\mathbf{z^*}) - 2 c\\mathbf{z^*}'\\mathbf{e}.\\tag{3.7}\n\\end{equation}\\]\nNow \\(\\mathbf{z^*}'\\mathbf{e} = \\mathbf{z^*}'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}) = \\mathbf{z^*}'\\mathbf{y}\\) \\(\\mathbf{z}^*\\) residuals OLS regression \\(\\mathbf{X}\\). Since \\(c = (\\mathbf{z^*}'\\mathbf{z^*})^{-1}\\mathbf{z^*}'\\mathbf{y^*}\\) (application Theorem 3.2), \\((\\mathbf{z^*}'\\mathbf{z^*})c = \\mathbf{z^*}'\\mathbf{y^*}\\) , therefore, \\(\\mathbf{z^*}'\\mathbf{e} = (\\mathbf{z^*}'\\mathbf{z^*})c\\). Inserting Eq. (3.7) leads results. \\(\\square\\)Proposition 3.5  (Change coefficient determination variable added) Denoting \\(R_W^2\\) coefficient determination regression \\(\\mathbf{y}\\) variable \\(\\mathbf{W}\\), :\n\\[\nR_{\\mathbf{X},\\mathbf{z}}^2 = R_{\\mathbf{X}}^2 + (1-R_{\\mathbf{X}}^2)(r_{yz}^\\mathbf{X})^2,\n\\]\n\\(r_{yz}^\\mathbf{X}\\) coefficient partial correlation.Proof. Let’s use notations Prop. @ref{prp:chgeR2}. Theorem 3.2 implies \\(c = (\\mathbf{z^*}'\\mathbf{z^*})^{-1}\\mathbf{z^*}'\\mathbf{y^*}\\). Using Eq. (3.6) gives \\(\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e} - (\\mathbf{z^*}'\\mathbf{y^*})^2/(\\mathbf{z^*}'\\mathbf{z^*})\\). Using definition partial correlation (Eq. (3.5)), get \\(\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e}\\left(1 - (r_{yz}^\\mathbf{X})^2\\right)\\). results obtained dividing sides previous equation \\(\\mathbf{y}'\\mathbf{M}_0\\mathbf{y}\\). \\(\\square\\)previous theorem shows necessarily increase \\(R^2\\) add variables, even irrelevant.adjusted \\(R^2\\), denoted \\(\\bar{R}^2\\), fit measure penalizes large numbers regressors:\n\\[\\begin{equation*}\n\\boxed{\\bar{R}^2 = 1 - \\frac{\\mathbf{e}'\\mathbf{e}/(n-K)}{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}/(n-1)} = 1 - \\frac{n-1}{n-K}(1-R^2).}\n\\end{equation*}\\]","code":"\npar(mfrow=c(1,2))\npar(plt=c(.3,.95,.2,.85))\nN <- 100\neps <- rnorm(N)\nX <- rnorm(N)\nY <- 1 + X + eps\nplot(X,Y,pch=19,main=\"(a) Low R2\")\nY <- 1 + X + .1*eps\nplot(X,Y,pch=19,main=\"(b) High R2\")"},{"path":"linear-regressions.html","id":"inference-and-prediction","chapter":"3 Linear Regressions","heading":"3.2.5 Inference and Prediction","text":"normality assumption (Assumption 3.5), know distribution \\(\\mathbf{b}\\) (conditional \\(\\mathbf{X}\\)). Indeed, \\((\\mathbf{b}|\\mathbf{X}) \\equiv (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y}\\) multivariate Gaussian:\n\\[\\begin{equation}\n\\mathbf{b}|\\mathbf{X} \\sim \\mathcal{N}(\\beta,\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}).\\tag{3.8}\n\\end{equation}\\]Problem: practice, know \\(\\sigma^2\\) (population parameter).Proposition 3.6  3.1 3.4, unbiased estimate \\(\\sigma^2\\) given :\n\\[\\begin{equation}\ns^2 = \\frac{\\mathbf{e}'\\mathbf{e}}{n-K}.\\tag{3.9}\n\\end{equation}\\]\n(sometimes denoted \\(\\sigma^2_{OLS}\\).)Proof. \\(\\mathbb{E}(\\mathbf{e}'\\mathbf{e}|\\mathbf{X})=\\mathbb{E}(\\boldsymbol{\\varepsilon}'\\mathbf{M}\\boldsymbol{\\varepsilon}|\\mathbf{X})=\\mathbb{E}(\\mbox{Tr}(\\boldsymbol{\\varepsilon}'\\mathbf{M}\\boldsymbol{\\varepsilon})|\\mathbf{X})) =\\mbox{Tr}(\\mathbf{M}\\mathbb{E}(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'|\\mathbf{X}))=\\sigma^2 \\mbox{Tr}(\\mathbf{M})\\). (Note \\(\\mathbb{E}(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'|\\mathbf{X})=\\sigma^2Id\\) Assumptions 3.3 3.4, see Prop. 3.2.) Finally: \\(\\mbox{Tr}(\\mathbf{M})=n-\\mbox{Tr}(\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')=n-\\mbox{Tr}((\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X})=n-\\mbox{Tr}(Id_{K\\times K})\\). \\(\\square\\)Two results prove important perform hypothesis testing:know distribution \\(s^2\\) (Prop. 3.7).\\(s^2\\) \\(\\mathbf{b}\\) independent random variables (Prop. 3.8).Proposition 3.7  3.1 3.5, : \\(\\dfrac{s^2}{\\sigma^2} | \\mathbf{X} \\sim \\chi^2(n-K)/(n-K)\\).Proof. \\(\\mathbf{e}'\\mathbf{e}=\\boldsymbol\\varepsilon'\\mathbf{M}\\boldsymbol\\varepsilon\\). \\(\\mathbf{M}\\) idempotent symmetric matrix. Therefore can decomposed \\(PDP'\\) \\(D\\) diagonal matrix \\(P\\) orthogonal matrix. result \\(\\mathbf{e}'\\mathbf{e} = (P'\\boldsymbol\\varepsilon)'D(P'\\boldsymbol\\varepsilon)\\), .e. \\(\\mathbf{e}'\\mathbf{e}\\) weighted sum independent squared Gaussian variables (entries \\(P'\\boldsymbol\\varepsilon\\) independent Gaussian – 3.5 – uncorrelated). variance ..d. Gaussian variable \\(\\sigma^2\\). \\(\\mathbf{M}\\) idempotent symmetric matrix, eigenvalues either 0 1, rank equals trace. , trace equal \\(n-K\\) (see proof Eq. (3.9)). Therefore \\(D\\) \\(n-K\\) entries equal 1 \\(K\\) equal 0.Hence, \\(\\mathbf{e}'\\mathbf{e} = (P'\\boldsymbol\\varepsilon)'D(P'\\boldsymbol\\varepsilon)\\) sum \\(n-K\\) squared independent Gaussian variables variance \\(\\sigma^2\\). Therefore \\(\\frac{\\mathbf{e}'\\mathbf{e}}{\\sigma^2} = (n-K)\\frac{s^2}{\\sigma^2}\\) sum \\(n-k\\) squared ..d. standard normal variables. \\(\\square\\)Proposition 3.8  Hypotheses 3.1 3.5, \\(\\mathbf{b}\\) \\(s^2\\) independent.Proof. \\(\\mathbf{b}=\\boldsymbol\\beta + [\\mathbf{X}'{\\mathbf{X}}]^{-1}\\mathbf{X}\\boldsymbol\\varepsilon\\) \\(s^2 = \\boldsymbol\\varepsilon' \\mathbf{M} \\boldsymbol\\varepsilon/(n-K)\\). Hence \\(\\mathbf{b}\\) affine combination \\(\\boldsymbol\\varepsilon\\) \\(s^2\\) quadratic combination Gaussian shocks. One can write \\(s^2\\) \\(s^2 = (\\mathbf{M}\\boldsymbol\\varepsilon)' \\mathbf{M} \\boldsymbol\\varepsilon/(n-K)\\) \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta + \\mathbf{T}\\boldsymbol\\varepsilon\\). Since \\(\\mathbf{T}\\mathbf{M}=0\\), \\(\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{M}\\boldsymbol\\varepsilon\\) independent (two uncorrelated Gaussian variables independent), therefore \\(\\mathbf{b}\\) \\(s^2\\), functions respective independent variables, independent. \\(\\square\\)Hypotheses 3.1 3.5, let us consider \\(b_k\\), \\(k^{th}\\) entry \\(\\mathbf{b}\\):\n\\[\nb_k | \\mathbf{X} \\sim \\mathcal{N}(\\beta_k,\\sigma^2 v_k),\n\\]\n\\(v_k\\) k\\(^{th}\\) component diagonal \\((\\mathbf{X}'\\mathbf{X})^{-1}\\).Besides, (Prop. 3.7):\n\\[\n\\frac{(n-K)s^2}{\\sigma^2} | \\mathbf{X} \\sim \\chi ^2 (n-K).\n\\]result (using Props. 3.7 3.8), :\n\\[\\begin{equation}\n\\boxed{t_k = \\frac{\\frac{b_k - \\beta_k}{\\sqrt{\\sigma^2 v_k}}}{\\sqrt{\\frac{(n-K)s^2}{\\sigma^2(n-K)}}} = \\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} \\sim t(n-K),}\\tag{3.10}\n\\end{equation}\\]\n\\(t(n-K)\\) denotes \\(t\\) distribution \\(n-K\\) degrees freedom.Remark: \\(\\frac{b_k - \\beta_k}{\\sqrt{\\sigma^2 v_k}} | \\mathbf{X} \\sim \\mathcal{N}(0,1)\\) \\(\\frac{(n-K)s^2}{\\sigma^2} | \\mathbf{X} \\sim \\chi ^2 (n-K)\\). two distributions depend \\(\\mathbf{X}\\) \\(\\Rightarrow\\) marginal distribution \\(t_k\\) also \\(t\\).Note \\(s^2 v_k\\) exactly conditional variance \\(b_k\\): variance \\(b_k\\) conditional \\(\\mathbf{X}\\) \\(\\sigma^2 v_k\\). However \\(s^2 v_k\\) unbiased estimate \\(\\sigma^2 v_k\\) (Prop. 3.6).previous result (Eq. (3.10)) can extended linear combinations elements \\(\\mathbf{b}\\) (Eq. (3.10) \\(k^{th}\\) component ).Let us consider \\(\\boldsymbol\\alpha'\\mathbf{b}\\), OLS estimate \\(\\boldsymbol\\alpha'\\boldsymbol\\beta\\). Eq. (3.8), :\n\\[\n\\boldsymbol\\alpha'\\mathbf{b} | \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol\\alpha'\\boldsymbol\\beta,\\sigma^2 \\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha).\n\\]\nTherefore:\n\\[\n\\frac{\\boldsymbol\\alpha'\\mathbf{b} - \\boldsymbol\\alpha'\\boldsymbol\\beta}{\\sqrt{\\sigma^2 \\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha}} | \\mathbf{X} \\sim \\mathcal{N}(0,1).\n\\]\nUsing approach one used derive Eq. (3.10), one can show Props. 3.7 3.8 imply :\n\\[\\begin{equation}\n\\boxed{\\frac{\\boldsymbol\\alpha'\\mathbf{b} - \\boldsymbol\\alpha'\\boldsymbol\\beta}{\\sqrt{s^2\\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha}} \\sim t(n-K).}\\tag{3.11}\n\\end{equation}\\]\nFigure 3.3: chart shows higher degree freedom, closer distribution \\(t( u)\\) gets normal distribution.\n","code":"\npar(plt=c(.2,.95,.2,.95))\nxx <- seq(-3.5,3.5,by=.01)\nplot(xx,dnorm(xx),xlab=\"X\",ylab=\"\",type=\"l\",lwd=2)\nlines(xx,dt(xx,df=3),col=\"red\",lwd=2)\nlines(xx,dt(xx,df=7),col=\"red\",lwd=2,lty=2)\nlines(xx,dt(xx,df=20),col=\"blue\",lwd=3,lty=3)\nlegend(\"topright\",\n       c(\"N(0,1)\",\"t(3)\",\"t(7)\",\"t(20)\"),\n       lty=c(1,1,2,3), # gives the legend appropriate symbols (lines)       \n       lwd=c(2,2,2,3), # line width\n       col=c(\"black\",\"red\",\"red\",\"blue\"))"},{"path":"linear-regressions.html","id":"confidence-interval-of-beta_k","chapter":"3 Linear Regressions","heading":"3.2.6 Confidence interval of \\(\\beta_k\\)","text":"Assume want compute (symmetrical) confidence interval \\([I_{d,1-\\alpha},I_{u,1-\\alpha}]\\) \\(\\mathbb{P}(\\beta_k \\[I_{d,1-\\alpha},I_{u,1-\\alpha}])=1-\\alpha\\). particular, want : \\(\\mathbb{P}(\\beta_k < I_{d,1-\\alpha})=\\frac{\\alpha}{2}\\).purpose, make use \\(t_k = \\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} \\sim t(n-K)\\) (Eq. (3.10)).:\n\\[\n\\mathbb{P}(\\beta_k < I_{d,1-\\alpha})=\\frac{\\alpha}{2} \\Leftrightarrow\n\\]\n\\[\\begin{eqnarray*}\n\\mathbb{P}\\left(\\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} > \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} \\Leftrightarrow \\mathbb{P}\\left(t_k > \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} \\Leftrightarrow&&\\\\\n1 - \\mathbb{P}\\left(t_k \\le \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} \\Leftrightarrow \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}} = \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right),&&\n\\end{eqnarray*}\\]\n\\(\\Phi_{t(n-K)}(\\alpha)\\) c.d.f. \\(t(n-K)\\) distribution (Table @ref{tab:Studenttable}).\\(I_{u,1-\\alpha}\\), obtain:\n\\[\\begin{eqnarray*}\n&&[I_{d,1-\\alpha},I_{u,1-\\alpha}] =\\\\\n&&\\left[b_k - \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\sqrt{s^2v_k},b_k + \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\sqrt{s^2v_k}\\right].\n\\end{eqnarray*}\\]","code":""},{"path":"linear-regressions.html","id":"example","chapter":"3 Linear Regressions","heading":"3.2.7 Example","text":"following example based HRS dataset (Health Retirement Study). consider subset large dataset, focusing variables, year 2018 (wave 14). R script builds reduced dataset.last two columns give test statistic p-values associated test whose null hypothesis :\n\\[\nH_0: \\beta_k=0.\n\\]\nt-statistics, \\(b_k/\\sqrt{s^2 v_k}\\), test statistic test. \\(H_0\\), t-statistic \\(t(n-K)\\) (see Eq. (3.10)). Hence, critical region test size \\(\\alpha\\) :\n\\[\n\\left]-\\infty,-\\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\right] \\cup \\left[\\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right),+\\infty\\right[.\n\\]\np-value defined probability \\(|Z| > |t|\\), \\(t\\) (computed) t statistics \\(Z \\sim t(n-K)\\). , p-value given \\(2(1 - \\Phi_{t(n-K)}(|t_k|))\\).See webpage details regarding link critical regions, p-value, test outcomes.","code":"\nreducedHRS <- read.csv(\"https://raw.githubusercontent.com/jrenne/Data4courses/master/HRS_RAND/reduced_version/reducedHRS.csv\")\neq <- lm(riearn~raedyrs+ragey_b+I(ragey_b^2)+rfemale,data=reducedHRS)\nprint(summary(eq))## \n## Call:\n## lm(formula = riearn ~ raedyrs + ragey_b + I(ragey_b^2) + rfemale, \n##     data = reducedHRS)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -82512 -29447  -8144  18083 394724 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -1.336e+05  3.245e+04  -4.116 3.92e-05 ***\n## raedyrs       5.216e+03  2.384e+02  21.876  < 2e-16 ***\n## ragey_b       4.758e+03  1.086e+03   4.382 1.20e-05 ***\n## I(ragey_b^2) -4.441e+01  9.097e+00  -4.882 1.09e-06 ***\n## rfemale      -1.499e+04  1.498e+03 -10.007  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 50130 on 4540 degrees of freedom\n## Multiple R-squared:  0.1173, Adjusted R-squared:  0.1165 \n## F-statistic: 150.8 on 4 and 4540 DF,  p-value: < 2.2e-16"},{"path":"linear-regressions.html","id":"set-of-linear-restrictions","chapter":"3 Linear Regressions","heading":"3.2.8 Set of linear restrictions","text":"consider following model:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon, \\quad \\varepsilon \\sim ..d. \\mathcal{N}(0,\\sigma^2).\n\\]\nwant test joint validity set restrictions involving components \\(\\boldsymbol\\beta\\) linear way.Set linear restrictions:\n\\[\\begin{equation}\\label{eq:restrictions}\n\\begin{array}{ccc}\nr_{1,1} \\beta_1 + \\dots + r_{1,K} \\beta_K &=& q_1\\\\\n\\vdots && \\vdots\\\\\nr_{J,1} \\beta_1 + \\dots + r_{J,K} \\beta_K &=& q_J,\n\\end{array}\n\\end{equation}\\]\ncan written matrix form:\n\\[\\begin{equation}\n\\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}.\n\\end{equation}\\]Defin Discrepancy vector \\(\\mathbf{m} = \\mathbf{R}\\mathbf{b} - \\mathbf{q}\\). null hypothesis:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{m}|\\mathbf{X}) &=& \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} = 0 \\quad \\mbox{} \\\\\n\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X}) &=& \\mathbf{R} \\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\mathbf{R}'.\n\\end{eqnarray*}\\]\nHypotheses 3.1 3.4, \\(\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X}) = \\sigma^2 \\mathbf{R} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\) (see Prop. 3.3).Consider test:\n\\[\\begin{equation}\n\\boxed{H_0: \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} = 0 \\mbox{ } H_1: \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} \\ne 0.}\\tag{3.12}\n\\end{equation}\\]perform Wald test. 3.1 3.5 –need normality assumption– \\(H_0\\), can shown :\n\\[\\begin{equation}\nW = \\mathbf{m}'\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X})^{-1}\\mathbf{m} \\sim \\chi^2(J). \\tag{3.13}\n\\end{equation}\\]\nHowever, \\(\\sigma^2\\) unknown. Hence compute \\(W\\).can however approximate replacing \\(\\sigma^2\\) \\(s^2\\). distribution new statistic \\(\\chi^2(J)\\) ;\n\\(\\mathcal{F}\\) distribution, test called \\(F\\) test.Proposition 3.9  Hypotheses 3.1 3.5 Eq. (3.12) holds, :\n\\[\\begin{equation}\nF = \\frac{W}{J}\\frac{\\sigma^2}{s^2} = \\frac{\\mathbf{m}'(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\mathbf{m}}{s^2J} \\sim \\mathcal{F}(J,n-K),\\tag{3.14}\n\\end{equation}\\]\n\\(\\mathcal{F}\\) distribution F-statistic.Proof. According Eq. (3.13), \\(W/J \\sim \\chi^2(J)/J\\). Moreover, denominator (\\(s^2/\\sigma^2\\)) \\(\\sim \\chi^2(n-K)\\). Therefore, \\(F\\) ratio r.v. distributed \\(\\chi^2(J)/J\\) another distributed \\(\\chi^2(n-K)/(n-K)\\). remains verify r.v. independent.\\(H_0\\), \\(\\mathbf{m} = \\mathbf{R}(\\mathbf{b}-\\boldsymbol\\beta) = \\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon\\).\nTherefore \\(\\mathbf{m}'(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\mathbf{m}\\) form \\(\\boldsymbol\\varepsilon'\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{T}=\\mathbf{D}'\\mathbf{C}\\mathbf{D}\\) \\(\\mathbf{D}=\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) \\(\\mathbf{C}=(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\). Hypotheses 3.1 3.4, covariance \\(\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{M}\\boldsymbol\\varepsilon\\) \\(\\sigma^2\\mathbf{T}\\mathbf{M} = \\mathbf{0}\\). Therefore, 3.5, variables Gaussian variables 0 covariance. Hence independent. \\(\\square\\)Remark: large \\(n-K\\), \\(\\mathcal{F}_{J,n-K}\\) distribution converges \\(\\mathcal{F}_{J,\\infty}=\\chi^2(J)/J\\).Proposition 3.10  F-statistic defined Eq. (3.14) also equal :\n\\[\\begin{equation}\nF = \\frac{(R^2-R_*^2)/J}{(1-R^2)/(n-K)} =  \\frac{(SSR_{restr}-SSR_{unrestr})/J}{SSR_{unrestr}/(n-K)},\\tag{3.15}\n\\end{equation}\\]\n\\(R_*^2\\) coef. determination (Eq. (eq:RR2)) ``restricted regression’’ (SSR: sum squared residuals.)Proof. Let’s denote \\(\\mathbf{e}_*=\\mathbf{y}-\\mathbf{X}\\mathbf{b}_*\\) vector residuals associated restricted regression (.e. \\(\\mathbf{R}\\mathbf{b}_*=\\mathbf{q}\\)).\n\\(\\mathbf{e}_*=\\mathbf{e} - \\mathbf{X}(\\mathbf{b}_*-\\mathbf{b})\\). Using \\(\\mathbf{e}'\\mathbf{X}=0\\), get \\(\\mathbf{e}_*'\\mathbf{e}_*=\\mathbf{e}'\\mathbf{e} + (\\mathbf{b}_*-\\mathbf{b})'\\mathbf{X}'\\mathbf{X}(\\mathbf{b}_*-\\mathbf{b}) \\ge \\mathbf{e}'\\mathbf{e}\\).Prop. @ref(prp:constrained_LS), know \\(\\mathbf{b}_*-\\mathbf{b}=-(\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\mathbf{b} - \\mathbf{q})\\). Therefore:\n\\[\n\\mathbf{e}_*'\\mathbf{e}_* - \\mathbf{e}'\\mathbf{e} = (\\mathbf{R}\\mathbf{b} - \\mathbf{q})'[\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}']^{-1}(\\mathbf{R}\\mathbf{b} - \\mathbf{q}).\n\\]\nimplies F statistic defined Prop. 3.9 also equal :\n\\[\n\\frac{(\\mathbf{e}_*'\\mathbf{e}_* - \\mathbf{e}'\\mathbf{e})/J}{\\mathbf{e}'\\mathbf{e}/(n-K)}. \\square\n\\]null hypothesis \\(H_0\\) (Eq. (3.12)) F-test rejected \\(F\\) –defined Eq. (3.14) (3.15)– higher \\(\\mathcal{F}_{1-\\alpha}(J,n-K)\\). (Hence, test one-sided test.)","code":""},{"path":"linear-regressions.html","id":"common-pitfalls","chapter":"3 Linear Regressions","heading":"3.2.9 Common pitfalls","text":"","code":""},{"path":"linear-regressions.html","id":"multicollinearity","chapter":"3 Linear Regressions","heading":"3.2.10 Multicollinearity","text":"Consider model: \\(y_i = \\beta_1 x_{,1} + \\beta_2 x_{,2} + \\varepsilon_i\\), variables zero-mean \\(\\mathbb{V}ar(\\varepsilon_i)=\\sigma^2\\). \n\\[\n\\mathbf{X}'\\mathbf{X} = \\left[ \\begin{array}{cc}\n\\sum_i x_{,1}^2 & \\sum_i x_{,1} x_{,2} \\\\\n\\sum_i x_{,1} x_{,2} & \\sum_i x_{,2}^2\n\\end{array}\\right],\n\\]\ntherefore:\n\\[\\begin{eqnarray*}\n(\\mathbf{X}'\\mathbf{X})^{-1} &=& \\frac{1}{\\sum_i x_{,1}^2\\sum_i x_{,2}^2 - (\\sum_i x_{,1} x_{,2})^2} \\left[ \\begin{array}{cc}\n\\sum_i x_{,2}^2 & -\\sum_i x_{,1} x_{,2} \\\\\n-\\sum_i x_{,1} x_{,2} & \\sum_i x_{,1}^2\n\\end{array}\\right].\n\\end{eqnarray*}\\]\ninverse upper-left parameter \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) :\n\\[\\begin{equation}\n\\sum_i x_{,1}^2 - \\frac{(\\sum_i x_{,1} x_{,2})^2}{\\sum_i x_{,2}^2} = \\sum_i x_{,1}^2(1 - correl_{1,2}^2),\\tag{3.16}\n\\end{equation}\\]\n\\(correl_{1,2}\\) sample correlation \\(\\mathbf{x}_{1}\\) \\(\\mathbf{x}_{2}\\).Hence, closer one \\(correl_{1,2}\\), higher variance \\(b_1\\) (recall variance \\(b_1\\) upper-left component \\(\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}\\)).","code":""},{"path":"linear-regressions.html","id":"omitted-variables","chapter":"3 Linear Regressions","heading":"3.2.11 Omitted variables","text":"Consider following model, called ``True model’’:\n\\[\n\\mathbf{y} = \\underbrace{\\mathbf{X}_1}_{n \\times K_1}\\underbrace{\\boldsymbol\\beta_1}_{K_1 \\times 1} + \\underbrace{\\mathbf{X}_2}_{n\\times K_2}\\underbrace{\\boldsymbol\\beta_2}_{K_2 \\times 1} + \\boldsymbol\\varepsilon\n\\]\n, one computes \\(\\mathbf{b}_1\\) regressing \\(\\mathbf{y}\\) \\(\\mathbf{X}_1\\) , get:\n\\[\n\\mathbf{b}_1 = (\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\mathbf{y} = \\boldsymbol\\beta_1 + (\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\mathbf{X}_2\\boldsymbol\\beta_2 +\n(\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\boldsymbol\\varepsilon.\n\\]Hence, obtain omitted-variable formula:\n\\[\n\\boxed{\\mathbb{E}(\\mathbf{b}_1|\\mathbf{X}) = \\boldsymbol\\beta_1 + \\underbrace{(\\mathbf{X}_1'\\mathbf{X}_1)^{-1}(\\mathbf{X}_1'\\mathbf{X}_2)}_{K_1 \\times K_2}\\boldsymbol\\beta_2}\n\\]\n(column \\((\\mathbf{X}_1'\\mathbf{X}_1)^{-1}(\\mathbf{X}_1'\\mathbf{X}_2)\\) OLS regressors obtained regressing columns \\(\\mathbf{X}_2\\) \\(\\mathbf{X}_1\\)).Example 3.1  Consider ``true model’’:\n\\[\\begin{equation}\nwage_i = \\beta_0 +\\beta_1 edu_i + \\beta_2 ability_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim ..d.\\,\\mathcal{N}(0,\\sigma_\\varepsilon^2)\n\\end{equation}\\]\n, assume \\(edu\\) variable correlated \\(ability\\). Specifically:\n\\[\nedu_i = \\alpha_0 +\\alpha_1 ability_i + \\eta_i, \\quad \\eta_i \\sim ..d.\\,\\mathcal{N}(0,\\sigma_\\eta^2).\n\\]\nAssume mistakingly run regression omitting \\(ability\\) variable:\n\\[\\begin{equation}\nwage_i = \\gamma_0 +\\gamma_1 edu_i + \\xi_i.\n\\end{equation}\\]\ncan seen \\(\\xi_i = \\varepsilon_i - (\\beta_2/\\alpha_1) \\eta_i \\sim ..d.\\,\\mathcal{N}(0,\\sigma_\\varepsilon^2+(\\beta_2/\\alpha_1)^2\\sigma_\\eta^2)\\) population regression coefficient \\(\\gamma_1 = \\beta_1 + \\beta_2/\\alpha_1 \\ne \\beta_1\\).Example 3.2  Let us use California Test Score dataset (package AER). Assume want measure effect students--teacher ratio (`str) student test scores (testscr). folowing regressions show effect lower controls added.","code":"\nlibrary(AER); data(\"CASchools\")\nCASchools$str <- CASchools$students/CASchools$teachers\nCASchools$testscr <- .5 * (CASchools$math + CASchools$read)\neq <- lm(testscr~str,data=CASchools)\nsummary(eq)$coefficients##               Estimate Std. Error   t value      Pr(>|t|)\n## (Intercept) 698.932949  9.4674911 73.824516 6.569846e-242\n## str          -2.279808  0.4798255 -4.751327  2.783308e-06\neq <- lm(testscr~str+lunch,data=CASchools)\nsummary(eq)$coefficients##                Estimate Std. Error    t value      Pr(>|t|)\n## (Intercept) 702.9113020 4.70024626 149.547760  0.000000e+00\n## str          -1.1172255 0.24035528  -4.648225  4.498554e-06\n## lunch        -0.5997501 0.01676439 -35.775242 3.709097e-129\neq <- lm(testscr~str+lunch+english,data=CASchools)\nsummary(eq)$coefficients##                Estimate Std. Error    t value     Pr(>|t|)\n## (Intercept) 700.1499572 4.68568672 149.423126 0.000000e+00\n## str          -0.9983090 0.23875428  -4.181324 3.535873e-05\n## lunch        -0.5473454 0.02159885 -25.341418 2.303048e-86\n## english      -0.1215735 0.03231728  -3.761872 1.928369e-04"},{"path":"linear-regressions.html","id":"irrelevant-variable","chapter":"3 Linear Regressions","heading":"3.2.12 Irrelevant variable","text":"Consider True model:\n\\[\n\\mathbf{y} = \\mathbf{X}_1\\boldsymbol\\beta_1 + \\boldsymbol\\varepsilon,\n\\]\nEstimated model :\n\\[\n\\mathbf{y} = \\mathbf{X}_1\\boldsymbol\\beta_1 + \\mathbf{X}_2\\boldsymbol\\beta_2 + \\boldsymbol\\varepsilon\n\\]estimates unbiased. However, adding irrelevant explanatory variables increases variance estimate \\(\\boldsymbol\\beta_1\\) (compared case one uses correct explanatory variables). case unless correlation \\(\\mathbf{X}_1\\) \\(\\mathbf{X}_2\\) null, see Eq. (3.16).words, estimator inefficient, .e., exists alternative consistent estimator whose variance lower. inefficiency problem can serious consequences testing hypotheses type \\(H_0: \\beta_1 = 0\\) due loss power, might infer relevant variables truly (Type-II error; False Negative).","code":""},{"path":"linear-regressions.html","id":"large-sample-properties","chapter":"3 Linear Regressions","heading":"3.3 Large Sample Properties","text":"Even relax normality assumption (Hypothesis 3.5), can approximate finite-sample behavior estimators using large-sample asymptotic properties.begin , proceed Hypothesis 3.1 3.4. (see later deal –partial– relaxations Hypothesis 3.3 3.4.)regularity assumptions, 3.1 3.4, even residuals normally-distributed, least square estimators can asymptotically normal inference can performed small samples 3.1 3.5 hold. derives Prop. 3.11 (). F-test (Prop. (prp:Ftest)) t-test can performed.Proposition 3.11  Assumptions 3.1 3.4, assuming :\n\\[\\begin{equation}\nQ = \\mbox{plim}_{n \\rightarrow \\infty} \\frac{\\mathbf{X}'\\mathbf{X}}{n},\\tag{3.17}\n\\end{equation}\\]\n\\((\\mathbf{x}_i,\\varepsilon_i)\\)s independent (across entities \\(\\)), :\n\\[\\begin{equation}\n\\sqrt{n}(\\mathbf{b} - \\boldsymbol\\beta)\\overset{d} {\\rightarrow} \\mathcal{N}\\left(0,\\sigma^2Q^{-1}\\right).\\tag{3.18}\n\\end{equation}\\]Proof. Since \\(\\mathbf{b} = \\boldsymbol\\beta + \\left( \\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1}\\left(\\frac{\\mathbf{X}'\\boldsymbol\\varepsilon}{n}\\right)\\), : \\(\\sqrt{n}(\\mathbf{b} - \\boldsymbol\\beta) = \\left( \\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1} \\left(\\frac{1}{\\sqrt{n}}\\right)\\mathbf{X}'\\boldsymbol\\varepsilon\\). Since \\(f:\\rightarrow ^{-1}\\) continuous function (\\(\\ne \\mathbf{0}\\)), \\(\\mbox{plim}_{n \\rightarrow \\infty} \\left(\\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1} = \\mathbf{Q}^{-1}\\). Let us denote \\(V_i\\) vector \\(\\mathbf{x}_i \\varepsilon_i\\). \\((\\mathbf{x}_i,\\varepsilon_i)\\)s independent, \\(V_i\\)s independent well. covariance matrix \\(\\sigma^2\\mathbb{E}(\\mathbf{x}_i \\mathbf{x}_i')=\\sigma^2Q\\). Applying multivariate central limit theorem \\(V_i\\)s gives \\(\\sqrt{n}\\left(\\frac{1}{n}\\sum_{=1}^n \\mathbf{x}_i \\varepsilon_i\\right) = \\left(\\frac{1}{\\sqrt{n}}\\right)\\mathbf{X}'\\boldsymbol\\varepsilon \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2Q)\\). application Slutsky’s theorem leads results. \\(\\square\\)practice, \\(\\sigma^2\\) estimated \\(\\frac{\\mathbf{e}'\\mathbf{e}}{n-K}\\) (Eq. (3.9)) \\(\\mathbf{Q}^{-1}\\) \\(\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1}\\).Eqs. (3.17) (3.18) respectively correspond convergences probability distribution.","code":""},{"path":"linear-regressions.html","id":"instrumental-variables","chapter":"3 Linear Regressions","heading":"3.4 Instrumental Variables","text":", want relax Hypothesis 3.2 –conditional mean zero assumption, implying particular \\(\\mathbf{x}_i\\) \\(\\varepsilon_i\\) uncorrelated.consider following model:\n\\[\\begin{equation}\ny_i = \\mathbf{x_i}'\\boldsymbol\\beta + \\varepsilon_i, \\quad \\mbox{} \\mathbb{E}(\\varepsilon_i)=0  \\mbox{ } \\mathbf{x_i}\\\\perp \\varepsilon_i.\\tag{3.19}\n\\end{equation}\\]Definition 3.2  \\(L\\)-dimensional random variable \\(\\mathbf{z}_i\\) valid set instruments :\\(\\mathbf{z}_i\\) correlated \\(\\mathbf{x}_i\\);\\(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{Z})=0\\) andthe orthogonal projections \\(\\mathbf{x}_i\\)s \\(\\mathbf{z}_i\\)s multicollinear.Example. Let us make assumption \\(\\mathbf{x}_i\\\\perp \\varepsilon_i\\) ((3.19)) precise:\n\\[\\begin{equation}\n\\mathbb{E}(\\varepsilon_i)=0 \\quad \\mbox{} \\quad \\mathbb{E}(\\varepsilon_i \\mathbf{x_i})=\\boldsymbol\\gamma.\\tag{3.20}\n\\end{equation}\\]\nlaw large numbers, \\(\\mbox{plim}_{n \\rightarrow \\infty} \\mathbf{X}'\\boldsymbol\\varepsilon / n = \\boldsymbol\\gamma\\). \\(\\mathbf{Q}_{xx} := \\mbox{plim } \\mathbf{X}'\\mathbf{X}/n\\), OLS estimator consistent \n\\[\n\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon \\overset{p}{\\rightarrow} \\boldsymbol\\beta + \\mathbf{Q}_{xx}^{-1}\\boldsymbol\\gamma \\ne \\boldsymbol\\beta.\n\\]\\(\\mathbf{z}_i\\) valid set instruments, :\n\\[\n\\mbox{plim}\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right) =\\mbox{plim}\\left( \\frac{\\mathbf{Z}'(\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon)}{n} \\right) = \\mbox{plim}\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\boldsymbol\\beta\n\\]\nIndeed, law large numbers, \\(\\frac{\\mathbf{Z}'\\boldsymbol\\varepsilon}{n} \\overset{p}{\\rightarrow}\\mathbb{E}(\\mathbf{z}_i\\varepsilon_i)=0\\).\\(L = K\\), matrix \\(\\frac{\\mathbf{Z}'\\mathbf{X}}{n}\\) dimension \\(K \\times K\\) :\n\\[\n\\left[\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\right]^{-1}\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right) = \\boldsymbol\\beta.\n\\]\ncontinuity inverse funct.: \\(\\left[\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\right]^{-1}=\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1}\\).\nSlutsky Theorem implies \n\\[\n\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1} \\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right)  = \\mbox{plim }\\left( \\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1} \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right).\n\\]\nHence \\(\\mathbf{b}_{iv}\\) consistent defined :\n\\[\n\\boxed{\\mathbf{b}_{iv} = (\\mathbf{Z}'\\mathbf{X})^{-1}\\mathbf{Z}'\\mathbf{y}.}\n\\]Proposition 3.12  \\(\\mathbf{z}_i\\) \\(L\\)-dimensional random variable constitutes valid set instruments (see Def. 3.2) \\(L=K\\), asymptotic distribution \\(\\mathbf{b}_{iv}\\) :\n\\[\n\\mathbf{b}_{iv} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(\\boldsymbol\\beta,\\frac{\\sigma^2}{n}\\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\\right]^{-1}\\right)\n\\]\n\\(\\mbox{plim } \\mathbf{Z}'\\mathbf{Z}/n =: \\mathbf{Q}_{zz}\\), \\(\\mbox{plim } \\mathbf{Z}'\\mathbf{X}/n =: \\mathbf{Q}_{zx}\\), \\(\\mbox{plim } \\mathbf{X}'\\mathbf{Z}/n =: \\mathbf{Q}_{xz}\\).Proof. proof similar Prop. 3.11, starting point \\(\\mathbf{b}_{iv} = \\boldsymbol\\beta + (\\mathbf{Z}'\\mathbf{X})^{-1}\\mathbf{Z}'\\boldsymbol\\varepsilon\\). \\(L=K\\), :\n\\[\n\\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\\right]^{-1}=Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}\n\\]\npractice, estimate \\(\\mathbb{V}ar(\\mathbf{b}_{iv}) = \\frac{\\sigma^2}{n}Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}\\), replace \\(\\sigma^2\\) :\n\\[\ns_{iv}^2 = \\frac{1}{n}\\sum_{=1}^{n} (y_i - \\mathbf{x}_i'\\mathbf{b}_{iv})^2\n\\]\\(L > K\\)? Idea: First regress \\(\\mathbf{X}\\) space spanned \\(\\mathbf{Z}\\) regress \\(\\mathbf{y}\\) fitted values \\(\\hat{\\mathbf{X}}:=\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}\\). \\(\\mathbf{b}_{iv} = (\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}})^{-1}\\hat{\\mathbf{X}}'\\mathbf{y}\\):\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{iv} = [\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{Y}.} \\tag{3.21}\n\\end{equation}\\]case, Prop. 3.12 still holds, \\(\\mathbf{b}_{iv}\\) given Eq. (3.21).\\(\\mathbf{b}_{iv}\\) also result regression \\(\\mathbf{y}\\) \\(\\mathbf{X^*}\\), columns \\(\\mathbf{X}^*\\) (othogonal) projections \\(\\mathbf{X}\\) \\(\\mathbf{Z}\\), .e. \\(\\mathbf{X^*} = \\mathbf{P^{Z}X}\\) (using notations introduced Eq. (3.3)). Hence names estimator: Two-Stage Least Squares (TSLS).instruments properly satisfy Condition () Def. 3.2 (.e. \\(\\mathbf{x}_i\\) \\(\\mathbf{z}_i\\) loosely related), instruments said weak.Relevant citation: Andrews, Stock, Sun (2019).problem instance discussed Stock Yogo (2003). See also Stock Watson pp.,489-490.Hausman test can used test IV necessary. IV techniques required \\(\\mbox{plim}_{n \\rightarrow \\infty} \\mathbf{X}'\\boldsymbol\\varepsilon / n \\ne 0\\). Hausman (1978) proposes test efficiency estimators. null hypothesis two estimators, \\(\\mathbf{b}_0\\) \\(\\mathbf{b}_1\\), consistent \\(\\mathbf{b}_0\\) (asymptotically) efficient relative \\(\\mathbf{b}_1\\). alternative hypothesis, \\(\\mathbf{b}_1\\) (IV present case) remains consistent \\(\\mathbf{b}_0\\) (OLS present case).test statistic :\n\\[\nH = (\\mathbf{b}_1 - \\mathbf{b}_0)' MPI(\\mathbb{V}ar(\\mathbf{b}_1) - \\mathbb{V}ar(\\mathbf{b}_0))(\\mathbf{b}_1 - \\mathbf{b}_0),\n\\]\n\\(MPI\\) Moore-Penrose pseudo-inverse. null hypothesis, \\(H \\sim \\chi^2(q)\\), \\(q\\) rank \\(\\mathbb{V}ar(\\mathbf{b}_1) - \\mathbb{V}ar(\\mathbf{b}_0)\\).Example 3.3  Estimation price elasticitySee e.g. estimation tobacco price elasticity demand.want estimate effect demand exogenous increase prices cigarettes (say).model :\n\\[\\begin{eqnarray*}\n\\underbrace{q^d_t}_{\\mbox{log(demand)}} &=& \\alpha_0 + \\alpha_1 \\underbrace{\\times p_t}_{\\mbox{log(price)}} + \\alpha_2 \\underbrace{\\times w_t}_{\\mbox{income}} + \\varepsilon_t^d\\\\\n\\underbrace{q^s_t}_{\\mbox{log(supply)}} &=& \\gamma_0 + \\gamma_1 \\times p_t + \\gamma_2 \\underbrace{\\times \\mathbf{y}_t}_{\\mbox{cost factors}} + \\varepsilon_t^s,\n\\end{eqnarray*}\\]\n\\(\\mathbf{y}_t\\), \\(w_t\\), \\(\\varepsilon_t^s \\sim \\mathcal{N}(0,\\sigma^2_s)\\) \\(\\varepsilon_t^d \\sim \\mathcal{N}(0,\\sigma^2_d)\\) independent.Equilibrium: \\(q^d_t = q^s_t\\). implies prices endogenous:\n\\[\np_t = \\frac{\\alpha_0 + \\alpha_2 w_t + \\varepsilon_t^d - \\gamma_0 - \\gamma_2 \\mathbf{y}_t - \\varepsilon_t^s}{\\gamma_1 - \\alpha_1}.\n\\]\nparticular \\(\\mathbb{E}(p_t \\varepsilon_t^d) = \\frac{\\sigma^2_d}{\\gamma_1 - \\alpha_1} \\ne 0\\) \\(\\Rightarrow\\) Regressing OLS \\(q_t^d\\) \\(p_t\\) gives biased estimates (see Eq. (3.20)).\nFigure 3.4: figure illustrates situation prevailing estimating price-elasticity (price endogenous).\nEstimation price elasticity cigarette demand. Instrument: real tax cigarettes arising state’s general sales tax. Presumption: states larger general sales tax, cigarette prices higher, general tax determined forces affecting \\(\\varepsilon_t^d\\).","code":"\ndata(\"CigarettesSW\", package = \"AER\")\nCigarettesSW$rprice  <- with(CigarettesSW, price/cpi)\nCigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)\nCigarettesSW$tdiff   <- with(CigarettesSW, (taxs - tax)/cpi)\n\n## model \nfm <- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + tdiff + I(tax/cpi),\n            data = CigarettesSW, subset = year == \"1995\")\neq.no.IV <- lm(log(packs) ~ log(rprice) + log(rincome),\n               data = CigarettesSW, subset = year == \"1995\")\nsummary(fm, vcov = sandwich, diagnostics = TRUE)## \n## Call:\n## ivreg(formula = log(packs) ~ log(rprice) + log(rincome) | log(rincome) + \n##     tdiff + I(tax/cpi), data = CigarettesSW, subset = year == \n##     \"1995\")\n## \n## Residuals:\n##        Min         1Q     Median         3Q        Max \n## -0.6006931 -0.0862222 -0.0009999  0.1164699  0.3734227 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    9.8950     0.9288  10.654 6.89e-14 ***\n## log(rprice)   -1.2774     0.2417  -5.286 3.54e-06 ***\n## log(rincome)   0.2804     0.2458   1.141     0.26    \n## \n## Diagnostic tests:\n##                  df1 df2 statistic p-value    \n## Weak instruments   2  44   228.738  <2e-16 ***\n## Wu-Hausman         1  44     3.823  0.0569 .  \n## Sargan             1  NA     0.333  0.5641    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1879 on 45 degrees of freedom\n## Multiple R-Squared: 0.4294,  Adjusted R-squared: 0.4041 \n## Wald test: 17.25 on 2 and 45 DF,  p-value: 2.743e-06\nsummary(eq.no.IV)## \n## Call:\n## lm(formula = log(packs) ~ log(rprice) + log(rincome), data = CigarettesSW, \n##     subset = year == \"1995\")\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.59077 -0.07856 -0.00149  0.11860  0.35442 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   10.3420     1.0227  10.113 3.66e-13 ***\n## log(rprice)   -1.4065     0.2514  -5.595 1.24e-06 ***\n## log(rincome)   0.3439     0.2350   1.463     0.15    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1873 on 45 degrees of freedom\n## Multiple R-squared:  0.4327, Adjusted R-squared:  0.4075 \n## F-statistic: 17.16 on 2 and 45 DF,  p-value: 2.884e-06\nfm2 <- ivreg(log(packs) ~ log(rprice) | tdiff, data = CigarettesSW, subset = year == \"1995\")\nanova(fm, fm2)## Analysis of Variance Table\n## \n## Model 1: log(packs) ~ log(rprice) + log(rincome) | log(rincome) + tdiff + \n##     I(tax/cpi)\n## Model 2: log(packs) ~ log(rprice) | tdiff\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     45 1.5880                           \n## 2     46 1.6668 -1 -0.078748 1.3815  0.246\nlibrary(sem)\ndata(\"CollegeDistance\", package = \"AER\")\nsimple.ed.1s<- lm(education ~ urban + gender + ethnicity + unemp + distance,\n                  data = CollegeDistance)\nCollegeDistance$ed.pred<- predict(simple.ed.1s)\nsimple.ed.2s<- lm(wage ~ urban + gender + ethnicity + unemp + ed.pred ,\n                  data = CollegeDistance)\n\nsimple.comp<- encomptest(wage ~ urban + gender + ethnicity + unemp + ed.pred ,\n                         wage ~ urban + gender + ethnicity + unemp + education ,\n                         data = CollegeDistance)\nfsttest<- encomptest(education ~ tuition + gender + ethnicity + urban ,\n                     education ~ distance ,\n                     data = CollegeDistance)\n\neqOLS <- lm(wage ~ urban + gender + ethnicity + unemp + education,\n            data=CollegeDistance)\n\nsummary(eqOLS)## \n## Call:\n## lm(formula = wage ~ urban + gender + ethnicity + unemp + education, \n##     data = CollegeDistance)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3484 -0.8408  0.1808  0.8119  3.9875 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)        8.641490   0.157008  55.039   <2e-16 ***\n## urbanyes           0.070117   0.044727   1.568   0.1170    \n## genderfemale      -0.085242   0.037069  -2.300   0.0215 *  \n## ethnicityafam     -0.556056   0.052167 -10.659   <2e-16 ***\n## ethnicityhispanic -0.544007   0.048670 -11.177   <2e-16 ***\n## unemp              0.133101   0.006711  19.834   <2e-16 ***\n## education          0.005369   0.010362   0.518   0.6044    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.268 on 4732 degrees of freedom\n## Multiple R-squared:  0.1098, Adjusted R-squared:  0.1087 \n## F-statistic: 97.27 on 6 and 4732 DF,  p-value: < 2.2e-16\nsummary(simple.ed.2s)## \n## Call:\n## lm(formula = wage ~ urban + gender + ethnicity + unemp + ed.pred, \n##     data = CollegeDistance)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.1692 -0.8294  0.1502  0.8482  3.9537 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       -0.359032   1.412087  -0.254  0.79931    \n## urbanyes           0.046144   0.044691   1.033  0.30188    \n## genderfemale      -0.070753   0.036978  -1.913  0.05576 .  \n## ethnicityafam     -0.227240   0.072984  -3.114  0.00186 ** \n## ethnicityhispanic -0.351291   0.057021  -6.161 7.84e-10 ***\n## unemp              0.139163   0.006748  20.622  < 2e-16 ***\n## ed.pred            0.647099   0.100592   6.433 1.38e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.263 on 4732 degrees of freedom\n## Multiple R-squared:  0.1175, Adjusted R-squared:  0.1163 \n## F-statistic:   105 on 6 and 4732 DF,  p-value: < 2.2e-16\neqTSLS <- tsls(wage ~ urban + gender + ethnicity + unemp + education,\n               ~ urban + gender + ethnicity + unemp + distance,\n               data=CollegeDistance)\n\neqTSLS <- ivreg(wage ~ urban + gender + ethnicity + unemp + education|\n                  urban + gender + ethnicity + unemp + distance,\n                data=CollegeDistance)\n\nsummary(eqTSLS, vcov = sandwich, diagnostics = TRUE)## \n## Call:\n## ivreg(formula = wage ~ urban + gender + ethnicity + unemp + education | \n##     urban + gender + ethnicity + unemp + distance, data = CollegeDistance)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -5.20896 -1.14578 -0.02361  1.33303  4.77571 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)       -0.35903    1.91755  -0.187   0.8515    \n## urbanyes           0.04614    0.05926   0.779   0.4362    \n## genderfemale      -0.07075    0.04974  -1.422   0.1550    \n## ethnicityafam     -0.22724    0.09539  -2.382   0.0172 *  \n## ethnicityhispanic -0.35129    0.07577  -4.636 3.64e-06 ***\n## unemp              0.13916    0.00934  14.899  < 2e-16 ***\n## education          0.64710    0.13691   4.727 2.35e-06 ***\n## \n## Diagnostic tests:\n##                   df1  df2 statistic  p-value    \n## Weak instruments    1 4732     50.19 1.60e-12 ***\n## Wu-Hausman          1 4731     40.30 2.38e-10 ***\n## Sargan              0   NA        NA       NA    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.706 on 4732 degrees of freedom\n## Multiple R-Squared: -0.6118, Adjusted R-squared: -0.6138 \n## Wald test: 57.08 on 6 and 4732 DF,  p-value: < 2.2e-16"},{"path":"linear-regressions.html","id":"general-regression-model","chapter":"3 Linear Regressions","heading":"3.5 General Regression Model","text":"want relax assumption according disturbances uncorrelated (Hypothesis @ref(hyp:noncorrel_resid)) homoskedasticity Hypothesis 3.3.replace latter two assumptions general formulation:\n\\[\\begin{eqnarray}\n\\mathbb{E}(\\boldsymbol\\varepsilon \\boldsymbol\\varepsilon'| \\mathbf{X}) &=& \\boldsymbol\\Sigma. \\tag{3.22}\n\\end{eqnarray}\\]Note Eq. ((3.22)) general Hypothesis 3.3 @ref(hyp:noncorrel_resid) diagonal entries \\(\\boldsymbol\\Sigma\\) may different (case Hypothesis 3.3), non-diagonal entries \\(\\boldsymbol\\Sigma\\) can \\(\\ne 0\\) (contrary Hypothesis 3.4).Definition 3.3  Hypothesis 3.1 3.2, together Eq. (3.22), form General Regression Model (GRM) framework.Note regression model Hypotheses 3.1 3.4 hold specific case GRM framework.GRM context notably allows model heteroskedasticity autocorrelation.Heteroskedasticity:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]. \\tag{3.23}\n\\end{equation}\\]Autocorrelation:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc}\n1 & \\rho_{2,1} & \\dots & \\rho_{n,1} \\\\\n\\rho_{2,1} & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho_{n,n-1} \\\\\n\\rho_{n,1} & \\rho_{n,2} & \\dots & 1\n\\end{array} \\right]. \\tag{3.24}\n\\end{equation}\\]Example 3.4  Autocorrelation , particular, recurrent problem time-series data used (see Section @ref(section:TS}).time-series context, subscript \\(\\) refers date. Assume instance :\n\\[\\begin{equation}\ny_i = \\mathbf{x}_i' \\boldsymbol\\beta + \\varepsilon_i \\tag{3.25}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\varepsilon_i = \\rho \\varepsilon_{-1} + v_i, \\quad v_i \\sim \\mathcal{N}(0,\\sigma_v^2).\\tag{3.26}\n\\end{equation}\\]\ncase, GRM context, :\n\n3.5.1 Generalized Least Squares\nAssume \\(\\boldsymbol\\Sigma\\) known (``feasible GLS’’). \\(\\boldsymbol\\Sigma\\) symmetric positive, admits spectral decomposition form \\(\\boldsymbol\\Sigma = \\mathbf{C} \\boldsymbol\\Lambda \\mathbf{C}'\\), \\(\\mathbf{C}\\) orthogonal matrix (.e. \\(\\mathbf{C}\\mathbf{C}'=Id\\)) \\(\\boldsymbol\\Lambda\\) diagonal matrix (diagonal entries eigenvalues \\(\\boldsymbol\\Sigma\\)).\n\\(\\boldsymbol\\Sigma = (\\mathbf{P}\\mathbf{P}')^{-1}\\) \\(\\mathbf{P} = \\mathbf{C}\\boldsymbol\\Lambda^{-1/2}\\).\nConsider transformed model:\n\\[\n\\mathbf{P}'\\mathbf{y} = \\mathbf{P}'\\mathbf{X}\\boldsymbol\\beta + \\mathbf{P}'\\boldsymbol\\varepsilon \\quad \\mbox{} \\quad \\mathbf{y}^* = \\mathbf{X}^*\\boldsymbol\\beta + \\boldsymbol\\varepsilon^*.\n\\]\nvariance \\(\\boldsymbol\\varepsilon^*\\) \\(\\mathbf{}\\). transformed model, OLS BLUE (Gauss-Markow Theorem 3.1).\nGeneralized least squares estimator \\(\\boldsymbol\\beta\\) :\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{GLS} = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}}.\\tag{3.28}\n\\end{equation}\\]\n:\n\\[\n\\mathbb{V}ar(\\mathbf{b}_{GLS}|\\mathbf{X}) = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}.\n\\]\n\\(\\boldsymbol\\Sigma\\) unknown, GLS estimator said infeasible. structure required. Assume \\(\\boldsymbol\\Sigma\\) admits parametric form \\(\\boldsymbol\\Sigma(\\theta)\\). estimation becomes feasible (FGLS) one replaces \\(\\boldsymbol\\Sigma(\\theta)\\) \\(\\boldsymbol\\Sigma(\\hat\\theta)\\).\n\\(\\hat\\theta\\) consistent estimator \\(\\theta\\), FGLS asymptotically efficient (see Example ??).\ncontrast, \\(\\boldsymbol\\Sigma\\) obvious structure: OLS (IV) estimator available. remains unbiased, consistent, asymptotically normally distributed, efficient. Standard inference procedures appropriate longer.\nAutocorrelation time-series context. Consider case presented Example 3.4. OLS estimate \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta\\) consistent, estimates \\(e_i\\)s \\(\\varepsilon_i\\)s also . Consistent estimators \\(\\rho\\) \\(\\sigma_v\\) obtained regressing \\(e_i\\)s \\(e_{-1}\\)s. Using estimates Eq. (3.27) provides consistent estimate \\(\\boldsymbol\\Sigma\\).\nSee Cochrane Orcutt (2012).\n\nProposition 3.13  Conditionally \\(\\mathbf{X}\\), :\n\\[\\begin{equation}\n\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\frac{1}{n}\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\right)\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\\tag{3.29}\n\\end{equation}\\]\nHypothesis 3.5, since \\(\\mathbf{b}\\) linear \\(\\boldsymbol\\varepsilon\\), :\n\\[\\begin{equation}\n\\mathbf{b}|\\mathbf{X} \\sim \\mathcal{N}\\left(\\boldsymbol\\beta,\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\right)\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\right).\n\\end{equation}\\]\n\nNote variance estimator \\(\\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) , using \\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) inference may misleading.\n\nProposition 3.14  \\(\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, \\(\\mbox{plim }(\\mathbf{b})=\\boldsymbol\\beta\\).\n\n\nProof. \\(\\mathbb{V}ar(\\mathbf{b})=\\mathbb{E}[\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})]+\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]\\). Since \\(\\mathbb{E}(\\mathbf{b}|\\mathbf{X})=\\boldsymbol\\beta\\), \\(\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]=0\\). Eq. (3.29) implies \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\rightarrow 0\\). Hence \\(\\mathbf{b}\\) converges mean square therefore probability. \\(\\square\\)\n\n\nProposition 3.15  \\(Q_{xx}=\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, :\n\\[\n\\sqrt{n}(\\mathbf{b}-\\boldsymbol\\beta) \\overset{d}{\\rightarrow} \\mathcal{N}(0,Q_{xx}^{-1}Q_{x\\boldsymbol\\Sigma x}Q_{xx}^{-1}).\n\\]\n\n\nProposition 3.16  regressors IV variables ``well-behaved’’, :\n\\[\n\\mathbf{b}_{iv} \\overset{}{\\sim} \\mathcal{N}(\\boldsymbol\\beta,\\mathbf{V}_{iv}),\n\\]\n\n\\[\n\\mathbf{V}_{iv} = \\frac{1}{n}(\\mathbf{Q}^*)\\mbox{ plim }\\left( \\frac{1}{n} \\mathbf{Z}'\\boldsymbol\\Sigma \\mathbf{Z}\\right)(\\mathbf{Q}^*)',\n\\]\n\n\\[\n\\mathbf{Q}^* = [\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}\\mathbf{Q}_{zx}]^{-1}\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}.\n\\]\n\npractical purposes, one needs estimates \\(\\boldsymbol\\Sigma\\) Props. 3.13, 3.15 3.16.\nIdea: instead estimating \\(\\boldsymbol\\Sigma\\) (dimension \\(n \\times n\\)) directly, one can estimate \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\), dimension \\(K \\times K\\) (\\(\\frac{1}{n}\\mathbf{Z}'\\boldsymbol\\Sigma\\mathbf{Z}\\) IV case). Indeed, expression (\\(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\)) eventually appears formulas – instance Eq. (3.29).\n:\n\\[\\begin{equation}\n\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X} = \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j. \\tag{3.30}\n\\end{equation}\\]\nRobust estimation asymptotic covariance matrices look estimates previous matrix. computation based fact \\(\\mathbf{b}\\) consistent, \\(e_i\\)s consistent (pointwise) estimators \\(\\varepsilon_i\\)s.\n\nExample 3.5  Heteroskedasticity.\ncase Eq. (3.23).\nneed estimate \\(\\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i\\). White (1980): general conditions:\n\\[\\begin{equation}\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right) =\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}e_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right). \\tag{3.31}\n\\end{equation}\\]\nestimator \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\) therefore :\n\\[\n\\frac{1}{n}\\mathbf{X}'\\mathbf{E}^2\\mathbf{X},\n\\]\n\\(\\mathbf{E}\\) \\(n \\times n\\) diagonal matrix whose diagonal elements estimated residuals \\(e_i\\).\nIllustration: Figure 3.2.\n\nLet us illustrate influence heteroskedasticity using simulations.\nconsider following model:\n\\[\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2).\n\\]\n\\(x_i\\)s ..d. \\(t(4)\\).\nsimulated sample (\\(n=200\\)) model:\n\nn <- 200\nx <- rt(n,df=5)\ny <- x + x*rnorm(n)\nplot(x,y,pch=19)\n\nsimulate 1000 samples model \\(n=200\\). sample, compute OLS estimate \\(\\beta\\) (=1). Using 1000 estimates \\(b\\), construct approximated (kernel-based) distribution OLS estimator (red figure).\n1000 OLS estimations, employ standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)) estimate variance \\(b\\). blue curve normal distribution centred 1 whose variance average 1000 previous variance estimates.\nvariance simulated \\(b\\) 0.040 (true one); average estimated variances based standard OLS formula 0.005 (bad estimate); average estimated variances based White robust covariance matrix 0.030 (better estimate).\nstandard OLS formula variance \\(b\\) overestimates precision estimator.\nalmost 50% simulations, 1 included 95% confidence interval \\(\\beta\\) computation interval based standard OLS formula variance \\(b\\).\nWhite robust covariance matrix used, 1 95% confidence interval \\(\\beta\\) less 10% simulations.\n\nn <- 200\nN <- 1000\nXX <- matrix(rt(n*N,df=5),n,N)\nYY <- matrix(XX + XX*rnorm(n),n,N)\nall_b       <- NULL\nall_V_OLS   <- NULL\nall_V_White <- NULL\n(j 1:N){\n  Y <- matrix(YY[,j],ncol=1)\n  X <- matrix(XX[,j],ncol=1)\n  b <- solve(t(X)%*%X) %*% t(X)%*%Y\n  e <- Y - X %*% b\n  S <- 1/n * t(X) %*% diag(c(e^2)) %*% X\n  V_OLS   <- solve(t(X)%*%X) * var(e)\n  V_White <- 1/n * (solve(1/n*t(X)%*%X)) %*% S %*% (solve(1/n*t(X)%*%X))\n  \n  all_b       <- c(all_b,b)\n  all_V_OLS   <- c(all_V_OLS,V_OLS)\n  all_V_White <- c(all_V_White,V_White)\n}\nplot(density(all_b))\nabline(v=mean(all_b),lty=2)\nabline(v=1)\nx <- seq(0,2,=.01)\nlines(x,dnorm(x,mean = 1,sd = mean(sqrt(all_V_OLS))),col=\"blue\")\nlines(x,dnorm(x,mean = 1,sd = mean(sqrt(all_V_White))),col=\"red\")\n\nAssume \\(\\boldsymbol\\Sigma\\) known (``feasible GLS’’). \\(\\boldsymbol\\Sigma\\) symmetric positive, admits spectral decomposition form \\(\\boldsymbol\\Sigma = \\mathbf{C} \\boldsymbol\\Lambda \\mathbf{C}'\\), \\(\\mathbf{C}\\) orthogonal matrix (.e. \\(\\mathbf{C}\\mathbf{C}'=Id\\)) \\(\\boldsymbol\\Lambda\\) diagonal matrix (diagonal entries eigenvalues \\(\\boldsymbol\\Sigma\\)).\\(\\boldsymbol\\Sigma = (\\mathbf{P}\\mathbf{P}')^{-1}\\) \\(\\mathbf{P} = \\mathbf{C}\\boldsymbol\\Lambda^{-1/2}\\).Consider transformed model:\n\\[\n\\mathbf{P}'\\mathbf{y} = \\mathbf{P}'\\mathbf{X}\\boldsymbol\\beta + \\mathbf{P}'\\boldsymbol\\varepsilon \\quad \\mbox{} \\quad \\mathbf{y}^* = \\mathbf{X}^*\\boldsymbol\\beta + \\boldsymbol\\varepsilon^*.\n\\]\nvariance \\(\\boldsymbol\\varepsilon^*\\) \\(\\mathbf{}\\). transformed model, OLS BLUE (Gauss-Markow Theorem 3.1).Generalized least squares estimator \\(\\boldsymbol\\beta\\) :\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{GLS} = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}}.\\tag{3.28}\n\\end{equation}\\]\n:\n\\[\n\\mathbb{V}ar(\\mathbf{b}_{GLS}|\\mathbf{X}) = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}.\n\\]\\(\\boldsymbol\\Sigma\\) unknown, GLS estimator said infeasible. structure required. Assume \\(\\boldsymbol\\Sigma\\) admits parametric form \\(\\boldsymbol\\Sigma(\\theta)\\). estimation becomes feasible (FGLS) one replaces \\(\\boldsymbol\\Sigma(\\theta)\\) \\(\\boldsymbol\\Sigma(\\hat\\theta)\\).\\(\\hat\\theta\\) consistent estimator \\(\\theta\\), FGLS asymptotically efficient (see Example ??).contrast, \\(\\boldsymbol\\Sigma\\) obvious structure: OLS (IV) estimator available. remains unbiased, consistent, asymptotically normally distributed, efficient. Standard inference procedures appropriate longer.Autocorrelation time-series context. Consider case presented Example 3.4. OLS estimate \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta\\) consistent, estimates \\(e_i\\)s \\(\\varepsilon_i\\)s also . Consistent estimators \\(\\rho\\) \\(\\sigma_v\\) obtained regressing \\(e_i\\)s \\(e_{-1}\\)s. Using estimates Eq. (3.27) provides consistent estimate \\(\\boldsymbol\\Sigma\\).See Cochrane Orcutt (2012).Proposition 3.13  Conditionally \\(\\mathbf{X}\\), :\n\\[\\begin{equation}\n\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\frac{1}{n}\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\right)\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\\tag{3.29}\n\\end{equation}\\]\nHypothesis 3.5, since \\(\\mathbf{b}\\) linear \\(\\boldsymbol\\varepsilon\\), :\n\\[\\begin{equation}\n\\mathbf{b}|\\mathbf{X} \\sim \\mathcal{N}\\left(\\boldsymbol\\beta,\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\right)\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\right).\n\\end{equation}\\]Note variance estimator \\(\\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) , using \\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) inference may misleading.Proposition 3.14  \\(\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, \\(\\mbox{plim }(\\mathbf{b})=\\boldsymbol\\beta\\).Proof. \\(\\mathbb{V}ar(\\mathbf{b})=\\mathbb{E}[\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})]+\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]\\). Since \\(\\mathbb{E}(\\mathbf{b}|\\mathbf{X})=\\boldsymbol\\beta\\), \\(\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]=0\\). Eq. (3.29) implies \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\rightarrow 0\\). Hence \\(\\mathbf{b}\\) converges mean square therefore probability. \\(\\square\\)Proposition 3.15  \\(Q_{xx}=\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, :\n\\[\n\\sqrt{n}(\\mathbf{b}-\\boldsymbol\\beta) \\overset{d}{\\rightarrow} \\mathcal{N}(0,Q_{xx}^{-1}Q_{x\\boldsymbol\\Sigma x}Q_{xx}^{-1}).\n\\]Proposition 3.16  regressors IV variables ``well-behaved’’, :\n\\[\n\\mathbf{b}_{iv} \\overset{}{\\sim} \\mathcal{N}(\\boldsymbol\\beta,\\mathbf{V}_{iv}),\n\\]\n\n\\[\n\\mathbf{V}_{iv} = \\frac{1}{n}(\\mathbf{Q}^*)\\mbox{ plim }\\left( \\frac{1}{n} \\mathbf{Z}'\\boldsymbol\\Sigma \\mathbf{Z}\\right)(\\mathbf{Q}^*)',\n\\]\n\n\\[\n\\mathbf{Q}^* = [\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}\\mathbf{Q}_{zx}]^{-1}\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}.\n\\]practical purposes, one needs estimates \\(\\boldsymbol\\Sigma\\) Props. 3.13, 3.15 3.16.Idea: instead estimating \\(\\boldsymbol\\Sigma\\) (dimension \\(n \\times n\\)) directly, one can estimate \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\), dimension \\(K \\times K\\) (\\(\\frac{1}{n}\\mathbf{Z}'\\boldsymbol\\Sigma\\mathbf{Z}\\) IV case). Indeed, expression (\\(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\)) eventually appears formulas – instance Eq. (3.29).:\n\\[\\begin{equation}\n\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X} = \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j. \\tag{3.30}\n\\end{equation}\\]Robust estimation asymptotic covariance matrices look estimates previous matrix. computation based fact \\(\\mathbf{b}\\) consistent, \\(e_i\\)s consistent (pointwise) estimators \\(\\varepsilon_i\\)s.Example 3.5  Heteroskedasticity.case Eq. (3.23).need estimate \\(\\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i\\). White (1980): general conditions:\n\\[\\begin{equation}\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right) =\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}e_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right). \\tag{3.31}\n\\end{equation}\\]\nestimator \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\) therefore :\n\\[\n\\frac{1}{n}\\mathbf{X}'\\mathbf{E}^2\\mathbf{X},\n\\]\n\\(\\mathbf{E}\\) \\(n \\times n\\) diagonal matrix whose diagonal elements estimated residuals \\(e_i\\).Illustration: Figure 3.2.Let us illustrate influence heteroskedasticity using simulations.consider following model:\n\\[\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2).\n\\]\n\\(x_i\\)s ..d. \\(t(4)\\).simulated sample (\\(n=200\\)) model:simulate 1000 samples model \\(n=200\\). sample, compute OLS estimate \\(\\beta\\) (=1). Using 1000 estimates \\(b\\), construct approximated (kernel-based) distribution OLS estimator (red figure).1000 OLS estimations, employ standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)) estimate variance \\(b\\). blue curve normal distribution centred 1 whose variance average 1000 previous variance estimates.variance simulated \\(b\\) 0.040 (true one); average estimated variances based standard OLS formula 0.005 (bad estimate); average estimated variances based White robust covariance matrix 0.030 (better estimate).standard OLS formula variance \\(b\\) overestimates precision estimator.almost 50% simulations, 1 included 95% confidence interval \\(\\beta\\) computation interval based standard OLS formula variance \\(b\\).White robust covariance matrix used, 1 95% confidence interval \\(\\beta\\) less 10% simulations.","code":"\nn <- 200\nx <- rt(n,df=5)\ny <- x + x*rnorm(n)\nplot(x,y,pch=19)\nn <- 200\nN <- 1000\nXX <- matrix(rt(n*N,df=5),n,N)\nYY <- matrix(XX + XX*rnorm(n),n,N)\nall_b       <- NULL\nall_V_OLS   <- NULL\nall_V_White <- NULL\nfor(j in 1:N){\n  Y <- matrix(YY[,j],ncol=1)\n  X <- matrix(XX[,j],ncol=1)\n  b <- solve(t(X)%*%X) %*% t(X)%*%Y\n  e <- Y - X %*% b\n  S <- 1/n * t(X) %*% diag(c(e^2)) %*% X\n  V_OLS   <- solve(t(X)%*%X) * var(e)\n  V_White <- 1/n * (solve(1/n*t(X)%*%X)) %*% S %*% (solve(1/n*t(X)%*%X))\n  \n  all_b       <- c(all_b,b)\n  all_V_OLS   <- c(all_V_OLS,V_OLS)\n  all_V_White <- c(all_V_White,V_White)\n}\nplot(density(all_b))\nabline(v=mean(all_b),lty=2)\nabline(v=1)\nx <- seq(0,2,by=.01)\nlines(x,dnorm(x,mean = 1,sd = mean(sqrt(all_V_OLS))),col=\"blue\")\nlines(x,dnorm(x,mean = 1,sd = mean(sqrt(all_V_White))),col=\"red\")"},{"path":"linear-regressions.html","id":"heteroskedasticity-and-autocorrelation-hac","chapter":"3 Linear Regressions","heading":"3.5.2 Heteroskedasticity and Autocorrelation (HAC)","text":"includes cases Eqs. (3.23) (3.24).Newey West (1987): correlation terms \\(\\) \\(j\\) gets sufficiently small \\(|-j|\\) increases:\n\\[\\begin{eqnarray}\n&&\\mbox{plim} \\left( \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j \\right) =  \\\\\n&&\\mbox{plim} \\left( \\frac{1}{n}\\sum_{t=1}^{n}e_{t}^2\\mathbf{x}_t\\mathbf{x}'_t +\n\\frac{1}{n}\\sum_{\\ell=1}^{L}\\sum_{t=\\ell+1}^{n}w_\\ell e_{t}e_{t-\\ell}(\\mathbf{x}_t\\mathbf{x}'_{t-\\ell} + \\mathbf{x}_{t-\\ell}\\mathbf{x}'_{t})\n\\right) \\nonumber \\tag{3.32}\n\\end{eqnarray}\\]\n\\(w_\\ell = 1 - \\ell/(L+1)\\).Let us illustrate influence autocorrelation using simulations.consider following model:\n\\[\\begin{equation}\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2),\\tag{3.33}\n\\end{equation}\\]\n\\(x_i\\)s \\(\\varepsilon_i\\)s :\n\\[\\begin{equation}\nx_i = 0.8 x_{-1} + u_i \\quad \\quad \\varepsilon_i = 0.8 \\varepsilon_{-1} + v_i, \\tag{3.34}\n\\end{equation}\\]\n\\(u_i\\)s \\(v_i\\)s ..d. \\(\\mathcal{N}(0,1)\\).simulated sample (\\(n=200\\)) model:\n\n\n\n\n\n\nsimulate 1000 samples model \\(n=200\\).sample, compute OLS estimate \\(\\beta\\) (=1).Using 1000 estimates \\(b\\), construct approximated (kernel-based) distribution OLS estimator (red figure).1000 OLS estimations, employ standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)) estimate variance \\(b\\). blue curve normal distribution centred 1 whose variance average 1000 previous variance estimates.variance simulated \\(b\\) 0.020 (true one); average estimated variances based standard OLS formula 0.005 (bad estimate); average estimated variances based White robust covariance matrix 0.015 (better estimate).standard OLS formula variance \\(b\\) overestimates precision estimator.35% simulations, 1 included 95% confidence interval \\(\\beta\\) computation interval based standard OLS formula variance \\(b\\).Newey-West robust covariance matrix used, 1 95% confidence interval \\(\\beta\\) 13% simulations.sake comparison, let us consider model auto-correlation (\\(x_i \\sim ..d. \\mathcal{N}(0,2.8)\\) \\(\\varepsilon_i \\sim ..d. \\mathcal{N}(0,2.8)\\)).","code":""},{"path":"linear-regressions.html","id":"how-to-detect-autocorrelation-in-residuals","chapter":"3 Linear Regressions","heading":"3.5.3 How to detect autocorrelation in residuals?","text":"Consider usual regression (say Eq. (3.25)).Durbin-Watson test typical autocorrelation test. test statistic :\n\\[\nDW = \\frac{\\sum_{=2}^{n}(e_i - e_{-1})^2}{\\sum_{=1}^{n}e_i^2}= 2(1 - r) - \\underbrace{\\frac{e_1^2 + e_n^2}{\\sum_{=1}^{n}e_i^2}}_{\\overset{p}{\\rightarrow} 0},\n\\]\n\\(r\\) slope regression \\(e_i\\)s \\(e_{-1}\\)s, .e.:\n\\[\nr = \\frac{\\sum_{=2}^{n}e_i e_{-1}}{\\sum_{=1}^{n-1}e_i^2}.\n\\]\n(\\(r\\) consistent estimator \\(\\mathbb{C}(\\varepsilon_i,\\varepsilon_{-1})\\), .e. \\(\\rho\\) Eq. (3.26).)Critical values depend T K: see e.g. tables CHECK.one-sided test \\(H_0\\): \\(\\rho=0\\) \\(H_1\\): \\(\\rho>0\\) carried comparing \\(DW\\) values \\(d_L(T, K)\\) \\(d_U(T, K)\\):\n\\[\n\\left\\{\n\\begin{array}{ll}\n\\mbox{$DW < d_L$,}&\\mbox{ null hypothesis rejected;}\\\\\n\\mbox{$DW > d_U$,}&\\mbox{ hypothesis rejected;}\\\\\n\\mbox{$d_L \\le DW \\le d_U$,} &\\mbox{ conclusion drawn.}\n\\end{array}\n\\right.\n\\]","code":""},{"path":"linear-regressions.html","id":"summary","chapter":"3 Linear Regressions","heading":"3.6 Summary","text":"\\(^*\\): see however Prop. 3.14 Prop. 3.15 additional hypotheses. Specifically \\(\\mathbf{X}'\\mathbf{X}/n\\) \\(\\mathbf{X}'\\boldsymbol{\\Sigma}\\mathbf{X}/n\\) must converge proba. finite positive definite matrices (\\(\\boldsymbol\\Sigma\\) defined Eq. (3.22)).Alors, combien vaut ? Answer: 1.","code":"\na <- 1"},{"path":"linear-regressions.html","id":"clusters","chapter":"3 Linear Regressions","heading":"3.7 Clusters","text":"MacKinnon, Nielsen, Webb (2022)nice reference MacKinnon, Nielsen, Webb (2022)Another one Cameron Miller (2014)See package fwildclusterboot wild cluster bootstrap.","code":""},{"path":"panel-regressions.html","id":"panel-regressions","chapter":"4 Panel regressions","heading":"4 Panel regressions","text":"standard panel situation following: lot entities (\\(\\\\{1,\\dots,n\\}\\)) , entity, observe different variables small number periods (\\(t \\\\{1,\\dots,T\\}\\)). longitudinal dataset.regression reads:\n\\[\ny_{,t} = \\mathbf{x}'_{,t}\\underbrace{\\boldsymbol\\beta}_{K \\times 1} + \\underbrace{\\mathbf{z}'_{}\\boldsymbol\\alpha}_{\\mbox{Individual effects}} + \\varepsilon_{,t}\n\\]\nobjective estimate previous equation.Figure 4.1. model \\(y_i = \\alpha_i + \\beta x_{,t} + \\varepsilon_{,t}\\), \\(t \\\\{1,2\\}\\). Panel (b), blue dots \\(t=1\\), red dots \\(t=2\\). lines relate dots associated entity \\(\\).\nFigure 4.1: data panels. Panel (b), blue dots \\(t=1\\), red dots \\(t=2\\). lines relate dots associated entity \\(\\).\nLet us know use Cigarette Consumption Panel dataset Stock Watson (2007)Cigarettes data Stock Watson (2003). Data U.S. states, 2 years: 1985 1995. colour corresponds given State.Airlines: cost versus fuel priceAirlines data Greene (2003). colour corresponds given airline.Notations:\n\\[\n\\mathbf{y}_i =\n\\underbrace{\\left[\n\\begin{array}{c}\ny_{,1}\\\\\n\\vdots\\\\\ny_{,T}\n\\end{array}\\right]}_{T \\times 1}, \\quad\n\\boldsymbol\\varepsilon_i =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\varepsilon_{,1}\\\\\n\\vdots\\\\\n\\varepsilon_{,T}\n\\end{array}\\right]}_{T \\times 1}, \\quad\n\\mathbf{x}_i =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\mathbf{x}_{,1}'\\\\\n\\vdots\\\\\n\\mathbf{x}_{,T}'\n\\end{array}\\right]}_{T \\times K}, \\quad\n\\mathbf{X} =\n\\underbrace{\\left[\n\\begin{array}{c}\n\\mathbf{x}_{1}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}\n\\end{array}\\right]}_{(nT) \\times K}.\n\\]\n\\[\n\\tilde{\\mathbf{y}}_i =\n\\left[\n\\begin{array}{c}\ny_{,1} - \\bar{y}_i\\\\\n\\vdots\\\\\ny_{,T} - \\bar{y}_i\n\\end{array}\\right], \\quad\n\\tilde{\\boldsymbol\\varepsilon}_i =\n\\left[\n\\begin{array}{c}\n\\varepsilon_{,1} - \\bar{\\varepsilon}_i\\\\\n\\vdots\\\\\n\\varepsilon_{,T} - \\bar{\\varepsilon}_i\n\\end{array}\\right],\n\\]\n\\[\n\\tilde{\\mathbf{x}}_i =\n\\left[\n\\begin{array}{c}\n\\mathbf{x}_{,1}' - \\bar{\\mathbf{x}}_i'\\\\\n\\vdots\\\\\n\\mathbf{x}_{,T}' - \\bar{\\mathbf{x}}_i'\n\\end{array}\\right], \\quad\n\\tilde{\\mathbf{X}} =\n\\left[\n\\begin{array}{c}\n\\tilde{\\mathbf{x}}_{1}\\\\\n\\vdots\\\\\n\\tilde{\\mathbf{x}}_{n}\n\\end{array}\\right], \\quad\n\\tilde{\\mathbf{Y}} =\n\\left[\n\\begin{array}{c}\n\\tilde{\\mathbf{y}}_{1}\\\\\n\\vdots\\\\\n\\tilde{\\mathbf{y}}_{n}\n\\end{array}\\right],\n\\]\n\n\\[\n\\bar{y}_i = \\frac{1}{T} \\sum_{t=1}^T y_{,t}, \\quad \\bar{\\varepsilon}_i = \\frac{1}{T}\\sum_{t=1}^T \\varepsilon_{,t} \\quad \\mbox{} \\quad \\bar{\\mathbf{x}}_i = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{x}_{,t}.\n\\]","code":"\nT <- 2 # 2 periods\nn <- 12 # 12 entities\nalpha <- 5*rnorm(n) # draw fixed effects\nx.1 <- rnorm(n) - .5*alpha # note: x_i's correlate to alpha_i's\nx.2 <- rnorm(n) - .5*alpha\nbeta <- 5; sigma <- .3\ny.1 <- alpha + x.1 + sigma*rnorm(n)\ny.2 <- alpha + x.2 + sigma*rnorm(n)\nx <- c(x.1,x.2) # pooled x\ny <- c(y.1,y.2) # pooled y\npar(mfrow=c(1,2))\nplot(x,y,col=\"black\",pch=19,xlab=\"x\",ylab=\"y\",main=\"(a)\")\nplot(x,y,col=\"black\",pch=19,xlab=\"x\",ylab=\"y\",main=\"(b)\")\npoints(x.1,y.1,col=\"blue\",pch=19)\npoints(x.2,y.2,col=\"red\",pch=19)\nfor(i in 1:n){\n  lines(c(x.1[i],x.2[i]),c(y.1[i],y.2[i]))\n}\ndata(\"CigarettesSW\", package = \"AER\")\nCigarettesSW$rprice  <- with(CigarettesSW, price/cpi)\nCigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)\nT <- length(levels(CigarettesSW$year))\nn <- length(levels(CigarettesSW$state))\neq.pooled <- lm(log(packs)~log(rprice)+log(rincome),data=CigarettesSW)\nprint(summary(eq.pooled)$coefficients)##                Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept)  10.0671678  0.5156035 19.525020 1.158630e-34\n## log(rprice)  -1.3341612  0.1353614 -9.856288 4.120472e-16\n## log(rincome)  0.3181371  0.1361194  2.337191 2.157508e-02\neq.LSDV <- lm(log(packs)~log(rprice)+log(rincome)+state,\n              data=CigarettesSW)\nprint(summary(eq.LSDV)$coefficients[1:3,])##                Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept)   9.9543751  0.2641889  37.67901 3.156258e-36\n## log(rprice)  -1.2103380  0.1138384 -10.63207 5.590562e-14\n## log(rincome)  0.1209004  0.1901069   0.63596 5.279541e-01\nCigarettesSW$year <- as.factor(CigarettesSW$year)\neq.LSDV2 <- lm(log(packs)~log(rprice)+log(rincome)+state+year,\n              data=CigarettesSW)\nprint(summary(eq.LSDV2)$coefficients[1:3,])##                Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept)   8.3597463  1.0485390  7.972757 3.776672e-10\n## log(rprice)  -1.0559739  0.1490905 -7.082770 7.682380e-09\n## log(rincome)  0.4974424  0.3042306  1.635084 1.090085e-01"},{"path":"panel-regressions.html","id":"three-standard-cases","chapter":"4 Panel regressions","heading":"4.0.1 Three standard cases","text":"Pooled regression: \\(\\mathbf{z}_i \\equiv 1\\).Fixed Effects: \\(\\mathbf{z}_i\\) unobserved, correlates \\(\\mathbf{x}_i\\) \\(\\Rightarrow\\) \\(\\mathbf{b}\\) biased inconsistent OLS regression \\(\\mathbf{y}\\) \\(\\mathbf{X}\\) (omitted variable, see XXX).Random Effects: \\(\\mathbf{z}_i\\) unobserved, uncorrelated \\(\\mathbf{x}_i\\). model writes:\n\\[\ny_{,t} = \\mathbf{x}'_{,t}\\boldsymbol\\beta + \\alpha +  \\underbrace{{\\color{blue}u_i + \\varepsilon_{,t}}}_{\\mbox{compound disturbance}},\n\\]\n\\(\\alpha = \\mathbb{E}(\\mathbf{z}'_{}\\boldsymbol\\alpha)\\) \\(u_i = \\mathbf{z}'_{}\\boldsymbol\\alpha - \\mathbb{E}(\\mathbf{z}'_{}\\boldsymbol\\alpha) \\perp \\mathbf{x}_i\\).Least squares consistent (inefficient, see GLS XXXX).","code":""},{"path":"panel-regressions.html","id":"estimation-of-fixed-effects-models","chapter":"4 Panel regressions","heading":"4.1 Estimation of Fixed Effects Models","text":"Hypothesis 4.1  (Fixed-effect model) assume :perfect multicollinearity among regressors.\\(\\mathbb{E}(\\varepsilon_{,t}|\\mathbf{X})=0\\), \\(,t\\).:\n\\[\n\\mathbb{E}(\\varepsilon_{,t}\\varepsilon_{j,s}|\\mathbf{X}) =\n\\left\\{\n\\begin{array}{cl}\n\\sigma^2 & \\mbox{$=j$ $s=t$},\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\\right.\n\\]assumptions analogous introduced standard linear regression:\\(\\leftrightarrow\\) 3.1, (ii) \\(\\leftrightarrow\\) 3.2, (iii) \\(\\leftrightarrow\\) 3.3 + 3.4.matrix form, given \\(\\), model writes:\n\\[\n\\mathbf{y}_i = \\mathbf{X}_i \\boldsymbol\\beta + \\mathbf{1}\\alpha_i + \\boldsymbol\\varepsilon_i,\n\\]\n\\(\\mathbf{1}\\) \\(T\\)-dimensional vector ones.Least Square Dummy Variable (LSDV) model:\n\\[\\begin{equation}\n\\boxed{\\mathbf{y} = [\\mathbf{X} \\quad \\mathbf{D}]\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\n\\end{array}\n\\right]\n+ \\boldsymbol\\varepsilon,} \\tag{4.1}\n\\end{equation}\\]\n:\n\\[\n\\mathbf{D} = \\underbrace{ \\left[\\begin{array}{cccc}\n\\mathbf{1}&\\mathbf{0}&\\dots&\\mathbf{0}\\\\\n\\mathbf{0}&\\mathbf{1}&\\dots&\\mathbf{0}\\\\\n&&\\vdots&\\\\\n\\mathbf{0}&\\mathbf{0}&\\dots&\\mathbf{1}\\\\\n\\end{array}\\right]}_{(nT \\times n)}.\n\\]linear regression (Eq. (4.1)) –dummy variables– satisfies Gauss-Markov conditions (Theorem 3.1). Hence, context, OLS estimator best linear unbiased estimator.Denoting \\(\\mathbf{Z}\\) matrix \\([\\mathbf{X} \\quad \\mathbf{D}]\\), \\(\\mathbf{b}\\) \\(\\mathbf{}\\) respective OLS estimates \\(\\boldsymbol\\beta\\) \\(\\boldsymbol\\alpha\\), :\n\\[\\begin{equation}\n\\boxed{\n\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\n\\right]\n= [\\mathbf{Z}'\\mathbf{Z}]^{-1}\\mathbf{Z}'\\mathbf{y}.} \\tag{4.2}\n\\end{equation}\\]asymptotical distribution \\([\\mathbf{b}',\\mathbf{}']'\\) derives standard OLS context: Prop. 3.11 can used replaced \\(\\mathbf{X}\\) \\(\\mathbf{Z}=[\\mathbf{X} \\quad \\mathbf{D}]\\).:\n\\[\\begin{equation}\n\\boxed{\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\n\\right] \\overset{d}{\\rightarrow}\n\\mathcal{N}\\left(\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\n\\end{array}\n\\right],\n\\sigma^2 \\frac{Q^{-1}}{nT}\n\\right)}\n\\end{equation}\\]\n\n\\[\nQ = \\mbox{plim}_{nT \\rightarrow \\infty} \\frac{1}{nT} \\mathbf{Z}'\\mathbf{Z}.\n\\]practice, estimator covariance matrix \\([\\mathbf{b}',\\mathbf{}']'\\) :\n\\[\ns^2 \\left( \\mathbf{Z}'\\mathbf{Z}\\right)^{-1} \\quad \\quad s^2 = \\frac{\\mathbf{e}'\\mathbf{e}}{nT - K - n},\n\\]\n\\(\\mathbf{e}\\) \\((nT) \\times 1\\) vector OLS residuals.alternative way expressing LSDV estimators.based matrix \\(\\mathbf{M_D}=\\mathbf{} - \\mathbf{D}(\\mathbf{D}'\\mathbf{D})^{-1}\\mathbf{D}'\\), acts operator removes entity-specific means, .e.:\n\\[\n\\tilde{\\mathbf{Y}} = \\mathbf{M_D}\\mathbf{Y}, \\quad \\tilde{\\mathbf{X}} = \\mathbf{M_D}\\mathbf{X} \\quad \\quad \\tilde{\\boldsymbol\\varepsilon} = \\mathbf{M_D}\\boldsymbol\\varepsilon.\n\\]notations, using Theorem 3.2, get:\n\\[\\begin{equation}\n\\boxed{\\mathbf{b} = [\\mathbf{X}'\\mathbf{M_D}\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{M_D}\\mathbf{y}.}\\tag{4.3}\n\\end{equation}\\]amounts regressing \\(\\tilde{y}_{,t}\\)s (\\(= y_{,t} - \\bar{y}_i\\)) \\(\\tilde{\\mathbf{x}}_{,t}\\)s (\\(=\\mathbf{x}_{,t} - \\bar{\\mathbf{x}}_i\\)).estimates \\(\\boldsymbol\\alpha\\) given :\n\\[\\begin{equation}\n\\boxed{\\mathbf{} = (\\mathbf{D}'\\mathbf{D})^{-1}\\mathbf{D}'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}),} \\tag{4.4}\n\\end{equation}\\]\nobtained developing second row \n\\[\n\\left[\n\\begin{array}{cc}\n\\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{D}\\\\\n\\mathbf{D}'\\mathbf{X} & \\mathbf{D}'\\mathbf{D}\n\\end{array}\\right]\n\\left[\n\\begin{array}{c}\n\\mathbf{b}\\\\\n\\mathbf{}\n\\end{array}\\right] =\n\\left[\n\\begin{array}{c}\n\\mathbf{X}'\\mathbf{Y}\\\\\n\\mathbf{D}'\\mathbf{Y}\n\\end{array}\\right],\n\\]\nfirst-order conditions resulting least squares problem (see Eq. (3.1)).XXXX Regression log airline costs log(output), log(fuel price) capacity utilization fleet (Data , see Fig. ??.XXXX Regression number cigarette packs (log) real income (log) real price cigarettes (log)Extension: Fixed time group effectsTime effects easily introduced:\n\\[\ny_{,t} = \\mathbf{x}_i'\\boldsymbol\\beta + \\alpha_i + \\gamma_t + \\varepsilon_{,t}.\n\\]LSDV (Eq. (4.1)) can extended:\n\\[\\begin{equation}\n\\mathbf{y} = [\\mathbf{X} \\quad \\mathbf{D} \\quad \\mathbf{C}]\n\\left[\n\\begin{array}{c}\n\\boldsymbol\\beta\\\\\n\\boldsymbol\\alpha\\\\\n\\boldsymbol\\gamma\n\\end{array}\n\\right]\n+ \\boldsymbol\\varepsilon, \\tag{4.5}\n\\end{equation}\\]\n:\n\\[\n\\mathbf{C} = \\left[\\begin{array}{cccc}\n\\boldsymbol{\\delta}_1&\\boldsymbol{\\delta}_2&\\dots&\\boldsymbol{\\delta}_{T-1}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\boldsymbol{\\delta}_1&\\boldsymbol{\\delta}_2&\\dots&\\boldsymbol{\\delta}_{T-1}\\\\\n\\end{array}\\right],\n\\]\n\\(T\\)-dimensional vector \\(\\boldsymbol\\delta_t\\) \n\\[\n[0,\\dots,0,\\underbrace{1}_{\\mbox{t$^{th}$ entry}},0,\\dots,0]'.\n\\]XXXX Geo-located data Airbnb, Z\"urichSource: Airbnb, date 22 June 2017. Regression price Entire home/apt number bedrooms, number people can accommodated.","code":""},{"path":"panel-regressions.html","id":"estimation-of-random-effects-models","chapter":"4 Panel regressions","heading":"4.2 Estimation of random effects models","text":", individual effects assumed correlated variables (\\(\\mathbf{x}_i\\)s).Random-effect models write:\n\\[\ny_{,t}=\\mathbf{x}'_{}\\boldsymbol\\beta + (\\alpha + \\underbrace{u_i}_{\\substack{\\text{Random}\\\\\\text{heterogeneity}}}) + \\varepsilon_{,t}\n\\]\n\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\varepsilon_{,t}|\\mathbf{X})&=&\\mathbb{E}(u_{}|\\mathbf{X}) =0,\\\\\n\\mathbb{E}(\\varepsilon_{,t}\\varepsilon_{j,s}|\\mathbf{X}) &=&\n\\left\\{\n\\begin{array}{cl}\n\\sigma_\\varepsilon^2 & \\mbox{ $=j$ $s=t$},\\\\\n0 & \\mbox{ otherwise.}\n\\end{array}\n\\right.\\\\\n\\mathbb{E}(u_{}u_{j}|\\mathbf{X}) &=&\n\\left\\{\n\\begin{array}{cl}\n\\sigma_u^2 & \\mbox{ $=j$},\\\\\n0 & \\mbox{otherwise.}\n\\end{array}\n\\right.\\\\\n\\mathbb{E}(\\varepsilon_{,t}u_{j}|\\mathbf{X})&=&0 \\quad \\text{$$, $j$ $t$}.\n\\end{eqnarray*}\\]Notation: \\(\\eta_{,t} = u_i + \\varepsilon_{,t}\\) \\(\\boldsymbol\\eta_i = [\\eta_{,1},\\eta_{,2},\\dots,\\eta_{,T}]'\\).\\(\\mathbb{E}(\\boldsymbol\\eta_i |\\mathbf{X}) = \\mathbf{0}\\) \\(\\mathbb{V}ar(\\boldsymbol\\eta_i | \\mathbf{X}) = \\boldsymbol\\Gamma\\) \n\\[\n\\boldsymbol\\Gamma = \\left[  \\begin{array}{ccccc}\n\\sigma_\\varepsilon^2+\\sigma_u^2 & \\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_u^2\\\\\n\\sigma_u^2 & \\sigma_\\varepsilon^2+\\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_u^2\\\\\n\\vdots && \\ddots && \\vdots \\\\\n\\sigma_u^2 & \\sigma_u^2 & \\sigma_u^2 & \\dots & \\sigma_\\varepsilon^2+\\sigma_u^2\\\\\n\\end{array}\n\\right] = \\sigma_\\varepsilon^2\\mathbf{} + \\sigma_u^2\\mathbf{1}\\mathbf{1}'.\n\\]Denoting \\(\\boldsymbol\\Sigma\\) covariance matrix \\(\\boldsymbol\\eta = [\\boldsymbol\\eta_1',\\dots,\\boldsymbol\\eta_n']'\\), :\n\\[\n\\boldsymbol\\Sigma = \\mathbf{} \\otimes \\boldsymbol\\Gamma.\n\\]knew \\(\\boldsymbol\\Sigma\\), apply (feasible) GLS (Eq. (3.28)):\n\\[\n\\boldsymbol\\beta = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}\n\\]\n(recall amounts regressing \\({\\boldsymbol\\Sigma^{-1/2}}'\\mathbf{y}\\) \\({\\boldsymbol\\Sigma^{-1/2}}'\\mathbf{X}\\)).can checked \\(\\boldsymbol\\Sigma^{-1/2} = \\mathbf{} \\otimes (\\boldsymbol\\Gamma^{-1/2})\\) \n\\[\n\\boldsymbol\\Gamma^{-1/2} = \\frac{1}{\\sigma_\\varepsilon}\\left( \\mathbf{} - \\frac{\\theta}{T}\\mathbf{1}\\mathbf{1}'\\right)\n\\]\n\n\\[\n\\theta = 1 - \\frac{\\sigma_\\varepsilon}{\\sqrt{\\sigma_\\varepsilon^2+T\\sigma_u^2}}.\n\\]Hence, knew \\(\\boldsymbol\\Sigma\\), transform data follows:\n\\[\n\\boldsymbol\\Gamma^{-1/2}\\mathbf{y}_i = \\frac{1}{\\sigma_\\varepsilon}\\left[\\begin{array}{c}y_{,1} - \\theta\\bar{y}_i\\\\y_{,2} - \\theta\\bar{y}_i\\\\\\vdots\\\\y_{,T} - \\theta\\bar{y}_i\\\\\\end{array}\\right].\n\\]\\(\\boldsymbol\\Sigma\\) unknown?Idea: taking deviations group means removes heterogeneity:\n\\[\ny_{,t} - \\bar{y}_i = [\\mathbf{x}_{,t} - \\bar{\\mathbf{x}}_i]'\\boldsymbol\\beta + (\\varepsilon_{,t} - \\bar{\\varepsilon}_i).\n\\]previous equation can consistently estimated OLS(residuals correlated across \\(t\\) within entity OLS remain consistent though, see Prop. @ref{prp:XXX)).\\(\\mathbb{E}\\left[\\sum_{=1}^{T}(\\varepsilon_{,t}-\\bar{\\varepsilon}_i)^2\\right] = (T-1)\\sigma_{\\varepsilon}^2\\).\\(\\varepsilon_{,t}\\)s observed \\(\\mathbf{b}\\) consistent estimator \\(\\boldsymbol\\beta\\); adjustment degrees freedom:\n\\[\n\\hat{\\sigma}_e^2 = \\frac{1}{nT-n-K}\\sum_{=1}^{n}\\sum_{t=1}^{T}(e_{,t} - \\bar{e}_i)^2.\n\\]\\(\\sigma_u^2\\)? can exploit fact OLS consistent pooled regression:\n\\[\n\\mbox{plim }s^2_{pooled} = \\mbox{plim }\\frac{\\mathbf{e}'\\mathbf{e}}{nT-K-1} = \\sigma_u^2 + \\sigma_\\varepsilon^2,\n\\]\ntherefore use \\(s^2_{pooled} - \\hat{\\sigma}_e^2\\) approximation \\(\\sigma_u^2\\).","code":""},{"path":"estimation-methods.html","id":"estimation-methods","chapter":"5 Estimation Methods","heading":"5 Estimation Methods","text":"Context Objective:observe sample \\(\\mathbf{y}=\\{y_1,\\dots,y_n\\}\\).know data generated model parameterized \\(\\theta_0 \\\\mathbb{R}^K\\).","code":""},{"path":"estimation-methods.html","id":"generalized-method-of-moments","chapter":"5 Estimation Methods","heading":"5.1 Generalized Method of Moments","text":"","code":""},{"path":"estimation-methods.html","id":"framework","chapter":"5 Estimation Methods","heading":"5.1.1 Framework","text":"denote \\(x_t\\) \\(p \\times 1\\) vector (stationary) variables observed date \\(t\\); \\(\\theta\\) \\(\\times 1\\) vector parameters, \\(h(x_t;\\theta)\\) continuous \\(r \\times 1\\) vector-valued function.denote \\(\\theta_0\\) true value \\(\\theta\\) assume \\(\\theta_0\\) satisfies:\n\\[\n\\mathbb{E}[h(x_t;\\theta_0)] = 0.\n\\]denote \\(\\underline{x_t}\\) information contained current past observations \\(x_t\\), : \\(\\underline{x_t} = \\{x_t,x_{t-1},\\dots,x_1\\}\\). denote \\(g(\\underline{x_T};\\theta)\\) sample average \\(h(x_t;\\theta)\\), .e.:\n\\[\ng(\\underline{x_T};\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} h(x_t;\\theta).\n\\]Intuition behind GMM: Choose \\(\\theta\\) make sample moment close possible 0.GMM estimator \\(\\theta_0\\) given :\n\\[\n\\hat{\\theta}_T = \\mbox{argmin}_{\\theta} \\quad g(\\underline{x_T};\\theta)'\\, W_T \\, g(\\underline{x_T};\\theta),\n\\]\n\\(W_T\\) positive definite matrix (may depend \\(\\underline{x_T}\\)).\\(= r\\), \\(\\hat{\\theta}_T\\) :\n\\[\ng(\\underline{x_T};\\hat{\\theta}_T) = 0.\n\\]\nregularity identification conditions:\n\\[\n\\hat{\\theta}_{T} \\overset{P}{\\rightarrow} \\theta_0,\n\\]\n.e. \\(\\forall \\varepsilon>0\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|\\hat{\\theta}_{T} - \\theta_0|>\\varepsilon) = 0\\).Optimal weighting matrix. GMM estimator achieving minimum asymptotic variance obtained \\(W_T\\) inverse matrix \\(S\\) defined :\n\\[\nS := \\sum_{\\nu = -\\infty}^{\\infty} \\Gamma_\\nu,\n\\]\n\\(\\Gamma_\\nu := \\mathbb{E}[h(x_t;\\theta_0) h(x_{t-\\nu};\\theta_0)']\\).\\(\\nu \\ge 0\\), let us define \\(\\hat{\\Gamma}_{\\nu,T}\\) :\n\\[\n\\hat{\\Gamma}_{\\nu,T} = \\frac{1}{T} \\sum_{t=\\nu + 1}^{T} h(x_t;\\hat{\\theta}_T)h(x_{t-\\nu};\\hat{\\theta}_T)',\n\\]\n\\(S\\) can approximated :\n\\[\\begin{eqnarray}\n&\\hat{\\Gamma}_{0,T}& \\quad \\mbox{$h(x_t;\\theta_0)$ serially uncorrelated } \\nonumber \\\\\n&\\hat{\\Gamma}_{0,T}& + \\sum_{\\nu=1}^{q}[1-\\nu/(q+1)](\\hat{\\Gamma}_{\\nu,T}+\\hat{\\Gamma}_{\\nu,T}') \\quad \\mbox{otherwise.}    \\tag{5.1}\n\\end{eqnarray}\\]Asymptotic distribution \\(\\hat\\theta_T\\):\n\\[\n\\sqrt{T}(\\hat\\theta_T - \\theta_0) \\overset{\\mathcal{L}}{\\rightarrow} \\mathcal{N}(0,V),\n\\]\n\\(V = (DS^{-1}D')^{-1}\\).\\(V\\) can approximated \\((\\hat{D}_T\\hat{S}_T^{-1}\\hat{D}_T')^{-1}\\),\n\\(\\hat{S}_T\\) given Eq. (5.1) \n\\[\n\\hat{D}'_T := \\left.\\frac{\\partial g(\\underline{x_T};\\theta)}{\\partial \\theta'}\\right|_{\\theta = \\hat\\theta_T}.\n\\]","code":""},{"path":"estimation-methods.html","id":"maximum-likelihood-estimation","chapter":"5 Estimation Methods","heading":"5.2 Maximum Likelihood Estimation","text":"Intuition behind Maximum Likelihood Estimation: Estimator = value \\(\\theta\\) probability observed \\(\\mathbf{y}\\) highest possible.Assume time periods arrivals two customers shop, denoted \\(y_i\\), ..d. follow exponential distribution, .e. \\(y_i \\sim \\mathcal{E}(\\lambda)\\).observed arrivals time, thereby constituting sample \\(\\{y_1,\\dots,y_n\\}\\). want estimate \\(\\lambda\\) (.e. case, vector parameters simply \\(\\theta = \\lambda\\)).density \\(Y\\) \\(f(y;\\lambda) = \\dfrac{1}{\\lambda}\\exp(-y/\\lambda)\\). Fig. 5.1 represents density functions different values \\(\\lambda\\).200 observations reported bottom Fig. 5.1 (red).\nbuild histogram report chart.\nFigure 5.1: red ticks, bottom, indicate observations (200 ). historgram based 200 observations\nestimate \\(\\lambda\\)?Now, assume four observations: \\(y_1=1.1\\), \\(y_2=2.2\\), \\(y_3=0.7\\) \\(y_4=5.0\\).\nprobability observing, small \\(\\varepsilon\\),\\(1.1-\\varepsilon \\le Y_1 < 1.1+\\varepsilon\\),\\(2.2-\\varepsilon \\le Y_2 < 2.2+\\varepsilon\\),\\(0.7-\\varepsilon \\le Y_3 < 0.7+\\varepsilon\\) \\(5.0-\\varepsilon \\le Y_4 < 5.0+\\varepsilon\\)?\\(y_i\\)s ..d., probability \\(\\prod_{=1}^4(2\\varepsilon f(y_i,\\lambda))\\).\nnext plot shows probability (divided \\(16\\varepsilon^4\\)) function \\(\\lambda\\).\nFigure 5.2: Proba. \\(y_i-\\varepsilon \\le Y_i < y_i+\\varepsilon\\), \\(\\\\{1,2,3,4\\}\\). vertical red line indicates maximum function.\nBack example 200 observations:\nFigure 5.3: Log-likelihood function associated 200 ..d. observations. vertical red line indicates maximum function.\n","code":""},{"path":"estimation-methods.html","id":"notations","chapter":"5 Estimation Methods","heading":"5.2.1 Notations","text":"\\(f(y;\\boldsymbol\\theta)\\) denotes probability density function (p.d.f.) random variable \\(Y\\) depends set parameters \\(\\boldsymbol\\theta\\).Density \\(n\\) independent identically distributed (..d.) observations \\(Y\\):\n\\[\nf(y_1,\\dots,y_n;\\boldsymbol\\theta) = \\prod_{=1}^n f(y_i;\\boldsymbol\\theta).\n\\]\n\\(\\mathbf{y}\\) denotes vector observations; \\(\\mathbf{y} = \\{y_1,\\dots,y_n\\}\\).Definition 5.1  (Likelihood function) \\(\\mathcal{L}: \\boldsymbol\\theta \\rightarrow \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})=f(y_1,\\dots,y_n;\\boldsymbol\\theta)\\) likelihood function.often work \\(\\log \\mathcal{L}\\), log-likelihood function.Example 5.1  (Gaussian distribution) \\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\n\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y}) = - \\frac{1}{2}\\sum_{=1}^n\\left( \\log \\sigma^2 + \\log 2\\pi + \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right).\n\\]Definition 5.2  (Score) score \\(S(y;\\boldsymbol\\theta)\\) given \\(\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\).\\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) (Example 5.1), \n\\[\n\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} =\n\\left[\\begin{array}{c}\n\\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\mu}\\\\\n\\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\sigma^2}\n\\end{array}\\right] =\n\\left[\\begin{array}{c}\n\\dfrac{y-\\mu}{\\sigma^2}\\\\\n\\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right)\n\\end{array}\\right].\n\\]Proposition 5.1  (Score expectation) expectation score zero.Proof. :\n\\[\\begin{eqnarray*}\n\\mathbb{E}\\left(\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\right) &=&\n\\int \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta) dy \\\\\n&=& \\int \\frac{\\partial f(y;\\boldsymbol\\theta)/\\partial \\boldsymbol\\theta}{f(y;\\boldsymbol\\theta)} f(y;\\boldsymbol\\theta) dy =\n\\frac{\\partial}{\\partial \\boldsymbol\\theta} \\int f(y;\\boldsymbol\\theta) dy =\n\\partial 1 /\\partial \\boldsymbol\\theta = 0.\n\\end{eqnarray*}\\]","code":""},{"path":"microeconometrics.html","id":"microeconometrics","chapter":"6 Microeconometrics","heading":"6 Microeconometrics","text":"","code":""},{"path":"time-series.html","id":"time-series","chapter":"7 Time Series","heading":"7 Time Series","text":"blabla","code":""},{"path":"appendix.html","id":"appendix","chapter":"8 Appendix","heading":"8 Appendix","text":"","code":""},{"path":"appendix.html","id":"statitical-tables","chapter":"8 Appendix","heading":"8.1 Statitical Tables","text":"Table 8.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 8.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 8.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 8.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]

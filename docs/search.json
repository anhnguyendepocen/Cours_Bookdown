[{"path":"index.html","id":"prerequisites","chapter":"\n1 Prerequisites\n\n\n\n\n\n\n\n\n\n\n\n\n","heading":"\n1 Prerequisites\n\n\n\n\n\n\n\n\n\n\n\n\n","text":"sample book written Markdown. can use anything Pandoc’s Markdown supports, e.g., math equation \\(^2 + b^2 = c^2\\).bookdown package can installed CRAN Github:Remember Rmd file contains one one chapter, chapter defined first-level heading #.compile example PDF, need XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.name/tinytex/.","code":"\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"linear-regressions.html","id":"linear-regressions","chapter":"2 Linear Regressions","heading":"2 Linear Regressions","text":"Definition 2.1  linear regression model defined :\n\\[\\begin{equation}\ny_i = \\boldsymbol\\beta'\\mathbf{x}_{} + \\varepsilon_i,\\tag{2.1}\n\\end{equation}\\]\n\\(\\mathbf{x}_{}=[x_{,1},\\dots,x_{,K}]'\\) vector dimension \\(K \\times 1\\).entity \\(\\), \\(x_{,k}\\)’s, \\(k \\\\{1,\\dots,K\\}\\), explanatory variables, regressors, covariates. variable interest, \\(y_i\\), often called dependent variable, regressand. last term specification, namely \\(\\varepsilon_i\\), called error, disturbance.researcher usually interested components vector \\(\\boldsymbol\\beta\\), denoted \\(\\beta_k\\), \\(k \\\\{1,\\dots,K\\}\\). usually aims estimating coefficients based observations \\(\\{y_i,\\mathbf{x}_{}\\}\\), \\(\\\\{1,\\dots,n\\}\\), constitutes sample (size \\(n\\)).intercept specification (2.1), one set \\(x_{,1}=1\\) \\(\\); \\(\\beta_1\\) corresponds intercept.","code":""},{"path":"linear-regressions.html","id":"linearHyp","chapter":"2 Linear Regressions","heading":"2.1 Hypotheses","text":"following, introduce different assumptions regarding covariates /errors. properties estimators used researcher depend assumptions satisfied.Hypothesis 2.1  (Full rank) exact linear relationship among independent variables (\\(x_{,k}\\)s, given \\(\\\\{1,\\dots,n\\}\\)).Intuitively, Hypothesis 2.1 satisfied, estimation model parameters unfeasible since, value \\(\\boldsymbol\\beta\\), changes explanatory variables exactly compensated changes another set explanatory variables, preventing identification effects.Let us denote \\(\\mathbf{X}\\) matrix containing explanatory variables, dimension \\(n \\times K\\). (, row \\(\\) \\(\\mathbf{X}\\) \\(\\mathbf{x}_i'\\).) following hypothesis concerns relationship errors (gathered \\(\\boldsymbol\\varepsilon\\), \\(n\\)-dimensional vector) explanatory variables \\(\\mathbf{X}\\):Hypothesis 2.2  (Conditional mean-zero assumption) \\[\\begin{equation}\n\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X}) = 0.\n\\end{equation}\\]following proposition states implications Hypothesis 2.2:Proposition 2.1  Hypothesis 2.2:\\(\\mathbb{E}(\\varepsilon_{})=0\\);\\(x_{ij}\\)s \\(\\varepsilon_{}\\)s uncorrelated, .e. \\(\\forall ,\\,j \\quad \\mathbb{C}orr(x_{ij},\\varepsilon_{})=0\\).Proof. Let us prove () (ii):law iterated expectations:\n\\[\n\\mathbb{E}(\\boldsymbol\\varepsilon)=\\mathbb{E}(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X}))=\\mathbb{E}(0)=0.\n\\]\\(\\mathbb{E}(x_{ij}\\varepsilon_i)=\\mathbb{E}(\\mathbb{E}(x_{ij}\\varepsilon_i|\\mathbf{X}))=\\mathbb{E}(x_{ij}\\underbrace{\\mathbb{E}(\\varepsilon_i|\\mathbf{X})}_{=0})=0\\).\\(\\square\\)Let us now present two hypotheses (?? ??) concerning stochastic properties errors \\(\\varepsilon_i\\):Hypothesis 2.3  (Homoskedasticity) \\[\n\\forall , \\quad \\mathbb{V}ar(\\varepsilon_i|\\mathbf{X}) = \\sigma^2.\n\\]following lines code generate figure comparing two situations: Panel () Figure 2.1 corresponds situation homoskedasticity, Panel (b) corresponds situation heteroskedasticity. Let us specific. two plots, \\(X_i \\sim \\mathcal{N}(0,1)\\) \\(\\varepsilon^*_i \\sim \\mathcal{N}(0,1)\\). Panel () (homoskedasticity): \\(Y_i = 2 + 2X_i + \\varepsilon^*_i\\). Panel (b) (heteroskedasticity): \\(Y_i = 2 + 2X_i + \\left(2\\mathbb{1}_{\\{X_i<0\\}}+0.2\\mathbb{1}_{\\{X_i\\ge0\\}}\\right)\\varepsilon^*_i\\).\nFigure 2.1: Homoskedasticity vs heteroskedasticity.\nFigure 2.2 shows situation heteroskedasticity, based data taken Swiss Household Panel. sample restricted persons younger 35 year 2019, completed least 19 years study. figure shows dispersion yearly income increases age.\nFigure 2.2: Income versus age. Data Swiss Household Panel. sample restricted persons completed least 19 years study. figure shows dispersion yearly income increases age.\nnext assumption concers correlation errors across entities.Hypothesis 2.4  (Uncorrelated errors) \\[\n\\forall \\ne j, \\quad \\mathbb{C}ov(\\varepsilon_i,\\varepsilon_j|\\mathbf{X})=0.\n\\]often work covariance matrices. Proposition 2.2 give specific form conditional covariance errors Hpoytheses 2.3 2.4 satisfied.Proposition 2.2  Hpoytheses 2.3 2.4 hold, :\n\\[\n\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})= \\sigma^2 Id,\n\\]\n\\(Id\\) \\(n \\times n\\) identity matrix.sometimes assume errors Gaussian — normal. work Hypothesis 2.5:Hypothesis 2.5  (Normal distribution) \\[\n\\forall , \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2).\n\\]","code":"\nlibrary(AEC)\ntable(shp$edyear19)## \n##    8    9   10   12   13   14   16   19   21 \n##   70  325  350 1985  454  117  990 1263  168\nshp_higherEd <- subset(shp,(edyear19>18)&age19<35)\nplot(i19wyg/1000~age19,data=shp_higherEd,pch=19,las=1,xlab=\"Age\",ylab=\"Yearly work income\")\nabline(lm(i19wyg/1000~age19,data=shp_higherEd),col=\"red\",lwd=2)"},{"path":"linear-regressions.html","id":"LSquares","chapter":"2 Linear Regressions","heading":"2.2 Least square estimation","text":"","code":""},{"path":"linear-regressions.html","id":"derivation-of-the-ols-formula","chapter":"2 Linear Regressions","heading":"2.2.1 Derivation of the OLS formula","text":"section, present study properties popular estimation approach called Ordinary Least Squares (OLS). suggested name, OLS estimator \\(\\boldsymbol\\beta\\) defined vector \\(\\mathbf{b}\\) minimizes sum squared residuals. (residuals estimates errors \\(\\varepsilon_i\\).)given vector coefficients \\(\\mathbf{b}=[b_1,\\dots,b_K]'\\), sum squared residuals :\n\\[\nf(\\mathbf{b}) =\\sum_{=1}^n \\left(y_i - \\sum_{j=1}^K x_{,j} b_j \\right)^2 = \\sum_{=1}^n (y_i - \\mathbf{x}_i' \\mathbf{b})^2.\n\\]\nMinimizing sum amounts minimizing:\n\\[\nf(\\mathbf{b}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{b})'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}).\n\\]Since:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = - 2 \\mathbf{X}'\\mathbf{y} + 2 \\mathbf{X}'\\mathbf{X}\\mathbf{b},\n\\]\ncomes necessary first-order condition (FOC) :\n\\[\\begin{equation}\n\\mathbf{X}'\\mathbf{X}\\mathbf{b} = \\mathbf{X}'\\mathbf{y}.\\tag{2.2}\n\\end{equation}\\]\nAssumption 2.1, \\(\\mathbf{X}'\\mathbf{X}\\) invertible. Hence:\n\\[\n\\boxed{\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y}.}\n\\]\nVector \\(\\mathbf{b}\\) minimizes sum squared residuals. (\\(f\\) non-negative quadratic function, therefore admits minimum.):\n\\[\n\\mathbf{y} = \\underbrace{\\mathbf{X}\\mathbf{b}}_{\\mbox{fitted values } (\\mathbf{y})} + \\underbrace{\\mathbf{e}}_{\\mbox{residuals}}\n\\]estimated residuals :\n\\[\\begin{equation}\n\\mathbf{e} = \\mathbf{y} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y} = \\mathbf{M} \\mathbf{y}\\tag{2.3}\n\\end{equation}\\]\n\\(\\mathbf{M} := \\mathbf{} - \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\) called residual maker matrix.Moreover, fitted values \\(\\hat{\\mathbf{y}}\\) given :\n\\[\\begin{equation}\n\\hat{\\mathbf{y}}=\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y} = \\mathbf{P} \\mathbf{y},\\tag{2.4}\n\\end{equation}\\]\n\\(\\mathbf{P}=\\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\) projection matrix.matrices \\(\\mathbf{M}\\) \\(\\mathbf{P}\\) :\\(\\mathbf{M} \\mathbf{X} = \\mathbf{0}\\): one regresses one explanatory variables \\(\\mathbf{X}\\), residuals null.\\(\\mathbf{M}\\mathbf{y}=\\mathbf{M}\\boldsymbol\\varepsilon\\) (\\(\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon\\) \\(\\mathbf{M} \\mathbf{X} = \\mathbf{0}\\)).additional properties \\(\\mathbf{M}\\) \\(\\mathbf{P}\\):\\(\\mathbf{M}\\) symmetric (\\(\\mathbf{M} = \\mathbf{M}'\\)) idempotent (\\(\\mathbf{M} = \\mathbf{M}^2 = \\mathbf{M}^k\\) \\(k>0\\)).\\(\\mathbf{P}\\) symmetric idempotent.\\(\\mathbf{P}\\mathbf{X} = \\mathbf{X}\\).\\(\\mathbf{P} \\mathbf{M} = \\mathbf{M} \\mathbf{P} = 0\\).\\(\\mathbf{y} = \\mathbf{P}\\mathbf{y} + \\mathbf{M}\\mathbf{y}\\) (decomposition \\(\\mathbf{y}\\) two orthogonal parts).easily checked \\(\\mathbf{X}'\\mathbf{e}=0\\). column \\(\\mathbf{X}\\) therefore orthogonal \\(\\mathbf{e}\\). particular, intercept included regression (\\(x_{,1} \\equiv 1\\), .e., first column \\(\\mathbf{X}\\) filled ones), average residuals null.Example 2.1  (Bivariate case) Consider bivariate situation, regress\\(y_i\\) constant explanatory variable \\(w_i\\). \\(K=2\\), \\(\\mathbf{X}\\) \\(n \\times 2\\) matrix whose \\(^{th}\\) row \\([x_{,1},x_{,2}]\\), \\(x_{,1}=1\\) (account intercept) \\(w_i = x_{,2}\\) (say).:\n\\[\\begin{eqnarray*}\n\\mathbf{X}'\\mathbf{X} &=&\n\\left[\\begin{array}{cc}\nn & \\sum_i w_i \\\\\n\\sum_i w_i & \\sum_i w_i^2\n\\end{array}\n\\right],\\\\\n(\\mathbf{X}'\\mathbf{X})^{-1} &=&\n\\frac{1}{n\\sum_i w_i^2-(\\sum_i w_i)^2}\n\\left[\\begin{array}{cc}\n\\sum_i w_i^2 & -\\sum_i w_i \\\\\n-\\sum_i w_i & n\n\\end{array}\n\\right],\\\\\n(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y} &=&\n\\frac{1}{n\\sum_i w_i^2-(\\sum_i w_i)^2}\n\\left[\\begin{array}{c}\n\\sum_i w_i^2\\sum_i y_i -\\sum_i w_i \\sum_i w_iy_i \\\\\n-\\sum_i w_i \\sum_i y_i + n \\sum_i w_i y_i\n\\end{array}\n\\right]\\\\\n&=& \\frac{1}{\\frac{1}{n}\\sum_i(w_i - \\bar{w})^2}\n\\left[\\begin{array}{c}\n\\frac{\\bar{y}}{n}\\sum_i w_i^2 -\\frac{\\bar{w}}{n}\\sum_i w_iy_i \\\\\n\\frac{1}{n}\\sum_i (w_i-\\bar{w})(y_i-\\bar{y})\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]can seen second element \\(\\mathbf{b}=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\) :\n\\[\nb_2 = \\frac{\\overline{\\mathbb{C}ov(W,Y)}}{\\overline{\\mathbb{V}ar(W)}},\n\\]\n\\(\\overline{\\mathbb{C}ov(W,Y)}\\) \\(\\overline{\\mathbb{V}ar(W)}\\) sample estimates.Since constant regression, \\(b_1 = \\bar{y} - b_2 \\bar{w}\\).","code":""},{"path":"linear-regressions.html","id":"properties-of-the-ols-estimate-small-sample","chapter":"2 Linear Regressions","heading":"2.2.2 Properties of the OLS estimate (small sample)","text":"Proposition 2.3 states first properties OLS estimator:Proposition 2.3  (Properties OLS estimator) :Assumptions 2.1 2.2, OLS estimator linear unbiased.Assumptions 2.1 2.2, OLS estimator linear unbiased.Hypotheses 2.1 2.4, conditional covariance matrix \\(\\mathbf{b}\\) : \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\).Hypotheses 2.1 2.4, conditional covariance matrix \\(\\mathbf{b}\\) : \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\).Proof. Hypothesis 2.1, \\(\\mathbf{X}'\\mathbf{X}\\) can inverted. :\n\\[\n\\mathbf{b} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}.\n\\]Let us consider expectation last term, .e. \\(\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon})\\). Using law iterated expectations, obtain:\n\\[\n\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}) = \\mathbb{E}(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}|\\mathbf{X}]) = \\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbb{E}[\\mathbf{\\varepsilon}|\\mathbf{X}]).\n\\]\nHypothesis 2.2, \\(\\mathbb{E}[\\mathbf{\\varepsilon}|\\mathbf{X}]=0\\). Hence \\(\\mathbb{E}((\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{\\varepsilon}) =0\\) result () follows.\\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}' \\mathbb{E}(\\boldsymbol\\varepsilon\\boldsymbol\\varepsilon'|\\mathbf{X}) \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1}\\).\nProp. 2.2, 2.3 2.4 hold, \\(\\mathbb{E}(\\boldsymbol\\varepsilon\\boldsymbol\\varepsilon'|\\mathbf{X})=\\mathbb{V}ar(\\boldsymbol\\varepsilon|\\mathbf{X})=\\sigma^2 Id\\).Together, Hypotheses 2.1 2.4 form Gauss-Markov set assumptions. assumptions, OLS estimator feature lowest possible variance:Theorem 2.1  (Gauss-Markov Theorem) Assumptions 2.1 2.4, vector \\(w\\), minimum-variance linear unbiased estimator \\(w' \\boldsymbol\\beta\\) \\(w' \\mathbf{b}\\), \\(\\mathbf{b}\\) least squares estimator. (BLUE: Best Linear Unbiased Estimator.)Proof. Consider \\(\\mathbf{b}^* = C \\mathbf{y}\\), another linear unbiased estimator \\(\\boldsymbol\\beta\\). Since unbiased, must \\(\\mathbb{E}(C\\mathbf{y}|\\mathbf{X}) = \\mathbb{E}(C\\mathbf{X}\\boldsymbol\\beta + C\\boldsymbol\\varepsilon|\\mathbf{X}) = \\boldsymbol\\beta\\). \\(\\mathbb{E}(C\\boldsymbol\\varepsilon|\\mathbf{X})=C\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X})=0\\) (2.2).Therefore \\(\\mathbf{b}^*\\) unbiased \\(\\mathbb{E}(C\\mathbf{X})\\boldsymbol\\beta=\\boldsymbol\\beta\\). case \\(\\boldsymbol\\beta\\), implies must \\(C\\mathbf{X}=\\mathbf{}\\).Let us compute \\(\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X})\\). , introduce \\(D = C - (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\), \\(D\\mathbf{y}=\\mathbf{b}^*-\\mathbf{b}\\). fact \\(C\\mathbf{X}=\\mathbf{}\\) implies \\(D\\mathbf{X} = \\mathbf{0}\\).\\(\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X}) = \\mathbb{V}ar(C \\mathbf{y}|\\mathbf{X}) =\\mathbb{V}ar(C \\boldsymbol\\varepsilon|\\mathbf{X}) = \\sigma^2CC'\\) (Assumptions 2.3 2.4, see Prop. 2.2). Using \\(C=D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) exploiting fact \\(D\\mathbf{X} = \\mathbf{0}\\) leads :\n\\[\n\\mathbb{V}ar(\\mathbf{b^*}|\\mathbf{X}) =\\sigma^2\\left[(D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')(D+(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')'\\right] = \\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) + \\sigma^2 \\mathbf{D}\\mathbf{D}'.\n\\]\nTherefore, \\(\\mathbb{V}ar(w'\\mathbf{b^*}|\\mathbf{X})=w'\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})w + \\sigma^2 w'\\mathbf{D}\\mathbf{D}'w\\ge w'\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})w=\\mathbb{V}ar(w'\\mathbf{b}|\\mathbf{X})\\).Frish-Waugh theorem (Theorem 2.2) reveals relationship OLS estimator notion partial correlation coefficient. Consider linear least square regression \\(\\mathbf{y}\\) \\(\\mathbf{X}\\). introduce notations:\\(\\mathbf{b}^{\\mathbf{y}/\\mathbf{X}}\\): OLS estimates \\(\\boldsymbol\\beta\\),\\(\\mathbf{M}^{\\mathbf{X}}\\): residual-maker matrix regression \\(\\mathbf{X}\\),\\(\\mathbf{P}^{\\mathbf{X}}\\): projection matrix regression \\(\\mathbf{X}\\).Let us split set explanatory variables two: \\(\\mathbf{X} = [\\mathbf{X}_1,\\mathbf{X}_2]\\). obvious notations: \\(\\mathbf{b}^{\\mathbf{y}/\\mathbf{X}}=[\\mathbf{b}_1',\\mathbf{b}_2']'\\).Theorem 2.2  (Frisch-Waugh Theorem) :\n\\[\n\\mathbf{b}_2 = \\mathbf{b}^{\\mathbf{M^{\\mathbf{X}_1}y}/\\mathbf{M^{\\mathbf{X}_1}\\mathbf{X}_2}}.\n\\]Proof. minimization least squares leads (first-order conditions, see Eq. (2.2)):\n\\[\n\\left[ \\begin{array}{cc} \\mathbf{X}_1'\\mathbf{X}_1 & \\mathbf{X}_1'\\mathbf{X}_2 \\\\ \\mathbf{X}_2'\\mathbf{X}_1 & \\mathbf{X}_2'\\mathbf{X}_2\\end{array}\\right]\n\\left[ \\begin{array}{c} \\mathbf{b}_1 \\\\ \\mathbf{b}_2\\end{array}\\right] =\n\\left[ \\begin{array}{c} \\mathbf{X}_1' \\mathbf{y} \\\\ \\mathbf{X}_2' \\mathbf{y} \\end{array}\\right].\n\\]\nUse first-row block equations solve \\(\\mathbf{b}_1\\) first; comes function \\(\\mathbf{b}_2\\). use second set equations solve \\(\\mathbf{b}_2\\), leads :\n\\[\n\\mathbf{b}_2 = [\\mathbf{X}_2'\\mathbf{X}_2 - \\mathbf{X}_2'\\mathbf{X}_1(\\mathbf{X}_1'\\mathbf{X}_1)\\mathbf{X}_1'\\mathbf{X}_2]^{-1}\\mathbf{X}_2'(Id - \\mathbf{X}_1(\\mathbf{X}_1'\\mathbf{X}_1)\\mathbf{X}_1')\\mathbf{y}=[\\mathbf{X}_2' \\mathbf{M}^{\\mathbf{X}_1}\\mathbf{X}_2]^{-1}\\mathbf{X}_2'\\mathbf{M}^{\\mathbf{X}_1}\\mathbf{y}.\n\\]\nUsing fact \\(\\mathbf{M}^{\\mathbf{X}_1}\\) idempotent symmetric leads result.suggests second way estimating \\(\\mathbf{b}_2\\):Regress \\(Y\\) \\(X_1\\), regress \\(X_2\\) \\(X_1\\).Regress residuals associated former regression ones associated withe latter regressions.illustrated following code, run different regressions involving number Google searches “parapluie” (umbrella French) broad specification, regress French precipitations month dummies. Next, deseasonalize dependent variable precipitations regressing month dummies. stated Theorem 2.2, regressing deseasonalized Google searches deseasonalized precipitations give coefficient baseline regression.\\(b_2\\) scalar (\\(\\mathbf{X}_2\\) dimension \\(n \\times 1\\)), Theorem 2.2 gives expression partial regression coefficient \\(b_2\\):\n\\[\nb_2 = \\frac{\\mathbf{X}_2'M^{\\mathbf{X}_1}\\mathbf{y}}{\\mathbf{X}_2'M^{\\mathbf{X}_1}\\mathbf{X}_2}.\n\\]","code":"\nlibrary(AEC)\ndummies <- as.matrix(parapluie[,4:14])\neq_all <- lm(parapluie~precip+dummies,data=parapluie)\ndeseas_parapluie <- lm(parapluie~dummies,data=parapluie)$residuals\ndeseas_precip    <- lm(precip~dummies,data=parapluie)$residuals\neq_frac <- lm(deseas_parapluie~deseas_precip)\nrbind(eq_all$coefficients,\n      c(eq_frac$coefficients,rep(NaN,11)))##        (Intercept)    precip  dummies1  dummies2  dummies3  dummies4  dummies5\n## [1,] -2.953448e+00 0.1300055 -8.599068 -13.32904 -7.982958 -3.392353 -3.703816\n## [2,] -1.038345e-15 0.1300055       NaN       NaN       NaN       NaN       NaN\n##       dummies6  dummies7  dummies8 dummies9 dummies10 dummies11\n## [1,] -3.360641 -7.315881 -7.717277  -4.6492 -5.109199   1.98077\n## [2,]       NaN       NaN       NaN      NaN       NaN       NaN"},{"path":"linear-regressions.html","id":"goodness-of-fit","chapter":"2 Linear Regressions","heading":"2.2.3 Goodness of fit","text":"Define total variation \\(y\\) sum squared deviations:\n\\[\nTSS = \\sum_{=1}^{n} (y_i - \\bar{y})^2.\n\\]\n:\n\\[\n\\mathbf{y} = \\mathbf{X}\\mathbf{b} + \\mathbf{e} = \\hat{\\mathbf{y}} + \\mathbf{e}\n\\]\nfollowing, assume regression includes constant (.e. \\(\\), \\(x_{,1}=1\\)). Denote \\(\\mathbf{M}^0\\) matrix transforms observations deviations sample means. Using \\(\\mathbf{M}^0 \\mathbf{e} = \\mathbf{e}\\) \\(\\mathbf{X}' \\mathbf{e}=0\\), :\n\\[\\begin{eqnarray*}\n\\underbrace{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}}_{\\mbox{Total sum sq.}} &=& (\\mathbf{X}\\mathbf{b} + \\mathbf{e})' \\mathbf{M}^0 (\\mathbf{X}\\mathbf{b} + \\mathbf{e})\\\\\n&=& \\underbrace{\\mathbf{b}' \\mathbf{X}' \\mathbf{M}^0 \\mathbf{X}\\mathbf{b}}_{\\mbox{\"Explained\" sum sq.}} + \\underbrace{\\mathbf{e}'\\mathbf{e}}_{\\mbox{Sum sq. residuals}}\\\\\nTSS &=& Expl.SS + SSR.\n\\end{eqnarray*}\\]can now define coefficient determination:\n\\[\\begin{equation}\n\\boxed{\\mbox{Coefficient determination} = \\frac{Expl.SS}{TSS} = 1 - \\frac{SSR}{TSS} = 1 - \\frac{\\mathbf{e}'\\mathbf{e}}{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}}.}\\tag{2.5}\n\\end{equation}\\]can shown (Greene 2003, sec. 3.5) :\n\\[\n\\mbox{Coefficient determination} = \\frac{[\\sum_{=1}^n(y_i - \\bar{y})(\\hat{y_i} - \\bar{y})]^2}{\\sum_{=1}^n(y_i - \\bar{y})^2 \\sum_{=1}^n(\\hat{y_i} - \\bar{y})^2}.\n\\]\n, \\(R^2\\) sample squared correlation \\(y\\) (regression-implied) \\(y\\)’s predictions.hgher \\(R^2\\), higher goodness fit model. One however cautious \\(R^2\\). Indeed, easy increase : suffices add explanatory variables. stated Proposition 2.5, adding explanatory variable (even truly relate dependent variable) results increase \\(R^2\\). limit, taking set \\(n\\) non-linearly-dependent explanatory variables (.e., variables satisfying Hypothesis 2.1) results \\(R^2\\) equal one.Proposition 2.4  (Change SSR variable added) :\n\\[\\begin{equation}\n\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e} - c^2(\\mathbf{z^*}'\\mathbf{z^*}) \\qquad (\\le \\mathbf{e}'\\mathbf{e}) \\tag{2.6}\n\\end{equation}\\]\n() \\(\\mathbf{u}\\) \\(\\mathbf{e}\\) residuals regressions \\(\\mathbf{y}\\) \\([\\mathbf{X},\\mathbf{z}]\\) \\(\\mathbf{y}\\) \\(\\mathbf{X}\\), respectively, (ii) \\(c\\) regression coefficient \\(\\mathbf{z}\\) former regression \\(\\mathbf{z}^*\\) residuals regression \\(\\mathbf{z}\\) \\(\\mathbf{X}\\).Proof. OLS estimates \\([\\mathbf{d}',\\mathbf{c}]'\\) regression \\(\\mathbf{y}\\) \\([\\mathbf{X},\\mathbf{z}]\\) satisfies (first-order cond., Eq. (2.2))\n\\[\n\\left[ \\begin{array}{cc} \\mathbf{X}'\\mathbf{X} & \\mathbf{X}'\\mathbf{z} \\\\ \\mathbf{z}'\\mathbf{X} & \\mathbf{z}'\\mathbf{z}\\end{array}\\right]\n\\left[ \\begin{array}{c} \\mathbf{d} \\\\ \\mathbf{c}\\end{array}\\right] =\n\\left[ \\begin{array}{c} \\mathbf{X}' \\mathbf{y} \\\\ \\mathbf{z}' \\mathbf{y} \\end{array}\\right].\n\\]\nHence, particular \\(\\mathbf{d} = \\mathbf{b} - (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{z}\\mathbf{c}\\), \\(\\mathbf{b}\\) OLS \\(\\mathbf{y}\\) \\(\\mathbf{X}\\). Substituting \\(\\mathbf{u} = \\mathbf{y} - \\mathbf{X}\\mathbf{d} - \\mathbf{z}c\\), get \\(\\mathbf{u} = \\mathbf{e} - \\mathbf{z}^*c\\). therefore :\n\\[\\begin{equation}\n\\mathbf{u}'\\mathbf{u} = (\\mathbf{e} - \\mathbf{z}^*c)(\\mathbf{e} - \\mathbf{z}^*c)= \\mathbf{e}'\\mathbf{e} + c^2(\\mathbf{z^*}'\\mathbf{z^*}) - 2 c\\mathbf{z^*}'\\mathbf{e}.\\tag{2.7}\n\\end{equation}\\]\nNow \\(\\mathbf{z^*}'\\mathbf{e} = \\mathbf{z^*}'(\\mathbf{y} - \\mathbf{X}\\mathbf{b}) = \\mathbf{z^*}'\\mathbf{y}\\) \\(\\mathbf{z}^*\\) residuals OLS regression \\(\\mathbf{X}\\). Since \\(c = (\\mathbf{z^*}'\\mathbf{z^*})^{-1}\\mathbf{z^*}'\\mathbf{y^*}\\) (application Theorem 2.2), \\((\\mathbf{z^*}'\\mathbf{z^*})c = \\mathbf{z^*}'\\mathbf{y^*}\\) , therefore, \\(\\mathbf{z^*}'\\mathbf{e} = (\\mathbf{z^*}'\\mathbf{z^*})c\\). Inserting Eq. (2.7) leads results.Proposition 2.5  (Change coefficient determination variable added) Denoting \\(R_W^2\\) coefficient determination regression \\(\\mathbf{y}\\) variable \\(\\mathbf{W}\\), :\n\\[\nR_{\\mathbf{X},\\mathbf{z}}^2 = R_{\\mathbf{X}}^2 + (1-R_{\\mathbf{X}}^2)(r_{yz}^\\mathbf{X})^2,\n\\]\n\\(r_{yz}^\\mathbf{X}\\) coefficient partial correlation (see Definition 3.1).Proof. Let’s use notations Prop. @ref{prp:chgeR2}. Theorem 2.2 implies \\(c = (\\mathbf{z^*}'\\mathbf{z^*})^{-1}\\mathbf{z^*}'\\mathbf{y^*}\\). Using Eq. (2.6) gives \\(\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e} - (\\mathbf{z^*}'\\mathbf{y^*})^2/(\\mathbf{z^*}'\\mathbf{z^*})\\). Using definition partial correlation (Eq. (3.1)), get \\(\\mathbf{u}'\\mathbf{u} = \\mathbf{e}'\\mathbf{e}\\left(1 - (r_{yz}^\\mathbf{X})^2\\right)\\). results obtained dividing sides previous equation \\(\\mathbf{y}'\\mathbf{M}_0\\mathbf{y}\\).Figure 2.3, , illustrates fact one can obtain \\(R^2\\) one regressing sample length \\(n\\) set \\(n\\) linearly-independent variables.\nFigure 2.3: figure illustrates monotonous increase \\(R^2\\) function number explanatory variables. true model, explanatory variables, .e., \\(y_i = \\varepsilon_i\\). take (independent) regressors regress \\(y\\) latter, progresively increasing set regressors.\norder address risk adding irrelevant explanatory variables, measures adjusted \\(R^2\\) proposed. Compared standard \\(R^2\\), measures add penalties depend number covariates employed regression. common adjusted \\(R^2\\) measure, denoted \\(\\bar{R}^2\\), following:\n\\[\\begin{equation*}\n\\boxed{\\bar{R}^2 = 1 - \\frac{\\mathbf{e}'\\mathbf{e}/(n-K)}{\\mathbf{y}'\\mathbf{M}^0\\mathbf{y}/(n-1)} = 1 - \\frac{n-1}{n-K}(1-R^2).}\n\\end{equation*}\\]","code":"\nn <- 30;Y <- rnorm(n);X <- matrix(rnorm(n^2),n,n)\nall_R2 <- NULL;all_adjR2 <- NULL\nfor(j in 0:(n-1)){\n  if(j==0){eq <- lm(Y~1)\n  }else{eq <- lm(Y~X[,1:j])}\n  all_R2 <- c(all_R2,summary(eq)$r.squared)\n  all_adjR2 <- c(all_adjR2,summary(eq)$adj.r.squared)\n}\nplot(all_R2,pch=19,ylim=c(min(all_adjR2,na.rm = TRUE),1),xlab=\"number of regressors\",ylab=\"R2\")\npoints(all_adjR2,pch=3);abline(h=0,col=\"light grey\",lwd=2)\nlegend(\"topleft\",c(\"R2\",\"Adjusted R2\"),\n       lty=NaN,col=c(\"black\"),pch=c(19,3),lwd=2)"},{"path":"linear-regressions.html","id":"inference-and-confidence-intervals-in-small-sample","chapter":"2 Linear Regressions","heading":"2.2.4 Inference and confidence intervals (in small sample)","text":"normality assumption (Assumption 2.5), know distribution \\(\\mathbf{b}\\) (conditional \\(\\mathbf{X}\\)). Indeed, \\((\\mathbf{b}|\\mathbf{X}) \\equiv (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y}\\) multivariate Gaussian:\n\\[\\begin{equation}\n\\mathbf{b}|\\mathbf{X} \\sim \\mathcal{N}(\\beta,\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}).\\tag{2.8}\n\\end{equation}\\]Eq. (2.8) can used conduct inference tests. However, practice, know \\(\\sigma^2\\) (population parameter). following proposition gives unbiased estimate \\(\\sigma^2\\).Proposition 2.6  2.1 2.4, unbiased estimate \\(\\sigma^2\\) given :\n\\[\\begin{equation}\ns^2 = \\frac{\\mathbf{e}'\\mathbf{e}}{n-K}.\\tag{2.9}\n\\end{equation}\\]\n(sometimes denoted \\(\\sigma^2_{OLS}\\).)Proof. \\(\\mathbb{E}(\\mathbf{e}'\\mathbf{e}|\\mathbf{X})=\\mathbb{E}(\\boldsymbol{\\varepsilon}'\\mathbf{M}\\boldsymbol{\\varepsilon}|\\mathbf{X})=\\mathbb{E}(\\mbox{Tr}(\\boldsymbol{\\varepsilon}'\\mathbf{M}\\boldsymbol{\\varepsilon})|\\mathbf{X})) =\\mbox{Tr}(\\mathbf{M}\\mathbb{E}(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'|\\mathbf{X}))=\\sigma^2 \\mbox{Tr}(\\mathbf{M})\\). (Note \\(\\mathbb{E}(\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'|\\mathbf{X})=\\sigma^2Id\\) Assumptions 2.3 2.4, see Prop. 2.2.) Finally:\n\\[\\begin{eqnarray*}\n\\mbox{Tr}(\\mathbf{M})&=&n-\\mbox{Tr}(\\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}')\\\\\n&=&n-\\mbox{Tr}((\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{X})=n-\\mbox{Tr}(Id_{K \\times K}),\n\\end{eqnarray*}\\]\nleads result.Two results prove important produce inference:know distribution \\(s^2\\) (Prop. 2.7).\\(s^2\\) \\(\\mathbf{b}\\) independent random variables (Prop. 2.8).Proposition 2.7  2.1 2.5, : \\(\\dfrac{s^2}{\\sigma^2} | \\mathbf{X} \\sim \\chi^2(n-K)/(n-K)\\).Proof. \\(\\mathbf{e}'\\mathbf{e}=\\boldsymbol\\varepsilon'\\mathbf{M}\\boldsymbol\\varepsilon\\). \\(\\mathbf{M}\\) idempotent symmetric matrix. Therefore can decomposed \\(PDP'\\) \\(D\\) diagonal matrix \\(P\\) orthogonal matrix. result \\(\\mathbf{e}'\\mathbf{e} = (P'\\boldsymbol\\varepsilon)'D(P'\\boldsymbol\\varepsilon)\\), .e. \\(\\mathbf{e}'\\mathbf{e}\\) weighted sum independent squared Gaussian variables (entries \\(P'\\boldsymbol\\varepsilon\\) independent Gaussian —2.5— uncorrelated). variance ..d. Gaussian variable \\(\\sigma^2\\). \\(\\mathbf{M}\\) idempotent symmetric matrix, eigenvalues either 0 1, rank equals trace (see Propositions 3.3 3.4). , trace equal \\(n-K\\) (see proof Eq. (2.9)). Therefore \\(D\\) \\(n-K\\) entries equal 1 \\(K\\) equal 0. Hence, \\(\\mathbf{e}'\\mathbf{e} = (P'\\boldsymbol\\varepsilon)'D(P'\\boldsymbol\\varepsilon)\\) sum \\(n-K\\) squared independent Gaussian variables variance \\(\\sigma^2\\). Therefore \\(\\frac{\\mathbf{e}'\\mathbf{e}}{\\sigma^2} = (n-K)\\frac{s^2}{\\sigma^2}\\) sum \\(n-k\\) squared ..d. standard normal variables.Proposition 2.8  Hypotheses 2.1 2.5, \\(\\mathbf{b}\\) \\(s^2\\) independent.Proof. \\(\\mathbf{b}=\\boldsymbol\\beta + [\\mathbf{X}'{\\mathbf{X}}]^{-1}\\mathbf{X}\\boldsymbol\\varepsilon\\) \\(s^2 = \\boldsymbol\\varepsilon' \\mathbf{M} \\boldsymbol\\varepsilon/(n-K)\\). Hence \\(\\mathbf{b}\\) affine combination \\(\\boldsymbol\\varepsilon\\) \\(s^2\\) quadratic combination Gaussian shocks. One can write \\(s^2\\) \\(s^2 = (\\mathbf{M}\\boldsymbol\\varepsilon)' \\mathbf{M} \\boldsymbol\\varepsilon/(n-K)\\) \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta + \\mathbf{T}\\boldsymbol\\varepsilon\\). Since \\(\\mathbf{T}\\mathbf{M}=0\\), \\(\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{M}\\boldsymbol\\varepsilon\\) independent (two uncorrelated Gaussian variables independent), therefore \\(\\mathbf{b}\\) \\(s^2\\), functions two sets independent variables, independent.Consistently Eq. (2.8), Hypotheses 2.1 2.5, \\(k^{th}\\) entry \\(\\mathbf{b}\\) satisfies:\n\\[\nb_k | \\mathbf{X} \\sim \\mathcal{N}(\\beta_k,\\sigma^2 v_k),\n\\]\n\\(v_k\\) k\\(^{th}\\) component diagonal \\((\\mathbf{X}'\\mathbf{X})^{-1}\\).Moreover, (Prop. 2.7):\n\\[\n\\frac{(n-K)s^2}{\\sigma^2} | \\mathbf{X} \\sim \\chi ^2 (n-K).\n\\]result (using Propositions 2.7 2.8), :\n\\[\\begin{equation}\n\\boxed{t_k = \\frac{\\frac{b_k - \\beta_k}{\\sqrt{\\sigma^2 v_k}}}{\\sqrt{\\frac{(n-K)s^2}{\\sigma^2(n-K)}}} = \\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} \\sim t(n-K),}\\tag{2.10}\n\\end{equation}\\]\n\\(t(n-K)\\) denotes \\(t\\) distribution \\(n-K\\) degrees freedom.1Note \\(s^2 v_k\\) exactly conditional variance \\(b_k\\): variance \\(b_k\\) conditional \\(\\mathbf{X}\\) \\(\\sigma^2 v_k\\). However \\(s^2 v_k\\) unbiased estimate \\(\\sigma^2 v_k\\) (Prop. 2.6).previous result (Eq. (2.10)) can extended linear combinations elements \\(\\mathbf{b}\\) (Eq. (2.10) \\(k^{th}\\) component ).Let us consider \\(\\boldsymbol\\alpha'\\mathbf{b}\\), OLS estimate \\(\\boldsymbol\\alpha'\\boldsymbol\\beta\\). Eq. (2.8), :\n\\[\n\\boldsymbol\\alpha'\\mathbf{b} | \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol\\alpha'\\boldsymbol\\beta,\\sigma^2 \\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha).\n\\]\nTherefore:\n\\[\n\\frac{\\boldsymbol\\alpha'\\mathbf{b} - \\boldsymbol\\alpha'\\boldsymbol\\beta}{\\sqrt{\\sigma^2 \\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha}} | \\mathbf{X} \\sim \\mathcal{N}(0,1).\n\\]\nUsing approach one used derive Eq. (2.10), one can show Props. 2.7 2.8 also imply :\n\\[\\begin{equation}\n\\boxed{\\frac{\\boldsymbol\\alpha'\\mathbf{b} - \\boldsymbol\\alpha'\\boldsymbol\\beta}{\\sqrt{s^2\\boldsymbol\\alpha'(\\mathbf{X}'\\mathbf{X})^{-1}\\boldsymbol\\alpha}} \\sim t(n-K).}\\tag{2.11}\n\\end{equation}\\]\nFigure 2.4: higher degree freedom, closer distribution \\(t(\\nu)\\) gets normal distribution. (Convergence distribution.)\nprecedes widely exploited statistical inference context linear regressions. Indeed, Eq. (2.10) gives sense distances \\(b_k\\) \\(\\beta_k\\) can deemed “likely”. instance, implies , \\(\\sqrt{v_k s^2}\\) equal 1 (say), probability obtain \\(b_k\\) smaller \\(\\beta_k-\\) 4.587 \\(\\times \\sqrt{v_k s^2}\\) larger \\(\\beta_k+\\) 4.587 \\(\\times \\sqrt{v_k s^2}\\) equal 0.1% \\(n-K=10\\).means instance , assumption \\(\\beta_k=0\\), extremely unlikely obtained \\(b_k/\\sqrt{v_k s^2}\\) smaller 4.587 larger 4.587. generally, shows t-statistic, .e., ratio \\(b_k/\\sqrt{v_k s^2}\\), test statistic associated null hypothesis:\n\\[\nH_0: \\beta_k=0.\n\\]\nnull hypothesis, test statistic follows Student-t distribution \\(n-K\\) degrees freedom. t-statistic therefore particular importance, , result, routinely reported regression outputs. illustrated , regression aims determining covariates households’ income. example makes use data Swiss Household Panel (SHP). edyear19 number years education age19 age respondent, 2019.last two columns give t-statistic p-values associated t-test, whose critical region test size \\(\\alpha\\) :\n\\[\n\\left]-\\infty,-\\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\right] \\cup \\left[\\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right),+\\infty\\right[.\n\\]\nrecall p-value defined probability \\(|Z| > |t|\\), \\(t\\) (computed) t statistics \\(Z \\sim t(n-K)\\). , p-value given \\(2(1 - \\Phi_{t(n-K)}(|t_k|))\\). See webpage details regarding link critical regions, p-value, test outcomes.Now, suppose want compute (symmetrical) confidence interval \\([I_{d,1-\\alpha},I_{u,1-\\alpha}]\\) \\(\\mathbb{P}(\\beta_k \\[I_{d,1-\\alpha},I_{u,1-\\alpha}])=1-\\alpha\\). particular, want : \\(\\mathbb{P}(\\beta_k < I_{d,1-\\alpha})=\\frac{\\alpha}{2}\\).purpose, make use Eq. (2.10), .e., \\(t_k = \\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} \\sim t(n-K)\\). :\n\\[\\begin{eqnarray*}\n\\mathbb{P}(\\beta_k < I_{d,1-\\alpha})=\\frac{\\alpha}{2} &\\Leftrightarrow& \\\\\n\\mathbb{P}\\left(\\frac{b_k - \\beta_k}{\\sqrt{s^2v_k}} > \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} &\\Leftrightarrow& \\mathbb{P}\\left(t_k > \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} \\Leftrightarrow\\\\\n1 - \\mathbb{P}\\left(t_k \\le \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}}\\right)=\\frac{\\alpha}{2} &\\Leftrightarrow& \\frac{b_k - I_{d,1-\\alpha}}{\\sqrt{s^2v_k}} = \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right),\n\\end{eqnarray*}\\]\n\\(\\Phi_{t(n-K)}(\\alpha)\\) c.d.f. \\(t(n-K)\\) distribution (Table 3.2).\\(I_{u,1-\\alpha}\\), obtain:\n\\[\\begin{eqnarray*}\n&&[I_{d,1-\\alpha},I_{u,1-\\alpha}] =\\\\\n&&\\left[b_k - \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\sqrt{s^2v_k},b_k + \\Phi^{-1}_{t(n-K)}\\left(1-\\frac{\\alpha}{2}\\right)\\sqrt{s^2v_k}\\right].\n\\end{eqnarray*}\\]Using results previous regression, compute lower upper bounds 95/% confidence intervals estimated parameters follows:","code":"\nlibrary(AEC)\nlibrary(sandwich)\nshp$income <- shp$i19ptotn/1000\nshp$female <- 1*(shp$sex19==2)\neq <- lm(income ~ edyear19 + age19 + I(age19^2) + female,data=shp)\nn <- length(eq$residuals); K <- length(eq$coefficients)\nlower.b <- eq$coefficients - pt(.025,df=n-K)*sqrt(diag(vcov(eq)))\nupper.b <- eq$coefficients + pt(.025,df=n-K)*sqrt(diag(vcov(eq)))\ncbind(lower.b,upper.b)##                 lower.b      upper.b\n## (Intercept) -74.8848532 -69.06276141\n## edyear19      4.7334839   4.95504836\n## age19         3.1272532   3.34998983\n## I(age19^2)   -0.0300164  -0.02788323\n## female      -32.5523381 -31.06546311"},{"path":"linear-regressions.html","id":"Ftest","chapter":"2 Linear Regressions","heading":"2.2.5 Testing a set of linear restrictions","text":"sometimes want test set restrictions jointly consistent data hand. Let us formalize set (\\(J\\)) linear restrictions:\n\\[\\begin{equation}\\label{eq:restrictions}\n\\begin{array}{ccc}\nr_{1,1} \\beta_1 + \\dots + r_{1,K} \\beta_K &=& q_1\\\\\n\\vdots && \\vdots\\\\\nr_{J,1} \\beta_1 + \\dots + r_{J,K} \\beta_K &=& q_J.\n\\end{array}\n\\end{equation}\\]\nmatrix form, get:\n\\[\\begin{equation}\n\\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}.\n\\end{equation}\\]Define Discrepancy vector \\(\\mathbf{m} = \\mathbf{R}\\mathbf{b} - \\mathbf{q}\\). null hypothesis:\n\\[\\begin{eqnarray*}\n\\mathbb{E}(\\mathbf{m}|\\mathbf{X}) &=& \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} = 0 \\quad \\mbox{} \\\\\n\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X}) &=& \\mathbf{R} \\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\mathbf{R}'.\n\\end{eqnarray*}\\]notations, assumption test :\n\\[\\begin{equation}\n\\boxed{H_0: \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} = 0 \\mbox{ } H_1: \\mathbf{R}\\boldsymbol\\beta - \\mathbf{q} \\ne 0.}\\tag{2.12}\n\\end{equation}\\]Hypotheses 2.1 2.4, \\(\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X}) = \\sigma^2 \\mathbf{R} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\) (see Prop. 2.3). add normality assumption (Hypothesis 2.5), :\n\\[\\begin{equation}\nW = \\mathbf{m}'\\mathbb{V}ar(\\mathbf{m}|\\mathbf{X})^{-1}\\mathbf{m} \\sim \\chi^2(J). \\tag{2.13}\n\\end{equation}\\]\\(\\sigma^2\\) known, conduct Wald test. case practice compute \\(W\\). can, however, approximate replacing \\(\\sigma^2\\) \\(s^2\\). distribution new statistic \\(\\chi^2(J)\\) ; \\(\\mathcal{F}\\) distribution (whose quantiles shown Table 3.4), test called \\(F\\) test.Proposition 2.9  Hypotheses 2.1 2.5 Eq. (2.12) holds, :\n\\[\\begin{equation}\nF = \\frac{W}{J}\\frac{\\sigma^2}{s^2} = \\frac{\\mathbf{m}'(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\mathbf{m}}{s^2J} \\sim \\mathcal{F}(J,n-K),\\tag{2.14}\n\\end{equation}\\]\n\\(\\mathcal{F}\\) distribution F-statistic.Proof. According Eq. (2.13), \\(W/J \\sim \\chi^2(J)/J\\). Moreover, denominator (\\(s^2/\\sigma^2\\)) \\(\\sim \\chi^2(n-K)\\). Therefore, \\(F\\) ratio r.v. distributed \\(\\chi^2(J)/J\\) another distributed \\(\\chi^2(n-K)/(n-K)\\). remains verify r.v. independent.\\(H_0\\), \\(\\mathbf{m} = \\mathbf{R}(\\mathbf{b}-\\boldsymbol\\beta) = \\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon\\).\nTherefore \\(\\mathbf{m}'(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\mathbf{m}\\) form \\(\\boldsymbol\\varepsilon'\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{T}=\\mathbf{D}'\\mathbf{C}\\mathbf{D}\\) \\(\\mathbf{D}=\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) \\(\\mathbf{C}=(\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}')^{-1}\\). Hypotheses 2.1 2.4, covariance \\(\\mathbf{T}\\boldsymbol\\varepsilon\\) \\(\\mathbf{M}\\boldsymbol\\varepsilon\\) \\(\\sigma^2\\mathbf{T}\\mathbf{M} = \\mathbf{0}\\). Therefore, 2.5, variables Gaussian variables 0 covariance. Hence independent.large \\(n-K\\), \\(\\mathcal{F}_{J,n-K}\\) distribution converges \\(\\mathcal{F}_{J,\\infty}=\\chi^2(J)/J\\).following proposition proposes another computation F-statistic, based \\(R^2\\) restricted unrestricted linear models.Proposition 2.10  F-statistic defined Eq. (2.14) also equal :\n\\[\\begin{equation}\nF = \\frac{(R^2-R_*^2)/J}{(1-R^2)/(n-K)} =  \\frac{(SSR_{restr}-SSR_{unrestr})/J}{SSR_{unrestr}/(n-K)},\\tag{2.15}\n\\end{equation}\\]\n\\(R_*^2\\) coef. determination (Eq. (2.5)) “restricted regression” (SSR: sum squared residuals.)Proof. Let’s denote \\(\\mathbf{e}_*=\\mathbf{y}-\\mathbf{X}\\mathbf{b}_*\\) vector residuals associated restricted regression (.e. \\(\\mathbf{R}\\mathbf{b}_*=\\mathbf{q}\\)).\n\\(\\mathbf{e}_*=\\mathbf{e} - \\mathbf{X}(\\mathbf{b}_*-\\mathbf{b})\\). Using \\(\\mathbf{e}'\\mathbf{X}=0\\), get \\(\\mathbf{e}_*'\\mathbf{e}_*=\\mathbf{e}'\\mathbf{e} + (\\mathbf{b}_*-\\mathbf{b})'\\mathbf{X}'\\mathbf{X}(\\mathbf{b}_*-\\mathbf{b}) \\ge \\mathbf{e}'\\mathbf{e}\\).Proposition 3.5 (Appendix ??), : \\(\\mathbf{b}_*-\\mathbf{b}=-(\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\mathbf{b} - \\mathbf{q})\\). Therefore:\n\\[\n\\mathbf{e}_*'\\mathbf{e}_* - \\mathbf{e}'\\mathbf{e} = (\\mathbf{R}\\mathbf{b} - \\mathbf{q})'[\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}']^{-1}(\\mathbf{R}\\mathbf{b} - \\mathbf{q}).\n\\]\nimplies F statistic defined Prop. 2.9 also equal :\n\\[\n\\frac{(\\mathbf{e}_*'\\mathbf{e}_* - \\mathbf{e}'\\mathbf{e})/J}{\\mathbf{e}'\\mathbf{e}/(n-K)},\n\\]\nleads result.null hypothesis \\(H_0\\) (Eq. (2.12)) F-test rejected \\(F\\) –defined Eq. (2.14) (2.15)– higher \\(\\mathcal{F}_{1-\\alpha}(J,n-K)\\). (Hence, test one-sided test.)","code":""},{"path":"linear-regressions.html","id":"largeSample","chapter":"2 Linear Regressions","heading":"2.2.6 Large Sample Properties","text":"Even relax normality assumption (Hypothesis 2.5), can approximate finite-sample behavior estimators using large-sample asymptotic properties.begin , proceed Hypothesis 2.1 2.4. (see, later , deal –partial– relaxations Hypothesis 2.3 2.4.)regularity assumptions, Hypotheses 2.1 2.4, even residuals normally-distributed, least square estimators can asymptotically normal inference can performed small samples Hypotheses 2.1 2.5 hold. derives Prop. 2.11 (). F-test (Prop. 2.10) t-test (Eq. (2.10)) can performed.Proposition 2.11  Assumptions 2.1 2.4, assuming :\n\\[\\begin{equation}\nQ = \\mbox{plim}_{n \\rightarrow \\infty} \\frac{\\mathbf{X}'\\mathbf{X}}{n},\\tag{2.16}\n\\end{equation}\\]\n\\((\\mathbf{x}_i,\\varepsilon_i)\\)s independent (across entities \\(\\)), :\n\\[\\begin{equation}\n\\sqrt{n}(\\mathbf{b} - \\boldsymbol\\beta)\\overset{d} {\\rightarrow} \\mathcal{N}\\left(0,\\sigma^2Q^{-1}\\right).\\tag{2.17}\n\\end{equation}\\]Proof. Since \\(\\mathbf{b} = \\boldsymbol\\beta + \\left( \\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1}\\left(\\frac{\\mathbf{X}'\\boldsymbol\\varepsilon}{n}\\right)\\), : \\(\\sqrt{n}(\\mathbf{b} - \\boldsymbol\\beta) = \\left( \\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1} \\left(\\frac{1}{\\sqrt{n}}\\right)\\mathbf{X}'\\boldsymbol\\varepsilon\\). Since \\(f:\\rightarrow ^{-1}\\) continuous function (\\(\\ne \\mathbf{0}\\)), \\(\\mbox{plim}_{n \\rightarrow \\infty} \\left(\\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1} = \\mathbf{Q}^{-1}\\) (see Prop. 3.7). Let us denote \\(V_i\\) vector \\(\\mathbf{x}_i \\varepsilon_i\\). \\((\\mathbf{x}_i,\\varepsilon_i)\\)s independent, \\(V_i\\)s independent well. covariance matrix \\(\\sigma^2\\mathbb{E}(\\mathbf{x}_i \\mathbf{x}_i')=\\sigma^2Q\\). Applying multivariate central limit theorem vectors \\(V_i\\) gives \\(\\sqrt{n}\\left(\\frac{1}{n}\\sum_{=1}^n \\mathbf{x}_i \\varepsilon_i\\right) = \\left(\\frac{1}{\\sqrt{n}}\\right)\\mathbf{X}'\\boldsymbol\\varepsilon \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2Q)\\). application Slutsky’s theorem (Prop. 3.7) leads results.practice, \\(\\sigma^2\\) estimated \\(\\frac{\\mathbf{e}'\\mathbf{e}}{n-K}\\) (Eq. (2.9)) \\(\\mathbf{Q}^{-1}\\) \\(\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{n}\\right)^{-1}\\). , covariance matrix estimator approximated :\n\\[\\begin{equation}\n\\widehat{\\mathbb{V}ar}(\\mathbf{b}) = s^2 (\\mathbf{X}'\\mathbf{X})^{-1}.\\tag{2.18}\n\\end{equation}\\]Eqs. (2.16) (2.17) respectively correspond convergences probability distribution (see Definitions 3.9 3.12, respectively).","code":""},{"path":"linear-regressions.html","id":"CommonPitfalls","chapter":"2 Linear Regressions","heading":"2.3 Common pitfalls in linear regressions","text":"","code":""},{"path":"linear-regressions.html","id":"multicollinearity","chapter":"2 Linear Regressions","heading":"2.3.1 Multicollinearity","text":"Consider model: \\(y_i = \\beta_1 x_{,1} + \\beta_2 x_{,2} + \\varepsilon_i\\), variables zero-mean \\(\\mathbb{V}ar(\\varepsilon_i)=\\sigma^2\\). \n\\[\n\\mathbf{X}'\\mathbf{X} = \\left[ \\begin{array}{cc}\n\\sum_i x_{,1}^2 & \\sum_i x_{,1} x_{,2} \\\\\n\\sum_i x_{,1} x_{,2} & \\sum_i x_{,2}^2\n\\end{array}\\right],\n\\]\ntherefore:\n\\[\\begin{eqnarray*}\n(\\mathbf{X}'\\mathbf{X})^{-1} &=& \\frac{1}{\\sum_i x_{,1}^2\\sum_i x_{,2}^2 - (\\sum_i x_{,1} x_{,2})^2} \\left[ \\begin{array}{cc}\n\\sum_i x_{,2}^2 & -\\sum_i x_{,1} x_{,2} \\\\\n-\\sum_i x_{,1} x_{,2} & \\sum_i x_{,1}^2\n\\end{array}\\right].\n\\end{eqnarray*}\\]\ninverse upper-left parameter \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) :\n\\[\\begin{equation}\n\\sum_i x_{,1}^2 - \\frac{(\\sum_i x_{,1} x_{,2})^2}{\\sum_i x_{,2}^2} = \\sum_i x_{,1}^2(1 - correl_{1,2}^2),\\tag{2.19}\n\\end{equation}\\]\n\\(correl_{1,2}\\) sample correlation \\(\\mathbf{x}_{1}\\) \\(\\mathbf{x}_{2}\\).Hence, closer one \\(correl_{1,2}\\), higher variance \\(b_1\\) (recall variance \\(b_1\\) upper-left component \\(\\sigma^2(\\mathbf{X}'\\mathbf{X})^{-1}\\)).","code":""},{"path":"linear-regressions.html","id":"omitted-variables","chapter":"2 Linear Regressions","heading":"2.3.2 Omitted variables","text":"Consider following model (“True model”):\n\\[\n\\mathbf{y} = \\underbrace{\\mathbf{X}_1}_{n \\times K_1}\\underbrace{\\boldsymbol\\beta_1}_{K_1 \\times 1} + \\underbrace{\\mathbf{X}_2}_{n\\times K_2}\\underbrace{\\boldsymbol\\beta_2}_{K_2 \\times 1} + \\boldsymbol\\varepsilon\n\\]\none computes \\(\\mathbf{b}_1\\) regressing \\(\\mathbf{y}\\) \\(\\mathbf{X}_1\\) , one gets:\n\\[\n\\mathbf{b}_1 = (\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\mathbf{y} = \\boldsymbol\\beta_1 + (\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\mathbf{X}_2\\boldsymbol\\beta_2 +\n(\\mathbf{X}_1'\\mathbf{X}_1)^{-1}\\mathbf{X}_1'\\boldsymbol\\varepsilon.\n\\]results omitted-variable formula:\n\\[\n\\boxed{\\mathbb{E}(\\mathbf{b}_1|\\mathbf{X}) = \\boldsymbol\\beta_1 + \\underbrace{(\\mathbf{X}_1'\\mathbf{X}_1)^{-1}(\\mathbf{X}_1'\\mathbf{X}_2)}_{K_1 \\times K_2}\\boldsymbol\\beta_2}\n\\]\n(column \\((\\mathbf{X}_1'\\mathbf{X}_1)^{-1}(\\mathbf{X}_1'\\mathbf{X}_2)\\) OLS regressors obtained regressing columns \\(\\mathbf{X}_2\\) \\(\\mathbf{X}_1\\)). Unless variables included \\(\\mathbf{X}_1\\) orthogonal \\(\\mathbf{X}_2\\), obtain bias.Example 2.2  Let us use California Test Score dataset (package AER). Assume want measure effect students--teacher ratio (str) student test scores (testscr). following regressions show effect lower controls added.","code":"\nlibrary(AER); data(\"CASchools\")\nCASchools$str <- CASchools$students/CASchools$teachers\nCASchools$testscr <- .5 * (CASchools$math + CASchools$read)\neq1 <- lm(testscr~str,data=CASchools)\neq2 <- lm(testscr~str+lunch,data=CASchools)\neq3 <- lm(testscr~str+lunch+english,data=CASchools)\nstargazer::stargazer(eq1,eq2,eq3,type=\"text\",no.space = TRUE)## \n## =============================================================================================\n##                                                Dependent variable:                           \n##                     -------------------------------------------------------------------------\n##                                                      testscr                                 \n##                               (1)                     (2)                      (3)           \n## ---------------------------------------------------------------------------------------------\n## str                        -2.280***               -1.117***                -0.998***        \n##                             (0.480)                 (0.240)                  (0.239)         \n## lunch                                              -0.600***                -0.547***        \n##                                                     (0.017)                  (0.022)         \n## english                                                                     -0.122***        \n##                                                                              (0.032)         \n## Constant                  698.933***               702.911***               700.150***       \n##                             (9.467)                 (4.700)                  (4.686)         \n## ---------------------------------------------------------------------------------------------\n## Observations                  420                     420                      420           \n## R2                           0.051                   0.767                    0.775          \n## Adjusted R2                  0.049                   0.766                    0.773          \n## Residual Std. Error    18.581 (df = 418)        9.222 (df = 417)         9.080 (df = 416)    \n## F Statistic         22.575*** (df = 1; 418) 685.756*** (df = 2; 417) 476.306*** (df = 3; 416)\n## =============================================================================================\n## Note:                                                             *p<0.1; **p<0.05; ***p<0.01"},{"path":"linear-regressions.html","id":"irrelevant","chapter":"2 Linear Regressions","heading":"2.3.3 Irrelevant variable","text":"Consider True model:\n\\[\n\\mathbf{y} = \\mathbf{X}_1\\boldsymbol\\beta_1 + \\boldsymbol\\varepsilon,\n\\]\nEstimated model :\n\\[\n\\mathbf{y} = \\mathbf{X}_1\\boldsymbol\\beta_1 + \\mathbf{X}_2\\boldsymbol\\beta_2 + \\boldsymbol\\varepsilon\n\\]estimates unbiased. However, adding irrelevant explanatory variables increases variance estimate \\(\\boldsymbol\\beta_1\\) (compared case one uses correct explanatory variables). case unless correlation \\(\\mathbf{X}_1\\) \\(\\mathbf{X}_2\\) null, see Eq. (2.19).words, estimator inefficient, .e., exists alternative consistent estimator whose variance lower. inefficiency problem can serious consequences testing hypotheses type \\(H_0: \\beta_1 = 0\\) due loss power, might infer relevant variables truly (Type-II error; False Negative).","code":""},{"path":"linear-regressions.html","id":"IV","chapter":"2 Linear Regressions","heading":"2.4 Instrumental Variables","text":"Nice interpretation testsThe conditional mean zero assumption (Hypothesis 2.2), according \\(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{X})=0\\) —implies particular \\(\\mathbf{x}_i\\) \\(\\varepsilon_i\\) uncorrelated— sometimes consistent considered economic framework. case, parameters interest may still estimated consistently resorting instrumental variable techniques.Consider following model:\n\\[\\begin{equation}\ny_i = \\mathbf{x_i}'\\boldsymbol\\beta + \\varepsilon_i, \\quad \\mbox{} \\mathbb{E}(\\varepsilon_i)=0  \\mbox{ } \\mathbf{x_i}\\\\perp \\varepsilon_i.\\tag{2.20}\n\\end{equation}\\]Let us illustrate situation may result biased OLS estimate. Consider instance situation :\n\\[\\begin{equation}\n\\mathbb{E}(\\varepsilon_i)=0 \\quad \\mbox{} \\quad \\mathbb{E}(\\varepsilon_i \\mathbf{x_i})=\\boldsymbol\\gamma,\\tag{2.21}\n\\end{equation}\\]\ncase \\(\\mathbf{x}_i\\\\perp \\varepsilon_i\\) (consistently Eq. (2.20)).law large numbers, \\(\\mbox{plim}_{n \\rightarrow \\infty} \\mathbf{X}'\\boldsymbol\\varepsilon / n = \\boldsymbol\\gamma\\). \\(\\mathbf{Q}_{xx} := \\mbox{plim } \\mathbf{X}'\\mathbf{X}/n\\), OLS estimator consistent \n\\[\n\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon \\overset{p}{\\rightarrow} \\boldsymbol\\beta + \\mathbf{Q}_{xx}^{-1}\\boldsymbol\\gamma \\ne \\boldsymbol\\beta.\n\\]Let us now introduce notion instruments.Definition 2.2  (Instrumental variables) \\(L\\)-dimensional random variable \\(\\mathbf{z}_i\\) valid set instruments :\\(\\mathbf{z}_i\\) correlated \\(\\mathbf{x}_i\\);\\(\\mathbb{E}(\\boldsymbol\\varepsilon|\\mathbf{Z})=0\\) andthe orthogonal projections \\(\\mathbf{x}_i\\)s \\(\\mathbf{z}_i\\)s multicollinear.\\(\\mathbf{z}_i\\) valid set instruments, :\n\\[\n\\mbox{plim}\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right) =\\mbox{plim}\\left( \\frac{\\mathbf{Z}'(\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon)}{n} \\right) = \\mbox{plim}\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\boldsymbol\\beta\n\\]\nIndeed, law large numbers, \\(\\frac{\\mathbf{Z}'\\boldsymbol\\varepsilon}{n} \\overset{p}{\\rightarrow}\\mathbb{E}(\\mathbf{z}_i\\varepsilon_i)=0\\).\\(L = K\\), matrix \\(\\frac{\\mathbf{Z}'\\mathbf{X}}{n}\\) dimension \\(K \\times K\\) :\n\\[\n\\left[\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\right]^{-1}\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right) = \\boldsymbol\\beta.\n\\]\ncontinuity inverse funct.: \\(\\left[\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)\\right]^{-1}=\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1}\\).\nSlutsky Theorem (Prop. 3.7) implies :\n\\[\n\\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1} \\mbox{plim }\\left( \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right)  = \\mbox{plim }\\left( \\left( \\frac{\\mathbf{Z}'\\mathbf{X}}{n} \\right)^{-1} \\frac{\\mathbf{Z}'\\mathbf{y}}{n} \\right).\n\\]\nHence \\(\\mathbf{b}_{iv}\\) consistent defined :\n\\[\n\\boxed{\\mathbf{b}_{iv} = (\\mathbf{Z}'\\mathbf{X})^{-1}\\mathbf{Z}'\\mathbf{y}.}\n\\]Proposition 2.12  (Asymptotic distribution IV estimator) \\(\\mathbf{z}_i\\) \\(L\\)-dimensional random variable constitutes valid set instruments (see Def. 2.2) \\(L=K\\), asymptotic distribution \\(\\mathbf{b}_{iv}\\) :\n\\[\n\\mathbf{b}_{iv} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(\\boldsymbol\\beta,\\frac{\\sigma^2}{n}\\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\\right]^{-1}\\right)\n\\]\n\\(\\mbox{plim } \\mathbf{Z}'\\mathbf{Z}/n =: \\mathbf{Q}_{zz}\\), \\(\\mbox{plim } \\mathbf{Z}'\\mathbf{X}/n =: \\mathbf{Q}_{zx}\\), \\(\\mbox{plim } \\mathbf{X}'\\mathbf{Z}/n =: \\mathbf{Q}_{xz}\\).Proof. proof similar Prop. 2.11, starting point \\(\\mathbf{b}_{iv} = \\boldsymbol\\beta + (\\mathbf{Z}'\\mathbf{X})^{-1}\\mathbf{Z}'\\boldsymbol\\varepsilon\\).\\(L=K\\), :\n\\[\n\\left[Q_{xz}Q_{zz}^{-1}Q_{zx}\\right]^{-1}=Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}\n\\]\npractice, estimate \\(\\mathbb{V}ar(\\mathbf{b}_{iv}) = \\frac{\\sigma^2}{n}Q_{zx}^{-1}Q_{zz}Q_{xz}^{-1}\\), replace \\(\\sigma^2\\) :\n\\[\ns_{iv}^2 = \\frac{1}{n}\\sum_{=1}^{n} (y_i - \\mathbf{x}_i'\\mathbf{b}_{iv})^2.\n\\]\\(L > K\\)? case, proceed follows:Regress \\(\\mathbf{X}\\) space spanned \\(\\mathbf{Z}\\) andRegress \\(\\mathbf{y}\\) fitted values \\(\\hat{\\mathbf{X}}:=\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}\\).results :\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{iv} = [\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{Y}.} \\tag{2.22}\n\\end{equation}\\]case, Prop. 2.12 still holds, \\(\\mathbf{b}_{iv}\\) given Eq. (2.22).\\(\\mathbf{b}_{iv}\\) also result regression \\(\\mathbf{y}\\) \\(\\mathbf{X}^*\\), columns \\(\\mathbf{X}^*\\) (othogonal) projections \\(\\mathbf{X}\\) \\(\\mathbf{Z}\\), .e. \\(\\mathbf{X}^* = \\mathbf{P^{Z}X}\\) (using notations introduced Eq. (2.4)). Hence names estimator: Two-Stage Least Squares (TSLS).instruments properly satisfy Condition () Def. 2.2 (.e. \\(\\mathbf{x}_i\\) \\(\\mathbf{z}_i\\) loosely related), instruments said weak (see, e.g., J. H. Stock Yogo (2005), available Andrews, Stock, Sun (2019)). simple standard way test weak instruments consist looking F-statistic associated first stage estimation. easier reject null hypothesis (large test statistic), less weak instruments.Durbin-Wu-Hausman test (Durbin (1954), Wu (1973), Hausman (1978)) can used test IV necessary. IV techniques required \\(\\mbox{plim}_{n \\rightarrow \\infty} \\mathbf{X}'\\boldsymbol\\varepsilon / n \\ne 0\\). Hausman (1978) proposes test efficiency estimators. null hypothesis two estimators, \\(\\mathbf{b}_0\\) \\(\\mathbf{b}_1\\), consistent \\(\\mathbf{b}_0\\) (asymptotically) efficient relative \\(\\mathbf{b}_1\\). alternative hypothesis, \\(\\mathbf{b}_1\\) (IV present case) remains consistent \\(\\mathbf{b}_0\\) (OLS present case). , reject null hypothesis, means OLS estimator consistent, potentially due endogeneity issue.test statistic :\n\\[\nH = (\\mathbf{b}_1 - \\mathbf{b}_0)' MPI(\\mathbb{V}ar(\\mathbf{b}_1) - \\mathbb{V}ar(\\mathbf{b}_0))(\\mathbf{b}_1 - \\mathbf{b}_0),\n\\]\n\\(MPI\\) Moore-Penrose pseudo-inverse. null hypothesis, \\(H \\sim \\chi^2(q)\\), \\(q\\) rank \\(\\mathbb{V}ar(\\mathbf{b}_1) - \\mathbb{V}ar(\\mathbf{b}_0)\\).Example 2.3  (Estimation price elasticity) See e.g. estimation tobacco price elasticity demand.want estimate effect demand exogenous increase prices cigarettes (say).model :\n\\[\\begin{eqnarray*}\n\\underbrace{q^d_t}_{\\mbox{log(demand)}} &=& \\alpha_0 + \\alpha_1 \\underbrace{\\times p_t}_{\\mbox{log(price)}} + \\alpha_2 \\underbrace{\\times w_t}_{\\mbox{income}} + \\varepsilon_t^d\\\\\n\\underbrace{q^s_t}_{\\mbox{log(supply)}} &=& \\gamma_0 + \\gamma_1 \\times p_t + \\gamma_2 \\underbrace{\\times \\mathbf{y}_t}_{\\mbox{cost factors}} + \\varepsilon_t^s,\n\\end{eqnarray*}\\]\n\\(\\mathbf{y}_t\\), \\(w_t\\), \\(\\varepsilon_t^s \\sim \\mathcal{N}(0,\\sigma^2_s)\\) \\(\\varepsilon_t^d \\sim \\mathcal{N}(0,\\sigma^2_d)\\) independent.Equilibrium: \\(q^d_t = q^s_t\\). implies prices endogenous:\n\\[\np_t = \\frac{\\alpha_0 + \\alpha_2 w_t + \\varepsilon_t^d - \\gamma_0 - \\gamma_2 \\mathbf{y}_t - \\varepsilon_t^s}{\\gamma_1 - \\alpha_1}.\n\\]\nparticular \\(\\mathbb{E}(p_t \\varepsilon_t^d) = \\frac{\\sigma^2_d}{\\gamma_1 - \\alpha_1} \\ne 0\\) \\(\\Rightarrow\\) Regressing OLS \\(q_t^d\\) \\(p_t\\) gives biased estimates (see Eq. (2.21)).\nFigure 2.5: figure illustrates situation prevailing estimating price-elasticity (price endogenous).\nLet us use IV regressions estimate price elasticity cigarette demand. purpose, use CigarettesSW dataset package AER (data used J. Stock Watson (2003)). panel dataset documents cigarette consumption 48 continental US States 1985–1995. instrument real tax cigarettes arising state’s general sales tax. rationale larger general sales tax drives cigarette prices , general tax determined forces affecting \\(\\varepsilon_t^d\\).Example 2.4  (Education wage) example, make use another dataset proposed J. Stock Watson (2003), namely CollegeDistance dataset.2 objective estimate effect education wages. Education choice suspected endogenous variable, calls IV strategy. instrumental variable distance college.","code":"\ndata(\"CigarettesSW\", package = \"AER\")\nCigarettesSW$rprice  <- with(CigarettesSW, price/cpi)\nCigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)\nCigarettesSW$tdiff   <- with(CigarettesSW, (taxs - tax)/cpi)\n\n## model \neq.IV1 <- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + tdiff + I(tax/cpi),\n                data = CigarettesSW, subset = year == \"1995\")\neq.IV2 <- ivreg(log(packs) ~ log(rprice) | tdiff,\n                data = CigarettesSW, subset = year == \"1995\")\neq.no.IV <- lm(log(packs) ~ log(rprice) + log(rincome),\n               data = CigarettesSW, subset = year == \"1995\")\nstargazer::stargazer(eq.no.IV,eq.IV1,eq.IV2,type=\"text\",no.space = TRUE)## \n## ==========================================================================\n##                                      Dependent variable:                  \n##                     ------------------------------------------------------\n##                                           log(packs)                      \n##                              OLS                    instrumental          \n##                                                       variable            \n##                              (1)                 (2)             (3)      \n## --------------------------------------------------------------------------\n## log(rprice)               -1.407***           -1.277***       -1.084***   \n##                            (0.251)             (0.263)         (0.317)    \n## log(rincome)                0.344               0.280                     \n##                            (0.235)             (0.239)                    \n## Constant                  10.342***           9.895***        9.720***    \n##                            (1.023)             (1.059)         (1.514)    \n## --------------------------------------------------------------------------\n## Observations                  48                 48              48       \n## R2                          0.433               0.429           0.401     \n## Adjusted R2                 0.408               0.404           0.388     \n## Residual Std. Error    0.187 (df = 45)     0.188 (df = 45) 0.190 (df = 46)\n## F Statistic         17.165*** (df = 2; 45)                                \n## ==========================================================================\n## Note:                                          *p<0.1; **p<0.05; ***p<0.01\nsummary(eq.IV1,diagnostics = TRUE)$diagnostics##                  df1 df2   statistic      p-value\n## Weak instruments   2  44 244.7337536 1.444054e-24\n## Wu-Hausman         1  44   3.0678163 8.682505e-02\n## Sargan             1  NA   0.3326221 5.641191e-01\nlibrary(sem)\ndata(\"CollegeDistance\", package = \"AER\")\neq.1st.stage <- lm(education ~ urban + gender + ethnicity + unemp + distance,\n                   data = CollegeDistance)\nCollegeDistance$ed.pred<- predict(eq.1st.stage)\neq.2nd.stage <- lm(wage ~ urban + gender + ethnicity + unemp + ed.pred ,\n                   data = CollegeDistance)\neqOLS <- lm(wage ~ urban + gender + ethnicity + unemp + education,\n            data=CollegeDistance)\neqTSLS <- ivreg(wage ~ urban + gender + ethnicity + unemp + education|\n                  urban + gender + ethnicity + unemp + distance,\n                data=CollegeDistance)\nstargazer::stargazer(eq.1st.stage,eq.2nd.stage,eqTSLS,eqOLS,type=\"text\",no.space = TRUE)## \n## ===========================================================================\n##                                             Dependent variable:            \n##                                 -------------------------------------------\n##                                 education               wage               \n##                                    OLS       OLS     instrumental    OLS   \n##                                                        variable            \n##                                    (1)       (2)         (3)         (4)   \n## ---------------------------------------------------------------------------\n## urbanyes                         -0.092     0.046       0.046       0.070  \n##                                  (0.065)   (0.045)     (0.060)     (0.045) \n## genderfemale                     -0.025    -0.071*      -0.071    -0.085** \n##                                  (0.052)   (0.037)     (0.050)     (0.037) \n## ethnicityafam                   -0.524*** -0.227***    -0.227**   -0.556***\n##                                  (0.072)   (0.073)     (0.099)     (0.052) \n## ethnicityhispanic               -0.275*** -0.351***   -0.351***   -0.544***\n##                                  (0.068)   (0.057)     (0.077)     (0.049) \n## unemp                             0.010    0.139***    0.139***   0.133*** \n##                                  (0.010)   (0.007)     (0.009)     (0.007) \n## distance                        -0.087***                                  \n##                                  (0.012)                                   \n## ed.pred                                    0.647***                        \n##                                            (0.101)                         \n## education                                              0.647***     0.005  \n##                                                        (0.136)     (0.010) \n## Constant                        14.061***   -0.359      -0.359    8.641*** \n##                                  (0.083)   (1.412)     (1.908)     (0.157) \n## ---------------------------------------------------------------------------\n## Observations                      4,739     4,739       4,739       4,739  \n## R2                                0.023     0.117       -0.612      0.110  \n## Adjusted R2                       0.022     0.116       -0.614      0.109  \n## Residual Std. Error (df = 4732)   1.770     1.263       1.706       1.268  \n## F Statistic (df = 6; 4732)      18.552*** 104.971***              97.274***\n## ===========================================================================\n## Note:                                           *p<0.1; **p<0.05; ***p<0.01"},{"path":"linear-regressions.html","id":"general-regression-model-grm-and-robust-covariance-matrices","chapter":"2 Linear Regressions","heading":"2.5 General Regression Model (GRM) and robust covariance matrices","text":"statistical inference presented relies strong assumptions regarding stochastic properties errors. Namely, assumed mutually uncorrelated (Hypothesis @ref(hyp:noncorrel_resid)) homoskedastic (Hypothesis 2.3.objective section present approaches aimed adjusting estimate covariance matrix OLS estimator (\\((\\mathbf{X}'\\mathbf{X})^{-1}s^2\\), see Eq. (2.18)), previous hypotheses hold.","code":""},{"path":"linear-regressions.html","id":"presentation-of-the-general-regression-model-grm","chapter":"2 Linear Regressions","heading":"2.5.1 Presentation of the General Regression Model (GRM)","text":"prove useful introduce following notation:\n\\[\\begin{eqnarray}\n\\mathbb{V}ar(\\boldsymbol\\varepsilon | \\mathbf{X}) = \\mathbb{E}(\\boldsymbol\\varepsilon \\boldsymbol\\varepsilon'| \\mathbf{X}) &=& \\boldsymbol\\Sigma. \\tag{2.23}\n\\end{eqnarray}\\]Note Eq. ((2.23)) general Hypothesis 2.3 @ref(hyp:noncorrel_resid) diagonal entries \\(\\boldsymbol\\Sigma\\) may different (opposed Hypothesis 2.3), non-diagonal entries \\(\\boldsymbol\\Sigma\\) can non-null (opposed Hypothesis 2.4).Definition 2.3  (General Regression Model (GRM)) Hypothesis 2.1 2.2, together Eq. (2.23), form General Regression Model (GRM) framework.Naturally, regression model Hypotheses 2.1 2.4 hold specific case GRM framework.GRM context notably encompasses situations heteroskedasticity autocorrelation:Heteroskedasticity:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]. \\tag{2.24}\n\\end{equation}\\]Heteroskedasticity:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]. \\tag{2.24}\n\\end{equation}\\]Autocorrelation:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc}\n1 & \\rho_{2,1} & \\dots & \\rho_{n,1} \\\\\n\\rho_{2,1} & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho_{n,n-1} \\\\\n\\rho_{n,1} & \\rho_{n,2} & \\dots & 1\n\\end{array} \\right]. \\tag{2.25}\n\\end{equation}\\]Autocorrelation:\n\\[\\begin{equation}\n\\boldsymbol\\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc}\n1 & \\rho_{2,1} & \\dots & \\rho_{n,1} \\\\\n\\rho_{2,1} & 1 &  & \\vdots \\\\\n\\vdots && \\ddots& \\rho_{n,n-1} \\\\\n\\rho_{n,1} & \\rho_{n,2} & \\dots & 1\n\\end{array} \\right]. \\tag{2.25}\n\\end{equation}\\]Example 2.5  (Auto-regressive processes) Autocorrelation , particular, recurrent problem time-series data used (see Section ??).time-series context, subscript \\(\\) refers date. Assume instance :\n\\[\\begin{equation}\ny_i = \\mathbf{x}_i' \\boldsymbol\\beta + \\varepsilon_i \\tag{2.26}\n\\end{equation}\\]\n\n\\[\\begin{equation}\n\\varepsilon_i = \\rho \\varepsilon_{-1} + v_i, \\quad v_i \\sim \\mathcal{N}(0,\\sigma_v^2).\\tag{2.27}\n\\end{equation}\\]\ncase, GRM context, :\n\\(\\boldsymbol\\Sigma\\)— one can determine better (accurate) estimator OLS one. approach called Generalized Least Squares (GLS), present .","code":""},{"path":"linear-regressions.html","id":"GLS","chapter":"2 Linear Regressions","heading":"2.5.2 Generalized Least Squares","text":"Assume \\(\\boldsymbol\\Sigma\\) known (“feasible GLS”). \\(\\boldsymbol\\Sigma\\) symmetric positive, admits spectral decomposition form \\(\\boldsymbol\\Sigma = \\mathbf{C} \\boldsymbol\\Lambda \\mathbf{C}'\\), \\(\\mathbf{C}\\) orthogonal matrix (.e. \\(\\mathbf{C}\\mathbf{C}'=Id\\)) \\(\\boldsymbol\\Lambda\\) diagonal matrix (diagonal entries eigenvalues \\(\\boldsymbol\\Sigma\\)).\\(\\boldsymbol\\Sigma = (\\mathbf{P}\\mathbf{P}')^{-1}\\) \\(\\mathbf{P} = \\mathbf{C}\\boldsymbol\\Lambda^{-1/2}\\). Consider transformed model:\n\\[\n\\mathbf{P}'\\mathbf{y} = \\mathbf{P}'\\mathbf{X}\\boldsymbol\\beta + \\mathbf{P}'\\boldsymbol\\varepsilon \\quad \\mbox{} \\quad \\mathbf{y}^* = \\mathbf{X}^*\\boldsymbol\\beta + \\boldsymbol\\varepsilon^*.\n\\]\nvariance \\(\\boldsymbol\\varepsilon^*\\) \\(\\mathbf{}\\). transformed model, OLS BLUE (Gauss-Markow Theorem 2.1).Generalized least squares estimator \\(\\boldsymbol\\beta\\) :\n\\[\\begin{equation}\n\\boxed{\\mathbf{b}_{GLS} = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{y}}.\\tag{2.29}\n\\end{equation}\\]\n:\n\\[\n\\mathbb{V}ar(\\mathbf{b}_{GLS}|\\mathbf{X}) = (\\mathbf{X}'\\boldsymbol\\Sigma^{-1}\\mathbf{X})^{-1}.\n\\]However, general, \\(\\boldsymbol\\Sigma\\) unknown. GLS estimator said infeasible. structure required. Assume \\(\\boldsymbol\\Sigma\\) admits parametric form \\(\\boldsymbol\\Sigma(\\theta)\\). estimation becomes feasible (FGLS) one replaces \\(\\boldsymbol\\Sigma(\\theta)\\) \\(\\boldsymbol\\Sigma(\\hat\\theta)\\), \\(\\hat\\theta\\) consistent estimator \\(\\theta\\). case, FGLS asymptotically efficient (see Example ??).\\(\\boldsymbol\\Sigma\\) obvious structure: OLS (IV) estimator available. regularity assumptions, remains unbiased, consistent, asymptotically normally distributed, efficient. Standard inference procedures longer appropriate.Example 2.6  (GLS auto-correlation case) Consider case presented Example 2.5. OLS estimate \\(\\mathbf{b}\\) \\(\\boldsymbol\\beta\\) consistent, estimates \\(e_i\\)s \\(\\varepsilon_i\\)s also . Consistent estimators \\(\\rho\\) \\(\\sigma_v\\) obtained regressing \\(e_i\\)s \\(e_{-1}\\)s. Using estimates Eq. (2.28) provides consistent estimate \\(\\boldsymbol\\Sigma\\). Applying steps recursively gives efficient estimator \\(\\boldsymbol\\beta\\) (Cochrane Orcutt (1949)).","code":""},{"path":"linear-regressions.html","id":"asymptotic-properties-of-the-ols-estimator-in-the-grm-framework","chapter":"2 Linear Regressions","heading":"2.5.3 Asymptotic properties of the OLS estimator in the GRM framework","text":"GRM framework, :\n\\[\\begin{equation}\n\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) = \\frac{1}{n}\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\right)\\left(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\\tag{2.30}\n\\end{equation}\\]conditional covariance matrix OLS estimator therefore \\(\\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) longer Therefore, using \\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\) inference may misleading. , see appropriate construct appropriate estimates covariance matrix \\(\\mathbf{b}\\). , , let us prove OLS estimator remains consistent GRM framework.Proposition 2.13  (Consistency OLS estimator GRM framework) \\(\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, \\(\\mbox{plim }(\\mathbf{b})=\\boldsymbol\\beta\\).Proof. \\(\\mathbb{V}ar(\\mathbf{b})=\\mathbb{E}[\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X})]+\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]\\). Since \\(\\mathbb{E}(\\mathbf{b}|\\mathbf{X})=\\boldsymbol\\beta\\), \\(\\mathbb{V}ar[\\mathbb{E}(\\mathbf{b}|\\mathbf{X})]=0\\). Eq. (2.30) implies \\(\\mathbb{V}ar(\\mathbf{b}|\\mathbf{X}) \\rightarrow 0\\). Hence \\(\\mathbf{b}\\) converges mean square, therefore probability (see Prop. ??).Prop. 2.14 gives asymptotic distribution OLS estimator GRM framework.Proposition 2.14  (Asymptotic distribution OLS estimator GRM framework) \\(Q_{xx}=\\mbox{plim }(\\mathbf{X}'\\mathbf{X}/n)\\) \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) finite positive definite matrices, :\n\\[\n\\sqrt{n}(\\mathbf{b}-\\boldsymbol\\beta) \\overset{d}{\\rightarrow} \\mathcal{N}(0,Q_{xx}^{-1}Q_{x\\boldsymbol\\Sigma x}Q_{xx}^{-1}).\n\\]IV estimator also features normal asymptotic distribution:Proposition 2.15  (Asymptotic distribution OLS estimator GRM framework) regressors IV variables “well-behaved”, :\n\\[\n\\mathbf{b}_{iv} \\overset{}{\\sim} \\mathcal{N}(\\boldsymbol\\beta,\\mathbf{V}_{iv}),\n\\]\n\n\\[\n\\mathbf{V}_{iv} = \\frac{1}{n}(\\mathbf{Q}^*)\\mbox{ plim }\\left(\\frac{1}{n} \\mathbf{Z}'\\boldsymbol\\Sigma \\mathbf{Z}\\right)(\\mathbf{Q}^*)',\n\\]\n\n\\[\n\\mathbf{Q}^* = [\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}\\mathbf{Q}_{zx}]^{-1}\\mathbf{Q}_{xz}\\mathbf{Q}_{zz}^{-1}.\n\\]practical purposes, one needs estimates \\(\\boldsymbol\\Sigma\\) Props. 2.14 2.15. complication comes fact \\(\\boldsymbol\\Sigma\\) dimension \\(n \\times n\\), estimation —based sample length \\(n\\)— therefore infeasible general case. Notwithstanding, looking Eq. (2.30), appears one can focus estimation \\(Q_{x\\boldsymbol\\Sigma x}=\\mbox{plim }(\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}/n)\\) (\\(\\mbox{plim }\\left(\\frac{1}{n} \\mathbf{Z}'\\boldsymbol\\Sigma \\mathbf{Z}\\right)\\) IV case). matrix dimension \\(K \\times K\\), estimation easier.:\n\\[\\begin{equation}\n\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X} = \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j. \\tag{2.31}\n\\end{equation}\\]-called robust covariance matrices estimates previous matrix. computation based fact \\(\\mathbf{b}\\) consistent, \\(e_i\\)’s consistent (pointwise) estimators \\(\\varepsilon_i\\)’s. Let us present robust covariance matrices two basic situations: heteroskedasticity (Example 2.7) auto-correlation residuals (Example ??)Example 2.7  (Heteroskedasticity) case Eq. (2.24).need estimate \\(\\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i\\). White (1980): general conditions:\n\\[\\begin{equation}\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}\\sigma_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right) =\n\\mbox{plim}\\left( \\frac{1}{n}\\sum_{=1}^{n}e_{}^2\\mathbf{x}_i\\mathbf{x}'_i \\right). \\tag{2.32}\n\\end{equation}\\]\nestimator \\(\\frac{1}{n}\\mathbf{X}'\\boldsymbol\\Sigma\\mathbf{X}\\) therefore :\n\\[\\begin{equation}\n\\frac{1}{n}\\mathbf{X}'\\mathbf{E}^2\\mathbf{X},\\tag{2.33}\n\\end{equation}\\]\n\\(\\mathbf{E}\\) \\(n \\times n\\) diagonal matrix whose diagonal elements estimated residuals \\(e_i\\).Illustration: Figure ??.Let us illustrate influence heteroskedasticity using simulations.consider following model:\n\\[\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2).\n\\]\n\\(x_i\\)s ..d. \\(t(4)\\).simulated sample (\\(n=200\\)) model:simulate 1000 samples model \\(n=200\\). sample, compute OLS estimate \\(\\beta\\) (=1). Using 1000 estimates \\(b\\), construct approximated (kernel-based) distribution OLS estimator (red figure).1000 OLS estimations, employ standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)) estimate variance \\(b\\). blue curve normal distribution centred 1 whose variance average 1000 previous variance estimates.variance simulated \\(b\\) 0.040 (true one); average estimated variances based standard OLS formula 0.005 (bad estimate); average estimated variances based White robust covariance matrix 0.030 (better estimate).standard OLS formula variance \\(b\\) overestimates precision estimator.almost 50% simulations, 1 included 95% confidence interval \\(\\beta\\) computation interval based standard OLS formula variance \\(b\\).White robust covariance matrix used, 1 95% confidence interval \\(\\beta\\) less 10% simulations.Example 2.8  (Heteroskedasticity Autocorrelation (HAC)) includes cases Eqs. (2.24) (2.25).Newey West (1987): correlation terms \\(\\) \\(j\\) gets sufficiently small \\(|-j|\\) increases:\n\\[\\begin{eqnarray}\n&&\\mbox{plim} \\left( \\frac{1}{n}\\sum_{=1}^{n}\\sum_{j=1}^{n}\\sigma_{,j}\\mathbf{x}_i\\mathbf{x}'_j \\right) =  \\\\\n&&\\mbox{plim} \\left( \\frac{1}{n}\\sum_{t=1}^{n}e_{t}^2\\mathbf{x}_t\\mathbf{x}'_t +\n\\frac{1}{n}\\sum_{\\ell=1}^{L}\\sum_{t=\\ell+1}^{n}w_\\ell e_{t}e_{t-\\ell}(\\mathbf{x}_t\\mathbf{x}'_{t-\\ell} + \\mathbf{x}_{t-\\ell}\\mathbf{x}'_{t})\n\\right) \\nonumber \\tag{2.34}\n\\end{eqnarray}\\]\n\\(w_\\ell = 1 - \\ell/(L+1)\\).Let us illustrate influence autocorrelation using simulations.consider following model:\n\\[\\begin{equation}\ny_i = x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,x_i^2),\\tag{2.35}\n\\end{equation}\\]\n\\(x_i\\)s \\(\\varepsilon_i\\)s :\n\\[\\begin{equation}\nx_i = 0.8 x_{-1} + u_i \\quad \\quad \\varepsilon_i = 0.8 \\varepsilon_{-1} + v_i, \\tag{2.36}\n\\end{equation}\\]\n\\(u_i\\)s \\(v_i\\)s ..d. \\(\\mathcal{N}(0,1)\\).simulated sample (\\(n=200\\)) model:\n\n\n\n\n\n\nsimulate 1000 samples model \\(n=200\\).sample, compute OLS estimate \\(\\beta\\) (=1).Using 1000 estimates \\(b\\), construct approximated (kernel-based) distribution OLS estimator (red figure).1000 OLS estimations, employ standard OLS variance formula (\\(s^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\)) estimate variance \\(b\\). blue curve normal distribution centred 1 whose variance average 1000 previous variance estimates.variance simulated \\(b\\) 0.020 (true one); average estimated variances based standard OLS formula 0.005 (bad estimate); average estimated variances based White robust covariance matrix 0.015 (better estimate).standard OLS formula variance \\(b\\) overestimates precision estimator.35% simulations, 1 included 95% confidence interval \\(\\beta\\) computation interval based standard OLS formula variance \\(b\\).Newey-West robust covariance matrix used, 1 95% confidence interval \\(\\beta\\) 13% simulations.sake comparison, let us consider model auto-correlation (\\(x_i \\sim ..d. \\mathcal{N}(0,2.8)\\) \\(\\varepsilon_i \\sim ..d. \\mathcal{N}(0,2.8)\\)).","code":"\nn <- 200\nx <- rt(n,df=5)\ny <- x + x*rnorm(n)\nplot(x,y,pch=19)\nn <- 200\nN <- 1000\nXX <- matrix(rt(n*N,df=5),n,N)\nYY <- matrix(XX + XX*rnorm(n),n,N)\nall_b       <- NULL\nall_V_OLS   <- NULL\nall_V_White <- NULL\nfor(j in 1:N){\n  Y <- matrix(YY[,j],ncol=1)\n  X <- matrix(XX[,j],ncol=1)\n  b <- solve(t(X)%*%X) %*% t(X)%*%Y\n  e <- Y - X %*% b\n  S <- 1/n * t(X) %*% diag(c(e^2)) %*% X\n  V_OLS   <- solve(t(X)%*%X) * var(e)\n  V_White <- 1/n * (solve(1/n*t(X)%*%X)) %*% S %*% (solve(1/n*t(X)%*%X))\n  \n  all_b       <- c(all_b,b)\n  all_V_OLS   <- c(all_V_OLS,V_OLS)\n  all_V_White <- c(all_V_White,V_White)\n}\nplot(density(all_b))\nabline(v=mean(all_b),lty=2)\nabline(v=1)\nx <- seq(0,2,by=.01)\nlines(x,dnorm(x,mean = 1,sd = mean(sqrt(all_V_OLS))),col=\"blue\")\nlines(x,dnorm(x,mean = 1,sd = mean(sqrt(all_V_White))),col=\"red\")"},{"path":"linear-regressions.html","id":"how-to-detect-autocorrelation-in-residuals","chapter":"2 Linear Regressions","heading":"2.5.4 How to detect autocorrelation in residuals?","text":"Consider usual regression (say Eq. (2.26)).Durbin-Watson test typical autocorrelation test. test statistic :\n\\[\nDW = \\frac{\\sum_{=2}^{n}(e_i - e_{-1})^2}{\\sum_{=1}^{n}e_i^2}= 2(1 - r) - \\underbrace{\\frac{e_1^2 + e_n^2}{\\sum_{=1}^{n}e_i^2}}_{\\overset{p}{\\rightarrow} 0},\n\\]\n\\(r\\) slope regression \\(e_i\\)s \\(e_{-1}\\)s, .e.:\n\\[\nr = \\frac{\\sum_{=2}^{n}e_i e_{-1}}{\\sum_{=1}^{n-1}e_i^2}.\n\\]\n(\\(r\\) consistent estimator \\(\\mathbb{C}(\\varepsilon_i,\\varepsilon_{-1})\\), .e. \\(\\rho\\) Eq. (2.27).)Critical values depend T K: see e.g. tables CHECK.one-sided test \\(H_0\\): \\(\\rho=0\\) \\(H_1\\): \\(\\rho>0\\) carried comparing \\(DW\\) values \\(d_L(T, K)\\) \\(d_U(T, K)\\):\n\\[\n\\left\\{\n\\begin{array}{ll}\n\\mbox{$DW < d_L$,}&\\mbox{ null hypothesis rejected;}\\\\\n\\mbox{$DW > d_U$,}&\\mbox{ hypothesis rejected;}\\\\\n\\mbox{$d_L \\le DW \\le d_U$,} &\\mbox{ conclusion drawn.}\n\\end{array}\n\\right.\n\\]","code":""},{"path":"linear-regressions.html","id":"summary","chapter":"2 Linear Regressions","heading":"2.6 Summary","text":"\\(^*\\): see however Prop. 2.13 Prop. 2.14 additional hypotheses. Specifically \\(\\mathbf{X}'\\mathbf{X}/n\\) \\(\\mathbf{X}'\\boldsymbol{\\Sigma}\\mathbf{X}/n\\) must converge proba. finite positive definite matrices (\\(\\boldsymbol\\Sigma\\) defined Eq. (2.23)).","code":""},{"path":"linear-regressions.html","id":"clusters","chapter":"2 Linear Regressions","heading":"2.7 Clusters","text":"XXXX HC0, HC1… Davidson MacKinnon 2004 Section 5.5 XXXXMacKinnon, Nielsen, Webb (2022)nice reference MacKinnon, Nielsen, Webb (2022)Another one Cameron Miller (2014)See package fwildclusterboot wild cluster bootstrap.XXXXXXBased MacKinnon, Nielsen, Webb (2022)::\n\\[\\begin{equation}\n\\mathbf{b} = \\boldsymbol\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon.\\tag{2.37}\n\\end{equation}\\]\nConsider set \\(\\{n_1,n_2,\\dots,n_G\\}\\) s.t. \\(n=\\sum_g n_g\\), based following decomposition \\(\\mathbf{X}\\):\n\\[\n\\mathbf{X} = \\left[\n\\begin{array}{c}\n\\mathbf{X}_1 \\\\\n\\mathbf{X}_1 \\\\\n\\vdots\\\\\n\\mathbf{X}_G\n\\end{array}\n\\right].\n\\]\nnotations, Eq. (2.37) rewrites:\n\\[\\begin{equation}\n\\mathbf{b} - \\boldsymbol\\beta = \\left(\\sum_{g=1}^G \\mathbf{X}_g'\\mathbf{X}_g\\right)^{-1}\\mathbf{X}'\\sum_{g=1}^G \\mathbf{s}_g,\\tag{2.38}\n\\end{equation}\\]\n\\(\\mathbf{s}_g = \\mathbf{X}_g'\\boldsymbol\\varepsilon_g\\) denotes score vector (dimension \\(K \\times 1\\)) associoated \\(g^{th}\\) cluster.model correctly specified \\(\\mathbb{E}(\\mathbf{s}_g))0\\) clusters \\(g\\). Note Eq. (2.38) valid partition \\(\\{1,\\dots,n\\}\\). Nevertheless, dividing sample clusters really becomes meaningful assume following hypothesis holds:Hypothesis 2.6  :\n\\[\n()\\; \\mathbb{E}(\\mathbf{s}_g\\mathbf{s}_g')=\\Sigma_g,\\quad (ii)\\; \\mathbb{E}(\\mathbf{s}_g\\mathbf{s}_q')=0,\\;g \\ne q.\n\\]real assumotion \\((ii)\\). first one simply gives notation covariance matrix score assiciated \\(g^{th}\\) cluster. Remark covariance matrices can differ across clusters. , cluster-based inference robust heteroskedasticity intra-cluster dependence without imposing restrictions (unknown) form either .choice clustering structure sometimes debatable, structure generally assumed known theoretical applied work.Matrix \\(\\Sigma_g\\) depends covariance structure \\(\\varepsilon\\)’s. particular, \\(\\Omega_g = \\mathbb{E}(\\boldsymbol\\varepsilon_g\\boldsymbol\\varepsilon_g'|\\mathbf{X}_g)\\), \\(\\Sigma_g = \\mathbb{E}(\\mathbf{X}_g'\\Omega_g\\mathbf{X}_g)\\).Hypothesis 2.6, comes covariance matrix \\(\\mathbf{b}\\) :\n\\[\\begin{equation}\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\sum_{g=1}^G \\Sigma_g\\right)\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\tag{2.39}\n\\end{equation}\\]Let us denote \\(\\varepsilon_{g,}\\) error associated \\(^{th}\\) component vector \\(\\boldsymbol\\varepsilon_g\\). Consider special case \\(\\mathbb{E}(\\varepsilon_{g,} \\varepsilon_{g,j}|\\mathbf{X}_g)=\\sigma^2\\mathbb{}_{\\{=j\\}}\\), Eq. (2.39) gives standard expression \\(\\sigma^2\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\).\\(\\mathbb{E}(\\varepsilon_{gi} \\varepsilon_{gj}|\\mathbf{X}_g)=\\sigma_{gi}^2\\mathbb{}_{\\{=j\\}}\\), fall case addressed White formula (see Eq. (2.33)), .e.:\n\\[\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}'\\left[  \\begin{array}{cccc}\n\\sigma_1^2 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2^2 &  & 0 \\\\\n\\vdots && \\ddots& \\vdots \\\\\n0 & \\dots & 0 & \\sigma_n^2\n\\end{array} \\right]\\mathbf{X}\\right)\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\n\\]\nnatural way estimate Eq. (2.39) consists replacing \\(\\Sigma_g\\) sample equivalent, .e. \\(\\widehat{\\Sigma}_g=\\mathbf{X}_g'\\mathbf{e}_g\\mathbf{e}_g'\\mathbf{X}_g\\). Adding corrections degrees freedom, leads following estimate covariance matrix \\(\\mathbf{b}\\):\n\\[\\begin{equation}\n\\frac{G(n-1)}{(G-1)(n-K)}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\sum_{g=1}^G\\widehat{\\Sigma}_g\\right) \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}. \\tag{2.40}\n\\end{equation}\\]\nprevious estimate CRCV1 MacKinnon, Nielsen, Webb (2022).Note indeed find White estimator \\(G=n\\) (see Eq. (2.33)).Remark, one cluster, neglecting degree--freedom correction, , \\(G=1\\):\n\\[\n\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}'\\mathbf{e}\\mathbf{e}'\\mathbf{X}\\right) \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1} = 0\n\\]\n\\(\\mathbf{X}'\\mathbf{e}=0\\). Hence, large clusters necessarily increase variance.","code":"\nlibrary(AEC)\nlibrary(sandwich)\nshp$income <- shp$i19ptotn/1000\nshp$female <- 1*(shp$sex19==2)\neq <- lm(income ~ edyear19 + age19 + I(age19^2) + female,data=shp)\n#eq <- lm(income ~ edyear19 + age19 + I(age19^2) + female + I(female*ownkid19*(age19<40)),data=shp)\n#lmtest::coeftest(eq,vcov. = sandwich)\n#lmtest::coeftest(eq,vcov. = vcovHC)\n#X <- cbind(1,shp$edyear19,shp$age19,shp$age19^2,shp$female)\n#solve(t(X) %*% X) %*% t(X) %*% diag(eq$residuals^2) %*% X %*% solve(t(X) %*% X)\n#vcovHC(eq,type=\"HC0\")\n#sandwich(eq)\n#vcovHC(eq,type=\"HC1\")"},{"path":"linear-regressions.html","id":"two-way-clustering","chapter":"2 Linear Regressions","heading":"2.7.1 Two-way clustering","text":"Let’s add second dimension data (e.g., time). now two partitions data: one index \\(g\\), \\(g \\\\{1,\\dots,G\\}\\), index \\(h\\), \\(h \\\\{1,\\dots,H\\}\\). Accordingly, denote \\(\\mathbf{X}_{g,h}\\) submatrix \\(\\mathbf{X}\\) contains explanatory variables corresponding clusters \\(g\\) \\(h\\) (e.g., firms given country \\(g\\) given date \\(h\\)). also denote \\(\\mathbf{X}_{g,\\bullet}\\) (respectively \\(\\mathbf{X}_{\\bullet,h}\\)) submatrix \\(\\mathbf{X}\\) containing explanatory variables pertaining cluster \\(g\\), possible values \\(h\\) (resp. cluster \\(h\\), possible values \\(g\\)).Consider follwing hypothesis:Hypothesis 2.7  :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}(\\mathbf{s}_{g,\\bullet}\\mathbf{s}_{g,\\bullet}')=\\Sigma_g,\\quad \\mathbb{E}(\\mathbf{s}_{\\bullet,h}\\mathbf{s}_{\\bullet,h}')=\\Sigma^*_h,\\quad \\mathbb{E}(\\mathbf{s}_{g,h}\\mathbf{s}_{g,h}')=\\Sigma_{g,h},\\\\ &&\\mathbb{E}(\\mathbf{s}_{g,h}\\mathbf{s}_{q,k}')=0\\;\\mbox{}g\\neq q\\mbox{ }h \\ne k.\n\\end{eqnarray*}\\]assumption, matrix covariance scores given :\n\\[\n\\Sigma = \\sum_{g=1}^G \\Sigma_{g} + \\sum_{h=1}^H \\Sigma^*_{h} - \\sum_{g=1}^G\\sum_{h=1}^H \\Sigma_{g,h}.\n\\]\nlast term right-hand side must subtracted order avoid double counting.Proof. :\n\\[\\begin{eqnarray*}\n\\Sigma &=& \\sum_{g=1}^G\\sum_{q=1}^G\\sum_{h=1}^H\\sum_{k=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{q,k}'\\\\\n&=& \\sum_{g=1}^G\\underbrace{\\left(\\sum_{h=1}^H\\sum_{k=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{g,k}'\\right)}_{=\\Sigma_g}+\\sum_{h=1}^H\\underbrace{\\left(\\sum_{g=1}^G\\sum_{q=1}^G \\mathbf{s}_{g,h}\\mathbf{s}_{q,h}'\\right)}_{=\\Sigma^*_h}-\\sum_{g=1}^G\\sum_{h=1}^H \\mathbf{s}_{g,h}\\mathbf{s}_{g,h}',\n\\end{eqnarray*}\\]\ngives result.asymptotic theory can based tow different approaches: () large number clusters (common case), (ii) fixed number clusters large number observations cluster (see SUbsections 4.1 4.2 MacKinnon, Nielsen, Webb (2022)). variable \\(N_g\\)’s, less reliable asymptotic inference based Eq. (2.40), especially clusters unusually large, distribution data heavy-tailed (fewer moments). issues somehow mitigated clusters approximate factor structure.practice, \\(\\Sigma\\) estimated :\n\\[\n\\widehat{\\Sigma} = \\sum_{g=1}^G \\widehat{\\mathbf{s}}_{g,\\bullet}\\widehat{\\mathbf{s}}_{g,\\bullet}' + \\sum_{h=1}^H \\widehat{\\mathbf{s}}_{\\bullet,h}\\widehat{\\mathbf{s}}_{\\bullet,h} - \\sum_{g=1}^G\\sum_{h=1}^H \\widehat{\\mathbf{s}}_{g,h}\\widehat{\\mathbf{s}}_{g,h}',\n\\]\nuse:\n\\[\n\\widehat{\\mathbb{V}ar}(\\mathbf{b}) = \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\widehat{\\Sigma}\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}.\n\\]alternative asymptotic approximation distribution statistic interest, one can resort bootstrap approximation (see Section 5 MacKinnon, Nielsen, Webb (2022)). R, packge fwildclusterboot allows implement approaches (see, e.g., tutorial Alexander Fischer).","code":""},{"path":"linear-regressions.html","id":"shrinkage-methods","chapter":"2 Linear Regressions","heading":"2.8 Shrinkage methods","text":"Chosing right variables often complicated, especially presence many potentially relevant covariates. Keeping large number covariates results large standard deviations estimated parameters (see Section 2.3.3). order address issue, shrinkage methods designed. objective methods help select limited number variables (shrinking regression coefficients less useful variables towards zero). two best-known shrinkage techniques ridge regression lasso approach.3In cases (ridge lasso), OLS minimization problem (see Section 2.2), :\n\\[\\begin{equation}\n\\mathbf{b} = \\underset{\\boldsymbol\\beta}{\\mbox{argmin}}\\; \\sum_{=1}^n(y_i - \\mathbf{x}_i'\\boldsymbol\\beta)\n\\end{equation}\\]\nreplaced following:\n\\[\\begin{equation}\n\\mathbf{b}_\\lambda = \\underset{\\boldsymbol\\beta}{\\mbox{argmin}}\\; \\sum_{=1}^n(y_i - \\mathbf{x}_i'\\boldsymbol\\beta) + \\lambda f(\\boldsymbol\\beta),\\tag{2.41}\n\\end{equation}\\]\n\\(\\lambda f(\\boldsymbol\\beta)\\) penalty term positively depends “size” comppments \\(\\boldsymbol\\beta\\). term called shrinkage penalty term.Specifically, assuming vector \\(\\mathbf{x}_i\\), contains whole set potential covariates, dimension \\(K \\times 1\\), :\n\\[\\begin{eqnarray*}\nf(\\boldsymbol\\beta) & = & \\sum_{j=1}^K \\beta_j^2 \\quad \\mbox{ridge case ($\\ell_2$ norm)},\\\\\nf(\\boldsymbol\\beta) & = & \\sum_{j=1}^K |\\beta_j| \\quad \\mbox{lasso case ($\\ell_1$ norm)}.\n\\end{eqnarray*}\\]cases, want involve intercept set parameters shrink, preceding equations respectively replaced :\n\\[\\begin{eqnarray*}\nf(\\boldsymbol\\beta) & = & \\sum_{j=2}^K \\beta_j^2 \\quad \\mbox{(ridge)},\\\\\nf(\\boldsymbol\\beta) & = & \\sum_{j=2}^K |\\beta_j| \\quad \\mbox{(lasso)}.\n\\end{eqnarray*}\\]nature penalty (based \\(\\ell_1\\) \\(\\ell_2\\) norms) implies different behaviour parameter estimates \\(\\lambda\\) –thetuning parameter– grows. ridge regression, coefficient estimates go zero (shrinkage); lasso case, coefficients reach zero \\(\\lambda\\) reach values. words, ridge regression acheive shrinkage, lasso regressions acheive shrinkage variable selection.Parameter \\(\\lambda\\) , determined separately minimization problem Eq. (2.41). One can combine standard criteria (e.g., BIC Akaike) lasso regressions help determine \\(\\lambda\\).R, one can use glmnet package run ridge lasso regressions. folowing example, employ package model interest rates proposed debtors. data come Lending Club.begin , let us define variables want consider:Let us standardize data:Next, define set \\(\\lambda\\) use, run ridge lasso regressions:following figure shows estimated parameters depend \\(\\lambda\\):Let us take two values \\(\\lambda\\) see associated estimated parameters context lasso regressions:glmnet package (see Hastie et al. (2021)) also offers tools implement cross-validation:","code":"\nlibrary(AEC)\nlibrary(glmnet)## Loading required package: Matrix## Loaded glmnet 4.1-4\ncredit$owner <- 1*(credit$home_ownership==\"OWN\")\ncredit$renter <- 1*(credit$home_ownership==\"MORTGAGE\")\ncredit$verification_status <- 1*(credit$verification_status==\"Not Verified\")\ncredit$emp_length_10 <- 1*(credit$emp_length_10)\ncredit$log_annual_inc <- log(credit$annual_inc)\ncredit$log_funded_amnt <- log(credit$funded_amnt)\ncredit$annual_inc2 <- (credit$annual_inc)^2\ncredit$funded_amnt2 <- (credit$funded_amnt)^2\nx <- subset(credit,select = c(delinq_2yrs, annual_inc, annual_inc2, log_annual_inc, dti, installment, verification_status, funded_amnt, funded_amnt2, log_funded_amnt, pub_rec, emp_length_10, owner, renter, pub_rec_bankruptcies, revol_util, revol_bal))\ny <- credit$int_rate/sd(credit$int_rate,na.rm = TRUE)\nstdv.x <- apply(x,2,function(a){sd(a,na.rm = TRUE)})\nx <- x/t(matrix(stdv.x,dim(x)[2],dim(x)[1]))\ngrid.lambda <- seq(0,.2,by=.005)\nresult.ridge <- glmnet(x, y, alpha = 0, lambda = grid.lambda)\nresult.lasso <- glmnet(x, y, alpha = 1, lambda = grid.lambda)\nvariab <- 3\nplot(result.ridge$lambda,coef(result.ridge)[variab,],type=\"l\",\n     ylim=c(min(coef(result.ridge)[variab,],coef(result.lasso)[variab,]),\n            max(coef(result.ridge)[variab,],coef(result.lasso)[variab,])),\n     xlab=expression(lambda),ylab=\"Estimated parameter\")\nlines(result.lasso$lambda,coef(result.lasso)[variab,],col=\"red\")\ni <- 20; j <- 40\ncbind(result.lasso$lambda[i],result.lasso$lambda[j])##       [,1]  [,2]\n## [1,] 0.105 0.005\ncbind(coef(result.lasso)[,i],coef(result.lasso)[,j])##                             [,1]          [,2]\n## (Intercept)           3.24385583  6.4716266208\n## delinq_2yrs           0.06308870  0.0689352682\n## annual_inc            0.00000000  0.0045956535\n## annual_inc2           0.00000000  0.0000000000\n## log_annual_inc        0.00000000 -0.0361238200\n## dti                   0.00000000  0.0224224582\n## installment           0.14767959  8.2287289816\n## verification_status   0.00000000 -0.0009750047\n## funded_amnt           0.00000000 -7.3091694210\n## funded_amnt2          0.00000000 -0.4711846250\n## log_funded_amnt       0.00000000 -0.2460932367\n## pub_rec               0.03390816  0.0599725219\n## emp_length_10         0.00000000 -0.0192494122\n## owner                 0.00000000 -0.0244459908\n## renter               -0.03882640 -0.0624308746\n## pub_rec_bankruptcies  0.00000000  0.0000000000\n## revol_util            0.00000000  0.0000000000\n## revol_bal             0.00000000  0.0024022685\n# Compute values of y predicted by the model, for all lambdas:\npred1 <- predict(result.lasso,as.matrix(x))\n# Compute values of y predicted by the model, for a specific value:\npred2 <- predict(result.lasso,as.matrix(x),s=0.085)\n# cross validation:\ncvglmnet <- cv.glmnet(as.matrix(x),y)\nplot(cvglmnet)\ncvglmnet$lambda.min # value of lambda.min, that is the value of lambda that gives minimum mean cross-validated error## [1] 0.003727602\ncvglmnet$lambda.1se # largest value of lambda that is such that the sample cost is within the +1/-1 standard-deviation band stemming from the cv procedure## [1] 0.00448991\ncoef(cvglmnet, s = \"lambda.min\") # associated parameters## 18 x 1 sparse Matrix of class \"dgCMatrix\"\n##                                s1\n## (Intercept)           6.627817184\n## delinq_2yrs           0.065166059\n## annual_inc            0.006603198\n## annual_inc2           .          \n## log_annual_inc       -0.038648540\n## dti                   0.019932058\n## installment           8.636645286\n## verification_status  -0.003667472\n## funded_amnt          -7.662840777\n## funded_amnt2         -0.517617243\n## log_funded_amnt      -0.256890033\n## pub_rec               0.057255481\n## emp_length_10        -0.018817829\n## owner                -0.024997467\n## renter               -0.059573041\n## pub_rec_bankruptcies  .          \n## revol_util            .          \n## revol_bal             0.003361683\npredict(cvglmnet, newx = as.matrix(x)[1:5,], s = \"lambda.min\") # predicted values of y for specific values of x##       lambda.min\n## 21529   3.811212\n## 21547   3.415492\n## 21579   4.027977\n## 21583   3.256566\n## 21608   3.337846"},{"path":"append.html","id":"append","chapter":"3 Appendix","heading":"3 Appendix","text":"","code":""},{"path":"append.html","id":"statistical-tables","chapter":"3 Appendix","heading":"3.1 Statistical Tables","text":"Table 3.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 3.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 3.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 3.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"append.html","id":"variousResults","chapter":"3 Appendix","heading":"3.2 Statistics: definitions and results","text":"Definition 3.1  (Partial correlation) partial correlation \\(y\\) \\(z\\), controlling variables \\(\\mathbf{X}\\) sample correlation \\(y^*\\) \\(z^*\\), latter two variables residuals regressions \\(y\\) \\(\\mathbf{X}\\) \\(z\\) \\(\\mathbf{X}\\), respectively.correlation denoted \\(r_{yz}^\\mathbf{X}\\). definition, :\n\\[\\begin{equation}\nr_{yz}^\\mathbf{X} = \\frac{\\mathbf{z^*}'\\mathbf{y^*}}{\\sqrt{(\\mathbf{z^*}'\\mathbf{z^*})(\\mathbf{y^*}'\\mathbf{y^*})}}.\\tag{3.1}\n\\end{equation}\\]Definition 3.2  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).\\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]\\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Definition 3.3  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 3.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 3.4  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 3.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 3.5  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 3.4 quantiles.)Definition 3.6  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 3.2 quantiles.)Definition 3.7  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 3.3 quantiles.)Definition 3.8  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 3.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 3.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 3.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 3.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 3.6  (Chebychev's inequality) \\(\\mathbb{E}(|X|^r)\\) finite \\(r>0\\) :\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}.\n\\]\nparticular, \\(r=2\\):\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}.\n\\]Proof. Remark \\(\\varepsilon^r \\mathbb{}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) take expectation sides.Definition 3.9  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 3.10  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 3.11  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 3.12  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Proposition 3.7  (Rules limiting distributions (Slutsky)) :Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Proposition 3.8  (Implications stochastic convergences) :\n\\[\\begin{align*}\n&\\boxed{\\overset{L^s}{\\rightarrow}}& &\\underset{1 \\le r \\le s}{\\Rightarrow}& &\\boxed{\\overset{L^r}{\\rightarrow}}&\\\\\n&& && &\\Downarrow&\\\\\n&\\boxed{\\overset{.s.}{\\rightarrow}}& &\\Rightarrow& &\\boxed{\\overset{p}{\\rightarrow}}& \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}.\n\\end{align*}\\]Proof. (fact \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting \\(F\\) \\(F_n\\) c.d.f. \\(X\\) \\(X_n\\), respectively:\n\\[\\begin{equation}\nF_n(x) = \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X > x+\\varepsilon) \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\\tag{3.2}\n\\end{equation}\\]\nBesides,\n\\[\nF(x-\\varepsilon) = \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n > x) \\le F_n(x) + \\mathbb{P}(|X_n - X|>\\varepsilon),\n\\]\nimplies:\n\\[\\begin{equation}\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x).\\tag{3.3}\n\\end{equation}\\]\nEqs. (3.2) (3.3) imply:\n\\[\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x)  \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\n\\]\nTaking limits \\(n \\rightarrow \\infty\\) yields\n\\[\nF(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x)  \\le F(x+\\varepsilon).\n\\]\nresult obtained taking limits \\(\\varepsilon \\rightarrow 0\\) (\\(F\\) continuous \\(x\\)).Proposition 3.9  (Convergence distribution constant) \\(X_n\\) converges distribution constant \\(c\\), \\(X_n\\) converges probability \\(c\\).Proof. \\(\\varepsilon>0\\), \\(\\mathbb{P}(X_n < c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) .e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) \\(\\mathbb{P}(X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\),\ngives result.Example \\(plim\\) \\(L^r\\) convergence: Let \\(\\{x_n\\}_{n \\\\mathbb{N}}\\) series random variables defined :\n\\[\nx_n = n u_n,\n\\]\n\\(u_n\\) independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\).\\(x_n \\overset{p}{\\rightarrow} 0\\) \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\).Theorem 3.1  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 3.2  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]Definition 3.13  (Characteristic function) real-valued random variable \\(X\\), characteristic function defined :\n\\[\n\\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)].\n\\]Theorem 3.3  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. 3.13) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 3.4  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n &=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).Proposition 3.10  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Proposition 3.11  \\(\\mathbf{}\\) idempotent \\(\\mathbf{x}\\) Gaussian, \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}\\) independent \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\).Proof. \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\), two Gaussian vectors \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{}\\mathbf{x}\\) independent. implies independence function \\(\\mathbf{L}\\mathbf{x}\\) function \\(\\mathbf{}\\mathbf{x}\\). results follows observation \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}=(\\mathbf{}\\mathbf{x})'(\\mathbf{}\\mathbf{x})\\), function \\(\\mathbf{}\\mathbf{x}\\).Proposition 3.12  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.Theorem 3.5  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 3.14  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 3.13  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Definition 3.15  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha,\n\\]\n\\(S_n\\) test statistic \\(\\Theta\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\\\Theta\\).Definition 3.16  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1,\n\\]\n\\(S_n\\) test statistic \\(\\Theta^c\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\notin \\Theta^c\\).Definition 3.17  (Kullback discrepancy) Given two p.d.f. \\(f\\) \\(f^*\\), Kullback discrepancy defined :\n\\[\n(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy.\n\\]Proposition 3.14  (Properties Kullback discrepancy) :\\((f,f^*) \\ge 0\\)\\((f,f^*) \\ge 0\\)\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).Proof. \\(x \\rightarrow -\\log(x)\\) convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves ()). Since \\(x \\rightarrow -\\log(x)\\) strictly convex, equality () holds \\(f(Y)/f^*(Y)\\) constant (proves (ii)).Proposition 3.15  (Square absolute summability) :\n\\[\n\\underbrace{\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty}_{\\mbox{Square summability}}.\n\\]Proof. See Appendix 3.Hamilton. Idea: Absolute summability implies exist \\(N\\) , \\(j>N\\), \\(|\\theta_j| < 1\\) (deduced Cauchy criterion, Theorem 3.1 therefore \\(\\theta_j^2 < |\\theta_j|\\).","code":""},{"path":"append.html","id":"GaussianVar","chapter":"3 Appendix","heading":"3.3 Some properties of Gaussian variables","text":"Proposition 3.16  (Bayesian update vector Gaussian variables) \n\\[\n\\left[\n\\begin{array}{c}\nY_1\\\\\nY_2\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\n\\left(0,\n\\left[\\begin{array}{cc}\n\\Omega_{11} & \\Omega_{12}\\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right]\n\\right),\n\\]\n\n\\[\nY_{2}|Y_{1} \\sim \\mathcal{N}\n\\left(\n\\Omega_{21}\\Omega_{11}^{-1}Y_{1},\\Omega_{22}-\\Omega_{21}\\Omega_{11}^{-1}\\Omega_{12}\n\\right).\n\\]\n\\[\nY_{1}|Y_{2} \\sim \\mathcal{N}\n\\left(\n\\Omega_{12}\\Omega_{22}^{-1}Y_{2},\\Omega_{11}-\\Omega_{12}\\Omega_{22}^{-1}\\Omega_{21}\n\\right).\n\\]Proposition 3.17  (Truncated distributions) \\(X\\) random variable distributed according p.d.f. \\(f\\), c.d.f. \\(F\\), infinite support. p.d.f. \\(X|\\le X < b\\) \n\\[\ng(x) = \\frac{f(x)}{F(b)-F()}\\mathbb{}_{\\{\\le x < b\\}},\n\\]\n\\(<b\\).partiucular, Gaussian variable \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\nf(X=x|\\le X<b) = \\dfrac{\\dfrac{1}{\\sigma}\\phi\\left(\\dfrac{x - \\mu}{\\sigma}\\right)}{Z}.\n\\]\n\\(Z = \\Phi(\\beta)-\\Phi(\\alpha)\\), \\(\\alpha = \\dfrac{- \\mu}{\\sigma}\\) \\(\\beta = \\dfrac{b - \\mu}{\\sigma}\\).Moreover:\n\\[\\begin{eqnarray}\n\\mathbb{E}(X|\\le X<b) &=& \\mu - \\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\sigma. \\tag{3.4}\n\\end{eqnarray}\\]also :\n\\[\\begin{eqnarray}\n&& \\mathbb{V}ar(X|\\le X<b) \\nonumber\\\\\n&=& \\sigma^2\\left[\n1 -  \\frac{\\beta\\phi\\left(\\beta\\right)-\\alpha\\phi\\left(\\alpha\\right)}{Z} -  \\left(\\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\right)^2 \\right] \\tag{3.5}\n\\end{eqnarray}\\]particular, \\(b \\rightarrow \\infty\\), get:\n\\[\\begin{equation}\n\\mathbb{V}ar(X|< X) = \\sigma^2\\left[1 + \\alpha\\lambda(-\\alpha) - \\lambda(-\\alpha)^2 \\right], \\tag{3.6}\n\\end{equation}\\]\n\\(\\lambda(x)=\\dfrac{\\phi(x)}{\\Phi(x)}\\) called inverse Mills ratio.Consider case \\(\\rightarrow - \\infty\\) (.e. conditioning set \\(X<b\\)) \\(\\mu=0\\), \\(\\sigma=1\\). Eq. (3.4) gives \\(\\mathbb{E}(X|X<b) = - \\lambda(b) = - \\dfrac{\\phi(b)}{\\Phi(b)}\\), \\(\\lambda\\) function computing inverse Mills ratio.\nFigure 3.1: \\(\\mathbb{E}(X|X<b)\\) function \\(b\\) \\(X\\sim \\mathcal{N}(0,1)\\) (black).\nProposition 3.18  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"proofs","chapter":"3 Appendix","heading":"3.4 Proofs","text":"","code":""},{"path":"append.html","id":"MLEproperties","chapter":"3 Appendix","heading":"3.4.1 Proof of Proposition ??","text":"Proof. Assumptions () (ii) (set Assumptions ??) imply \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\)).\\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) can interpreted sample mean r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) ..d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – exists (Assumption iv).latter convergence uniform (Assumption v), solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges solution limit problem:\n\\[\n\\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy.\n\\]Properties Kullback information measure (see Prop. 3.14), together identifiability assumption (ii) implies solution limit problem unique equal \\(\\boldsymbol\\theta_0\\).Consider r.v. sequence \\(\\boldsymbol\\theta\\) converges \\(\\boldsymbol\\theta_0\\). Taylor expansion score neighborood \\(\\boldsymbol\\theta_0\\) yields :\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\]\\(\\boldsymbol\\theta_{MLE}\\) converges \\(\\boldsymbol\\theta_0\\) satisfies likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}\\). Therefore:\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]\nequivalently:\n\\[\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]law large numbers, : \\(\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\mathbf{}(\\boldsymbol\\theta_0) = \\mathcal{}_Y(\\boldsymbol\\theta_0)\\).Besides, :\n\\[\\begin{eqnarray*}\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} &=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\\n&=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right)\n\\end{eqnarray*}\\]\nconverges \\(\\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0))\\) CLT.Collecting preceding results leads (b). fact \\(\\boldsymbol\\theta_{MLE}\\) achieves FDCR bound proves (c).","code":""},{"path":"append.html","id":"Walddistri","chapter":"3 Appendix","heading":"3.4.2 Proof of Proposition ??","text":"Proof. \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (eq:normMLE)). Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields :\n\\[\\begin{equation}\n\\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{3.7}\n\\end{equation}\\]\n\\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore:\n\\[\\begin{equation}\n\\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{3.8}\n\\end{equation}\\]\nHence\n\\[\n\\sqrt{n} \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right).\n\\]\nTaking quadratic form, obtain:\n\\[\nn h(\\hat{\\boldsymbol\\theta}_{n})'  \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]fact test asymptotic level \\(\\alpha\\) directly stems precedes. Consistency test: Consider \\(\\theta_0 \\\\Theta\\). MLE consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (3.7) still valid. implies \\(\\xi^W_n\\) converges \\(+\\infty\\) therefore \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\).","code":""},{"path":"append.html","id":"LMdistri","chapter":"3 Appendix","heading":"3.4.3 Proof of Proposition ??","text":"Proof. Notations: “\\(\\approx\\)” means “equal term converges 0 probability”. \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes unconstrained one.combine two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (definition) get:\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{3.9}\n\\end{equation}\\]\nBesides, (using definition information matrix):\n\\[\\begin{equation}\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{3.10}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{3.11}\n\\end{equation}\\]\nTaking difference multiplying \\(\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\):\n\\[\\begin{equation}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\mathcal{}(\\boldsymbol\\theta_0).\\tag{3.12}\n\\end{equation}\\]\nEqs. (3.9) (3.12) yield :\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}.\\tag{3.13}\n\\end{equation}\\]Recall \\(\\hat{\\boldsymbol\\theta}^0_n\\) MLE \\(\\boldsymbol\\theta_0\\) constraint \\(h(\\boldsymbol\\theta)=0\\). vector Lagrange multipliers \\(\\hat\\lambda_n\\) associated program satisfies:\n\\[\\begin{equation}\n\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{3.14}\n\\end{equation}\\]\nSubstituting latter equation Eq. (3.13) gives:\n\\[\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}}\n\\]\nyields:\n\\[\\begin{equation}\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{3.15}\n\\end{equation}\\]\nfollows, Eq. (3.8), :\n\\[\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\\right).\n\\]\nTaking quadratic form last equation gives:\n\\[\n\\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]\nUsing Eq. (3.14), appears left-hand side term last equation \\(\\xi^{LM}\\) defined Eq. (??). Consistency: see Remark 17.3 Gouriéroux Monfort (1995).","code":""},{"path":"append.html","id":"equivWaldLM","chapter":"3 Appendix","heading":"3.4.4 Proof of Proposition ??","text":"Proof. (using Eq. (eq:multiplier)):\n\\[\n\\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n.\n\\]\nSince, \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (3.15) therefore implies :\n\\[\n\\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)' \\left(\n\\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\nh(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W},\n\\]\ngives result.","code":""},{"path":"append.html","id":"equivLRLM","chapter":"3 Appendix","heading":"3.4.5 Proof of Proposition ??","text":"Proof. second-order taylor expansions \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y})\\) \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y})\\) :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nTaking difference, obtain:\n\\[\n\\xi_n^{LR} \\approx 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nUsing \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (3.11)), :\n\\[\n\\xi_n^{LR} \\approx\n2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)'\\mathcal{}(\\boldsymbol\\theta_0)\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n)\n+ n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nsecond three terms sum, replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) develop associated product. leads :\n\\[\\begin{equation}\n\\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)' \\mathcal{}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{3.16}\n\\end{equation}\\]\ndifference Eqs. (3.10) (3.11) implies:\n\\[\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n),\n\\]\n, associated Eq. @(eq:lr10), gives:\n\\[\n\\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}.\n\\]\nHence \\(\\xi_n^{LR}\\) asymptotic distribution \\(\\xi_n^{LM}\\).Let’s show test consistent. , note :\n\\[\n\\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\mathbf{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\mathbf{y})}{n} = \\frac{1}{n} \\sum_{=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)] \\rightarrow \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)],\n\\]\n\\(\\boldsymbol\\theta_\\infty\\), pseudo true value, \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (definition \\(H_1\\)). Kullback inequality asymptotic identifiability \\(\\boldsymbol\\theta_0\\), follows \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] >0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) \\(H_1\\).","code":""},{"path":"append.html","id":"proofTVTCL","chapter":"3 Appendix","heading":"3.4.6 Proof of Eq. (??)","text":"Proof. :\n\\[\\begin{eqnarray*}\n&&T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\\n&=& T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s<t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\\n&=& \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\\n&&+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\\n&=&  \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} .\n\\end{eqnarray*}\\]\nTherefore:\n\\[\\begin{eqnarray*}\nT\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j &=& - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots\n\\end{eqnarray*}\\]\n:\n\\[\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| \\le 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\]\\(q \\le T\\), :\n\\[\\begin{eqnarray*}\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\\n&&2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots  + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\\n&\\le& \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\\n&&2|\\gamma_{q+1}| + \\dots  + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]Consider \\(\\varepsilon > 0\\). fact autocovariances absolutely summable implies exists \\(q_0\\) (Cauchy criterion, Theorem 3.1):\n\\[\n2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots < \\varepsilon/2.\n\\]\n, \\(T > q_0\\), comes :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2.\n\\]\n\\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) \n\\[\n\\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2.\n\\]\n, \\(T>f(q_0)\\) \\(T>q_0\\), .e. \\(T>\\max(f(q_0),q_0)\\), :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon.\n\\]","code":""},{"path":"append.html","id":"smallestMSE","chapter":"3 Appendix","heading":"3.4.7 Proof of Proposition ??","text":"Proof. :\n\\[\\begin{eqnarray}\n\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\\n&=&  \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)\\nonumber\\\\\n&& + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right). \\tag{3.17}\n\\end{eqnarray}\\]\nLet us focus last term. :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right)\\\\\n&=& \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\\mbox{function $x_t$}}}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.\n\\end{eqnarray*}\\]Therefore, Eq. (3.17) becomes:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\\n&=&  \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ depend $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ depends $y^*_{t+1}$}}.\n\\end{eqnarray*}\\]\nimplies \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) always larger \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), therefore minimized second term equal zero, \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\).","code":""},{"path":"append.html","id":"estimVARGaussian","chapter":"3 Appendix","heading":"3.4.8 Proof of Proposition ??","text":"Proof. Using Proposition ?? (Appendix ??), obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.","code":""},{"path":"append.html","id":"OLSVAR","chapter":"3 Appendix","heading":"3.4.9 Proof of Proposition ??","text":"Proof. Let us drop \\(\\) subscript. Rearranging Eq. (??), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"append.html","id":"additional-codes","chapter":"3 Appendix","heading":"3.5 Additional codes","text":"","code":""},{"path":"append.html","id":"App:GEV","chapter":"3 Appendix","heading":"3.5.1 Simulating GEV distributions","text":"following lines code used generate Figure ??.","code":"\nn.sim <- 4000\npar(mfrow=c(1,3),\n    plt=c(.2,.95,.2,.85))\nall.rhos <- c(.3,.6,.95)\nfor(j in 1:length(all.rhos)){\n  theta <- 1/all.rhos[j]\n  v1 <- runif(n.sim)\n  v2 <- runif(n.sim)\n  w <- rep(.000001,n.sim)\n  # solve for f(w) = w*(1 - log(w)/theta) - v2 = 0\n  for(i in 1:20){\n    f.i <- w * (1 - log(w)/theta) - v2\n    f.prime <- 1 - log(w)/theta - 1/theta\n    w <- w - f.i/f.prime\n  }\n  u1 <- exp(v1^(1/theta) * log(w))\n  u2 <- exp((1-v1)^(1/theta) * log(w))\n\n  # Get eps1 and eps2 using the inverse of the Gumbel distribution's cdf:\n  eps1 <- -log(-log(u1))\n  eps2 <- -log(-log(u2))\n  cbind(cor(eps1,eps2),1-all.rhos[j]^2)\n  plot(eps1,eps2,pch=19,col=\"#FF000044\",\n       main=paste(\"rho = \",toString(all.rhos[j]),sep=\"\"),\n       xlab=expression(epsilon[1]),\n       ylab=expression(epsilon[2]),\n       cex.lab=2,cex.main=1.5)\n}"},{"path":"append.html","id":"IRFDELTA","chapter":"3 Appendix","heading":"3.5.2 Computing the covariance matrix of IRF using the delta method","text":"","code":"\nirf.function <- function(THETA){\n  c <- THETA[1]\n  phi <- THETA[2:(p+1)]\n  if(q>0){\n    theta <- c(1,THETA[(1+p+1):(1+p+q)])\n  }else{\n    theta <- 1\n  }\n  sigma <- THETA[1+p+q+1]\n  r <- dim(Matrix.of.Exog)[2] - 1\n  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]\n  \n  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60,y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,\n                  X=NaN,beta=NaN)\n  return(irf)\n}\n\nIRF.0 <- 100*irf.function(x$THETA)\neps <- .00000001\nd.IRF <- NULL\nfor(i in 1:length(x$THETA)){\n  THETA.i <- x$THETA\n  THETA.i[i] <- THETA.i[i] + eps\n  IRF.i <- 100*irf.function(THETA.i)\n  d.IRF <- cbind(d.IRF,\n                 (IRF.i - IRF.0)/eps\n                 )\n}\nmat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]

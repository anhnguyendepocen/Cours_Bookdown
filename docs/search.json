[{"path":"index.html","id":"prerequisites","chapter":"1 Prerequisites","heading":"1 Prerequisites","text":"sample book written Markdown. can use anything Pandoc’s Markdown supports, e.g., math equation \\(^2 + b^2 = c^2\\).bookdown package can installed CRAN Github:Remember Rmd file contains one one chapter, chapter defined first-level heading #.compile example PDF, need XeLaTeX. recommended install TinyTeX (includes XeLaTeX): https://yihui.name/tinytex/.","code":"\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"can label chapter section titles using {#label} , e.g., can reference Chapter 2. manually label , automatic labels anyway, e.g., Chapter ??.Figures tables captions placed figure table environments, respectively.\nFigure 2.1: nice figure!\nReference figure code chunk label fig: prefix, e.g., see Figure 2.1. Similarly, can reference tables generated knitr::kable(), e.g., see Table 2.1.Table 2.1: nice table!can write citations, . example, using bookdown package (Xie 2022) sample book, built top R Markdown knitr (Xie 2015).example borrowed Petersen.XXXXSargan-Hansen () test. Sargan (1958) Hansen (1982)Durbin-Wu-Hausman test: Durbin (1954) / Wu (1973) / Hausman (1978)Use R! excellent tutorial.\n(notably plm Arellano-Bond example, 140 UK firms)Program evaluation (good survey): Abadie Cattaneo (2018)\nMostly harmless: Angrist Pischke (2008)Diff--Diff: Card Krueger (1994)Diff--Diff:\nMeyer, Viscusi, Durbin (1995)\ndata wooldridge package. See page","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(iris, 20), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)\nlibrary(sandwich)\n## Petersen's data\ndata(\"PetersenCL\", package = \"sandwich\")\nm <- lm(y ~ x, data = PetersenCL)\n\n## clustered covariances\n## one-way\nvcovCL(m, cluster = ~ firm)##               (Intercept)             x\n## (Intercept)  4.490702e-03 -6.473517e-05\n## x           -6.473517e-05  2.559927e-03\nvcovCL(m, cluster = PetersenCL$firm) ## same##               (Intercept)             x\n## (Intercept)  4.490702e-03 -6.473517e-05\n## x           -6.473517e-05  2.559927e-03\n## one-way with HC2\nvcovCL(m, cluster = ~ firm, type = \"HC2\")##               (Intercept)             x\n## (Intercept)  4.494487e-03 -6.592912e-05\n## x           -6.592912e-05  2.568236e-03\n## two-way\nvcovCL(m, cluster = ~ firm + year)##               (Intercept)             x\n## (Intercept)  4.233313e-03 -2.845344e-05\n## x           -2.845344e-05  2.868462e-03\nvcovCL(m, cluster = PetersenCL[, c(\"firm\", \"year\")]) ## same##               (Intercept)             x\n## (Intercept)  4.233313e-03 -2.845344e-05\n## x           -2.845344e-05  2.868462e-03"},{"path":"vector-auto-regressive-var-models.html","id":"vector-auto-regressive-var-models","chapter":"3 Vector Auto-Regressive (VAR) models","heading":"3 Vector Auto-Regressive (VAR) models","text":"Kilian (1998) See pageSign restrictions: package, Danne (2015).Pfaff (2008)\nHlavac (2022)VARs widely used macroeconomic analysis. simple easy estimate, make possible conveniently capture dynamics complex multivariate systems. VAR popularity notably due Sims (1980)’s influential work. economics, VAR models often employed order identify structural shocks. First, present VAR models. Second, study structural extension (SVAR models).Definition 3.1  ((S)VAR model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector random variables. Process \\(y_{t}\\) follows \\(p^{th}\\)-order VAR , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t,& \\varepsilon_t:\\mbox{ (correlated) innovation}\\\\\nSVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B \\eta_t,& \\eta_t:\\mbox{ ($\\bot$) structural shock},\n\\end{array}\\tag{3.1}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B\\eta_t\\). assume \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.first line Eq. (3.1) corresponds reduced-form VAR model (structural form second line).Eq. (3.1) can also written:\n\\[\ny_{t}=c+\\Phi(L)y_{t-1}+\\varepsilon_{t},\n\\]\n\\(\\Phi(L) = \\Phi_1 + \\Phi_2 L + \\dots + \\Phi_p L^{p-1}\\).Consequently:\n\\[\ny_{t}\\mid y_{t-1},y_{t-2},\\ldots,y_{-p+1}\\sim \\mathcal{N}(c+\\Phi_{1}y_{t-1}+\\ldots\\Phi_{p}y_{t-p},\\Omega).\n\\]Using Hamilton (1994)’s notations, denote \\(\\Pi\\) matrix \\(\\left[\\begin{array}{ccccc} c & \\Phi_{1} & \\Phi_{2} & \\ldots & \\Phi_{p}\\end{array}\\right]'\\) \\(x_{t}\\) vector \\(\\left[\\begin{array}{ccccc} 1 & y'_{t-1} & y'_{t-2} & \\ldots & y'_{t-p}\\end{array}\\right]'\\), :\n\\[\\begin{equation}\ny_{t}= \\Pi'x_{t} + \\varepsilon_{t}. \\tag{3.2}\n\\end{equation}\\]\nprevious representation convenient discuss estimation VAR model, parameters gathered two matrices : \\(\\Pi\\) \\(\\Omega\\).","code":"\nlibrary(VAR.etp)\nlibrary(vars) #standard VAR models\ndata(dat) # part of VAR.etp package\na <- VAR.Boot(dat,p=2,nb=200,type=\"const\")\nb <- VAR(dat,p=2)\nrbind(a$coef[1,],(a$coef+a$Bias)[1,],b$varresult$inv$coefficients)##        inv(-1)    inc(-1)  con(-1)    inv(-2)  inc(-2)   con(-2)       const\n## [1,] -0.311982 0.08913291 0.951436 -0.1418598 0.143103 1.0466672 -0.01891006\n## [2,] -0.319631 0.14598883 0.961219 -0.1605511 0.114605 0.9343938 -0.01672199\n## [3,] -0.319631 0.14598883 0.961219 -0.1605511 0.114605 0.9343938 -0.01672199"},{"path":"vector-auto-regressive-var-models.html","id":"var-estimation","chapter":"3 Vector Auto-Regressive (VAR) models","heading":"3.0.1 VAR estimation","text":"Let us start case shocks Gaussian.Proposition 3.1  (MLE Gaussian VAR) \\(y_t\\) follows VAR(p) (see Definition 3.1), \\(\\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,\\Omega)\\), ML estimate \\(\\Pi\\), denoted \\(\\hat{\\Pi}\\) (see Eq. (3.2)), given \n\\[\\begin{equation}\n\\hat{\\Pi}=\\left[\\sum_{t=1}^{T}x_{t}x'_{t}\\right]^{-1}\\left[\\sum_{t=1}^{T}y_{t}'x_{t}\\right]= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y},\\tag{3.3}\n\\end{equation}\\]\n\\(\\mathbf{X}\\) \\(T \\times (np)\\) matrix whose \\(t^{th}\\) row \\(x_t\\) \\(\\mathbf{y}\\) \\(T \\times n\\) matrix whose \\(t^{th}\\) row \\(y_{t}'\\)., \\(^{th}\\) column \\(\\hat{\\Pi}\\) (\\(b_i\\), say) OLS estimate \\(\\beta_i\\), :\n\\[\\begin{equation}\ny_{,t} = \\beta_i'x_t + \\varepsilon_{,t},\\tag{3.4}\n\\end{equation}\\]\n(.e., \\(\\beta_i' = [c_i,\\phi_{,1}',\\dots,\\phi_{,p}']'\\)).ML estimate \\(\\Omega\\), denoted \\(\\hat{\\Omega}\\), coincides sample covariance matrix \\(n\\) series OLS residuals Eq. (3.4), .e.:\n\\[\\begin{equation}\n\\hat{\\Omega} = \\frac{1}{T} \\sum_{=1}^T \\hat{\\varepsilon}_t\\hat{\\varepsilon}_t',\\quad\\mbox{} \\hat{\\varepsilon}_t= y_t - \\hat{\\Pi}'x_t.\n\\end{equation}\\]asymptotic distributions estimators ones resulting standard OLS formula.Proof. See Appendix 4.4.8.stated Proposition 4.4.9, shocks Gaussian, OLS regressions still provide consistent estimates model parameters. However, since \\(x_t\\) correlates \\(\\varepsilon_s\\) \\(s<t\\), OLS estimator \\(\\mathbf{b}_i\\) \\(\\boldsymbol\\beta_i\\) biased small sample. (also case ML estimator.)Indeed, denoting \\(\\boldsymbol\\varepsilon_i\\) \\(T \\times 1\\) vector \\(\\varepsilon_{,t}\\)’s, using notations \\(b_i\\) \\(\\beta_i\\) introduced Proposition 3.1, :\n\\[\\begin{equation}\n\\mathbf{b}_i = \\beta_i + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i.\\tag{3.5}\n\\end{equation}\\]\nnon-zero correlation \\(x_t\\) \\(\\varepsilon_{,s}\\) \\(s<t\\) , therefore, \\(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i] \\ne 0\\).However, \\(y_t\\) covariance stationary, \\(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\) converges positive definite matrix \\(\\mathbf{Q}\\), \\(\\frac{1}{n}X'\\boldsymbol\\varepsilon_i\\) converges 0. Hence \\(\\mathbf{b}_i \\overset{p}{\\rightarrow} \\beta_i\\). precisely:Proposition 3.2  (Asymptotic distribution OLS estimate $\\beta_i$) \\(y_t\\) follows VAR model, defined Definition 3.1, :\n\\[\n\\sqrt{T}(\\mathbf{b}_i-\\beta_i) =  \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T x_t x_t' \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}}\n\\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T x_t\\varepsilon_{,t} \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma_i^2\\mathbf{Q})},\n\\]\n\\(\\sigma_i = \\mathbb{V}ar(\\varepsilon_{,t})\\) \\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) given :\n\\[\\begin{equation}\n\\mathbf{Q} = \\left[\n\\begin{array}{ccccc}\n1 & \\mu' &\\mu' & \\dots & \\mu' \\\\\n\\mu & \\gamma_0 + \\mu\\mu' & \\gamma_1 + \\mu\\mu' & \\dots & \\gamma_{p-1} + \\mu\\mu'\\\\\n\\mu & \\gamma_1 + \\mu\\mu' & \\gamma_0 + \\mu\\mu' & \\dots & \\gamma_{p-2} + \\mu\\mu'\\\\\n\\vdots &\\vdots &\\vdots &\\dots &\\vdots \\\\\n\\mu & \\gamma_{p-1} + \\mu\\mu' & \\gamma_{p-2} + \\mu\\mu' & \\dots & \\gamma_{0} + \\mu\\mu'\n\\end{array}\n\\right].\\tag{3.6}\n\\end{equation}\\]Proof. See Appendix 4.4.9.following proposition extends previous proposition includes covariances different \\(\\beta_i\\)’s well asymptotic distribution ML estimates \\(\\Omega\\).Proposition 3.3  (Asymptotic distribution OLS estimates) \\(y_t\\) follows VAR model, defined Definition 3.1, :\n\\[\\begin{equation}\n\\sqrt{T}\\left[\n\\begin{array}{c}\nvec(\\hat\\Pi - \\Pi)\\\\\nvec(\\hat\\Omega - \\Omega)\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\\left(0,\n\\left[\n\\begin{array}{cc}\n\\Omega \\otimes \\mathbf{Q}^{-1} & 0\\\\\n0 & \\Sigma_{22}\n\\end{array}\n\\right]\\right),\\tag{3.7}\n\\end{equation}\\]\ncomponent \\(\\Sigma_{22}\\) corresponding covariance \\(\\hat\\sigma_{,j}\\) \\(\\hat\\sigma_{k,l}\\) (\\(,j,l,m \\\\{1,\\dots,n\\}^4\\)) equal \\(\\sigma_{,l}\\sigma_{j,m}+\\sigma_{,m}\\sigma_{j,l}\\).Proof. See Hamilton (1994), Appendix Chapter 11.Naturally, practice, \\(\\Omega\\) replaced \\(\\hat{\\Omega}\\), \\(Q\\) replaced \\(\\hat{\\mathbf{Q}} = \\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) \\(\\Sigma\\) matrix whose components form \\(\\hat\\sigma_{,l}\\hat\\sigma_{j,m}+\\hat\\sigma_{,m}\\hat\\sigma_{j,l}\\), \\(\\hat\\sigma_{,l}\\)’s components \\(\\hat\\Omega\\).simplicity VAR framework tractability MLE open way convenient econometric testing. Let’s illustrate likelihood ratio test. maximum value achieved MLE \n\\[\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega}) = -\\frac{Tn}{2}\\log(2\\pi)+\\frac{T}{2}\\log\\left|\\hat{\\Omega}^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\]\nlast term :\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t} &=& \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right] = \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right]\\\\\n&=&\\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right] = \\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\left(T\\hat{\\Omega}\\right)\\right]=Tn.\n\\end{eqnarray*}\\]\nTherefore, optimized log-likelihood simply obtained :\n\\[\\begin{equation}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega})=-(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\hat{\\Omega}^{-1}\\right|-Tn/2.\\tag{3.8}\n\\end{equation}\\]Assume want test null hypothesis set variables follows VAR(\\(p_{0}\\)) alternative\nspecification \\(p_{1}\\) (\\(>p_{0}\\)).Let us denote \\(\\hat{L}_{0}\\) \\(\\hat{L}_{1}\\) maximum log-likelihoods obtained \\(p_{0}\\) \\(p_{1}\\) lags, respectively.null hypothesis (\\(H_0\\): \\(p=p_0\\)), :\n\\[\\begin{eqnarray*}\n2\\left(\\hat{L}_{1}-\\hat{L}_{0}\\right)&=&T\\left(\\log\\left|\\hat{\\Omega}_{1}^{-1}\\right|-\\log\\left|\\hat{\\Omega}_{0}^{-1}\\right|\\right)  \\sim \\chi^2(n^{2}(p_{1}-p_{0})).\n\\end{eqnarray*}\\]Block exogeneityLet’s decompose \\(y_t\\) two subvectors \\(y^{(1)}_{t}\\) (\\(n_1 \\times 1\\)) \\(y^{(2)}_{t}\\) (\\(n_2 \\times 1\\)), \\(y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']\\) (therefore \\(n=n_1 +n_2\\)), :\n\\[\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t}\\\\\ny^{(2)}_{t}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\n\\Phi^{(1,1)} & \\Phi^{(1,2)}\\\\\n\\Phi^{(2,1)} & \\Phi^{(2,2)}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t-1}\\\\\ny^{(2)}_{t-1}\n\\end{array}\n\\right] + \\varepsilon_t.\n\\]\nOne can easily test block exogeneity \\(y_t^{(2)}\\) (say). null assumption ca n expressed \\(\\Phi^{(2,1)}=0\\) \\(\\Sigma^{(2,1)}=0\\).Lag selectionIn VAR, adding lags consumes numerous degrees freedom: \\(p\\) lags, \\(n\\) equations VAR contains \\(n\\times p\\) coefficients plus intercept term.Adding lags improve -sample fit, likely result -parameterization affect {--sample} prediction performance.select appropriate lag length, -called selection criteria can used (see Definition ??). criteria minimized. , best specification one giving lowest criteria.context VAR models, using Eq. (3.8), :\n\\[\\begin{eqnarray*}\nAIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{2}{T}N\\\\\nBIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{\\log T}{T}N,\n\\end{eqnarray*}\\]\n\\(N=p \\times n^{2}\\).Companion Form Stability VAR processLet us introduce vector \\(y_{t}^{*}\\), whihc stacks last \\(p\\) values \\(y_t\\):\n\\[\ny_{t}^{*}=\\left[\\begin{array}{cccc}\ny'_{t} & y'_{t-1} & \\ldots & y'_{t-p+1}\\end{array}\\right]^{'},\n\\]\nEq. (3.1) can rewritten companion form:\n\\[\\begin{equation}\ny_{t}^{*} =\n\\underbrace{\\left[\\begin{array}{c}\nc\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{=c^*}+\n\\underbrace{\\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]}_{=\\Phi}\ny_{t-1}^{*}+\n\\underbrace{\\left[\\begin{array}{c}\n\\varepsilon_{t}\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{\\varepsilon_t^*}\\tag{3.9}\n\\end{equation}\\]Matrices \\(\\Phi\\) \\(\\Sigma^* = \\mathbb{V}ar(\\varepsilon_t^*)\\) dimension \\(np \\times np\\). \\(\\Sigma^*\\) filled zeros, except \\(n\\times n\\) upper-left block equal \\(\\Sigma = \\mathbb{V}ar(\\varepsilon_t)\\).:\n\\[\\begin{eqnarray*}\ny_{t}^{*} & = & c^{*}+\\Phi\\left(c^{*}+\\Phi y_{t-2}^{*}+\\varepsilon_{t-1}^{*}\\right)+\\varepsilon_{t}^{*} \\nonumber \\\\\n& = & c^{*}+\\varepsilon_{t}^{*}+\\Phi(c^{*}+\\varepsilon_{t-1}^{*})+\\ldots+\\Phi^{k}(c^{*}+\\varepsilon_{t-k}^{*})+\\Phi^k y_{t-k}^{*}.\n\\end{eqnarray*}\\]eigenvalues \\(\\Phi\\) strictly within unit circle, \\(\\Phi^k\\) geometrically decays zero matrix get following Wold decomposition \\(y_t\\):\n\\[\\begin{eqnarray}\ny_{t}^{*}  & = & c^{*}+\\varepsilon_{t}^{*}+\\Phi(c^{*}+\\varepsilon_{t-1}^{*})+\\ldots+\\Phi^{k}(c^{*}+\\varepsilon_{t-k}^{*})+\\ldots \\nonumber \\\\\n& = & \\mu^{*} +\\varepsilon_{t}^{*}+\\Phi\\varepsilon_{t-1}^{*}+\\ldots+\\Phi^{k}\\varepsilon_{t-k}^{*}+\\ldots,\\tag{3.10}\n\\end{eqnarray}\\]\n\\(\\mu^* = (- \\Phi)^{-1} c^*\\).(can also seen \\(\\mu^{*} = [\\mu',\\dots,\\mu']'\\), \\(\\mu = (- \\Phi_1 - \\dots - \\Phi_p)^{-1}c\\)).unconditional variance \\(y_t\\) can derived Eq. (3.10), exploiting fact \\(\\varepsilon_{t}^{*}\\) serially uncorrelated:\n\\[\n\\mathbb{V}ar(y_t^*)=\\Omega^*+\\Phi\\Omega^*\\Phi'+\\ldots+\\Phi^{k}\\Omega^*\\Phi'^{k}+\\ldots,\n\\]\n\\(\\mathbb{V}ar(\\varepsilon_t^*)=\\Omega^*\\).unconditional variance \\(y_t\\) upper-left \\(n\\times n\\) block matrix \\(\\mathbb{V}ar(y_t^*)\\).Eq. (3.10) also implies \\(\\Psi_k\\) matrices defining IRFs (see Eq. (??)) given : \\(\\Psi_k = \\widetilde{\\Phi^k}B\\), \\(\\widetilde{\\Phi^k}\\) upper-left matrix block \\(\\Phi^k\\).Granger CausalityGranger (1969) developed method explore causal relationships among variables. approach consists determining whether past values \\(y_{1,t}\\) can help explain current \\(y_{2,t}\\) (beyond information already included past values \\(y_{2,t}\\)).Formally, let us denote three information sets:\n\\[\\begin{eqnarray*}\n\\mathcal{}_{1,t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{2,t} & = & \\left\\{ y_{2,t},y_{2,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots y_{2,t},y_{2,t-1},\\ldots\\right\\}.\n\\end{eqnarray*}\\]\nsay \\(y_{1,t}\\) Granger-causes \\(y_{2,t}\\) \n\\[\n\\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{2,t-1}\\right]\\neq \\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{t-1}\\right].\n\\]get intuition behind testing procedure, consider following\nbivariate VAR(\\(p\\)) process:\n\\[\\begin{eqnarray*}\ny_{1,t} & = & c_1+\\Sigma_{=1}^{p}\\Phi_i^{(11)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(12)}y_{2,t-}+\\varepsilon_{1,t}\\\\\ny_{2,t} & = & c_2+\\Sigma_{=1}^{p}\\Phi_i^{(21)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(22)}y_{2,t-}+\\varepsilon_{2,t},\n\\end{eqnarray*}\\]\n\\(\\Phi_k^{(ij)}\\) denotes element \\((,j)\\) \\(\\Phi_k\\)., \\(y_{1,t}\\) said Granger-cause \\(y_{2,t}\\) \n\\[\n\\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0.\n\\]\nTherefore hypothesis testing \n\\[\n\\begin{cases}\nH_{0}: & \\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0\\\\\nH_{1}: & \\Phi_1^{(21)}\\neq0\\mbox{ }\\Phi_2^{(21)}\\neq0\\mbox{ }\\ldots\\Phi_p^{(21)}\\neq0.\\end{cases}\n\\]\nLoosely speaking, reject \\(H_{0}\\) coefficients lagged \\(y_{1,t}\\)’s statistically significant. Formally, can tested using \\(F\\)-test asymptotic chi-square test. \\(F\\)-statistic \n\\[\nF=\\frac{(RSS-USS)/p}{USS/(T-2p-1)},\n\\]\nRSS Restricted sum squared residuals USS Unrestricted sum squared residuals. \\(H_{0}\\), \\(F\\)-statistic distributed \\(\\mathcal{F}(p,T-2p-1)\\).Note \\(pF\\underset{T \\rightarrow \\infty}{\\rightarrow}\\chi^{2}(p)\\). Therefore, large samples \\(H_0\\):\n\\[\nF \\sim \\chi^{2}(p)/p.\n\\]Factor-Augmented VAR (FAVAR)VAR models subject curse dimensionality: \\(n\\), large, number parameters (\\(n^2\\)) explodes.case one suspects \\(y_{,t}\\)’s mainly driven small number random sources, factor structure may imposed (Bernanke, Boivin, Eliasz (2005)).Let us denote \\(f_t\\) \\(k\\)-dimensional vector latent factors accounting important shares variances \\(y_{,t}\\)’s (\\(k \\ll n\\)) \\(x_t\\) small \\(q\\)-dimensional subset \\(y_t\\) (\\(q \\ll n\\)). following factor structure posited:\n\\[\ny_t = \\Lambda^f f_t + \\Lambda^x x_t + e_t,\n\\]\n\\(e_t\\) ``small’’ serially mutually ..d. error terms.model complemented positing VAR dynamics \\([f_t',x_t']'\\):\n\\[\\begin{equation}\n\\left[\\begin{array}{c}f_t\\\\x_t\\end{array}\\right] = \\Phi(L)\\left[\\begin{array}{c}f_{t-1}\\\\x_{t-1}\\end{array}\\right] + v_t.\\tag{3.11}\n\\end{equation}\\]\\(f_t\\) e.g. \\(\\equiv\\) first \\(k\\) principal components \\(y_t\\).Standard identification techniques structural shocks can employed Eq. (3.11): Cholesky approach can used instance last component \\(x_t\\) short-term interest rate assumed MP shock contemporaneous impact macro-variables (\\(y_t\\)).","code":""},{"path":"vector-auto-regressive-var-models.html","id":"identification-problem-and-standard-identification-techniques","chapter":"3 Vector Auto-Regressive (VAR) models","heading":"3.1 Identification Problem and Standard Identification Techniques","text":"Identification Issue*previous section, seen estimate \\(\\Omega\\) \\(\\Phi_k\\) matrices context VAR model. IRFs functions \\(B\\) \\(\\Phi_k\\)’s, \\(\\Omega\\) \\(\\Phi_k\\)’s. \\(\\Omega = BB'\\), sufficient recover \\(B\\).Indeed, seen system equations whose unknowns \\(b_{,j}\\)’s (components \\(B\\)), system \\(\\Omega = BB'\\) contains \\(n(n+1)/2\\) linearly independent equations. Example \\(n=2\\):\n\\[\\begin{eqnarray*}\n&&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{array}\n\\right]\\left[\n\\begin{array}{cc}\nb_{11} & b_{21} \\\\\nb_{12} & b_{22}\n\\end{array}\n\\right]\\\\\n&\\Leftrightarrow&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11}^2+b_{12}^2 & {\\color{red}b_{11}b_{21}+b_{12}b_{22}} \\\\\n{\\color{red}b_{11}b_{21}+b_{12}b_{22}} & b_{22}^2 + b_{21}^2\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]3 linearly independent equations 4 unknowns. Therefore, \\(B\\) identified based second-order moments. Additional restrictions required identify \\(B\\).section covers two standard identification schemes: short-run long-run restrictions:short-run restriction (SRR) prevents structural shock affecting endogenous variable contemporaneously.Easy implement: appropriate entries \\(B\\) set 0.Particular case: Cholesky, recursive approach.Examples: Bernanke (1986), Sims (1986), Galí (1992), Ruibio-Ramírez, Waggoner, Zha (2010).long-run restriction (LRR) prevents structural shock cumulative impact one endogenous variables.Additional computations required implement . One needs compute cumulative effect one structural shocks \\(u_{t}\\) one endogenous variable.Examples: Blanchard Quah (1989), Faust Leeper (1997), Galí (1999), Erceg, Guerrieri, Gust (2005), Christiano, Eichenbaum, Vigfusson (2007).two approaches can combined (see, e.g., Gerlach Smets (1995)).Simple ExampleConsider following stylized economic dynamics:\n\\[\\begin{equation}\n\\begin{array}{clll}\ng_{t}&=& \\bar{g}-\\lambda(i_{t-1}-\\mathbb{E}_{t-1}\\pi_{t})+ \\underbrace{{\\color{blue}\\sigma_d \\eta_{d,t}}}_{\\mbox{demand shock}}& (\\mbox{curve})\\\\\n\\Delta \\pi_{t} & = & \\beta (g_{t} - \\bar{g})+ \\underbrace{{\\color{blue}\\sigma_{\\pi} \\eta_{\\pi,t}}}_{\\mbox{cost push shock}} & (\\mbox{Phillips curve})\\\\\ni_{t} & = & \\rho i_{t-1} + \\left[ \\gamma_\\pi \\mathbb{E}_{t}\\pi_{t+1}  + \\gamma_g (g_{t} - \\bar{g}) \\right]\\\\\n&& \\qquad \\qquad+\\underbrace{{\\color{blue}\\sigma_{mp} \\eta_{mp,t}}}_{\\mbox{Mon. Pol. shock}} & (\\mbox{Taylor rule}),\n\\end{array}\\tag{3.12}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\eta_t =\n\\left[\n\\begin{array}{c}\n\\eta_{\\pi,t}\\\\\n\\eta_{d,t}\\\\\n\\eta_{mp,t}\n\\end{array}\n\\right]\n\\sim ..d.\\,\\mathcal{N}(0,).\\tag{3.13}\n\\end{equation}\\]Vector \\(\\eta_t\\) vector Gaussian {structural shocks}, mutually serially independent.date \\(t\\):\\(g_t\\) contemporaneously affected \\(\\eta_{d,t}\\) ;\\(\\pi_t\\) contemporaneously affected \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\);\\(i_t\\) contemporaneously affected \\(\\eta_{mp,t}\\), \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\).System (3.12) rewritten form:\n\\[\\begin{equation}\n\\left[\\begin{array}{c}\nd_t\\\\\n\\pi_t\\\\\ni_t\n\\end{array}\\right]\n= \\Phi(L)\n\\left[\\begin{array}{c}\nd_{t-1}\\\\\n\\pi_{t-1}\\\\\ni_{t-1} +\n\\end{array}\\right] +\\underbrace{\\underbrace{\n\\left[\n\\begin{array}{ccc}\n0 & \\bullet & 0 \\\\\n\\bullet & \\bullet & 0 \\\\\n\\bullet & \\bullet & \\bullet\n\\end{array}\n\\right]}_{=B} \\eta_t}_{=\\varepsilon_t}\\tag{3.14}\n\\end{equation}\\]reduced-form model. representation suggests three additional restrictions entries \\(B\\); latter matrix therefore identified (signs columns) soon \\(\\Omega = BB'\\) known.particular cases well-known matrix decompositions \\(\\mathbb{V}ar(\\varepsilon_t)=\\Omega_\\varepsilon\\) can used easily estimate specific SVAR.Consider following context:first shock (say, \\(\\eta_{n_1,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) one endogenous variable (say, \\(y_{n_1,t}\\));second shock (say, \\(\\eta_{n_2,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) first two endogenous variables (say, \\(y_{n_1,t}\\)\n\\(y_{n_2,t}\\));…implies (1) column \\(n_1\\) \\(B\\) 1 non-zero entry (\\(n_1^{th}\\) entry), (2) column \\(n_2\\) \\(B\\) 2 non-zero entries (\\(n_1^{th}\\) \\(n_2^{th}\\) ones), …Without loss generality, can set \\(n_1=n\\), \\(n_2=n-1\\), … context, matrix \\(B\\) lower triangular.Cholesky decomposition \\(\\Omega_{\\varepsilon}\\) provides appropriate estimate \\(B\\), lower triangular matrix \\(B\\) :\n\\[\n\\Omega_\\varepsilon = BB'.\n\\]instance, Dedola Lippi (2005) estimate 5 structural VAR models US, UK, Germany, France Italy analyse monetary-policy transmission mechanisms. estimate SVAR(5) models period 1975-1997. shock-identification scheme based Cholesky decompositions, ordering endogenous variables : industrial production, consumer price index, commodity price index, short-term rate, monetary aggregate effective exchange rate (except US). ordering implies monetary policy reacts shocks affecting\nfirst three variables latter react monetary policy shocks one-period lag.Importantly, Cholesky approach can useful essentially interested one structural shock. case, e.g., Christiano, Eichenbaum, Evans (1996). identification based following relationship \\(\\varepsilon_t\\) \\(\\eta_t\\):\n\\[\n\\left[\\begin{array}{c}\n\\boldsymbol\\varepsilon_{S,t}\\\\\n\\varepsilon_{r,t}\\\\\n\\boldsymbol\\varepsilon_{F,t}\n\\end{array}\\right] =\n\\left[\\begin{array}{ccc}\nB_{SS} & 0 & 0 \\\\\nB_{rS} & B_{rr} & 0 \\\\\nB_{FS} & B_{Fr} & B_{FF}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n\\boldsymbol\\eta_{S,t}\\\\\n\\eta_{r,t}\\\\\n\\boldsymbol\\eta_{F,t}\n\\end{array}\\right],\n\\]\n\\(S\\), \\(r\\) \\(F\\) respectively correspond slow-moving variables, policy variable (short-term rate) fast-moving variables. \\(\\eta_{r,t}\\) scalar, \\(\\boldsymbol\\eta_{S,t}\\) \\(\\boldsymbol\\eta_{F,t}\\) may vectors. space spanned \\(\\boldsymbol\\varepsilon_{S,t}\\) spanned \\(\\boldsymbol\\eta_{S,t}\\). result, \\(\\varepsilon_{r,t}\\) linear combination \\(\\eta_{r,t}\\) \\(\\boldsymbol\\eta_{S,t}\\) (\\(\\perp\\)), comes \\(B_{rr}\\eta_{r,t}\\)’s (population) residuals regression \\(\\varepsilon_{r,t}\\) \\(\\boldsymbol\\varepsilon_{S,t}\\). \\(\\mathbb{V}ar(\\eta_{r,t})=1\\), \\(B_{rr}\\) given square root variance \\(B_{rr}\\eta_{r,t}\\). \\(B_{F,r}\\) finally obtained regressing components \\(\\boldsymbol\\varepsilon_{F,t}\\) \\(\\eta_{r,t}\\).equivalent approach consists computing Cholesky decomposition \\(BB'\\) keep column corresponding policy variable.Long-run restrictionsA second type restriction relates long-run influence shock endogenous variable. Let us consider instance structural shock assumed “long-run influence” GDP. express ? long-run change GDP can expressed \\(GDP_{t+h} - GDP_t\\), \\(h\\) large. Note :\n\\[\nGDP_{t+h} - GDP_t = \\Delta GDP_{t+h} +\\Delta GDP_{t+h-1} + \\dots + \\Delta GDP_{t+1}.\n\\]\nHence, fact given structural shock (\\(\\eta_{,t}\\), say) long-run influence GDP means \n\\[\n\\lim_{h\\rightarrow\\infty}\\frac{\\partial GDP_{t+h}}{\\partial \\eta_{,t}} = \\lim_{h\\rightarrow\\infty} \\frac{\\partial}{\\partial \\eta_{,t}}\\left(\\sum_{k=1}^h \\Delta  GDP_{t+k}\\right)= 0.\n\\]can easily formulated function \\(B\\) \\(y_t\\) (including \\(\\Delta GDP_t\\)) follows VAR(MA) process.shown previously (Eq. (3.9)), one can always write VAR(\\(p\\)) VAR(1). Consequently, let us focus VAR(1) case:\n\\[\\begin{eqnarray}\ny_{t} &=& c+\\Phi y_{t-1}+\\varepsilon_{t}\\\\\n& = & c+\\varepsilon_{t}+\\Phi(c+\\varepsilon_{t-1})+\\ldots+\\Phi^{k}(c+\\varepsilon_{t-k})+\\ldots \\nonumber \\\\\n& = & \\mu +\\varepsilon_{t}+\\Phi\\varepsilon_{t-1}+\\ldots+\\Phi^{k}\\varepsilon_{t-k}+\\ldots \\\\\n& = & \\mu +B\\eta_{t}+\\Phi B\\eta_{t-1}+\\ldots+\\Phi^{k}B\\eta_{t-k}+\\ldots,\n\\end{eqnarray}\\]sequence shocks \\(\\{\\eta_t\\}\\) determines sequence \\(\\{y_t\\}\\). \\(\\{\\eta_t\\}\\) replaced \\(\\{\\tilde{\\eta}_t\\}\\), \\(\\tilde{\\eta}_t=\\eta_t\\) \\(t \\ne s\\) \\(\\tilde{\\eta}_s=\\eta_s + \\gamma\\)?Assume \\(\\{\\tilde{y}_t\\}\\) associated “perturbated” sequence. \\(\\tilde{y}_t = y_t\\) \\(t<s\\). \\(t \\ge s\\), Wold decomposition \\(\\{\\tilde{y}_t\\}\\) implies:\n\\[\n\\tilde{y}_t = y_t + \\Phi^{t-s} B \\gamma.\n\\]Therefore, cumulative impact \\(\\gamma\\) \\(\\tilde{y}_t\\) (\\(t \\ge s\\)):\n\\[\\begin{eqnarray}\n(\\tilde{y}_t - y_t) +  (\\tilde{y}_{t-1} - y_{t-1}) + \\dots +  (\\tilde{y}_s - y_s) &=& \\nonumber \\\\\n(Id + \\Phi + \\Phi^2 + \\dots + \\Phi^{t-s}) B \\gamma.&& \\tag{3.15}\n\\end{eqnarray}\\]Consider shock \\(\\eta_{1,t}\\), magnitude \\(1\\). shock \\(\\gamma = [1,0,\\dots,0]'\\). Given Eq. @ref{eq:cumul), long-run cumulative effect shock endogenous variables given :\n\\[\n(Id+\\Phi+\\ldots+\\Phi^{k}+\\ldots)B\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right],\n\\]\nfirst column \\(\\Theta \\equiv (+\\Phi+\\ldots+\\Phi^{k}+\\ldots)B\\).context, consider following long-run restriction: “\\(j^{th}\\) structural shock cumulative impact \\(^{th}\\) endogenous variable” equivalent \n\\[\n\\Theta_{ij}=0,\n\\]\n\\(\\Theta_{ij}\\) element \\((,j)\\) \\(\\Theta\\).Blanchard Quah (1989) implement long-run restrictions small-scale VAR. Two variables considered: GDP unemployment. Consequently, VAR affected two types shocks. authors want identify supply shocks (can permanent effect output) demand shocks (permanent effect output).motivation authors regarding long-run restrictions can obtained traditional Keynesian view fluctuations. authors propose variant model Fischer (1977):\n\\[\\begin{eqnarray}\nY_{t} & = & M_{t}-P_{t}+.\\theta_{t}\\tag{3.16}\\\\\nY_{t} & = & N_{t}+\\theta_{t}\\tag{3.17}\\\\\nP_{t} & = & W_{t}-\\theta_{t}\\tag{3.18}\\\\\nW_{t} & = & W\\mid\\left\\{ \\mathbb{E}_{t-1}N_{t}=\\overline{N}\\right\\}. \\tag{3.19}\n\\end{eqnarray}\\]\nclose model, authors assume following dynamics money supply productivity:\n\\[\\begin{eqnarray*}\nM_{t} & = & M_{t-1}+\\varepsilon_{t}^{d}\\\\\n\\theta_{t} & = & \\theta_{t-1}+\\varepsilon_{t}^{s}.\n\\end{eqnarray*}\\]\ncontext, can shown \n\\[\\begin{eqnarray*}\n\\Delta Y_{t} & = & (\\varepsilon_{t}^{d}-\\varepsilon_{t-1}^{d})+.(\\varepsilon_{t}^{s}-\\varepsilon_{t-1}^{s})+\\varepsilon_{t}^{s}\\\\\nu_{t} & = & -\\varepsilon_{t}^{d}-\\varepsilon_{t}^{s}\n\\end{eqnarray*}\\]\n, appears demand shocks long-run cumulative impact \\(\\Delta Y_{t}\\), GDP growth, .e. long-term impact output \\(Y_t\\). vector endogenous variables \\(y_t = [\\Delta Y_{t} \\quad u_{t}]'\\) \\(\\Delta Y_{t}\\) denotes GDP growth. Estimation data quarterly, span period 1950:2 1987:4; 8 lags used VAR model.","code":""},{"path":"appendix.html","id":"appendix","chapter":"4 Appendix","heading":"4 Appendix","text":"","code":""},{"path":"appendix.html","id":"statistical-tables","chapter":"4 Appendix","heading":"4.1 Statistical Tables","text":"Table 4.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 4.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 4.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 4.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"appendix.html","id":"statistics-definitions-and-results","chapter":"4 Appendix","heading":"4.2 Statistics: definitions and results","text":"Definition 4.1  (Skewness kurtosis) Let \\(Y\\) random variable whose fourth moment exists. expectation \\(Y\\) denoted \\(\\mu\\).\\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^3]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{3/2}}.\n\\]\\(Y\\) given :\n\\[\n\\frac{\\mathbb{E}[(Y-\\mu)^4]}{\\{\\mathbb{E}[(Y-\\mu)^2]\\}^{2}}.\n\\]Definition 4.2  (Eigenvalues) eigenvalues matrix \\(M\\) numbers \\(\\lambda\\) :\n\\[\n|M - \\lambda | = 0,\n\\]\n\\(| \\bullet |\\) determinant operator.Proposition 4.1  (Properties determinant) :\\(|MN|=|M|\\times|N|\\).\\(|M^{-1}|=|M|^{-1}\\).\\(M\\) admits diagonal representation \\(M=TDT^{-1}\\), \\(D\\) diagonal matrix whose diagonal entries \\(\\{\\lambda_i\\}_{=1,\\dots,n}\\), :\n\\[\n|M - \\lambda |=\\prod_{=1}^n (\\lambda_i - \\lambda).\n\\]Definition 4.3  (Moore-Penrose inverse) \\(M \\\\mathbb{R}^{m \\times n}\\), Moore-Penrose pseudo inverse (exists ) unique matrix \\(M^* \\\\mathbb{R}^{n \\times m}\\) satisfies:\\(M M^* M = M\\)\\(M^* M M^* = M^*\\)\\((M M^*)'=M M^*\\)\n.iv \\((M^* M)'=M^* M\\).Proposition 4.2  (Properties Moore-Penrose inverse) \\(M\\) invertible \\(M^* = M^{-1}\\).pseudo-inverse zero matrix transpose.\n*\npseudo-inverse pseudo-inverse original matrix.Definition 4.4  (F distribution) Consider \\(n=n_1+n_2\\) ..d. \\(\\mathcal{N}(0,1)\\) r.v. \\(X_i\\). r.v. \\(F\\) defined :\n\\[\nF = \\frac{\\sum_{=1}^{n_1} X_i^2}{\\sum_{j=n_1+1}^{n_1+n_2} X_j^2}\\frac{n_2}{n_1}\n\\]\n\\(F \\sim \\mathcal{F}(n_1,n_2)\\). (See Table 4.4 quantiles.)Definition 4.5  (Student-t distribution) \\(Z\\) follows Student-t (\\(t\\)) distribution \\(\\nu\\) degrees freedom (d.f.) :\n\\[\nZ = X_0 \\bigg/ \\sqrt{\\frac{\\sum_{=1}^{\\nu}X_i^2}{\\nu}}, \\quad X_i \\sim ..d. \\mathcal{N}(0,1).\n\\]\n\\(\\mathbb{E}(Z)=0\\), \\(\\mathbb{V}ar(Z)=\\frac{\\nu}{\\nu-2}\\) \\(\\nu>2\\). (See Table 4.2 quantiles.)Definition 4.6  (Chi-square distribution) \\(Z\\) follows \\(\\chi^2\\) distribution \\(\\nu\\) d.f. \\(Z = \\sum_{=1}^{\\nu}X_i^2\\) \\(X_i \\sim ..d. \\mathcal{N}(0,1)\\).\n\\(\\mathbb{E}(Z)=\\nu\\). (See Table 4.3 quantiles.)Definition 4.7  (Idempotent matrix) Matrix \\(M\\) idempotent \\(M^2=M\\).\\(M\\) symmetric idempotent matrix, \\(M'M=M\\).Proposition 4.3  (Roots idempotent matrix) eigenvalues idempotent matrix either 1 0.Proof. \\(\\lambda\\) eigenvalue idempotent matrix \\(M\\) \\(\\exists x \\ne 0\\) s.t. \\(Mx=\\lambda x\\). Hence \\(M^2x=\\lambda M x \\Rightarrow (1-\\lambda)Mx=0\\). Either element \\(Mx\\) zero, case \\(\\lambda=0\\) least one element \\(Mx\\) nonzero, case \\(\\lambda=1\\).Proposition 4.4  (Idempotent matrix chi-square distribution) rank symmetric idempotent matrix equal trace.Proof. result follows Prop. 4.3, combined fact rank symmetric matrix equal number nonzero eigenvalues.Proposition 4.5  (Constrained least squares) solution following optimisation problem:\n\\[\\begin{eqnarray*}\n\\underset{\\boldsymbol\\beta}{\\min} && || \\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta ||^2 \\\\\n&& \\mbox{subject } \\mathbf{R}\\boldsymbol\\beta = \\mathbf{q}\n\\end{eqnarray*}\\]\ngiven :\n\\[\n\\boxed{\\boldsymbol\\beta^r = \\boldsymbol\\beta_0 - (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{R}'\\{\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\}^{-1}(\\mathbf{R}\\boldsymbol\\beta_0 - \\mathbf{q}),}\n\\]\n\\(\\boldsymbol\\beta_0=(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\).Proof. See instance Jackman, 2007.Proposition 4.6  (Chebychev's inequality) \\(\\mathbb{E}(|X|^r)\\) finite \\(r>0\\) :\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[|X - c|^r]}{\\varepsilon^r}.\n\\]\nparticular, \\(r=2\\):\n\\[\n\\forall \\varepsilon > 0, \\quad \\mathbb{P}(|X - c|>\\varepsilon) \\le \\frac{\\mathbb{E}[(X - c)^2]}{\\varepsilon^2}.\n\\]Proof. Remark \\(\\varepsilon^r \\mathbb{}_{\\{|X| \\ge \\varepsilon\\}} \\le |X|^r\\) take expectation sides.Definition 4.8  (Convergence probability) random variable sequence \\(x_n\\) converges probability constant \\(c\\) \\(\\forall \\varepsilon\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|x_n - c|>\\varepsilon) = 0\\).denoted : \\(\\mbox{plim } x_n = c\\).Definition 4.9  (Convergence Lr norm) \\(x_n\\) converges \\(r\\)-th mean (\\(L^r\\)-norm) towards \\(x\\), \\(\\mathbb{E}(|x_n|^r)\\) \\(\\mathbb{E}(|x|^r)\\) exist \n\\[\n\\lim_{n \\rightarrow \\infty} \\mathbb{E}(|x_n - x|^r) = 0.\n\\]\ndenoted : \\(x_n \\overset{L^r}{\\rightarrow} c\\).\\(r=2\\), convergence called mean square convergence.Definition 4.10  (Almost sure convergence) random variable sequence \\(x_n\\) converges almost surely \\(c\\) \\(\\mathbb{P}(\\lim_{n \\rightarrow \\infty} x_n = c) = 1\\).denoted : \\(x_n \\overset{.s.}{\\rightarrow} c\\).Definition 4.11  (Convergence distribution) \\(x_n\\) said converge distribution (law) \\(x\\) \n\\[\n\\lim_{n \\rightarrow \\infty} F_{x_n}(s) = F_{x}(s)\n\\]\n\\(s\\) \\(F_X\\) –cumulative distribution \\(X\\)– continuous.denoted : \\(x_n \\overset{d}{\\rightarrow} x\\).Proposition 4.7  (Rules limiting distributions (Slutsky)) :Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Slutsky’s theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(y_n \\overset{p}{\\rightarrow} c\\) \n\\[\\begin{eqnarray*}\nx_n y_n &\\overset{d}{\\rightarrow}& x c \\\\\nx_n + y_n &\\overset{d}{\\rightarrow}& x + c \\\\\nx_n/y_n &\\overset{d}{\\rightarrow}& x / c \\quad (\\mbox{}c \\ne 0)\n\\end{eqnarray*}\\]Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Continuous mapping theorem: \\(x_n \\overset{d}{\\rightarrow} x\\) \\(g\\) continuous function \\(g(x_n) \\overset{d}{\\rightarrow} g(x).\\)Proposition 4.8  (Implications stochastic convergences) :\n\\[\\begin{align*}\n&\\boxed{\\overset{L^s}{\\rightarrow}}& &\\underset{1 \\le r \\le s}{\\Rightarrow}& &\\boxed{\\overset{L^r}{\\rightarrow}}&\\\\\n&& && &\\Downarrow&\\\\\n&\\boxed{\\overset{.s.}{\\rightarrow}}& &\\Rightarrow& &\\boxed{\\overset{p}{\\rightarrow}}& \\Rightarrow \\qquad \\boxed{\\overset{d}{\\rightarrow}}.\n\\end{align*}\\]Proof. (fact \\(\\left(\\overset{p}{\\rightarrow}\\right) \\Rightarrow \\left( \\overset{d}{\\rightarrow}\\right)\\)). Assume \\(X_n \\overset{p}{\\rightarrow} X\\). Denoting \\(F\\) \\(F_n\\) c.d.f. \\(X\\) \\(X_n\\), respectively:\n\\[\\begin{equation}\nF_n(x) = \\mathbb{P}(X_n \\le x,X\\le x+\\varepsilon) + \\mathbb{P}(X_n \\le x,X > x+\\varepsilon) \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\\tag{4.1}\n\\end{equation}\\]\nBesides,\n\\[\nF(x-\\varepsilon) = \\mathbb{P}(X \\le x-\\varepsilon,X_n \\le x) + \\mathbb{P}(X \\le x-\\varepsilon,X_n > x) \\le F_n(x) + \\mathbb{P}(|X_n - X|>\\varepsilon),\n\\]\nimplies:\n\\[\\begin{equation}\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x).\\tag{4.2}\n\\end{equation}\\]\nEqs. (4.1) (4.2) imply:\n\\[\nF(x-\\varepsilon) - \\mathbb{P}(|X_n - X|>\\varepsilon) \\le F_n(x)  \\le F(x+\\varepsilon) + \\mathbb{P}(|X_n - X|>\\varepsilon).\n\\]\nTaking limits \\(n \\rightarrow \\infty\\) yields\n\\[\nF(x-\\varepsilon) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim inf}}\\; F_n(x) \\le \\underset{n \\rightarrow \\infty}{\\mbox{lim sup}}\\; F_n(x)  \\le F(x+\\varepsilon).\n\\]\nresult obtained taking limits \\(\\varepsilon \\rightarrow 0\\) (\\(F\\) continuous \\(x\\)).Proposition 4.9  (Convergence distribution constant) \\(X_n\\) converges distribution constant \\(c\\), \\(X_n\\) converges probability \\(c\\).Proof. \\(\\varepsilon>0\\), \\(\\mathbb{P}(X_n < c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\\) .e. \\(\\mathbb{P}(X_n \\ge c - \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\) \\(\\mathbb{P}(X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\). Therefore \\(\\mathbb{P}(c - \\varepsilon \\le X_n < c + \\varepsilon) \\underset{n \\rightarrow \\infty}{\\rightarrow} 1\\),\ngives result.Example \\(plim\\) \\(L^r\\) convergence: Let \\(\\{x_n\\}_{n \\\\mathbb{N}}\\) series random variables defined :\n\\[\nx_n = n u_n,\n\\]\n\\(u_n\\) independent random variables s.t. \\(u_n \\sim \\mathcal{B}(1/n)\\).\\(x_n \\overset{p}{\\rightarrow} 0\\) \\(x_n \\overset{L^r}{\\nrightarrow} 0\\) \\(\\mathbb{E}(|X_n-0|)=\\mathbb{E}(X_n)=1\\).Theorem 4.1  (Cauchy criterion (non-stochastic case)) \\(\\sum_{=0}^{T} a_i\\) converges (\\(T \\rightarrow \\infty\\)) iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\left|\\sum_{=N+1}^{M} a_i\\right| < \\eta.\n\\]Theorem 4.2  (Cauchy criterion (stochastic case)) \\(\\sum_{=0}^{T} \\theta_i \\varepsilon_{t-}\\) converges mean square (\\(T \\rightarrow \\infty\\)) random variable iff, \\(\\eta > 0\\), exists integer \\(N\\) , \\(M\\ge N\\),\n\\[\n\\mathbb{E}\\left[\\left(\\sum_{=N+1}^{M} \\theta_i \\varepsilon_{t-}\\right)^2\\right] < \\eta.\n\\]Definition 4.12  (Characteristic function) real-valued random variable \\(X\\), characteristic function defined :\n\\[\n\\phi_X: u \\rightarrow \\mathbb{E}[\\exp(iuX)].\n\\]Theorem 4.3  (Law large numbers) sample mean consistent estimator population mean.Proof. Let’s denote \\(\\phi_{X_i}\\) characteristic function r.v. \\(X_i\\). mean \\(X_i\\) \\(\\mu\\) Talyor expansion characteristic function :\n\\[\n\\phi_{X_i}(u) = \\mathbb{E}(\\exp(iuX)) = 1 + iu\\mu + o(u).\n\\]\nproperties characteristic function (see Def. 4.12) imply :\n\\[\n\\phi_{\\frac{1}{n}(X_1+\\dots+X_n)}(u) = \\prod_{=1}^{n} \\left(1 + \\frac{u}{n}\\mu + o\\left(\\frac{u}{n}\\right) \\right) \\rightarrow e^{iu\\mu}.\n\\]\nfacts () \\(e^{iu\\mu}\\) characteristic function constant \\(\\mu\\) (b) characteristic function uniquely characterises distribution imply sample mean converges distribution constant \\(\\mu\\), implies converges probability \\(\\mu\\).Theorem 4.4  (Lindberg-Levy Central limit theorem, CLT) \\(x_n\\) ..d. sequence random variables mean \\(\\mu\\) variance \\(\\sigma^2\\) (\\(\\]0,+\\infty[\\)), :\n\\[\n\\boxed{\\sqrt{n} (\\bar{x}_n - \\mu) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma^2), \\quad \\mbox{} \\quad \\bar{x}_n = \\frac{1}{n} \\sum_{=1}^{n} x_i.}\n\\]Proof. Let us introduce r.v. \\(Y_n:= \\sqrt{n}(\\bar{X}_n - \\mu)\\). \\(\\phi_{Y_n}(u) = \\left[ \\mathbb{E}\\left( \\exp(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)) \\right) \\right]^n\\). :\n\\[\\begin{eqnarray*}\n\\left[ \\mathbb{E}\\left( \\exp\\left(\\frac{1}{\\sqrt{n}} u (X_1 - \\mu)\\right) \\right) \\right]^n &=& \\left[ \\mathbb{E}\\left( 1 + \\frac{1}{\\sqrt{n}} u (X_1 - \\mu) - \\frac{1}{2n} u^2 (X_1 - \\mu)^2 + o(u^2) \\right) \\right]^n \\\\\n&=& \\left( 1 - \\frac{1}{2n}u^2\\sigma^2 + o(u^2)\\right)^n.\n\\end{eqnarray*}\\]\nTherefore \\(\\phi_{Y_n}(u) \\underset{n \\rightarrow \\infty}{\\rightarrow} \\exp \\left( - \\frac{1}{2}u^2\\sigma^2 \\right)\\), characteristic function \\(\\mathcal{N}(0,\\sigma^2)\\).Proposition 4.10  (Inverse partitioned matrix) :\n\\[\\begin{eqnarray*}\n&&\\left[ \\begin{array}{cc} \\mathbf{}_{11} & \\mathbf{}_{12} \\\\ \\mathbf{}_{21} & \\mathbf{}_{22} \\end{array}\\right]^{-1} = \\\\\n&&\\left[ \\begin{array}{cc} (\\mathbf{}_{11} - \\mathbf{}_{12}\\mathbf{}_{22}^{-1}\\mathbf{}_{21})^{-1} & - \\mathbf{}_{11}^{-1}\\mathbf{}_{12}(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\\\\n-(\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1}\\mathbf{}_{21}\\mathbf{}_{11}^{-1} & (\\mathbf{}_{22} - \\mathbf{}_{21}\\mathbf{}_{11}^{-1}\\mathbf{}_{12})^{-1} \\end{array} \\right].\n\\end{eqnarray*}\\]Proposition 4.11  \\(\\mathbf{}\\) idempotent \\(\\mathbf{x}\\) Gaussian, \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}\\) independent \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\).Proof. \\(\\mathbf{L}\\mathbf{}=\\mathbf{0}\\), two Gaussian vectors \\(\\mathbf{L}\\mathbf{x}\\) \\(\\mathbf{}\\mathbf{x}\\) independent. implies independence function \\(\\mathbf{L}\\mathbf{x}\\) function \\(\\mathbf{}\\mathbf{x}\\). results follows observation \\(\\mathbf{x}'\\mathbf{}\\mathbf{x}=(\\mathbf{}\\mathbf{x})'(\\mathbf{}\\mathbf{x})\\), function \\(\\mathbf{}\\mathbf{x}\\).Proposition 4.12  (Inner product multivariate Gaussian variable) Let \\(X\\) \\(n\\)-dimensional multivariate Gaussian variable: \\(X \\sim \\mathcal{N}(0,\\Sigma)\\). :\n\\[\nX' \\Sigma^{-1}X \\sim \\chi^2(n).\n\\]Proof. \\(\\Sigma\\) symmetrical definite positive matrix, admits spectral decomposition \\(PDP'\\) \\(P\\) orthogonal matrix (.e. \\(PP'=Id\\)) D diagonal matrix non-negative entries. Denoting \\(\\sqrt{D^{-1}}\\) diagonal matrix whose diagonal entries inverse \\(D\\), easily checked covariance matrix \\(Y:=\\sqrt{D^{-1}}P'X\\) \\(Id\\). Therefore \\(Y\\) vector uncorrelated Gaussian variables. properties Gaussian variables imply components \\(Y\\) also independent. Hence \\(Y'Y=\\sum_i Y_i^2 \\sim \\chi^2(n)\\).remains note \\(Y'Y=X'PD^{-1}P'X=X'\\mathbb{V}ar(X)^{-1}X\\) conclude.Theorem 4.5  (Cauchy-Schwarz inequality) :\n\\[\n|\\mathbb{C}ov(X,Y)| \\le \\sqrt{\\mathbb{V}ar(X)\\mathbb{V}ar(Y)}\n\\]\n, \\(X \\ne =\\) \\(Y \\ne 0\\), equality holds iff \\(X\\) \\(Y\\) affine transformation.Proof. \\(\\mathbb{V}ar(X)=0\\), trivial. case, let’s define \\(Z\\) \\(Z = Y - \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\). easily seen \\(\\mathbb{C}ov(X,Z)=0\\). , variance \\(Y=Z+\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\) equal sum variance \\(Z\\) variance \\(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X\\), :\n\\[\n\\mathbb{V}ar(Y) = \\mathbb{V}ar(Z) + \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X) \\ge \\left(\\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}\\right)^2\\mathbb{V}ar(X).\n\\]\nequality holds iff \\(\\mathbb{V}ar(Z)=0\\), .e. iff \\(Y = \\frac{\\mathbb{C}ov(X,Y)}{\\mathbb{V}ar(X)}X+cst\\).Definition 4.13  (Matrix derivatives) Consider fonction \\(f: \\mathbb{R}^K \\rightarrow \\mathbb{R}\\). first-order derivative :\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) =\n\\left[\\begin{array}{c}\n\\frac{\\partial f}{\\partial b_1}(\\mathbf{b})\\\\\n\\vdots\\\\\n\\frac{\\partial f}{\\partial b_K}(\\mathbf{b})\n\\end{array}\n\\right].\n\\]\nuse notation:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{b}'}(\\mathbf{b}) = \\left(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b})\\right)'.\n\\]Proposition 4.13  :\\(f(\\mathbf{b}) = ' \\mathbf{b}\\) \\(\\) \\(K \\times 1\\) vector \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = \\).\\(f(\\mathbf{b}) = \\mathbf{b}'\\mathbf{b}\\) \\(\\) \\(K \\times K\\) matrix, \\(\\frac{\\partial f}{\\partial \\mathbf{b}}(\\mathbf{b}) = 2A\\mathbf{b}\\).Definition 4.14  (Asymptotic level) asymptotic test critical region \\(\\Omega_n\\) equal \\(\\alpha\\) :\n\\[\n\\underset{\\theta \\\\Theta}{\\mbox{sup}} \\quad \\underset{n \\rightarrow \\infty}{\\mbox{lim}} \\mathbb{P}_\\theta (S_n \\\\Omega_n) = \\alpha,\n\\]\n\\(S_n\\) test statistic \\(\\Theta\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\\\Theta\\).Definition 4.15  (Asymptotically consistent test) asymptotic test critical region \\(\\Omega_n\\) consistent :\n\\[\n\\forall \\theta \\\\Theta^c, \\quad \\mathbb{P}_\\theta (S_n \\\\Omega_n) \\rightarrow 1,\n\\]\n\\(S_n\\) test statistic \\(\\Theta^c\\) null hypothesis \\(H_0\\) equivalent \\(\\theta \\notin \\Theta^c\\).Definition 4.16  (Kullback discrepancy) Given two p.d.f. \\(f\\) \\(f^*\\), Kullback discrepancy defined :\n\\[\n(f,f^*) = \\mathbb{E}^* \\left( \\log \\frac{f^*(Y)}{f(Y)} \\right) = \\int \\log \\frac{f^*(y)}{f(y)} f^*(y) dy.\n\\]Proposition 4.14  (Properties Kullback discrepancy) :\\((f,f^*) \\ge 0\\)\\((f,f^*) \\ge 0\\)\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).\\((f,f^*) = 0\\) iff \\(f \\equiv f^*\\).Proof. \\(x \\rightarrow -\\log(x)\\) convex function. Therefore \\(\\mathbb{E}^*(-\\log f(Y)/f^*(Y)) \\ge -\\log \\mathbb{E}^*(f(Y)/f^*(Y)) = 0\\) (proves ()). Since \\(x \\rightarrow -\\log(x)\\) strictly convex, equality () holds \\(f(Y)/f^*(Y)\\) constant (proves (ii)).Proposition 4.15  (Square absolute summability) :\n\\[\n\\underbrace{\\sum_{=0}^{\\infty}|\\theta_i| < + \\infty}_{\\mbox{Absolute summability}} \\Rightarrow \\underbrace{\\sum_{=0}^{\\infty} \\theta_i^2 < + \\infty}_{\\mbox{Square summability}}.\n\\]Proof. See Appendix 3.Hamilton. Idea: Absolute summability implies exist \\(N\\) , \\(j>N\\), \\(|\\theta_j| < 1\\) (deduced Cauchy criterion, Theorem 4.1 therefore \\(\\theta_j^2 < |\\theta_j|\\).","code":""},{"path":"appendix.html","id":"GaussianVar","chapter":"4 Appendix","heading":"4.3 Some properties of Gaussian variables","text":"Proposition 4.16  (Bayesian update vector Gaussian variables) \n\\[\n\\left[\n\\begin{array}{c}\nY_1\\\\\nY_2\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\n\\left(0,\n\\left[\\begin{array}{cc}\n\\Omega_{11} & \\Omega_{12}\\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right]\n\\right),\n\\]\n\n\\[\nY_{2}|Y_{1} \\sim \\mathcal{N}\n\\left(\n\\Omega_{21}\\Omega_{11}^{-1}Y_{1},\\Omega_{22}-\\Omega_{21}\\Omega_{11}^{-1}\\Omega_{12}\n\\right).\n\\]\n\\[\nY_{1}|Y_{2} \\sim \\mathcal{N}\n\\left(\n\\Omega_{12}\\Omega_{22}^{-1}Y_{2},\\Omega_{11}-\\Omega_{12}\\Omega_{22}^{-1}\\Omega_{21}\n\\right).\n\\]Proposition 4.17  (Truncated distributions) \\(X\\) random variable distributed according p.d.f. \\(f\\), c.d.f. \\(F\\), infinite support. p.d.f. \\(X|\\le X < b\\) \n\\[\ng(x) = \\frac{f(x)}{F(b)-F()}\\mathbb{}_{\\{\\le x < b\\}},\n\\]\n\\(<b\\).partiucular, Gaussian variable \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), \n\\[\nf(X=x|\\le X<b) = \\dfrac{\\dfrac{1}{\\sigma}\\phi\\left(\\dfrac{x - \\mu}{\\sigma}\\right)}{Z}.\n\\]\n\\(Z = \\Phi(\\beta)-\\Phi(\\alpha)\\), \\(\\alpha = \\dfrac{- \\mu}{\\sigma}\\) \\(\\beta = \\dfrac{b - \\mu}{\\sigma}\\).Moreover:\n\\[\\begin{eqnarray}\n\\mathbb{E}(X|\\le X<b) &=& \\mu - \\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\sigma. \\tag{4.3}\n\\end{eqnarray}\\]also :\n\\[\\begin{eqnarray}\n&& \\mathbb{V}ar(X|\\le X<b) \\nonumber\\\\\n&=& \\sigma^2\\left[\n1 -  \\frac{\\beta\\phi\\left(\\beta\\right)-\\alpha\\phi\\left(\\alpha\\right)}{Z} -  \\left(\\frac{\\phi\\left(\\beta\\right)-\\phi\\left(\\alpha\\right)}{Z}\\right)^2 \\right] \\tag{4.4}\n\\end{eqnarray}\\]particular, \\(b \\rightarrow \\infty\\), get:\n\\[\\begin{equation}\n\\mathbb{V}ar(X|< X) = \\sigma^2\\left[1 + \\alpha\\lambda(-\\alpha) - \\lambda(-\\alpha)^2 \\right], \\tag{4.5}\n\\end{equation}\\]\n\\(\\lambda(x)=\\dfrac{\\phi(x)}{\\Phi(x)}\\) called inverse Mills ratio.Consider case \\(\\rightarrow - \\infty\\) (.e. conditioning set \\(X<b\\)) \\(\\mu=0\\), \\(\\sigma=1\\). Eq. (4.3) gives \\(\\mathbb{E}(X|X<b) = - \\lambda(b) = - \\dfrac{\\phi(b)}{\\Phi(b)}\\), \\(\\lambda\\) function computing inverse Mills ratio.\nFigure 4.1: \\(\\mathbb{E}(X|X<b)\\) function \\(b\\) \\(X\\sim \\mathcal{N}(0,1)\\) (black).\nProposition 4.18  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"appendix.html","id":"proofs","chapter":"4 Appendix","heading":"4.4 Proofs","text":"","code":""},{"path":"appendix.html","id":"MLEproperties","chapter":"4 Appendix","heading":"4.4.1 Proof of Proposition ??","text":"Proof. Assumptions () (ii) (set Assumptions ??) imply \\(\\boldsymbol\\theta_{MLE}\\) exists (\\(=\\mbox{argmax}_\\theta (1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\)).\\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) can interpreted sample mean r.v. \\(\\log f(Y_i;\\boldsymbol\\theta)\\) ..d. Therefore \\((1/n)\\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})\\) converges \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) – exists (Assumption iv).latter convergence uniform (Assumption v), solution \\(\\boldsymbol\\theta_{MLE}\\) almost surely converges solution limit problem:\n\\[\n\\mbox{argmax}_\\theta \\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta)) = \\mbox{argmax}_\\theta \\int_{\\mathcal{Y}} \\log f(y;\\boldsymbol\\theta)f(y;\\boldsymbol\\theta_0) dy.\n\\]Properties Kullback information measure (see Prop. 4.14), together identifiability assumption (ii) implies solution limit problem unique equal \\(\\boldsymbol\\theta_0\\).Consider r.v. sequence \\(\\boldsymbol\\theta\\) converges \\(\\boldsymbol\\theta_0\\). Taylor expansion score neighborood \\(\\boldsymbol\\theta_0\\) yields :\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} + \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta - \\boldsymbol\\theta_0) + o_p(\\boldsymbol\\theta - \\boldsymbol\\theta_0)\n\\]\\(\\boldsymbol\\theta_{MLE}\\) converges \\(\\boldsymbol\\theta_0\\) satisfies likelihood equation \\(\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\mathbf{y})}{\\partial \\boldsymbol\\theta} = \\mathbf{0}\\). Therefore:\n\\[\n\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx - \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]\nequivalently:\n\\[\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right)\\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_0),\n\\]law large numbers, : \\(\\left(- \\frac{1}{n} \\sum_{=1}^n \\frac{\\partial^2 \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta'} \\right) \\overset{}\\rightarrow \\frac{1}{n} \\mathbf{}(\\boldsymbol\\theta_0) = \\mathcal{}_Y(\\boldsymbol\\theta_0)\\).Besides, :\n\\[\\begin{eqnarray*}\n\\frac{1}{\\sqrt{n}} \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} &=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right) \\\\\n&=& \\sqrt{n} \\left( \\frac{1}{n} \\sum_i \\left\\{ \\frac{\\partial \\log f(y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} - \\mathbb{E}_{\\boldsymbol\\theta_0} \\frac{\\partial \\log f(Y_i;\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta} \\right\\} \\right)\n\\end{eqnarray*}\\]\nconverges \\(\\mathcal{N}(0,\\mathcal{}_Y(\\boldsymbol\\theta_0))\\) CLT.Collecting preceding results leads (b). fact \\(\\boldsymbol\\theta_{MLE}\\) achieves FDCR bound proves (c).","code":""},{"path":"appendix.html","id":"Walddistri","chapter":"4 Appendix","heading":"4.4.2 Proof of Proposition ??","text":"Proof. \\(\\sqrt{n}(\\hat{\\boldsymbol\\theta}_{n} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{}(\\boldsymbol\\theta_0)^{-1})\\) (Eq. (eq:normMLE)). Taylor expansion around \\(\\boldsymbol\\theta_0\\) yields :\n\\[\\begin{equation}\n\\sqrt{n}(h(\\hat{\\boldsymbol\\theta}_{n}) - h(\\boldsymbol\\theta_{0})) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{4.6}\n\\end{equation}\\]\n\\(H_0\\), \\(h(\\boldsymbol\\theta_{0})=0\\) therefore:\n\\[\\begin{equation}\n\\sqrt{n} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\\right). \\tag{4.7}\n\\end{equation}\\]\nHence\n\\[\n\\sqrt{n} \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1/2} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,Id\\right).\n\\]\nTaking quadratic form, obtain:\n\\[\nn h(\\hat{\\boldsymbol\\theta}_{n})'  \\left(\n\\frac{\\partial h(\\boldsymbol\\theta_{0})}{\\partial \\boldsymbol\\theta'}\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{\\partial h(\\boldsymbol\\theta_{0})'}{\\partial \\boldsymbol\\theta}\n\\right)^{-1} h(\\hat{\\boldsymbol\\theta}_{n}) \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]fact test asymptotic level \\(\\alpha\\) directly stems precedes. Consistency test: Consider \\(\\theta_0 \\\\Theta\\). MLE consistent, \\(h(\\hat{\\boldsymbol\\theta}_{n})\\) converges \\(h(\\boldsymbol\\theta_0) \\ne 0\\). Eq. (4.6) still valid. implies \\(\\xi^W_n\\) converges \\(+\\infty\\) therefore \\(\\mathbb{P}_{\\boldsymbol\\theta}(\\xi^W_n \\ge \\chi^2_{1-\\alpha}(r)) \\rightarrow 1\\).","code":""},{"path":"appendix.html","id":"LMdistri","chapter":"4 Appendix","heading":"4.4.3 Proof of Proposition ??","text":"Proof. Notations: “\\(\\approx\\)” means “equal term converges 0 probability”. \\(H_0\\). \\(\\hat{\\boldsymbol\\theta}^0\\) constrained ML estimator; \\(\\hat{\\boldsymbol\\theta}\\) denotes unconstrained one.combine two Taylor expansion: \\(h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n - \\boldsymbol\\theta_0)\\) \\(h(\\hat{\\boldsymbol\\theta}_n^0) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n^0 - \\boldsymbol\\theta_0)\\) use \\(h(\\hat{\\boldsymbol\\theta}_n^0)=0\\) (definition) get:\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'}\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n - \\hat{\\boldsymbol\\theta}^0_n). \\tag{4.8}\n\\end{equation}\\]\nBesides, (using definition information matrix):\n\\[\\begin{equation}\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) \\tag{4.9}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n0=\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} - \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\\tag{4.10}\n\\end{equation}\\]\nTaking difference multiplying \\(\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\):\n\\[\\begin{equation}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}_n^0) \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\mathcal{}(\\boldsymbol\\theta_0).\\tag{4.11}\n\\end{equation}\\]\nEqs. (4.8) (4.11) yield :\n\\[\\begin{equation}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}.\\tag{4.12}\n\\end{equation}\\]Recall \\(\\hat{\\boldsymbol\\theta}^0_n\\) MLE \\(\\boldsymbol\\theta_0\\) constraint \\(h(\\boldsymbol\\theta)=0\\). vector Lagrange multipliers \\(\\hat\\lambda_n\\) associated program satisfies:\n\\[\\begin{equation}\n\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}+ \\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\\hat\\lambda_n = 0.\\tag{4.13}\n\\end{equation}\\]\nSubstituting latter equation Eq. (4.12) gives:\n\\[\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n) \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx\n- \\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\frac{\\hat\\lambda_n}{\\sqrt{n}}\n\\]\nyields:\n\\[\\begin{equation}\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\approx - \\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\n\\sqrt{n}h(\\hat{\\boldsymbol\\theta}_n).\\tag{4.14}\n\\end{equation}\\]\nfollows, Eq. (4.7), :\n\\[\n\\frac{\\hat\\lambda_n}{\\sqrt{n}} \\overset{d}{\\rightarrow} \\mathcal{N}\\left(0,\\left(\n\\dfrac{\\partial h(\\boldsymbol\\theta_0)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1}\n\\frac{\\partial h'(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\\right).\n\\]\nTaking quadratic form last equation gives:\n\\[\n\\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n \\overset{d}{\\rightarrow} \\chi^2(r).\n\\]\nUsing Eq. (4.13), appears left-hand side term last equation \\(\\xi^{LM}\\) defined Eq. (??). Consistency: see Remark 17.3 Gouriéroux Monfort (1995).","code":""},{"path":"appendix.html","id":"equivWaldLM","chapter":"4 Appendix","heading":"4.4.4 Proof of Proposition ??","text":"Proof. (using Eq. (eq:multiplier)):\n\\[\n\\xi^{LM}_n = \\frac{1}{n}\\hat\\lambda_n' \\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}^0_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}^0_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\hat\\lambda_n.\n\\]\nSince, \\(H_0\\), \\(\\hat{\\boldsymbol\\theta}_n^0\\approx\\hat{\\boldsymbol\\theta}_n \\approx {\\boldsymbol\\theta}_0\\), Eq. (4.14) therefore implies :\n\\[\n\\xi^{LM} \\approx n h(\\hat{\\boldsymbol\\theta}_n)' \\left(\n\\dfrac{\\partial h(\\hat{\\boldsymbol\\theta}_n)}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\hat{\\boldsymbol\\theta}_n)^{-1}\n\\frac{\\partial h'(\\hat{\\boldsymbol\\theta}_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta}\n\\right)^{-1}\nh(\\hat{\\boldsymbol\\theta}_n) = \\xi^{W},\n\\]\ngives result.","code":""},{"path":"appendix.html","id":"equivLRLM","chapter":"4 Appendix","heading":"4.4.5 Proof of Proposition ??","text":"Proof. second-order taylor expansions \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y})\\) \\(\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y})\\) :\n\\[\\begin{eqnarray*}\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\\\\n\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n,\\mathbf{y}) &\\approx& \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})\n+ \\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}(\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\n- \\frac{n}{2} (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0).\n\\end{eqnarray*}\\]\nTaking difference, obtain:\n\\[\n\\xi_n^{LR} \\approx 2\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0,\\mathbf{y})}{\\partial \\boldsymbol\\theta'}\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n) + n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nUsing \\(\\dfrac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta_0;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) (Eq. (4.10)), :\n\\[\n\\xi_n^{LR} \\approx\n2n(\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)'\\mathcal{}(\\boldsymbol\\theta_0)\n(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n)\n+ n (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0) - n (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)' \\mathcal{}(\\boldsymbol\\theta_0) (\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0).\n\\]\nsecond three terms sum, replace \\((\\hat{\\boldsymbol\\theta}^0_n-\\boldsymbol\\theta_0)\\) \\((\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n+\\hat{\\boldsymbol\\theta}_n-\\boldsymbol\\theta_0)\\) develop associated product. leads :\n\\[\\begin{equation}\n\\xi_n^{LR} \\approx n (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n)' \\mathcal{}(\\boldsymbol\\theta_0)^{-1} (\\hat{\\boldsymbol\\theta}^0_n-\\hat{\\boldsymbol\\theta}_n). \\tag{4.15}\n\\end{equation}\\]\ndifference Eqs. (4.9) (4.10) implies:\n\\[\n\\frac{1}{\\sqrt{n}}\\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx\n\\mathcal{}(\\boldsymbol\\theta_0)\\sqrt{n}(\\hat{\\boldsymbol\\theta}_n-\\hat{\\boldsymbol\\theta}^0_n),\n\\]\n, associated Eq. @(eq:lr10), gives:\n\\[\n\\xi_n^{LR} \\approx \\frac{1}{n} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta'} \\mathcal{}(\\boldsymbol\\theta_0)^{-1} \\frac{\\partial \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0_n;\\mathbf{y})}{\\partial \\boldsymbol\\theta} \\approx \\xi_n^{LM}.\n\\]\nHence \\(\\xi_n^{LR}\\) asymptotic distribution \\(\\xi_n^{LM}\\).Let’s show test consistent. , note :\n\\[\n\\frac{\\log \\mathcal{L}(\\hat{\\boldsymbol\\theta},\\mathbf{y}) - \\log \\mathcal{L}(\\hat{\\boldsymbol\\theta}^0,\\mathbf{y})}{n} = \\frac{1}{n} \\sum_{=1}^n[\\log f(y_i;\\hat{\\boldsymbol\\theta}_n) - \\log f(y_i;\\hat{\\boldsymbol\\theta}_n^0)] \\rightarrow \\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)],\n\\]\n\\(\\boldsymbol\\theta_\\infty\\), pseudo true value, \\(h(\\boldsymbol\\theta_\\infty) \\ne 0\\) (definition \\(H_1\\)). Kullback inequality asymptotic identifiability \\(\\boldsymbol\\theta_0\\), follows \\(\\mathbb{E}_0[\\log f(Y;\\boldsymbol\\theta_0) - \\log f(Y;\\boldsymbol\\theta_\\infty)] >0\\). Therefore \\(\\xi_n^{LR} \\rightarrow + \\infty\\) \\(H_1\\).","code":""},{"path":"appendix.html","id":"proofTVTCL","chapter":"4 Appendix","heading":"4.4.6 Proof of Eq. (??)","text":"Proof. :\n\\[\\begin{eqnarray*}\n&&T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right]\\\\\n&=& T\\mathbb{E}\\left[\\left(\\frac{1}{T}\\sum_{t=1}^T(y_t - \\mu)\\right)^2\\right] = \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^T(y_t - \\mu)^2+2\\sum_{s<t\\le T}(y_t - \\mu)(y_s - \\mu)\\right]\\\\\n&=& \\gamma_0 +\\frac{2}{T}\\left(\\sum_{t=2}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-1} - \\mu)\\right]\\right) +\\frac{2}{T}\\left(\\sum_{t=3}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-2} - \\mu)\\right]\\right) + \\dots \\\\\n&&+ \\frac{2}{T}\\left(\\sum_{t=T-1}^{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-2)} - \\mu)\\right]\\right) + \\frac{2}{T}\\mathbb{E}\\left[(y_t - \\mu)(y_{t-(T-1)} - \\mu)\\right]\\\\\n&=&  \\gamma_0 + 2 \\frac{T-1}{T}\\gamma_1 + \\dots + 2 \\frac{1}{T}\\gamma_{T-1} .\n\\end{eqnarray*}\\]\nTherefore:\n\\[\\begin{eqnarray*}\nT\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j &=& - 2\\frac{1}{T}\\gamma_1 - 2\\frac{2}{T}\\gamma_2 - \\dots - 2\\frac{T-1}{T}\\gamma_{T-1} - 2\\gamma_T - 2 \\gamma_{T+1} + \\dots\n\\end{eqnarray*}\\]\n:\n\\[\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| \\le 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\]\\(q \\le T\\), :\n\\[\\begin{eqnarray*}\n\\left|T\\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right| &\\le& 2\\frac{1}{T}|\\gamma_1| + 2\\frac{2}{T}|\\gamma_2| + \\dots + 2\\frac{q-1}{T}|\\gamma_{q-1}| +2\\frac{q}{T}|\\gamma_q| +\\\\\n&&2\\frac{q+1}{T}|\\gamma_{q+1}| + \\dots  + 2\\frac{T-1}{T}|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\\\\\n&\\le& \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q-1)|\\gamma_{q-1}| +q|\\gamma_q|\\right) +\\\\\n&&2|\\gamma_{q+1}| + \\dots  + 2|\\gamma_{T-1}| + 2|\\gamma_T| + 2 |\\gamma_{T+1}| + \\dots\n\\end{eqnarray*}\\]Consider \\(\\varepsilon > 0\\). fact autocovariances absolutely summable implies exists \\(q_0\\) (Cauchy criterion, Theorem 4.1):\n\\[\n2|\\gamma_{q_0+1}|+2|\\gamma_{q_0+2}|+2|\\gamma_{q_0+3}|+\\dots < \\varepsilon/2.\n\\]\n, \\(T > q_0\\), comes :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) + \\varepsilon/2.\n\\]\n\\(T \\ge 2\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right)/(\\varepsilon/2)\\) (\\(= f(q_0)\\), say) \n\\[\n\\frac{2}{T}\\left(|\\gamma_1| + 2|\\gamma_2| + \\dots + (q_0-1)|\\gamma_{q_0-1}| +q_0|\\gamma_{q_0}|\\right) \\le \\varepsilon/2.\n\\]\n, \\(T>f(q_0)\\) \\(T>q_0\\), .e. \\(T>\\max(f(q_0),q_0)\\), :\n\\[\n\\left|T \\mathbb{E}\\left[(\\bar{y}_T - \\mu)^2\\right] - \\sum_{j=-\\infty}^{+\\infty} \\gamma_j\\right|\\le \\varepsilon.\n\\]","code":""},{"path":"appendix.html","id":"smallestMSE","chapter":"4 Appendix","heading":"4.4.7 Proof of Proposition ??","text":"Proof. :\n\\[\\begin{eqnarray}\n\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) &=& \\mathbb{E}\\left([\\color{blue}{\\{y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)\\}} + \\color{red}{\\{\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}\\}}]^2\\right)\\nonumber\\\\\n&=&  \\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right) + \\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)\\nonumber\\\\\n&& + 2\\mathbb{E}\\left( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right). \\tag{4.16}\n\\end{eqnarray}\\]\nLet us focus last term. :\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}\\right)\\\\\n&=& \\mathbb{E}( \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}\\color{red}{ \\underbrace{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}_{\\mbox{function $x_t$}}}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\mathbb{E}( \\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}|x_t))\\\\\n&=& \\mathbb{E}( \\color{red}{ [\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]} \\color{blue}{\\underbrace{[\\mathbb{E}(y_{t+1}|x_t) - \\mathbb{E}(y_{t+1}|x_t)]}_{=0}})=0.\n\\end{eqnarray*}\\]Therefore, Eq. (4.16) becomes:\n\\[\\begin{eqnarray*}\n&&\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2) \\\\\n&=&  \\underbrace{\\mathbb{E}\\left(\\color{blue}{[y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]}^2\\right)}_{\\mbox{$\\ge 0$ depend $y^*_{t+1}$}} + \\underbrace{\\mathbb{E}\\left(\\color{red}{[\\mathbb{E}(y_{t+1}|x_t)  - y^*_{t+1}]}^2\\right)}_{\\mbox{$\\ge 0$ depends $y^*_{t+1}$}}.\n\\end{eqnarray*}\\]\nimplies \\(\\mathbb{E}([y_{t+1} - y^*_{t+1}]^2)\\) always larger \\(\\color{blue}{\\mathbb{E}([y_{t+1} - \\mathbb{E}(y_{t+1}|x_t)]^2)}\\), therefore minimized second term equal zero, \\(\\mathbb{E}(y_{t+1}|x_t) = y^*_{t+1}\\).","code":""},{"path":"appendix.html","id":"estimVARGaussian","chapter":"4 Appendix","heading":"4.4.8 Proof of Proposition 3.1","text":"Proof. Using Proposition ?? (Appendix ??), obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.","code":""},{"path":"appendix.html","id":"OLSVAR","chapter":"4 Appendix","heading":"4.4.9 Proof of Proposition 3.2","text":"Proof. Let us drop \\(\\) subscript. Rearranging Eq. (3.5), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"appendix.html","id":"additional-codes","chapter":"4 Appendix","heading":"4.5 Additional codes","text":"","code":""},{"path":"appendix.html","id":"App:GEV","chapter":"4 Appendix","heading":"4.5.1 Simulating GEV distributions","text":"following lines code used generate Figure ??.","code":"\nn.sim <- 4000\npar(mfrow=c(1,3),\n    plt=c(.2,.95,.2,.85))\nall.rhos <- c(.3,.6,.95)\nfor(j in 1:length(all.rhos)){\n  theta <- 1/all.rhos[j]\n  v1 <- runif(n.sim)\n  v2 <- runif(n.sim)\n  w <- rep(.000001,n.sim)\n  # solve for f(w) = w*(1 - log(w)/theta) - v2 = 0\n  for(i in 1:20){\n    f.i <- w * (1 - log(w)/theta) - v2\n    f.prime <- 1 - log(w)/theta - 1/theta\n    w <- w - f.i/f.prime\n  }\n  u1 <- exp(v1^(1/theta) * log(w))\n  u2 <- exp((1-v1)^(1/theta) * log(w))\n\n  # Get eps1 and eps2 using the inverse of the Gumbel distribution's cdf:\n  eps1 <- -log(-log(u1))\n  eps2 <- -log(-log(u2))\n  cbind(cor(eps1,eps2),1-all.rhos[j]^2)\n  plot(eps1,eps2,pch=19,col=\"#FF000044\",\n       main=paste(\"rho = \",toString(all.rhos[j]),sep=\"\"),\n       xlab=expression(epsilon[1]),\n       ylab=expression(epsilon[2]),\n       cex.lab=2,cex.main=1.5)\n}"},{"path":"appendix.html","id":"IRFDELTA","chapter":"4 Appendix","heading":"4.5.2 Computing the covariance matrix of IRF using the delta method","text":"","code":"\nirf.function <- function(THETA){\n  c <- THETA[1]\n  phi <- THETA[2:(p+1)]\n  if(q>0){\n    theta <- c(1,THETA[(1+p+1):(1+p+q)])\n  }else{\n    theta <- 1\n  }\n  sigma <- THETA[1+p+q+1]\n  r <- dim(Matrix.of.Exog)[2] - 1\n  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]\n  \n  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60,y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,\n                  X=NaN,beta=NaN)\n  return(irf)\n}\n\nIRF.0 <- 100*irf.function(x$THETA)\neps <- .00000001\nd.IRF <- NULL\nfor(i in 1:length(x$THETA)){\n  THETA.i <- x$THETA\n  THETA.i[i] <- THETA.i[i] + eps\n  IRF.i <- 100*irf.function(THETA.i)\n  d.IRF <- cbind(d.IRF,\n                 (IRF.i - IRF.0)/eps\n                 )\n}\nmat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]

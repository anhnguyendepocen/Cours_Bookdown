<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Estimation Methods | Advanced Econometrics</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Estimation Methods | Advanced Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Estimation Methods | Advanced Econometrics" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Jean-Paul Renne" />


<meta name="date" content="2022-08-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="panel-regressions.html"/>
<link rel="next" href="microeconometrics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Econometrics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="linear-regressions.html"><a href="linear-regressions.html"><i class="fa fa-check"></i><b>3</b> Linear Regressions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-regressions.html"><a href="linear-regressions.html#specification"><i class="fa fa-check"></i><b>3.1</b> Specification</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regressions.html"><a href="linear-regressions.html#least-square-estimation"><i class="fa fa-check"></i><b>3.2</b> Least square estimation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regressions.html"><a href="linear-regressions.html#bivariate-case"><i class="fa fa-check"></i><b>3.2.1</b> Bivariate case</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regressions.html"><a href="linear-regressions.html#gauss-markow-theorem"><i class="fa fa-check"></i><b>3.2.2</b> Gauss Markow Theorem</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regressions.html"><a href="linear-regressions.html#frish-waugh"><i class="fa fa-check"></i><b>3.2.3</b> Frish-Waugh</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-regressions.html"><a href="linear-regressions.html#goodness-of-fit"><i class="fa fa-check"></i><b>3.2.4</b> Goodness of fit</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear-regressions.html"><a href="linear-regressions.html#inference-and-prediction"><i class="fa fa-check"></i><b>3.2.5</b> Inference and Prediction</a></li>
<li class="chapter" data-level="3.2.6" data-path="linear-regressions.html"><a href="linear-regressions.html#confidence-interval-of-beta_k"><i class="fa fa-check"></i><b>3.2.6</b> Confidence interval of <span class="math inline">\(\beta_k\)</span></a></li>
<li class="chapter" data-level="3.2.7" data-path="linear-regressions.html"><a href="linear-regressions.html#example"><i class="fa fa-check"></i><b>3.2.7</b> Example</a></li>
<li class="chapter" data-level="3.2.8" data-path="linear-regressions.html"><a href="linear-regressions.html#set-of-linear-restrictions"><i class="fa fa-check"></i><b>3.2.8</b> Set of linear restrictions</a></li>
<li class="chapter" data-level="3.2.9" data-path="linear-regressions.html"><a href="linear-regressions.html#common-pitfalls"><i class="fa fa-check"></i><b>3.2.9</b> Common pitfalls</a></li>
<li class="chapter" data-level="3.2.10" data-path="linear-regressions.html"><a href="linear-regressions.html#multicollinearity"><i class="fa fa-check"></i><b>3.2.10</b> Multicollinearity</a></li>
<li class="chapter" data-level="3.2.11" data-path="linear-regressions.html"><a href="linear-regressions.html#omitted-variables"><i class="fa fa-check"></i><b>3.2.11</b> Omitted variables</a></li>
<li class="chapter" data-level="3.2.12" data-path="linear-regressions.html"><a href="linear-regressions.html#irrelevant-variable"><i class="fa fa-check"></i><b>3.2.12</b> Irrelevant variable</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-regressions.html"><a href="linear-regressions.html#large-sample-properties"><i class="fa fa-check"></i><b>3.3</b> Large Sample Properties</a></li>
<li class="chapter" data-level="3.4" data-path="linear-regressions.html"><a href="linear-regressions.html#instrumental-variables"><i class="fa fa-check"></i><b>3.4</b> Instrumental Variables</a></li>
<li class="chapter" data-level="3.5" data-path="linear-regressions.html"><a href="linear-regressions.html#general-regression-model"><i class="fa fa-check"></i><b>3.5</b> General Regression Model</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="linear-regressions.html"><a href="linear-regressions.html#generalized-least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-regressions.html"><a href="linear-regressions.html#heteroskedasticity-and-autocorrelation-hac"><i class="fa fa-check"></i><b>3.5.2</b> Heteroskedasticity and Autocorrelation (HAC)</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-regressions.html"><a href="linear-regressions.html#how-to-detect-autocorrelation-in-residuals"><i class="fa fa-check"></i><b>3.5.3</b> How to detect autocorrelation in residuals?</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-regressions.html"><a href="linear-regressions.html#summary"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="linear-regressions.html"><a href="linear-regressions.html#clusters"><i class="fa fa-check"></i><b>3.7</b> Clusters</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="panel-regressions.html"><a href="panel-regressions.html"><i class="fa fa-check"></i><b>4</b> Panel regressions</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="panel-regressions.html"><a href="panel-regressions.html#three-standard-cases"><i class="fa fa-check"></i><b>4.0.1</b> Three standard cases</a></li>
<li class="chapter" data-level="4.1" data-path="panel-regressions.html"><a href="panel-regressions.html#estimation-of-fixed-effects-models"><i class="fa fa-check"></i><b>4.1</b> Estimation of Fixed Effects Models</a></li>
<li class="chapter" data-level="4.2" data-path="panel-regressions.html"><a href="panel-regressions.html#estimation-of-random-effects-models"><i class="fa fa-check"></i><b>4.2</b> Estimation of random effects models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation-methods.html"><a href="estimation-methods.html"><i class="fa fa-check"></i><b>5</b> Estimation Methods</a>
<ul>
<li class="chapter" data-level="5.1" data-path="estimation-methods.html"><a href="estimation-methods.html#generalized-method-of-moments"><i class="fa fa-check"></i><b>5.1</b> Generalized Method of Moments</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="estimation-methods.html"><a href="estimation-methods.html#framework"><i class="fa fa-check"></i><b>5.1.1</b> Framework</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="estimation-methods.html"><a href="estimation-methods.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.2</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="estimation-methods.html"><a href="estimation-methods.html#notations"><i class="fa fa-check"></i><b>5.2.1</b> Notations</a></li>
<li class="chapter" data-level="5.2.2" data-path="estimation-methods.html"><a href="estimation-methods.html#to-sum-up-mle-in-practice"><i class="fa fa-check"></i><b>5.2.2</b> To sum up – MLE in practice</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="microeconometrics.html"><a href="microeconometrics.html"><i class="fa fa-check"></i><b>6</b> Microeconometrics</a></li>
<li class="chapter" data-level="7" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>7</b> Time Series</a></li>
<li class="chapter" data-level="8" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>8</b> Appendix</a>
<ul>
<li class="chapter" data-level="8.1" data-path="appendix.html"><a href="appendix.html#statitical-tables"><i class="fa fa-check"></i><b>8.1</b> Statitical Tables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimation-methods" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Estimation Methods<a href="estimation-methods.html#estimation-methods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Context and Objective:</p>
<ul>
<li>You observe a sample <span class="math inline">\(\mathbf{y}=\{y_1,\dots,y_n\}\)</span>.</li>
<li>You know that these data have been generated by a model parameterized by <span class="math inline">\(\theta_0 \in \mathbb{R}^K\)</span>.</li>
</ul>
<div id="generalized-method-of-moments" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Generalized Method of Moments<a href="estimation-methods.html#generalized-method-of-moments" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="framework" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Framework<a href="estimation-methods.html#framework" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We denote by <span class="math inline">\(x_t\)</span> a <span class="math inline">\(p \times 1\)</span> vector of (stationary) variables observed at date <span class="math inline">\(t\)</span>; by <span class="math inline">\(\theta\)</span> an <span class="math inline">\(a \times 1\)</span> vector of parameters, and by <span class="math inline">\(h(x_t;\theta)\)</span> a continuous <span class="math inline">\(r \times 1\)</span> vector-valued function.</p>
<p>We denote by <span class="math inline">\(\theta_0\)</span> the true value of <span class="math inline">\(\theta\)</span> and we assume that <span class="math inline">\(\theta_0\)</span> satisfies:
<span class="math display">\[
\mathbb{E}[h(x_t;\theta_0)] = 0.
\]</span></p>
<p>We denote by <span class="math inline">\(\underline{x_t}\)</span> the information contained in the current and past observations of <span class="math inline">\(x_t\)</span>, that is: <span class="math inline">\(\underline{x_t} = \{x_t,x_{t-1},\dots,x_1\}\)</span>. We denote by <span class="math inline">\(g(\underline{x_T};\theta)\)</span> the sample average of <span class="math inline">\(h(x_t;\theta)\)</span>, i.e.:
<span class="math display">\[
g(\underline{x_T};\theta) = \frac{1}{T} \sum_{t=1}^{T} h(x_t;\theta).
\]</span></p>
<p>Intuition behind GMM: Choose <span class="math inline">\(\theta\)</span> so as to make the sample moment as close as possible to 0.</p>
<p>A GMM estimator of <span class="math inline">\(\theta_0\)</span> is given by:
<span class="math display">\[
\hat{\theta}_T = \mbox{argmin}_{\theta} \quad g(\underline{x_T};\theta)&#39;\, W_T \, g(\underline{x_T};\theta),
\]</span>
where <span class="math inline">\(W_T\)</span> is a positive definite matrix (that may depend on <span class="math inline">\(\underline{x_T}\)</span>).</p>
<p>If <span class="math inline">\(a = r\)</span>, <span class="math inline">\(\hat{\theta}_T\)</span> is such that:
<span class="math display">\[
g(\underline{x_T};\hat{\theta}_T) = 0.
\]</span>
Under regularity and identification conditions:
<span class="math display">\[
\hat{\theta}_{T} \overset{P}{\rightarrow} \theta_0,
\]</span>
i.e. <span class="math inline">\(\forall \varepsilon&gt;0\)</span>, <span class="math inline">\(\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\theta}_{T} - \theta_0|&gt;\varepsilon) = 0\)</span>.</p>
<p><strong>Optimal weighting matrix</strong>. The GMM estimator achieving the minimum asymptotic variance is obtained when <span class="math inline">\(W_T\)</span> is the inverse of the matrix <span class="math inline">\(S\)</span> defined by:
<span class="math display">\[
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
\]</span>
where <span class="math inline">\(\Gamma_\nu := \mathbb{E}[h(x_t;\theta_0) h(x_{t-\nu};\theta_0)&#39;]\)</span>.</p>
<p>For <span class="math inline">\(\nu \ge 0\)</span>, let us define <span class="math inline">\(\hat{\Gamma}_{\nu,T}\)</span> by:
<span class="math display">\[
\hat{\Gamma}_{\nu,T} = \frac{1}{T} \sum_{t=\nu + 1}^{T} h(x_t;\hat{\theta}_T)h(x_{t-\nu};\hat{\theta}_T)&#39;,
\]</span>
<span class="math inline">\(S\)</span> can be approximated by:
<span class="math display" id="eq:Shat">\[\begin{eqnarray}
&amp;\hat{\Gamma}_{0,T}&amp; \quad \mbox{if the $h(x_t;\theta_0)$ are serially uncorrelated and} \nonumber \\
&amp;\hat{\Gamma}_{0,T}&amp; + \sum_{\nu=1}^{q}[1-\nu/(q+1)](\hat{\Gamma}_{\nu,T}+\hat{\Gamma}_{\nu,T}&#39;) \quad \mbox{otherwise.}    \tag{5.1}
\end{eqnarray}\]</span></p>
<p><strong>Asymptotic distribution of <span class="math inline">\(\hat\theta_T\)</span></strong></p>
<p>We have:
<span class="math display">\[
\sqrt{T}(\hat\theta_T - \theta_0) \overset{\mathcal{L}}{\rightarrow} \mathcal{N}(0,V),
\]</span>
where <span class="math inline">\(V = (DS^{-1}D&#39;)^{-1}\)</span>.</p>
<p><span class="math inline">\(V\)</span> can be approximated by <span class="math inline">\((\hat{D}_T\hat{S}_T^{-1}\hat{D}_T&#39;)^{-1}\)</span>,
where <span class="math inline">\(\hat{S}_T\)</span> is given by Eq. <a href="estimation-methods.html#eq:Shat">(5.1)</a> and
<span class="math display">\[
\hat{D}&#39;_T := \left.\frac{\partial g(\underline{x_T};\theta)}{\partial \theta&#39;}\right|_{\theta = \hat\theta_T}.
\]</span></p>
</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Maximum Likelihood Estimation<a href="estimation-methods.html#maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Intuition behind the Maximum Likelihood Estimation: Estimator = the value of <span class="math inline">\(\theta\)</span> that is such that the probability of having observed <span class="math inline">\(\mathbf{y}\)</span> is the highest possible.</p>
<p>Assume that the time periods between the arrivals of two customers in a shop, denoted by <span class="math inline">\(y_i\)</span>, are i.i.d. and follow an exponential distribution, i.e. <span class="math inline">\(y_i \sim \mathcal{E}(\lambda)\)</span>.</p>
<p>You have observed these arrivals for some time, thereby constituting a sample <span class="math inline">\(\{y_1,\dots,y_n\}\)</span>. You want to estimate <span class="math inline">\(\lambda\)</span> (i.e. in that case, the vector of parameters is simply <span class="math inline">\(\theta = \lambda\)</span>).</p>
<p>The density of <span class="math inline">\(Y\)</span> is <span class="math inline">\(f(y;\lambda) = \dfrac{1}{\lambda}\exp(-y/\lambda)\)</span>. Fig. <a href="estimation-methods.html#fig:MLE1">5.1</a> represents that density functions for different values of <span class="math inline">\(\lambda\)</span>.</p>
<p>Your 200 observations are reported at the bottom of Fig. <a href="estimation-methods.html#fig:MLE1">5.1</a> (red).
You build the histogram and report it on the same chart.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:MLE1"></span>
<img src="AdvECTS_files/figure-html/MLE1-1.png" alt="The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations" width="90%" />
<p class="caption">
Figure 5.1: The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations
</p>
</div>
<p>What is your estimate of <span class="math inline">\(\lambda\)</span>?</p>
<p>Now, assume that you have only four observations: <span class="math inline">\(y_1=1.1\)</span>, <span class="math inline">\(y_2=2.2\)</span>, <span class="math inline">\(y_3=0.7\)</span> and <span class="math inline">\(y_4=5.0\)</span>.
What was the probability of observing, for a small <span class="math inline">\(\varepsilon\)</span>,</p>
<ul>
<li><span class="math inline">\(1.1-\varepsilon \le Y_1 &lt; 1.1+\varepsilon\)</span>,</li>
<li><span class="math inline">\(2.2-\varepsilon \le Y_2 &lt; 2.2+\varepsilon\)</span>,</li>
<li><span class="math inline">\(0.7-\varepsilon \le Y_3 &lt; 0.7+\varepsilon\)</span> and</li>
<li><span class="math inline">\(5.0-\varepsilon \le Y_4 &lt; 5.0+\varepsilon\)</span>?</li>
</ul>
<p>Because the <span class="math inline">\(y_i\)</span>s are i.i.d., this probability is <span class="math inline">\(\prod_{i=1}^4(2\varepsilon f(y_i,\lambda))\)</span>.
The next plot shows the probability (divided by <span class="math inline">\(16\varepsilon^4\)</span>) as a function of <span class="math inline">\(\lambda\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:MLE2"></span>
<img src="AdvECTS_files/figure-html/MLE2-1.png" alt="Proba. that $y_i-\varepsilon \le Y_i &lt; y_i+\varepsilon$, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function." width="90%" />
<p class="caption">
Figure 5.2: Proba. that <span class="math inline">\(y_i-\varepsilon \le Y_i &lt; y_i+\varepsilon\)</span>, <span class="math inline">\(i \in \{1,2,3,4\}\)</span>. The vertical red line indicates the maximum of the function.
</p>
</div>
<p>Back to the example with 200 observations:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:MLE3"></span>
<img src="AdvECTS_files/figure-html/MLE3-1.png" alt="Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function." width="90%" />
<p class="caption">
Figure 5.3: Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.
</p>
</div>
<div id="notations" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Notations<a href="estimation-methods.html#notations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(f(y;\boldsymbol\theta)\)</span> denotes the probability density function (p.d.f.) of a random variable <span class="math inline">\(Y\)</span> which depends on a set of parameters <span class="math inline">\(\boldsymbol\theta\)</span>.</p>
<p>Density of <span class="math inline">\(n\)</span> independent and identically distributed (i.i.d.) observations of <span class="math inline">\(Y\)</span>:
<span class="math display">\[
f(y_1,\dots,y_n;\boldsymbol\theta) = \prod_{i=1}^n f(y_i;\boldsymbol\theta).
\]</span>
<span class="math inline">\(\mathbf{y}\)</span> denotes the vector of observations; <span class="math inline">\(\mathbf{y} = \{y_1,\dots,y_n\}\)</span>.</p>
<hr />
<div class="definition">
<p><span id="def:likelihood" class="definition"><strong>Definition 5.1  (Likelihood function) </strong></span><span class="math inline">\(\mathcal{L}: \boldsymbol\theta \rightarrow \mathcal{L}(\boldsymbol\theta;\mathbf{y})=f(y_1,\dots,y_n;\boldsymbol\theta)\)</span> is the <strong>likelihood function</strong>.</p>
</div>
<hr />
<p>We often work with <span class="math inline">\(\log \mathcal{L}\)</span>, the <strong>log-likelihood function</strong>.</p>
<div class="example">
<p><span id="exm:normal" class="example"><strong>Example 5.1  (Gaussian distribution) </strong></span>If <span class="math inline">\(y_i \sim \mathcal{N}(\mu,\sigma^2)\)</span>, then
<span class="math display">\[
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = - \frac{1}{2}\sum_{i=1}^n\left( \log \sigma^2 + \log 2\pi + \frac{(y_i-\mu)^2}{\sigma^2} \right).
\]</span></p>
</div>
<hr />
<div class="definition">
<p><span id="def:score" class="definition"><strong>Definition 5.2  (Score) </strong></span>The score <span class="math inline">\(S(y;\boldsymbol\theta)\)</span> is given by <span class="math inline">\(\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\)</span>.</p>
</div>
<hr />
<p>If <span class="math inline">\(y_i \sim \mathcal{N}(\mu,\sigma^2)\)</span> (Example <a href="estimation-methods.html#exm:normal">5.1</a>), then
<span class="math display">\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} =
\left[\begin{array}{c}
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \mu}\\
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \sigma^2}
\end{array}\right] =
\left[\begin{array}{c}
\dfrac{y-\mu}{\sigma^2}\\
\frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right)
\end{array}\right].
\]</span></p>
<hr />
<div class="proposition">
<p><span id="prp:score" class="proposition"><strong>Proposition 5.1  (Score expectation) </strong></span>The expectation of the score is zero.</p>
</div>
<hr />
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>We have:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left(\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\right) &amp;=&amp;
\int \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta) dy \\
&amp;=&amp; \int \frac{\partial f(y;\boldsymbol\theta)/\partial \boldsymbol\theta}{f(y;\boldsymbol\theta)} f(y;\boldsymbol\theta) dy =
\frac{\partial}{\partial \boldsymbol\theta} \int f(y;\boldsymbol\theta) dy =
\partial 1 /\partial \boldsymbol\theta = 0,
\end{eqnarray*}\]</span>
which gives the result.</p>
</div>
<div class="definition">
<p><span id="def:Fisher" class="definition"><strong>Definition 5.3  (Fisher information matrix) </strong></span>The <strong>information matrix</strong> is (minus) the the expectation of the second derivatives of the log-likelihood function:
<span class="math display">\[
\mathcal{I}_Y(\boldsymbol\theta) = - \mathbb{E} \left( \frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;} \right).
\]</span></p>
</div>
<hr />
<div class="proposition">
<p><span id="prp:Fisher" class="proposition"><strong>Proposition 5.2  </strong></span>We have <span class="math inline">\(\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E} \left[ \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right) \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)&#39; \right] = \mathbb{V}ar[S(Y;\boldsymbol\theta)]\)</span>.</p>
</div>
<hr />
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>We have <span class="math inline">\(\frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;} = \frac{\partial^2 f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;}\frac{1}{f(Y;\boldsymbol\theta)} - \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta&#39;}\)</span>. The expectation of the first right-hand side term is <span class="math inline">\(\partial^2 1 /(\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;) = \mathbf{0}\)</span>, which gives the result.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-17" class="example"><strong>Example 5.2  </strong></span>If <span class="math inline">\(y_i \sim\,i.i.d.\, \mathcal{N}(\mu,\sigma^2)\)</span>, let <span class="math inline">\(\boldsymbol\theta = [\mu,\sigma^2]&#39;\)</span> then
<span class="math display">\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} = \left[\frac{y-\mu}{\sigma^2} \quad \frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right) \right]&#39;
\]</span>
and
<span class="math display">\[
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E}\left( \frac{1}{\sigma^4}
\left[
\begin{array}{cc}
\sigma^2&amp;y-\mu\\
y-\mu &amp; \frac{(y-\mu)^2}{\sigma^2}-\frac{1}{2}
\end{array}\right]
\right)=
\left[
\begin{array}{cc}
1/\sigma^2&amp;0\\
0 &amp; 1/(2\sigma^4)
\end{array}\right].
\]</span></p>
</div>
<hr />
<div class="proposition">
<p><span id="prp:additiv" class="proposition"><strong>Proposition 5.3  (Additive property of the Info. mat.) </strong></span>The information matrix resulting from two independent experiments is the sum of the information matrices:
<span class="math display">\[
\mathcal{I}_{X,Y}(\boldsymbol\theta) = \mathcal{I}_X(\boldsymbol\theta) + \mathcal{I}_Y(\boldsymbol\theta).
\]</span></p>
</div>
<hr />
<p>:::{.proof} Immediately obtained from the definition (see Def. @ref{def:Fisher}).
:::</p>
<hr />
<div class="theorem">
<p><span id="thm:FDCR" class="theorem"><strong>Theorem 5.1  (Fr\'echet-Darmois-Cram\'er-Rao bound) </strong></span>Consider an unbiased estimator of <span class="math inline">\(\boldsymbol\theta\)</span> denoted by <span class="math inline">\(\hat{\boldsymbol\theta}(Y)\)</span>.</p>
<p>The variance of the random variable <span class="math inline">\(\boldsymbol\omega&#39;\hat{\boldsymbol\theta}\)</span> (which is a linear combination of the components of <span class="math inline">\(\hat{\boldsymbol\theta}\)</span>) is larger than:
<span class="math display">\[
(\boldsymbol\omega&#39;\boldsymbol\omega)^2/(\boldsymbol\omega&#39; \mathcal{I}_Y(\boldsymbol\theta) \boldsymbol\omega).
\]</span></p>
</div>
<hr />
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span>The Cauchy-Schwarz inequality implies that <span class="math inline">\(\sqrt{\mathbb{V}ar(\boldsymbol\omega&#39;\hat{\boldsymbol\theta}(Y))\mathbb{V}ar(\boldsymbol\omega&#39;S(Y;\boldsymbol\theta))} \ge |\boldsymbol\omega&#39;\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)]\boldsymbol\omega |\)</span>. Now, <span class="math inline">\(\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)] = \int_y \hat{\boldsymbol\theta}(y) \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta)dy = \frac{\partial}{\partial \boldsymbol\theta}\int_y \hat{\boldsymbol\theta}(y) f(y;\boldsymbol\theta)dy = \mathbf{I}\)</span> because <span class="math inline">\(\hat{\boldsymbol\theta}\)</span> is unbiased.</p>
<p>Therefore <span class="math inline">\(\mathbb{V}ar(\boldsymbol\omega&#39;\hat{\boldsymbol\theta}(Y)) \ge \mathbb{V}ar(\boldsymbol\omega&#39;S(Y;\boldsymbol\theta))^{-1} (\boldsymbol\omega&#39;\boldsymbol\omega)^2\)</span>. Prop. <a href="estimation-methods.html#prp:Fisher">5.2</a> leads to the result.</p>
</div>
<div class="definition">
<p><span id="def:identif" class="definition"><strong>Definition 5.4  </strong></span>The vector of parameters <span class="math inline">\(\boldsymbol\theta\)</span> is identifiable if, for any other vector <span class="math inline">\(\boldsymbol\theta^*\)</span>:
<span class="math display">\[
\boldsymbol\theta^* \ne \boldsymbol\theta \Rightarrow \mathcal{L}(\boldsymbol\theta^*;\mathbf{y}) \ne \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:MLEest" class="definition"><strong>Definition 5.5  (Maximum Likelihood Estimator (MLE)) </strong></span>The maximum likelihood estimator (MLE) is the vector <span class="math inline">\(\boldsymbol\theta\)</span> that maximizes the likelihood function. Formally:
<span class="math display" id="eq:MLEestimator">\[\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\mathbf{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).\tag{5.2}
\end{equation}\]</span></p>
</div>
<div class="definition">
<p><span id="def:likFunction" class="definition"><strong>Definition 5.6  (Likelihood equation) </strong></span>Necessary condition for maximizing the likelihood function:
<span class="math display">\[\begin{equation}
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta} = \mathbf{0}.
\end{equation}\]</span></p>
</div>
<div class="hypothesis">
<p><span id="hyp:MLEregularity" class="hypothesis"><strong>Hypothesis 5.1  (Regularity assumptions) </strong></span>We have:</p>
<ol style="list-style-type: lower-roman">
<li><p><span class="math inline">\(\boldsymbol\theta \in \Theta\)</span> where <span class="math inline">\(\Theta\)</span> is compact.</p></li>
<li><p><span class="math inline">\(\boldsymbol\theta_0\)</span> is identified.</p></li>
<li><p>The log-likelihood function is continuous in <span class="math inline">\(\boldsymbol\theta\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\)</span> exists.</p></li>
<li><p>The log-likelihood function is such that <span class="math inline">\((1/n)\log\mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span> converges almost surely to <span class="math inline">\(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\)</span>, uniformly in <span class="math inline">\(\boldsymbol\theta \in \Theta\)</span>.</p></li>
<li><p>The log-likelihood function is twice continuously differentiable in an open neighborood of <span class="math inline">\(\boldsymbol\theta_0\)</span>.</p></li>
<li><p>The matrix <span class="math inline">\(\mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;}\right) \quad \mbox{(Fisher Information matrix)}\)</span> exists and is nonsingular.</p></li>
</ol>
</div>
<hr />
<div class="proposition">
<p><span id="prp:MLEproperties" class="proposition"><strong>Proposition 5.4  (Properties of MLE) </strong></span>Under regularity conditions (Assumptions <a href="estimation-methods.html#hyp:MLEregularity">5.1</a>), the MLE is:</p>
<ol style="list-style-type: lower-alpha">
<li><p><strong>Consistent</strong>: <span class="math inline">\(\mbox{plim}\quad \boldsymbol\theta_{MLE} = \theta_0\)</span> (<span class="math inline">\(\theta_0\)</span> is the true vector of parameters).</p></li>
<li><p><strong>Asymptotically normal</strong>: <span class="math inline">\(\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\mathbf{I}(\boldsymbol\theta_0)^{-1})\)</span>, where
<span class="math display">\[
\mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0). \quad \mbox{(Fisher Info. matrix)}
\]</span></p></li>
<li><p><strong>Asymptotically efficient</strong>: <span class="math inline">\(\boldsymbol\theta_{MLE}\)</span> is asymptotically efficient and achieves the Fr'echet-Darmois-Cram'er-Rao lower bound for consistent estimators.</p></li>
<li><p><strong>Invariant</strong>: The MLE of <span class="math inline">\(g(\boldsymbol\theta_0)\)</span> is <span class="math inline">\(g(\boldsymbol\theta_{MLE})\)</span> if <span class="math inline">\(g\)</span> is a continuous and continuously differentiable function.</p></li>
</ol>
</div>
<hr />
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>Proof</em>. </span>See <a href="https://www.dropbox.com/s/8xelbghtvnd43yh/Proofs_MLE.pdf?dl=0">Online additional material</a>.</p>
</div>
<p>Note that (b) also writes:
<span class="math display" id="eq:normMLE">\[\begin{equation}
\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0)^{-1}). \tag{5.3}
\end{equation}\]</span></p>
<p>The asymptotic covariance matrix of the MLE is:
<span class="math display">\[
[\mathbf{I}(\boldsymbol\theta_0)]^{-1} = \left[- \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta&#39;}\right) \right]^{-1}.
\]</span>
A direct (analytical) evaluation of this expectation is often out of reach.</p>
<p>It can however be estimated by, either:
<span class="math display" id="eq:I2" id="eq:III1">\[\begin{eqnarray}
\hat{\mathbf{I}}_1^{-1} &amp;=&amp;  \left( - \frac{\partial^2 \log \mathcal{L}({\boldsymbol\theta_{MLE}};\mathbf{y})}{\partial {\boldsymbol\theta} \partial {\boldsymbol\theta}&#39;}\right)^{-1}, \tag{5.4}\\
\hat{\mathbf{I}}_2^{-1} &amp;=&amp;  \left( \sum_{i=1}^n \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta}} \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta&#39;}} \right)^{-1}.  \tag{5.5}
\end{eqnarray}\]</span></p>
</div>
<div id="to-sum-up-mle-in-practice" class="section level3 hasAnchor" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> To sum up – MLE in practice<a href="estimation-methods.html#to-sum-up-mle-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>A parametric model (depending on the vector of parameters <span class="math inline">\(\boldsymbol\theta\)</span> whose “true” value is <span class="math inline">\(\boldsymbol\theta_0\)</span>) is specified.</p></li>
<li><p>i.i.d. sources of randomness are identified.</p></li>
<li><p>The density associated to one observation <span class="math inline">\(y_i\)</span> is computed analytically (as a function of <span class="math inline">\(\boldsymbol\theta\)</span>): <span class="math inline">\(f(y;\boldsymbol\theta)\)</span>.</p></li>
<li><p>The log-likelihood is <span class="math inline">\(\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = \sum_i \log f(y_i;\boldsymbol\theta)\)</span>.</p></li>
<li><p>The MLE estimator results from the optimization problem (this is Eq. <a href="estimation-methods.html#eq:MLEestimator">(5.2)</a>):
<span class="math display">\[\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
\end{equation}\]</span></p></li>
<li><p>We have: <span class="math inline">\(\boldsymbol\theta_{MLE} \sim \mathcal{N}(\theta_0,\mathbf{I}(\boldsymbol\theta_0)^{-1})\)</span>, where <span class="math inline">\(\mathbf{I}(\boldsymbol\theta_0)^{-1}\)</span> is estimated by means of Eq. <a href="estimation-methods.html#eq:III1">(5.4)</a> or Eq. <a href="estimation-methods.html#eq:I2">(5.5)</a>. Most of the time, this computation is numerical.</p></li>
</ul>
<!-- \begin{frame}{} -->
<!-- \begin{scriptsize} -->
<!-- \begin{exmpl}[MLE estimation of a mixture of Gaussian distribution] -->
<!-- \begin{itemize} -->
<!-- \item Consider the returns of the SMI index used in Fig.\,\ref{fig:illuskernel_smi}. -->
<!-- \item Let's assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. $f(x;\boldsymbol\theta)$, with $\boldsymbol\theta = [\mu_1,\sigma_1,\mu_2,\sigma_2,p]'$, is given by: -->
<!-- $$ -->
<!-- p \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) + (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right). -->
<!-- $$ -->
<!-- \href{https://jrenne.shinyapps.io/density/}{\beamergotobutton{p.d.f. of mixtures of Gaussian dist.}} -->
<!-- \item The maximum likelihood estimate is $\boldsymbol\theta_{MLE}=[0.30,1.40,-1.45,3.61,0.87]'$. -->
<!-- \item The first two entries of the diagonal of $\hat{\bv{I}}_1^{-1}$ are $0.00528$ and $0.00526$. They are the estimates of $\mathbb{V}ar(\mu_{1,MLE})$ and of $\mathbb{V}ar(\sigma_{1,MLE})$, respectively. -->
<!-- 95\% confidence intervals for $\mu_1$ and $\sigma_1$ are, respectively: -->
<!-- $$ -->
<!-- 0.30 \pm 1.96\underbrace{\sqrt{0.00528}}_{=0.0726} \quad \mbox{ and } \quad 1.40 \pm 1.96\underbrace{\sqrt{0.00526}.}_{=0.0725} -->
<!-- $$ -->
<!-- \end{itemize} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[MLE estim. of a mixture of Gaussian distri. (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{Density of 5-day returns on SMI index} -->
<!-- \includegraphics[width=.9\linewidth]{../../figures/Figure_kernel_smi.pdf} -->
<!-- \label{fig:illuskernel_smi} -->
<!-- \begin{tiny} -->
<!-- Gaussian kernel, $h=0.5$ (in percent). -->
<!-- The data spans the period from 2 June 2006 to 23 February 2016 at the daily frequency. -->
<!-- Left-hand plot: the blue lines indicates $\mu \pm 2 \sigma$, where $\mu$ is the sample mean of the returns and $\sigma$ is their sample standard deviation. Right-hand plot: the red dotted line is the density $\mathcal{N}(\mu,\sigma^2)$ -->
<!-- \end{tiny} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
<!-- \begin{frame}{} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[MLE estim. of a mixture of Gaussian distri. (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{Estimated density (vs kernel-based estimate)} -->
<!-- \includegraphics[width=1\linewidth]{../../figures/Figure_kernel_smiMLE.pdf} -->
<!-- \label{fig:MLE1} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="panel-regressions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="microeconometrics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-EstimationMethods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["AdvECTS.pdf", "AdvECTS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

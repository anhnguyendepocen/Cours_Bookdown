<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Estimation Methods | Advanced Econometrics</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="Context and Objective: You observe a sample \(\mathbf{y}=\{y_1,\dots,y_n\}\). You know that these data have been generated by a model parameterized by \(\theta_0 \in \mathbb{R}^K\).  5.1...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 5 Estimation Methods | Advanced Econometrics">
<meta property="og:type" content="book">
<meta property="og:description" content="Context and Objective: You observe a sample \(\mathbf{y}=\{y_1,\dots,y_n\}\). You know that these data have been generated by a model parameterized by \(\theta_0 \in \mathbb{R}^K\).  5.1...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Estimation Methods | Advanced Econometrics">
<meta name="twitter:description" content="Context and Objective: You observe a sample \(\mathbf{y}=\{y_1,\dots,y_n\}\). You know that these data have been generated by a model parameterized by \(\theta_0 \in \mathbb{R}^K\).  5.1...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Advanced Econometrics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Prerequisites</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="" href="linear-regressions.html"><span class="header-section-number">3</span> Linear Regressions</a></li>
<li><a class="" href="panel-regressions.html"><span class="header-section-number">4</span> Panel regressions</a></li>
<li><a class="active" href="estimation-methods.html"><span class="header-section-number">5</span> Estimation Methods</a></li>
<li><a class="" href="microeconometrics.html"><span class="header-section-number">6</span> Microeconometrics</a></li>
<li><a class="" href="time-series.html"><span class="header-section-number">7</span> Time Series</a></li>
<li><a class="" href="appendix.html"><span class="header-section-number">8</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="estimation-methods" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Estimation Methods<a class="anchor" aria-label="anchor" href="#estimation-methods"><i class="fas fa-link"></i></a>
</h1>
<p>Context and Objective:</p>
<ul>
<li>You observe a sample <span class="math inline">\(\mathbf{y}=\{y_1,\dots,y_n\}\)</span>.</li>
<li>You know that these data have been generated by a model parameterized by <span class="math inline">\(\theta_0 \in \mathbb{R}^K\)</span>.</li>
</ul>
<div id="generalized-method-of-moments" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Generalized Method of Moments<a class="anchor" aria-label="anchor" href="#generalized-method-of-moments"><i class="fas fa-link"></i></a>
</h2>
<div id="framework" class="section level3" number="5.1.1">
<h3>
<span class="header-section-number">5.1.1</span> Framework<a class="anchor" aria-label="anchor" href="#framework"><i class="fas fa-link"></i></a>
</h3>
<p>We denote by <span class="math inline">\(x_t\)</span> a <span class="math inline">\(p \times 1\)</span> vector of (stationary) variables observed at date <span class="math inline">\(t\)</span>; by <span class="math inline">\(\theta\)</span> an <span class="math inline">\(a \times 1\)</span> vector of parameters, and by <span class="math inline">\(h(x_t;\theta)\)</span> a continuous <span class="math inline">\(r \times 1\)</span> vector-valued function.</p>
<p>We denote by <span class="math inline">\(\theta_0\)</span> the true value of <span class="math inline">\(\theta\)</span> and we assume that <span class="math inline">\(\theta_0\)</span> satisfies:
<span class="math display">\[
\mathbb{E}[h(x_t;\theta_0)] = 0.
\]</span></p>
<p>We denote by <span class="math inline">\(\underline{x_t}\)</span> the information contained in the current and past observations of <span class="math inline">\(x_t\)</span>, that is: <span class="math inline">\(\underline{x_t} = \{x_t,x_{t-1},\dots,x_1\}\)</span>. We denote by <span class="math inline">\(g(\underline{x_T};\theta)\)</span> the sample average of <span class="math inline">\(h(x_t;\theta)\)</span>, i.e.:
<span class="math display">\[
g(\underline{x_T};\theta) = \frac{1}{T} \sum_{t=1}^{T} h(x_t;\theta).
\]</span></p>
<p>Intuition behind GMM: Choose <span class="math inline">\(\theta\)</span> so as to make the sample moment as close as possible to 0.</p>
<p>A GMM estimator of <span class="math inline">\(\theta_0\)</span> is given by:
<span class="math display">\[
\hat{\theta}_T = \mbox{argmin}_{\theta} \quad g(\underline{x_T};\theta)'\, W_T \, g(\underline{x_T};\theta),
\]</span>
where <span class="math inline">\(W_T\)</span> is a positive definite matrix (that may depend on <span class="math inline">\(\underline{x_T}\)</span>).</p>
<p>If <span class="math inline">\(a = r\)</span>, <span class="math inline">\(\hat{\theta}_T\)</span> is such that:
<span class="math display">\[
g(\underline{x_T};\hat{\theta}_T) = 0.
\]</span>
Under regularity and identification conditions:
<span class="math display">\[
\hat{\theta}_{T} \overset{P}{\rightarrow} \theta_0,
\]</span>
i.e.Â <span class="math inline">\(\forall \varepsilon&gt;0\)</span>, <span class="math inline">\(\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\theta}_{T} - \theta_0|&gt;\varepsilon) = 0\)</span>.</p>
<p><strong>Optimal weighting matrix</strong>. The GMM estimator achieving the minimum asymptotic variance is obtained when <span class="math inline">\(W_T\)</span> is the inverse of the matrix <span class="math inline">\(S\)</span> defined by:
<span class="math display">\[
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
\]</span>
where <span class="math inline">\(\Gamma_\nu := \mathbb{E}[h(x_t;\theta_0) h(x_{t-\nu};\theta_0)']\)</span>.</p>
<p>For <span class="math inline">\(\nu \ge 0\)</span>, let us define <span class="math inline">\(\hat{\Gamma}_{\nu,T}\)</span> by:
<span class="math display">\[
\hat{\Gamma}_{\nu,T} = \frac{1}{T} \sum_{t=\nu + 1}^{T} h(x_t;\hat{\theta}_T)h(x_{t-\nu};\hat{\theta}_T)',
\]</span>
<span class="math inline">\(S\)</span> can be approximated by:
<span class="math display" id="eq:Shat">\[\begin{eqnarray}
&amp;\hat{\Gamma}_{0,T}&amp; \quad \mbox{if the $h(x_t;\theta_0)$ are serially uncorrelated and} \nonumber \\
&amp;\hat{\Gamma}_{0,T}&amp; + \sum_{\nu=1}^{q}[1-\nu/(q+1)](\hat{\Gamma}_{\nu,T}+\hat{\Gamma}_{\nu,T}') \quad \mbox{otherwise.}    \tag{5.1}
\end{eqnarray}\]</span></p>
<p><strong>Asymptotic distribution of <span class="math inline">\(\hat\theta_T\)</span></strong></p>
<p>We have:
<span class="math display">\[
\sqrt{T}(\hat\theta_T - \theta_0) \overset{\mathcal{L}}{\rightarrow} \mathcal{N}(0,V),
\]</span>
where <span class="math inline">\(V = (DS^{-1}D')^{-1}\)</span>.</p>
<p><span class="math inline">\(V\)</span> can be approximated by <span class="math inline">\((\hat{D}_T\hat{S}_T^{-1}\hat{D}_T')^{-1}\)</span>,
where <span class="math inline">\(\hat{S}_T\)</span> is given by Eq. <a href="estimation-methods.html#eq:Shat">(5.1)</a> and
<span class="math display">\[
\hat{D}'_T := \left.\frac{\partial g(\underline{x_T};\theta)}{\partial \theta'}\right|_{\theta = \hat\theta_T}.
\]</span></p>
</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Maximum Likelihood Estimation<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation"><i class="fas fa-link"></i></a>
</h2>
<p>Intuition behind the Maximum Likelihood Estimation: Estimator = the value of <span class="math inline">\(\theta\)</span> that is such that the probability of having observed <span class="math inline">\(\mathbf{y}\)</span> is the highest possible.</p>
<p>Assume that the time periods between the arrivals of two customers in a shop, denoted by <span class="math inline">\(y_i\)</span>, are i.i.d. and follow an exponential distribution, i.e.Â <span class="math inline">\(y_i \sim \mathcal{E}(\lambda)\)</span>.</p>
<p>You have observed these arrivals for some time, thereby constituting a sample <span class="math inline">\(\{y_1,\dots,y_n\}\)</span>. You want to estimate <span class="math inline">\(\lambda\)</span> (i.e.Â in that case, the vector of parameters is simply <span class="math inline">\(\theta = \lambda\)</span>).</p>
<p>The density of <span class="math inline">\(Y\)</span> is <span class="math inline">\(f(y;\lambda) = \dfrac{1}{\lambda}\exp(-y/\lambda)\)</span>. Fig. <a href="estimation-methods.html#fig:MLE1">5.1</a> represents that density functions for different values of <span class="math inline">\(\lambda\)</span>.</p>
<p>Your 200 observations are reported at the bottom of Fig. <a href="estimation-methods.html#fig:MLE1">5.1</a> (red).
You build the histogram and report it on the same chart.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:MLE1"></span>
<img src="AdvECTS_files/figure-html/MLE1-1.png" alt="The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations" width="90%"><p class="caption">
Figure 5.1: The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations
</p>
</div>
<p>What is your estimate of <span class="math inline">\(\lambda\)</span>?</p>
<p>Now, assume that you have only four observations: <span class="math inline">\(y_1=1.1\)</span>, <span class="math inline">\(y_2=2.2\)</span>, <span class="math inline">\(y_3=0.7\)</span> and <span class="math inline">\(y_4=5.0\)</span>.
What was the probability of observing, for a small <span class="math inline">\(\varepsilon\)</span>,</p>
<ul>
<li>
<span class="math inline">\(1.1-\varepsilon \le Y_1 &lt; 1.1+\varepsilon\)</span>,</li>
<li>
<span class="math inline">\(2.2-\varepsilon \le Y_2 &lt; 2.2+\varepsilon\)</span>,</li>
<li>
<span class="math inline">\(0.7-\varepsilon \le Y_3 &lt; 0.7+\varepsilon\)</span> and</li>
<li>
<span class="math inline">\(5.0-\varepsilon \le Y_4 &lt; 5.0+\varepsilon\)</span>?</li>
</ul>
<p>Because the <span class="math inline">\(y_i\)</span>s are i.i.d., this probability is <span class="math inline">\(\prod_{i=1}^4(2\varepsilon f(y_i,\lambda))\)</span>.
The next plot shows the probability (divided by <span class="math inline">\(16\varepsilon^4\)</span>) as a function of <span class="math inline">\(\lambda\)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:MLE2"></span>
<img src="AdvECTS_files/figure-html/MLE2-1.png" alt="Proba. that $y_i-\varepsilon \le Y_i &lt; y_i+\varepsilon$, $i \in \{1,2,3,4\}$. The vertical red line indicates the maximum of the function." width="90%"><p class="caption">
Figure 5.2: Proba. that <span class="math inline">\(y_i-\varepsilon \le Y_i &lt; y_i+\varepsilon\)</span>, <span class="math inline">\(i \in \{1,2,3,4\}\)</span>. The vertical red line indicates the maximum of the function.
</p>
</div>
<p>Back to the example with 200 observations:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:MLE3"></span>
<img src="AdvECTS_files/figure-html/MLE3-1.png" alt="Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function." width="90%"><p class="caption">
Figure 5.3: Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.
</p>
</div>
<div id="notations" class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> Notations<a class="anchor" aria-label="anchor" href="#notations"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math inline">\(f(y;\boldsymbol\theta)\)</span> denotes the probability density function (p.d.f.) of a random variable <span class="math inline">\(Y\)</span> which depends on a set of parameters <span class="math inline">\(\boldsymbol\theta\)</span>.</p>
<p>Density of <span class="math inline">\(n\)</span> independent and identically distributed (i.i.d.) observations of <span class="math inline">\(Y\)</span>:
<span class="math display">\[
f(y_1,\dots,y_n;\boldsymbol\theta) = \prod_{i=1}^n f(y_i;\boldsymbol\theta).
\]</span>
<span class="math inline">\(\mathbf{y}\)</span> denotes the vector of observations; <span class="math inline">\(\mathbf{y} = \{y_1,\dots,y_n\}\)</span>.</p>
<hr>
<div class="definition">
<p><span id="def:likelihood" class="definition"><strong>Definition 5.1  (Likelihood function) </strong></span><span class="math inline">\(\mathcal{L}: \boldsymbol\theta \rightarrow \mathcal{L}(\boldsymbol\theta;\mathbf{y})=f(y_1,\dots,y_n;\boldsymbol\theta)\)</span> is the <strong>likelihood function</strong>.</p>
</div>
<hr>
<p>We often work with <span class="math inline">\(\log \mathcal{L}\)</span>, the <strong>log-likelihood function</strong>.</p>
<div class="example">
<p><span id="exm:normal" class="example"><strong>Example 5.1  (Gaussian distribution) </strong></span>If <span class="math inline">\(y_i \sim \mathcal{N}(\mu,\sigma^2)\)</span>, then
<span class="math display">\[
\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = - \frac{1}{2}\sum_{i=1}^n\left( \log \sigma^2 + \log 2\pi + \frac{(y_i-\mu)^2}{\sigma^2} \right).
\]</span></p>
</div>
<hr>
<div class="definition">
<p><span id="def:score" class="definition"><strong>Definition 5.2  (Score) </strong></span>The score <span class="math inline">\(S(y;\boldsymbol\theta)\)</span> is given by <span class="math inline">\(\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\)</span>.</p>
</div>
<hr>
<p>If <span class="math inline">\(y_i \sim \mathcal{N}(\mu,\sigma^2)\)</span> (Example <a href="estimation-methods.html#exm:normal">5.1</a>), then
<span class="math display">\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} =
\left[\begin{array}{c}
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \mu}\\
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \sigma^2}
\end{array}\right] =
\left[\begin{array}{c}
\dfrac{y-\mu}{\sigma^2}\\
\frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right)
\end{array}\right].
\]</span></p>
<hr>
<div class="proposition">
<p><span id="prp:score" class="proposition"><strong>Proposition 5.1  (Score expectation) </strong></span>The expectation of the score is zero.</p>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>We have:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}\left(\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\right) &amp;=&amp;
\int \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta) dy \\
&amp;=&amp; \int \frac{\partial f(y;\boldsymbol\theta)/\partial \boldsymbol\theta}{f(y;\boldsymbol\theta)} f(y;\boldsymbol\theta) dy =
\frac{\partial}{\partial \boldsymbol\theta} \int f(y;\boldsymbol\theta) dy =
\partial 1 /\partial \boldsymbol\theta = 0,
\end{eqnarray*}\]</span>
which gives the result.</p>
</div>
<div class="definition">
<p><span id="def:Fisher" class="definition"><strong>Definition 5.3  (Fisher information matrix) </strong></span>The <strong>information matrix</strong> is (minus) the the expectation of the second derivatives of the log-likelihood function:
<span class="math display">\[
\mathcal{I}_Y(\boldsymbol\theta) = - \mathbb{E} \left( \frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right).
\]</span></p>
</div>
<hr>
<div class="proposition">
<p><span id="prp:Fisher" class="proposition"><strong>Proposition 5.2  </strong></span>We have <span class="math inline">\(\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E} \left[ \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right) \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)' \right] = \mathbb{V}ar[S(Y;\boldsymbol\theta)]\)</span>.</p>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>We have <span class="math inline">\(\frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = \frac{\partial^2 f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\frac{1}{f(Y;\boldsymbol\theta)} - \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta'}\)</span>. The expectation of the first right-hand side term is <span class="math inline">\(\partial^2 1 /(\partial \boldsymbol\theta \partial \boldsymbol\theta') = \mathbf{0}\)</span>, which gives the result.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-17" class="example"><strong>Example 5.2  </strong></span>If <span class="math inline">\(y_i \sim\,i.i.d.\, \mathcal{N}(\mu,\sigma^2)\)</span>, let <span class="math inline">\(\boldsymbol\theta = [\mu,\sigma^2]'\)</span> then
<span class="math display">\[
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} = \left[\frac{y-\mu}{\sigma^2} \quad \frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right) \right]'
\]</span>
and
<span class="math display">\[
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E}\left( \frac{1}{\sigma^4}
\left[
\begin{array}{cc}
\sigma^2&amp;y-\mu\\
y-\mu &amp; \frac{(y-\mu)^2}{\sigma^2}-\frac{1}{2}
\end{array}\right]
\right)=
\left[
\begin{array}{cc}
1/\sigma^2&amp;0\\
0 &amp; 1/(2\sigma^4)
\end{array}\right].
\]</span></p>
</div>
<hr>
<div class="proposition">
<p><span id="prp:additiv" class="proposition"><strong>Proposition 5.3  (Additive property of the Info. mat.) </strong></span>The information matrix resulting from two independent experiments is the sum of the information matrices:
<span class="math display">\[
\mathcal{I}_{X,Y}(\boldsymbol\theta) = \mathcal{I}_X(\boldsymbol\theta) + \mathcal{I}_Y(\boldsymbol\theta).
\]</span></p>
</div>
<hr>
<p>:::{.proof} Immediately obtained from the definition (see Def. @ref{def:Fisher}).
:::</p>
<hr>
<div class="theorem">
<p><span id="thm:FDCR" class="theorem"><strong>Theorem 5.1  (Fr\'echet-Darmois-Cram\'er-Rao bound) </strong></span>Consider an unbiased estimator of <span class="math inline">\(\boldsymbol\theta\)</span> denoted by <span class="math inline">\(\hat{\boldsymbol\theta}(Y)\)</span>.</p>
<p>The variance of the random variable <span class="math inline">\(\boldsymbol\omega'\hat{\boldsymbol\theta}\)</span> (which is a linear combination of the components of <span class="math inline">\(\hat{\boldsymbol\theta}\)</span>) is larger than:
<span class="math display">\[
(\boldsymbol\omega'\boldsymbol\omega)^2/(\boldsymbol\omega' \mathcal{I}_Y(\boldsymbol\theta) \boldsymbol\omega).
\]</span></p>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span>The Cauchy-Schwarz inequality implies that <span class="math inline">\(\sqrt{\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y))\mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))} \ge |\boldsymbol\omega'\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)]\boldsymbol\omega |\)</span>. Now, <span class="math inline">\(\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)] = \int_y \hat{\boldsymbol\theta}(y) \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta)dy = \frac{\partial}{\partial \boldsymbol\theta}\int_y \hat{\boldsymbol\theta}(y) f(y;\boldsymbol\theta)dy = \mathbf{I}\)</span> because <span class="math inline">\(\hat{\boldsymbol\theta}\)</span> is unbiased.</p>
<p>Therefore <span class="math inline">\(\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y)) \ge \mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))^{-1} (\boldsymbol\omega'\boldsymbol\omega)^2\)</span>. Prop. <a href="estimation-methods.html#prp:Fisher">5.2</a> leads to the result.</p>
</div>
<div class="definition">
<p><span id="def:identif" class="definition"><strong>Definition 5.4  </strong></span>The vector of parameters <span class="math inline">\(\boldsymbol\theta\)</span> is identifiable if, for any other vector <span class="math inline">\(\boldsymbol\theta^*\)</span>:
<span class="math display">\[
\boldsymbol\theta^* \ne \boldsymbol\theta \Rightarrow \mathcal{L}(\boldsymbol\theta^*;\mathbf{y}) \ne \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:MLEest" class="definition"><strong>Definition 5.5  (Maximum Likelihood Estimator (MLE)) </strong></span>The maximum likelihood estimator (MLE) is the vector <span class="math inline">\(\boldsymbol\theta\)</span> that maximizes the likelihood function. Formally:
<span class="math display" id="eq:MLEestimator">\[\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\mathbf{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).\tag{5.2}
\end{equation}\]</span></p>
</div>
<div class="definition">
<p><span id="def:likFunction" class="definition"><strong>Definition 5.6  (Likelihood equation) </strong></span>Necessary condition for maximizing the likelihood function:
<span class="math display">\[\begin{equation}
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta} = \mathbf{0}.
\end{equation}\]</span></p>
</div>
<div class="hypothesis">
<p><span id="hyp:MLEregularity" class="hypothesis"><strong>Hypothesis 5.1  (Regularity assumptions) </strong></span>We have:</p>
<ol style="list-style-type: lower-roman">
<li><p><span class="math inline">\(\boldsymbol\theta \in \Theta\)</span> where <span class="math inline">\(\Theta\)</span> is compact.</p></li>
<li><p><span class="math inline">\(\boldsymbol\theta_0\)</span> is identified.</p></li>
<li><p>The log-likelihood function is continuous in <span class="math inline">\(\boldsymbol\theta\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\)</span> exists.</p></li>
<li><p>The log-likelihood function is such that <span class="math inline">\((1/n)\log\mathcal{L}(\boldsymbol\theta;\mathbf{y})\)</span> converges almost surely to <span class="math inline">\(\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))\)</span>, uniformly in <span class="math inline">\(\boldsymbol\theta \in \Theta\)</span>.</p></li>
<li><p>The log-likelihood function is twice continuously differentiable in an open neighborood of <span class="math inline">\(\boldsymbol\theta_0\)</span>.</p></li>
<li><p>The matrix <span class="math inline">\(\mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \quad \mbox{(Fisher Information matrix)}\)</span> exists and is nonsingular.</p></li>
</ol>
</div>
<hr>
<div class="proposition">
<p><span id="prp:MLEproperties" class="proposition"><strong>Proposition 5.4  (Properties of MLE) </strong></span>Under regularity conditions (Assumptions <a href="estimation-methods.html#hyp:MLEregularity">5.1</a>), the MLE is:</p>
<ol style="list-style-type: lower-alpha">
<li><p><strong>Consistent</strong>: <span class="math inline">\(\mbox{plim}\quad \boldsymbol\theta_{MLE} = \theta_0\)</span> (<span class="math inline">\(\theta_0\)</span> is the true vector of parameters).</p></li>
<li><p><strong>Asymptotically normal</strong>: <span class="math inline">\(\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\mathbf{I}(\boldsymbol\theta_0)^{-1})\)</span>, where
<span class="math display">\[
\mathbf{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0). \quad \mbox{(Fisher Info. matrix)}
\]</span></p></li>
<li><p><strong>Asymptotically efficient</strong>: <span class="math inline">\(\boldsymbol\theta_{MLE}\)</span> is asymptotically efficient and achieves the Fr'echet-Darmois-Cram'er-Rao lower bound for consistent estimators.</p></li>
<li><p><strong>Invariant</strong>: The MLE of <span class="math inline">\(g(\boldsymbol\theta_0)\)</span> is <span class="math inline">\(g(\boldsymbol\theta_{MLE})\)</span> if <span class="math inline">\(g\)</span> is a continuous and continuously differentiable function.</p></li>
</ol>
</div>
<hr>
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>Proof</em>. </span>See <a href="https://www.dropbox.com/s/8xelbghtvnd43yh/Proofs_MLE.pdf?dl=0">Online additional material</a>.</p>
</div>
<p>Note that (b) also writes:
<span class="math display" id="eq:normMLE">\[\begin{equation}
\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0)^{-1}). \tag{5.3}
\end{equation}\]</span></p>
<p>The asymptotic covariance matrix of the MLE is:
<span class="math display">\[
[\mathbf{I}(\boldsymbol\theta_0)]^{-1} = \left[- \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\mathbf{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \right]^{-1}.
\]</span>
A direct (analytical) evaluation of this expectation is often out of reach.</p>
<p>It can however be estimated by, either:
<span class="math display" id="eq:I2">\[\begin{eqnarray}
\hat{\mathbf{I}}_1^{-1} &amp;=&amp;  \left( - \frac{\partial^2 \log \mathcal{L}({\boldsymbol\theta_{MLE}};\mathbf{y})}{\partial {\boldsymbol\theta} \partial {\boldsymbol\theta}'}\right)^{-1}, \tag{5.4}\\
\hat{\mathbf{I}}_2^{-1} &amp;=&amp;  \left( \sum_{i=1}^n \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta}} \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta'}} \right)^{-1}.  \tag{5.5}
\end{eqnarray}\]</span></p>
</div>
<div id="to-sum-up-mle-in-practice" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> To sum up â MLE in practice<a class="anchor" aria-label="anchor" href="#to-sum-up-mle-in-practice"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>A parametric model (depending on the vector of parameters <span class="math inline">\(\boldsymbol\theta\)</span> whose âtrueâ value is <span class="math inline">\(\boldsymbol\theta_0\)</span>) is specified.</p></li>
<li><p>i.i.d. sources of randomness are identified.</p></li>
<li><p>The density associated to one observation <span class="math inline">\(y_i\)</span> is computed analytically (as a function of <span class="math inline">\(\boldsymbol\theta\)</span>): <span class="math inline">\(f(y;\boldsymbol\theta)\)</span>.</p></li>
<li><p>The log-likelihood is <span class="math inline">\(\log \mathcal{L}(\boldsymbol\theta;\mathbf{y}) = \sum_i \log f(y_i;\boldsymbol\theta)\)</span>.</p></li>
<li><p>The MLE estimator results from the optimization problem (this is Eq. <a href="estimation-methods.html#eq:MLEestimator">(5.2)</a>):
<span class="math display">\[\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\mathbf{y}).
\end{equation}\]</span></p></li>
<li><p>We have: <span class="math inline">\(\boldsymbol\theta_{MLE} \sim \mathcal{N}(\theta_0,\mathbf{I}(\boldsymbol\theta_0)^{-1})\)</span>, where <span class="math inline">\(\mathbf{I}(\boldsymbol\theta_0)^{-1}\)</span> is estimated by means of Eq. <a href="estimation-methods.html#eq:III1">(5.4)</a> or Eq. <a href="estimation-methods.html#eq:I2">(5.5)</a>. Most of the time, this computation is numerical.</p></li>
</ul>
</div>
<div id="example-mle-estimation-of-a-mixture-of-gaussian-distribution" class="section level3" number="5.2.3">
<h3>
<span class="header-section-number">5.2.3</span> Example: MLE estimation of a mixture of Gaussian distribution<a class="anchor" aria-label="anchor" href="#example-mle-estimation-of-a-mixture-of-gaussian-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the returns of the SMI index. Letâs assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. <span class="math inline">\(f(x;\boldsymbol\theta)\)</span>, with <span class="math inline">\(\boldsymbol\theta = [\mu_1,\sigma_1,\mu_2,\sigma_2,p]'\)</span>, is given by:
<span class="math display">\[
p \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) + (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right).
\]</span>
(See <a href="https://jrenne.shinyapps.io/density/">p.d.f. of mixtures of Gaussian dist.</a>)</p>
<p>The maximum likelihood estimate is <span class="math inline">\(\boldsymbol\theta_{MLE}=[0.30,1.40,-1.45,3.61,0.87]'\)</span>.</p>
<p>The first two entries of the diagonal of <span class="math inline">\(\hat{\mathbf{I}}_1^{-1}\)</span> are <span class="math inline">\(0.00528\)</span> and <span class="math inline">\(0.00526\)</span>. They are the estimates of <span class="math inline">\(\mathbb{V}ar(\mu_{1,MLE})\)</span> and of <span class="math inline">\(\mathbb{V}ar(\sigma_{1,MLE})\)</span>, respectively.</p>
<p>95% confidence intervals for <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\sigma_1\)</span> are, respectively:
<span class="math display">\[
0.30 \pm 1.96\underbrace{\sqrt{0.00528}}_{=0.0726} \quad \mbox{ and } \quad 1.40 \pm 1.96\underbrace{\sqrt{0.00526}.}_{=0.0725}
\]</span></p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">smi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="st">"https://raw.githubusercontent.com/jrenne/Data4courses/master/SMI/SMI.csv"</span>,</span>
<span>                dec <span class="op">=</span> <span class="st">"."</span>,header <span class="op">=</span> <span class="cn">TRUE</span>, na.strings <span class="op">=</span> <span class="st">"null"</span><span class="op">)</span></span>
<span><span class="va">smi</span><span class="op">$</span><span class="va">Date</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/zoo/man/yearmon.html">as.Date</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">Date</span>,<span class="st">"%m/%d/%y"</span><span class="op">)</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">smi</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">h</span> <span class="op">&lt;-</span> <span class="fl">5</span> <span class="co"># holding period (one week)</span></span>
<span><span class="va">smi</span><span class="op">$</span><span class="va">r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="va">h</span><span class="op">)</span>,</span>
<span>           <span class="fl">100</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">Close</span><span class="op">[</span><span class="op">(</span><span class="fl">1</span><span class="op">+</span><span class="va">h</span><span class="op">)</span><span class="op">:</span><span class="cn">T</span><span class="op">]</span><span class="op">/</span><span class="va">smi</span><span class="op">$</span><span class="va">Close</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">-</span><span class="va">h</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">indic.dates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">1</span>,<span class="cn">T</span>,by<span class="op">=</span><span class="fl">5</span><span class="op">)</span>  <span class="co"># weekly returns</span></span>
<span><span class="va">smi</span> <span class="op">&lt;-</span> <span class="va">smi</span><span class="op">[</span><span class="va">indic.dates</span>,<span class="op">]</span></span>
<span><span class="va">smi</span> <span class="op">&lt;-</span> <span class="va">smi</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span><span class="op">(</span><span class="va">smi</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">Date</span>,<span class="va">smi</span><span class="op">$</span><span class="va">r</span>,type<span class="op">=</span><span class="st">"l"</span>,main<span class="op">=</span><span class="st">"5-day return on SMI index"</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">"in percent"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">+</span><span class="fl">2</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,lty<span class="op">=</span><span class="fl">3</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">-</span><span class="fl">2</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,lty<span class="op">=</span><span class="fl">3</span>,col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="AdvECTS_files/figure-html/smiData-1.png" width="672"></div>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span>,<span class="va">y</span><span class="op">)</span><span class="op">{</span> <span class="co"># Likelihood function</span></span>
<span>  <span class="va">mu.1</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>; <span class="va">mu.2</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="va">sigma.1</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>; <span class="va">sigma.2</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span></span>
<span>  <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">+</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">res</span> <span class="op">&lt;-</span> <span class="va">p</span><span class="op">*</span><span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">*</span><span class="va">sigma.1</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">y</span><span class="op">-</span><span class="va">mu.1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">sigma.1</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>               <span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">p</span><span class="op">)</span><span class="op">*</span><span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">pi</span><span class="op">*</span><span class="va">sigma.2</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">y</span><span class="op">-</span><span class="va">mu.2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">sigma.2</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">log.f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span>,<span class="va">y</span><span class="op">)</span><span class="op">{</span> <span class="co">#log-Likelihood function</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">f</span><span class="op">(</span><span class="va">theta</span>,<span class="va">y</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">res.optim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span>,<span class="fl">0.5</span>,<span class="fl">1.5</span>,<span class="fl">.5</span><span class="op">)</span>,</span>
<span>                   <span class="va">log.f</span>,</span>
<span>                   y<span class="op">=</span><span class="va">smi</span><span class="op">$</span><span class="va">r</span>,</span>
<span>                   method<span class="op">=</span><span class="st">"BFGS"</span>, <span class="co"># could be "Nelder-Mead"</span></span>
<span>                   control<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace<span class="op">=</span><span class="cn">FALSE</span>,maxit<span class="op">=</span><span class="fl">100</span><span class="op">)</span>,hessian<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="va">res.optim</span><span class="op">$</span><span class="va">par</span></span>
<span><span class="va">theta</span></span></code></pre></div>
<pre><code>## [1]  0.3012379 -1.3167476  1.7715072  4.8197596  1.9454889</code></pre>
<p>Now, let us compute estimates of the covariance matrix of the MLE:</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Hessian approach:</span></span>
<span><span class="va">J</span> <span class="op">&lt;-</span> <span class="va">res.optim</span><span class="op">$</span><span class="va">hessian</span></span>
<span><span class="va">I.1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">J</span><span class="op">)</span></span>
<span><span class="co"># Outer-product of gradient approach:</span></span>
<span><span class="va">log.f.0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">f</span><span class="op">(</span><span class="va">theta</span>,<span class="va">smi</span><span class="op">$</span><span class="va">r</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="fl">.00000001</span></span>
<span><span class="va">d.log.f</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">theta.i</span> <span class="op">&lt;-</span> <span class="va">theta</span></span>
<span>  <span class="va">theta.i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">theta.i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="va">epsilon</span></span>
<span>  <span class="va">log.f.i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">f</span><span class="op">(</span><span class="va">theta.i</span>,<span class="va">smi</span><span class="op">$</span><span class="va">r</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">d.log.f</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">d.log.f</span>,</span>
<span>                   <span class="op">(</span><span class="va">log.f.i</span> <span class="op">-</span> <span class="va">log.f.0</span><span class="op">)</span><span class="op">/</span><span class="va">epsilon</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">V</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">d.log.f</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">d.log.f</span></span>
<span><span class="va">I.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">d.log.f</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">d.log.f</span><span class="op">)</span></span>
<span><span class="co"># Misspecification-robust approach (sandwich formula):</span></span>
<span><span class="va">I.3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">J</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">V</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html">solve</a></span><span class="op">(</span><span class="va">J</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">I.1</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">I.2</span><span class="op">)</span>,<span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">I.3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>##             [,1]        [,2]       [,3]
## [1,] 0.003683422 0.003199481 0.00586160
## [2,] 0.226892824 0.194283391 0.38653389
## [3,] 0.005764271 0.002769579 0.01712255
## [4,] 0.194081311 0.047466419 0.83130838
## [5,] 0.092114437 0.040366005 0.31347858</code></pre>
<!-- \begin{figure} -->
<!-- \caption{Density of 5-day returns on SMI index} -->
<!-- \includegraphics[width=.9\linewidth]{../../figures/Figure_kernel_smi.pdf} -->
<!-- \label{fig:illuskernel_smi} -->
<!-- \begin{tiny} -->
<!-- Gaussian kernel, $h=0.5$ (in percent). -->
<!-- The data spans the period from 2 June 2006 to 23 February 2016 at the daily frequency. -->
<!-- Left-hand plot: the blue lines indicates $\mu \pm 2 \sigma$, where $\mu$ is the sample mean of the returns and $\sigma$ is their sample standard deviation. Right-hand plot: the red dotted line is the density $\mathcal{N}(\mu,\sigma^2)$ -->
<!-- \end{tiny} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->
<!-- \begin{frame}{} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[MLE estim. of a mixture of Gaussian distri. (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{Estimated density (vs kernel-based estimate)} -->
<!-- \includegraphics[width=1\linewidth]{../../figures/Figure_kernel_smiMLE.pdf} -->
<!-- \label{fig:MLE1} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="panel-regressions.html"><span class="header-section-number">4</span> Panel regressions</a></div>
<div class="next"><a href="microeconometrics.html"><span class="header-section-number">6</span> Microeconometrics</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimation-methods"><span class="header-section-number">5</span> Estimation Methods</a></li>
<li>
<a class="nav-link" href="#generalized-method-of-moments"><span class="header-section-number">5.1</span> Generalized Method of Moments</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#framework"><span class="header-section-number">5.1.1</span> Framework</a></li></ul>
</li>
<li>
<a class="nav-link" href="#maximum-likelihood-estimation"><span class="header-section-number">5.2</span> Maximum Likelihood Estimation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#notations"><span class="header-section-number">5.2.1</span> Notations</a></li>
<li><a class="nav-link" href="#to-sum-up-mle-in-practice"><span class="header-section-number">5.2.2</span> To sum up â MLE in practice</a></li>
<li><a class="nav-link" href="#example-mle-estimation-of-a-mixture-of-gaussian-distribution"><span class="header-section-number">5.2.3</span> Example: MLE estimation of a mixture of Gaussian distribution</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Advanced Econometrics</strong>" was written by Jean-Paul Renne. It was last built on 2022-08-11.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

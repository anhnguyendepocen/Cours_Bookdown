<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Statistical tests | Applied Econometrics</title>
<meta name="author" content="Jean-Paul Renne">
<meta name="description" content="We run a statistical test when we want to know whether some hypothesis about a vector of parameters \(\theta\) —that is imperfectly observed— is consistent with data that are seen as random, and...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 4 Statistical tests | Applied Econometrics">
<meta property="og:type" content="book">
<meta property="og:description" content="We run a statistical test when we want to know whether some hypothesis about a vector of parameters \(\theta\) —that is imperfectly observed— is consistent with data that are seen as random, and...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Statistical tests | Applied Econometrics">
<meta name="twitter:description" content="We run a statistical test when we want to know whether some hypothesis about a vector of parameters \(\theta\) —that is imperfectly observed— is consistent with data that are seen as random, and...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Applied Econometrics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Before starting</a></li>
<li><a class="" href="Basics.html"><span class="header-section-number">2</span> Basic statistical results</a></li>
<li><a class="" href="TCL.html"><span class="header-section-number">3</span> Central Limit Theorem</a></li>
<li><a class="active" href="Tests.html"><span class="header-section-number">4</span> Statistical tests</a></li>
<li><a class="" href="ChapterLS.html"><span class="header-section-number">5</span> Linear Regressions</a></li>
<li><a class="" href="Panel.html"><span class="header-section-number">6</span> Panel regressions</a></li>
<li><a class="" href="estimation-methods.html"><span class="header-section-number">7</span> Estimation Methods</a></li>
<li><a class="" href="microeconometrics.html"><span class="header-section-number">8</span> Microeconometrics</a></li>
<li><a class="" href="TS.html"><span class="header-section-number">9</span> Time Series</a></li>
<li><a class="" href="append.html"><span class="header-section-number">10</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="Tests" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Statistical tests<a class="anchor" aria-label="anchor" href="#Tests"><i class="fas fa-link"></i></a>
</h1>
<p>We run a statistical test when we want to know whether some hypothesis about a vector of parameters <span class="math inline">\(\theta\)</span> —that is imperfectly observed— is consistent with data that are seen as random, and whose randomness depend on <span class="math inline">\(\theta\)</span>.</p>
<p>Typically, assume you observe a sample <span class="math inline">\(\{x_1,\dots,x_n\}\)</span> where the <span class="math inline">\(x_i\)</span>’s are i.i.d., of mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (these two parameters being unobseved). One may want to know whether <span class="math inline">\(\mu=0\)</span> or, maybe, whether <span class="math inline">\(\sigma = 1\)</span>.</p>
<p>The hypothesis the researcher wants to test is called the <em>null hypothesis</em>. It is often denoted by <span class="math inline">\(H_0\)</span>. It is a conjecture about a given property of a population. Without loss of generality, it can be stated as:
<span class="math display">\[
H_0:\;\{\theta \in \Theta\}.
\]</span>
It can also be defined through a function <span class="math inline">\(h\)</span> (say):<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We can relate the previous and the next equation through &lt;span class="math inline"&gt;\(\Theta = \{\theta, \,s.t.\,h(\theta)=0\}\)&lt;/span&gt;.&lt;/p&gt;'><sup>3</sup></a>
<span class="math display">\[
H_0:\;h(\theta)=0.
\]</span></p>
<p>The <em>alternative hypothesis</em>, often denoted <span class="math inline">\(H_1\)</span> is then defined by <span class="math inline">\(H_1:\;\{\theta \in \Theta^c\}\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Note that researchers may want to find support for the null hypothesis &lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt; as well as for the alternative hypothesis &lt;span class="math inline"&gt;\(H_1\)&lt;/span&gt;.&lt;/p&gt;'><sup>4</sup></a></p>
<p>The ingredients of a statistical are as follows:</p>
<ul>
<li>a vector of (unknown) parameters (<span class="math inline">\(\theta\)</span>),</li>
<li>a <strong>test statistic</strong>, that is a function of the components of a sample (<span class="math inline">\(S(\mathbf{x})\)</span>, where <span class="math inline">\(\mathbf{x}=\{x_1,\dots,x_n\}\)</span>, say), and</li>
<li>a <strong>critical region</strong> (<span class="math inline">\(\Omega\)</span>, say), defined as a set of implausible values of <span class="math inline">\(S\)</span> under <span class="math inline">\(H_0\)</span>.</li>
</ul>
<p>To implement a test statistic, we need to know the distribution of <span class="math inline">\(S\)</span> under the null hypothesis <span class="math inline">\(H_0\)</span>. Equipped, with such a distribution, we compute <span class="math inline">\(S(\mathbf{x})\)</span> and we look at its location; more specifically, we look whether it lies within the critical region <span class="math inline">\(\Omega\)</span>. The latter corresponds to “implausible” regions of this distribution (typically the tails). If <span class="math inline">\(S \in \Omega\)</span>, then we reject the null hypothesis, which amounts to saying: if <span class="math inline">\(H_0\)</span> were true, it would have been unlikely to get such a large (or small) draw for <span class="math inline">\(S\)</span>.</p>
<p>Hence, there are two possible outcomes for a statisitcal test:</p>
<ul>
<li>
<span class="math inline">\(H_0\)</span> is rejected if <span class="math inline">\(S \in \Omega\)</span>;</li>
<li>
<span class="math inline">\(H_0\)</span> is not rejected if <span class="math inline">\(S \not\in \Omega\)</span>.</li>
</ul>
<p>This implies that there are two types of errors one can make in the context of a statistical test:</p>
<ul>
<li>
<strong>Type I error</strong>: Reject <span class="math inline">\(H_0\)</span> while it is true = False Positive (FP).</li>
<li>
<strong>Type II error</strong>: Absence of rejection of <span class="math inline">\(H_0\)</span> while it is false = False Negative (FN).</li>
</ul>
<div class="example">
<p><span id="exm:FPFN" class="example"><strong>Example 4.1  (Early Warning Signals (practical illustration of type-I and type-II errors)) </strong></span>Early warning systems (EWS) are approaches designed to warn policy makers of potential future economic and financial crises. See, e.g., <a href="http://www.ecb.europa.eu/events/pdf/conferences/140623/Vasicek-et-al_Comparing-Different-Early-Warning-Systems.pdf?F96bbb525a26071ecf97f9154fb3cc73">ECB (2014)</a></p>
<p>To implement such approaches, researchers look for signals/indices forecasting crises. Suppose one index (<span class="math inline">\(W\)</span>, say) appears to be large before financial crises; one may define an EWS by saying that a crisis is coming when <span class="math inline">\(W&gt;a\)</span>, where <span class="math inline">\(a\)</span> is a given threshold. It is easliy seen that the lower (respectively higeher) <span class="math inline">\(a\)</span>, the larger the fraction of FP (resp. of FN).</p>
</div>
<div id="size-and-power-of-a-test" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Size and power of a test<a class="anchor" aria-label="anchor" href="#size-and-power-of-a-test"><i class="fas fa-link"></i></a>
</h2>
<div class="definition">
<p><span id="def:sizepower" class="definition"><strong>Definition 4.1  (Size and Power of a test) </strong></span>For a given test,</p>
<ul>
<li>the probability of type-I errors, denoted by <span class="math inline">\(\alpha\)</span>, is called the <strong>size</strong>, or <strong>significance level</strong>, of the test,</li>
<li>the <strong>power</strong> of a test is equal to <span class="math inline">\(1 - \beta\)</span>, where <span class="math inline">\(\beta\)</span> is the probability of type-II errors.</li>
</ul>
</div>
<p>Formally, the previous definitions can be written as follows:
<span class="math display">\[\begin{eqnarray}
\alpha &amp;=&amp; \mathbb{P}(S \in \Omega|H_0) \quad \mbox{(Proba. of a false positive)}\\
\beta &amp;=&amp; \mathbb{P}(S \not\in \Omega|H_1) \quad \mbox{(Proba. of a false negative)}.
\end{eqnarray}\]</span></p>
<p>The power is the probability that the test will lead to the rejection of the null hypothesis if the latter is false (almost ruling out false negatives). Therefore, for a given size, we prefer tests with high power.</p>
<p>In most cases, there is a trade-off between size and power, whch is easily understood in the EWS context (Example <a href="Tests.html#exm:FPFN">4.1</a>): increasing the threshold <span class="math inline">\(a\)</span> reduces the fraction of FP (thereby reducing the size of the test), but it increases the fraction of FN (thereby reducing the power of the test).</p>
</div>
<div id="the-different-types-of-statistical-tests" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> The different types of statistical tests<a class="anchor" aria-label="anchor" href="#the-different-types-of-statistical-tests"><i class="fas fa-link"></i></a>
</h2>
<p>How to determine the critical region? Loosely speaking, we want the critical region to be a set of “implausible” values of the test statistic <span class="math inline">\(S\)</span> under the null hypothesis <span class="math inline">\(H_0\)</span>. The lower the size of the test (<span class="math inline">\(\alpha\)</span>), the more implausible these values. Recall that, by definition of the size of the test, <span class="math inline">\(\alpha = \mathbb{P}(S \in \Omega|H_0)\)</span>. That is, if <span class="math inline">\(\alpha\)</span> is small, there is only a small probability that <span class="math inline">\(S\)</span> lies in <span class="math inline">\(\Omega\)</span> under <span class="math inline">\(H_0\)</span>.</p>
<p>Consider the case where, under <span class="math inline">\(H_0\)</span>, the distribution of the test statistic is symmetrical (as in the case of the normal distribution or of the Student-t distribution). In this case, the critical region is usually defined by the union of the two tails of the distribution. The test is then said to be a <strong>two-tailed test</strong> or a <strong>two-sided test</strong>. This situation is illustrated by Figures <a href="Tests.html#fig:Illusttest1">4.1</a> and <a href="Tests.html#fig:Illusttest2">4.2</a>. (Use this <a href="https://jrenne.shinyapps.io/tests/">web interface</a> to explore alternative situations.)</p>
<p>Figure <a href="Tests.html#fig:Illusttest2">4.2</a> also illustrates the notion of <em>p-value</em> (in the case of a two-sided test). The p-value can be defined as the value of the size of the test <span class="math inline">\(\alpha\)</span> that is such that the computed test statistic, <span class="math inline">\(S\)</span>, is at the “frontier” of the critical region. Given this definition, if the p-value is smaller (respoectively larger) than the size of the test, we reject (resp. cannot rejet) the null hypothesis at the <span class="math inline">\(\alpha\)</span> significance level.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:Illusttest1"></span>
<img src="ApplEcts_files/figure-html/Illusttest1-1.png" alt="Two-sided test. Under $H_0$, $S \sim t(5)$. $\alpha$ is the size of the test." width="672"><p class="caption">
Figure 4.1: Two-sided test. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S \sim t(5)\)</span>. <span class="math inline">\(\alpha\)</span> is the size of the test.
</p>
</div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:Illusttest2"></span>
<img src="ApplEcts_files/figure-html/Illusttest2-1.png" alt="Two-sided test. Under $H_0$, $S \sim t(5)$. $\alpha$ is the size of the test." width="672"><p class="caption">
Figure 4.2: Two-sided test. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S \sim t(5)\)</span>. <span class="math inline">\(\alpha\)</span> is the size of the test.
</p>
</div>
<p>Figures <a href="Tests.html#fig:IllusTestOneSided1">4.3</a> and <a href="Tests.html#fig:IllusTestOneSided2">4.4</a> illustrate the <strong>one-tailed</strong>, or <strong>one-sided</strong> situation. These tests are tyxpically employed when the distribution of the test statistic under the null hypothesis has a support on <span class="math inline">\(\mathbb{R}^+\)</span> (e.g. <span class="math inline">\(\chi^2\)</span>).</p>
<p>Figure <a href="Tests.html#fig:IllusTestOneSided2">4.4</a> also illustrates the notion of p-value associated with a one-sided statistical test.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:IllusTestOneSided1"></span>
<img src="ApplEcts_files/figure-html/IllusTestOneSided1-1.png" alt="One-sided test. Under $H_0$, $S \sim \chi^2(5)$. $\alpha$ is the size of the test." width="672"><p class="caption">
Figure 4.3: One-sided test. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S \sim \chi^2(5)\)</span>. <span class="math inline">\(\alpha\)</span> is the size of the test.
</p>
</div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:IllusTestOneSided2"></span>
<img src="ApplEcts_files/figure-html/IllusTestOneSided2-1.png" alt="One-sided test. Under $H_0$, $S \sim \chi^2(5)$. $\alpha$ is the size of the test." width="672"><p class="caption">
Figure 4.4: One-sided test. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(S \sim \chi^2(5)\)</span>. <span class="math inline">\(\alpha\)</span> is the size of the test.
</p>
</div>
</div>
<div id="a-practical-illustration-of-size-and-power" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> A practical illustration of size and power<a class="anchor" aria-label="anchor" href="#a-practical-illustration-of-size-and-power"><i class="fas fa-link"></i></a>
</h2>
<p>Consider a factory that produces metal cylinders whose diameter has to be equal to 1cm. The tolerance is <span class="math inline">\(a=0.01cm\)</span>. That is, more than 90% of the pieces have to satisfy the tolerance for the whole production (say 1.000.000 pieces) to be bought by the client.</p>
<p>The production technology is such that a proportion <span class="math inline">\(\theta\)</span> (imperfectly known) of the pieces does not satisfy the tolerance. (The parameter <span class="math inline">\(\theta\)</span> could be computed by measuring all the pieces but this would be costly.) Instead, it is decided that <span class="math inline">\(n \ll 1.000.000\)</span> pieces will be measured.</p>
<p>In this ocntext, the null hypothesis <span class="math inline">\(H_0\)</span> is <span class="math inline">\(\theta &lt; 10\%\)</span>. The producing firm would like it to be true.</p>
<p>Let us denote by <span class="math inline">\(d_i\)</span> the binary indicator defined as:
<span class="math display">\[
d_i = \left\{
\begin{array}{cll}
0 &amp; \mbox{if the size of the $i^{th}$ cylinder is in $[1-a,1+a]$;}\\
1 &amp; \mbox{otherwise.}
\end{array}
\right.
\]</span></p>
<p>We set <span class="math inline">\(x_n=\sum_{i=1}^n d_i\)</span>. That is, <span class="math inline">\(x_n\)</span> is the number of measured pieces that do not satisfy the tolerance (out of <span class="math inline">\(n\)</span>).</p>
<p>A decision rule is: accept <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\dfrac{x_n}{n} \le b\)</span>, reject otherwise.</p>
<p>In that case, the test statistic is <span class="math inline">\(S_n=\frac{x_n}{n}\)</span> and the critical region is <span class="math inline">\(\Omega = [b,1]\)</span>. The probability to reject <span class="math inline">\(H_0\)</span> is:
<span class="math display">\[\begin{eqnarray*}
\mathbb{P}_\theta(S_n \in \Omega) = \sum_{i=b \times n+1}^{n}C_{n}^i\theta^i(1-\theta)^{n-i}.
\end{eqnarray*}\]</span></p>
<p><a href="https://jrenne.shinyapps.io/Factory/">web interface</a>.</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:Factory"></span>
<img src="ApplEcts_files/figure-html/Factory-1.png" alt="Factory example." width="672"><p class="caption">
Figure 4.5: Factory example.
</p>
</div>
<div class="definition">
<p><span id="def:asmyptlevel" class="definition"><strong>Definition 4.2  (Asymptotic level) </strong></span>An asymptotic test with critical region <span class="math inline">\(\Omega_n\)</span> has an asymptotic level equal to <span class="math inline">\(\alpha\)</span> if:
<span class="math display">\[
\underset{\theta \in \Theta}{\mbox{sup}} \quad \underset{n \rightarrow \infty}{\mbox{lim}} \mathbb{P}_\theta (S_n \in \Omega_n) = \alpha.
\]</span></p>
</div>
<div class="definition">
<p><span id="def:asmyptconsisttest" class="definition"><strong>Definition 4.3  (Asymptotically consistent test) </strong></span>An asymptotic test with critical region <span class="math inline">\(\Omega_n\)</span> is consistent if:
<span class="math display">\[
\forall \theta \in \Theta^c, \quad \mathbb{P}_\theta (S_n \in \Omega_n) \rightarrow 1.
\]</span></p>
</div>
<p>Back to the Factory example (Exmpl. <a href="#exm:factory"><strong>??</strong></a>): Asymptotic level</p>
<p>Because <span class="math inline">\(S_n =\bar{d}_n\)</span>, and since <span class="math inline">\(\mathbb{E}(d_i)=\theta\)</span> and <span class="math inline">\(\mathbb{V}ar(d_i)=\theta(1-\theta)\)</span>, the CLT (Thm <span class="math inline">\(\ref{thm:LindbergLevyCLT}\)</span>) leads to:
<span class="math display">\[
    S_n \sim \mathcal{N}\left(\theta,\frac{1}{n}\theta(1-\theta)\right) \quad or \quad \frac{\sqrt{n}(S_n-\theta)}{\sqrt{\theta(1-\theta)}} \sim \mathcal{N}(0,1)
    \]</span></p>
<p>Hence, <span class="math inline">\(\mathbb{P}_\theta (S_n \in \Omega_n)=\mathbb{P}_\theta (S_n &gt; b) \approx 1-\Phi\left(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}}\right)\)</span>.</p>
<p><span class="math inline">\(\theta \rightarrow 1-\Phi\left(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}}\right)\)</span> increases w.r.t. <span class="math inline">\(\theta\)</span>, therefore:
<span class="math display">\[
    \underset{\theta \in \Theta=[0,0.1]}{\mbox{sup}} \quad \mathbb{P}_\theta (S_n &gt; b_n) = \mathbb{P}_{\theta=0.1} (S_n \in \Omega_n)\approx\]</span>
<span class="math display">\[
    1-\Phi\left(\frac{\sqrt{n}(b_n-0.1)}{0.3}\right).
    \]</span>
Hence, if we set <span class="math inline">\(b_n = 0.1 + 0.3\Phi^{-1}(1-\alpha)/\sqrt{n}\)</span>, then we have <span class="math inline">\({\mbox{sup}}_{\theta \in \Theta=[0,0.1]} \quad \mathbb{P}_\theta (S_n &gt; b_n) \approx \alpha\)</span> for large values of <span class="math inline">\(n\)</span>.</p>
<p>We proceed under the assumption that <span class="math inline">\(\theta&gt;0.1\)</span> and we consider <span class="math inline">\(b_n = b = 0.1\)</span>.</p>
<p>We still have:
<span class="math display">\[
    \mathbb{P}_\theta (S_n \in \Omega_n)=\mathbb{P}_\theta (S_n &gt; b) \approx 1-\Phi\left(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}}\right).
    \]</span></p>
<p>Because <span class="math inline">\(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}} \underset{n \rightarrow \infty}{\rightarrow} -\infty\)</span>, we have
<span class="math display">\[
    \mathbb{P}_\theta (S_n &gt; b) \approx 1- \underbrace{\Phi\left(\frac{\sqrt{n}(b-\theta)}{\sqrt{\theta(1-\theta)}}\right)}_{\underset{n \rightarrow \infty}{\rightarrow} 0} \underset{n \rightarrow \infty}{\rightarrow} 1.
    \]</span>
Therefore, with <span class="math inline">\(b_n=b=0.1\)</span>, the test is consistent.</p>
</div>
<div id="normality-tests" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Normality tests<a class="anchor" aria-label="anchor" href="#normality-tests"><i class="fas fa-link"></i></a>
</h2>
<p>Let <span class="math inline">\(f\)</span> be the p.d.f. of <span class="math inline">\(Y\)</span>. The <span class="math inline">\(k^{th}\)</span> <strong>standardized moment</strong> of <span class="math inline">\(Y\)</span> is defined as:
<span class="math display">\[
    \psi_k = \frac{\mu_k}{\left(\sqrt{\mathbb{V}ar(Y)}\right)^k},
    \]</span>
where <span class="math inline">\(\mathbb{E}(Y)=\mu\)</span> and
<span class="math display">\[
    \mu_k = \mathbb{E}[(Y-\mu)^k]= \int_{-\infty}^{\infty} (y-\mu)^k f(y) dy
    \]</span>
is the <span class="math inline">\(k^{th}\)</span> {central moment} of <span class="math inline">\(Y\)</span>.</p>
<p>In particular, <span class="math inline">\(\mu_2 = \mathbb{V}ar(Y)=\sigma^2\)</span>, say. Therefore:
<span class="math display">\[
    \psi_k = \frac{\mu_k}{\left(\mu_2^{1/2}\right)^k},
    \]</span></p>
<p>The skewness corresponds to <span class="math inline">\(\psi_3\)</span> and the kurtosis to <span class="math inline">\(\psi_4\)</span> (Def. <a href="append.html#def:skewnesskurtosis">10.6</a>).</p>
<div class="proposition">
<p><span id="prp:normSkewKurt" class="proposition"><strong>Proposition 4.1  (Skewness and kurtosis of the normal distribution) </strong></span>For a Gaussian var., the skewness (<span class="math inline">\(\psi_3\)</span>) is 0 and the kurtosis (<span class="math inline">\(\psi_4\)</span>) is 3.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>For a centered Gaussian distribution, <span class="math inline">\((-y)^3f(-y)=-y^3f(y)\)</span>. This implies that <span class="math inline">\(\int_{-\infty}^{\infty}y^3f(y)dy=\int_{-\infty}^{0}y^3f(y)dy+\int_{0}^{\infty}y^3f(y)dy=-\int_{0}^{\infty}y^3f(y)dy+\int_{0}^{\infty}y^3f(y)dy=0\)</span>, which leads to the skewness result.</p>
<p>Moreover, for a Gaussian distribution, <span class="math inline">\(df(y)/dy=-yf(y)\)</span> and therefore <span class="math inline">\(\frac{d}{dy}(y^3f(y))=3y^2f(y)-y^4f(y)\)</span>. Partial integration leads to the kurtosis result.</p>
</div>
<p><span class="math inline">\(k^{th}\)</span> <strong>central sample moment</strong> of <span class="math inline">\(Y\)</span>:
<span class="math display">\[
    m_k = \frac{1}{n}\sum_{i=1}^n(y_i - \bar{y})^k.
    \]</span>
<span class="math inline">\(k^{th}\)</span> <strong>standardized sample moment</strong> of <span class="math inline">\(Y\)</span>:
<span class="math display">\[
    g_k = \frac{m_k}{m_2^{k/2}}.
    \]</span></p>
<div class="proposition">
<p><span id="prp:conssitCentralMoments" class="proposition"><strong>Proposition 4.2  (Consistency of central sample moments) </strong></span>If the <span class="math inline">\(k^{th}\)</span> central moment of <span class="math inline">\(Y\)</span>, exists, then the sample central moment <span class="math inline">\(m_k\)</span> is a consistent estimate of the central moment <span class="math inline">\(\mu_k\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:Asymptg3Normal" class="proposition"><strong>Proposition 4.3  (Asymptotic distribution of 3rd-order sample central moment of a normal distribution) </strong></span>If <span class="math inline">\(Y\sim\mathcal{N}(\mu,\sigma^2)\)</span>, then <span class="math inline">\(\sqrt{n}g_3 \overset{d}{\rightarrow} \mathcal{N}(0,6)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>See, e.g. <a href="http://www.springer.com/la/book/9780387985954">Lehmann (1999)</a>.</p>
</div>
<div class="proposition">
<p><span id="prp:Asymptg4Normal" class="proposition"><strong>Proposition 4.4  (Asymptotic distribution of 4th-order sample central moment of a normal distribution) </strong></span>If <span class="math inline">\(Y\sim\mathcal{N}(\mu,\sigma^2)\)</span>, then <span class="math inline">\(\sqrt{n}(g_4-3) \overset{d}{\rightarrow} \mathcal{N}(0,24)\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:Asymptg3g4Normal" class="proposition"><strong>Proposition 4.5  (Joint asymptotic distribution of 3rd and 4th-order sample central moments of a normal distribution) </strong></span>Asymptotically, the vector <span class="math inline">\((\sqrt{n}g_3,\sqrt{n}(g_4-3))\)</span> is bivariate Gaussian. Further its elements are uncorrelated and therefore independent.</p>
</div>
<p>The Jarque-Bera statistic is defined by:
<span class="math display">\[
JB = n \left( \frac{g_3^2}{6}+\frac{(g_4-3)^2}{24} \right) = \frac{n}{6}\left(g_3^2 + \frac{(g_4-3)^2}{4}\right).
\]</span></p>
<div class="proposition">
<p><span id="prp:JB" class="proposition"><strong>Proposition 4.6  (Jarque-Bera asympt. distri.) </strong></span>If <span class="math inline">\(Y\)</span> is Gaussian, <span class="math inline">\(JB \overset{d}{\rightarrow} \chi^2(2)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>This directly derives from Proposition <a href="Tests.html#prp:Asymptg3g4Normal">4.5</a>.</p>
</div>
<div class="example">
<p><span id="exm:JB" class="example"><strong>Example 4.2  (Consistency of the Jarque-Bera normality test) </strong></span>This example illustrates the consistency of the JB test (see Def. <a href="Tests.html#def:asmyptconsisttest">4.3</a>).</p>
<p>First, let us write the function computing the JB test statistic:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">JB</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">x.1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="va">x.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="va">x.2</span></span>
<span>  <span class="va">x.bar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x</span>,<span class="fl">1</span>,<span class="va">mean</span><span class="op">)</span></span>
<span>  <span class="va">x.x.bar</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">x.bar</span>,<span class="va">x.1</span>,<span class="va">x.2</span><span class="op">)</span></span>
<span>  <span class="va">m.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x.x.bar</span>,<span class="fl">1</span>,<span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span>
<span>  <span class="va">m.3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x.x.bar</span>,<span class="fl">1</span>,<span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">3</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span>
<span>  <span class="va">m.4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x.x.bar</span>,<span class="fl">1</span>,<span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">4</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span>
<span>  <span class="va">g.3</span> <span class="op">&lt;-</span> <span class="va">m.3</span><span class="op">/</span><span class="va">m.2</span><span class="op">^</span><span class="op">(</span><span class="fl">3</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">g.4</span> <span class="op">&lt;-</span> <span class="va">m.4</span><span class="op">/</span><span class="va">m.2</span><span class="op">^</span><span class="op">(</span><span class="fl">4</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">n</span><span class="op">*</span><span class="op">(</span><span class="va">g.3</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="fl">6</span> <span class="op">+</span> <span class="op">(</span><span class="va">g.4</span><span class="op">-</span><span class="fl">3</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="fl">24</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Let us first consider the case where <span class="math inline">\(H_0\)</span> (normality of the <span class="math inline">\(y_i\)</span>’s) is satisfied. Figure <a href="Tests.html#fig:JBTest2">4.6</a> displays the distribution of the JB statistics when the <span class="math inline">\(y_i\)</span>’s are normal, consistently with <span class="math inline">\(H_0\)</span>. It appears that when <span class="math inline">\(n\)</span> grows, the distribution indeed converges to the <span class="math inline">\(\chi^2(2)\)</span> distribution.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">all.n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>,<span class="fl">10</span>,<span class="fl">20</span>,<span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">nb.sim</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">all.n</span><span class="op">)</span><span class="op">)</span>,<span class="va">nb.sim</span>,<span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">all.n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span>;<span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">.95</span>,<span class="fl">.15</span>,<span class="fl">.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">all.n</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="va">all.n</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="fu">JB</span><span class="op">(</span><span class="va">y</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">]</span><span class="op">)</span>,nclass <span class="op">=</span> <span class="fl">200</span>,freq <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>       main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"n = "</span>,<span class="fu"><a href="https://rdrr.io/r/base/toString.html">toString</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span>,xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">xx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">10</span>,by<span class="op">=</span><span class="fl">.01</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">xx</span>,<span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">dchisq</a></span><span class="op">(</span><span class="va">xx</span>,df <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>,col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:JBTest2"></span>
<img src="ApplEcts_files/figure-html/JBTest2-1.png" alt="Distribution of the JB test statistic under $H_0$ (normality)." width="672"><p class="caption">
Figure 4.6: Distribution of the JB test statistic under <span class="math inline">\(H_0\)</span> (normality).
</p>
</div>
<p>Now, replace <code>rnorm</code> with <code>runif</code>. That is, let us now consider what happens for the distribution of the JB statistics when the <span class="math inline">\(y_i\)</span>’s are drawn from a uniform distribution. Figure <a href="Tests.html#fig:JBTest3">4.7</a> then shows that, when <span class="math inline">\(n\)</span> grows, the distributions shift to the right. This results in the consistency of the JB test (see Def. <a href="Tests.html#def:asmyptconsisttest">4.3</a>).</p>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:JBTest3"></span>
<img src="ApplEcts_files/figure-html/JBTest3-1.png" alt="Distribution of the JB test statistic when the $y_i$'s are drawn from a uniform distribution (hence $H_0$ is not satisfied)." width="672"><p class="caption">
Figure 4.7: Distribution of the JB test statistic when the <span class="math inline">\(y_i\)</span>’s are drawn from a uniform distribution (hence <span class="math inline">\(H_0\)</span> is not satisfied).
</p>
</div>
</div>
<!-- \begin{frame}{} -->
<!-- \begin{footnotesize} -->
<!--        \begin{figure} -->
<!--        \caption{The distr. of the JB stat. in small-sample ($Y\sim\mathcal{N}$).} -->
<!--            \includegraphics[width=.9\linewidth]{../../figures/Figure_Tests_JB1.pdf} -->
<!--        \tiny{10.000 samples of size $n$ are simulated (from a {\color{blue}normal distribution}). For each sample, the JB statistic is computed. The red line shows the $\chi^2(2)$ density. This figure illustrates the convergence of the JB statistic to the $\chi^2(2)$ distribution under $H_0$ (the null hypothesis of normality).} -->
<!--        \end{figure} -->
<!--        \begin{tiny} -->
<!--        \end{tiny} -->
<!-- \end{footnotesize} -->
<!-- \end{frame} -->
<!-- \begin{frame}{} -->
<!-- \begin{footnotesize} -->
<!--        \begin{figure} -->
<!--        \caption{The distr. of the JB stat. in small-sample ($Y\sim\mathcal{U}$).} -->
<!--            \includegraphics[width=.9\linewidth]{../../figures/Figure_Tests_JB2.pdf} -->
<!--        \tiny{10.000 samples of size $n$ are simulated (from a {\color{blue}uniform distribution}). For each sample, the JB statistic is computed. The red line shows the $\chi^2(2)$ density.  This figure illustrates the absence of convergence of the JB statistic to the $\chi^2(2)$ distribution under $H_1$.} -->
<!--        \end{figure} -->
<!--        \begin{tiny} -->
<!--        \end{tiny} -->
<!-- \end{footnotesize} -->
<!-- \end{frame} -->
<!-- %\begin{frame}{} -->
<!-- %\begin{footnotesize} -->
<!-- %      \begin{figure} -->
<!-- %      \caption{The distr. of the JB stat. in small-sample ($Y\sim t(7)$).} -->
<!-- %          \includegraphics[width=.9\linewidth]{../../figures/Figure_Tests_JB3.pdf} -->
<!-- % -->
<!-- %      \tiny{10.000 samples of size $n$ are simulated (from a {\color{blue}uniform distribution}). For each sample, the JB statistic is computed. The red line shows the $\chi^2(2)$ density.} -->
<!-- %      \end{figure} -->
<!-- %      \begin{tiny} -->
<!-- % -->
<!-- %      \end{tiny} -->
<!-- %\end{footnotesize} -->
<!-- %\end{frame} -->
<!-- % -->
<!-- % -->
<!-- %\begin{frame}{} -->
<!-- %\begin{footnotesize} -->
<!-- %      \begin{figure} -->
<!-- %      \caption{The distr. of the JB stat. in small-sample ($Y\sim t(20)$).} -->
<!-- %          \includegraphics[width=.9\linewidth]{../../figures/Figure_Tests_JB4.pdf} -->
<!-- % -->
<!-- %      \tiny{10.000 samples of size $n$ are simulated (from a {\color{blue}uniform distribution}). For each sample, the JB statistic is computed. The red line shows the $\chi^2(2)$ density.} -->
<!-- %      \end{figure} -->
<!-- %      \begin{tiny} -->
<!-- % -->
<!-- %      \end{tiny} -->
<!-- %\end{footnotesize} -->
<!-- %\end{frame} -->

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="TCL.html"><span class="header-section-number">3</span> Central Limit Theorem</a></div>
<div class="next"><a href="ChapterLS.html"><span class="header-section-number">5</span> Linear Regressions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#Tests"><span class="header-section-number">4</span> Statistical tests</a></li>
<li><a class="nav-link" href="#size-and-power-of-a-test"><span class="header-section-number">4.1</span> Size and power of a test</a></li>
<li><a class="nav-link" href="#the-different-types-of-statistical-tests"><span class="header-section-number">4.2</span> The different types of statistical tests</a></li>
<li><a class="nav-link" href="#a-practical-illustration-of-size-and-power"><span class="header-section-number">4.3</span> A practical illustration of size and power</a></li>
<li><a class="nav-link" href="#normality-tests"><span class="header-section-number">4.4</span> Normality tests</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Applied Econometrics</strong>" was written by Jean-Paul Renne. It was last built on 2023-01-08.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

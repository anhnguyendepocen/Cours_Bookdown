[["estimation-methods.html", "Chapter 5 Estimation Methods 5.1 Generalized Method of Moments (GMM) 5.2 Maximum Likelihood Estimation", " Chapter 5 Estimation Methods Context and Objective: You observe a sample \\(\\bv{y}=\\{y_1,\\dots,y_n\\}\\). You know that these data have been generated by a model parameterized by \\(\\theta_0 \\in \\mathbb{R}^K\\). 5.1 Generalized Method of Moments (GMM) 5.1.1 Framework We denote by \\(x_t\\) a \\(p \\times 1\\) vector of (stationary) variables observed at date \\(t\\); by \\(\\theta\\) an \\(a \\times 1\\) vector of parameters, and by \\(h(x_t;\\theta)\\) a continuous \\(r \\times 1\\) vector-valued function. We denote by \\(\\theta_0\\) the true value of \\(\\theta\\) and we assume that \\(\\theta_0\\) satisfies: \\[ \\mathbb{E}[h(x_t;\\theta_0)] = 0. \\] We denote by \\(\\underline{x_t}\\) the information contained in the current and past observations of \\(x_t\\), that is: \\(\\underline{x_t} = \\{x_t,x_{t-1},\\dots,x_1\\}\\). We denote by \\(g(\\underline{x_T};\\theta)\\) the sample average of \\(h(x_t;\\theta)\\), i.e.: \\[ g(\\underline{x_T};\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} h(x_t;\\theta). \\] Intuition behind GMM: Choose \\(\\theta\\) so as to make the sample moment as close as possible to 0. Definition 5.1 A GMM estimator of \\(\\theta_0\\) is given by: \\[ \\hat{\\theta}_T = \\mbox{argmin}_{\\theta} \\quad g(\\underline{x_T};\\theta)&#39;\\, W_T \\, g(\\underline{x_T};\\theta), \\] where \\(W_T\\) is a positive definite matrix (that may depend on \\(\\underline{x_T}\\)). If \\(a = r\\) (the dimension of \\(\\theta\\) is the same as that of \\(h(x_t;\\theta)\\), or \\(g(\\underline{x_T};\\theta)\\)), \\(\\hat{\\theta}_T\\) is such that: \\[ g(\\underline{x_T};\\hat{\\theta}_T) = 0. \\] Under regularity and identification conditions: \\[ \\hat{\\theta}_{T} \\overset{p}{\\rightarrow} \\theta_0, \\] i.e. \\(\\forall \\varepsilon&gt;0\\), \\(\\lim_{n \\rightarrow \\infty} \\mathbb{P}(|\\hat{\\theta}_{T} - \\theta_0|&gt;\\varepsilon) = 0\\). Optimal weighting matrix. The GMM estimator achieving the minimum asymptotic variance is obtained when \\(W_T\\) is the inverse of the matrix \\(S\\) defined by: \\[ S := \\sum_{\\nu = -\\infty}^{\\infty} \\Gamma_\\nu, \\] where \\(\\Gamma_\\nu := \\mathbb{E}[h(x_t;\\theta_0) h(x_{t-\\nu};\\theta_0)&#39;]\\). For \\(\\nu \\ge 0\\), let us define \\(\\hat{\\Gamma}_{\\nu,T}\\) by: \\[ \\hat{\\Gamma}_{\\nu,T} = \\frac{1}{T} \\sum_{t=\\nu + 1}^{T} h(x_t;\\hat{\\theta}_T)h(x_{t-\\nu};\\hat{\\theta}_T)&#39;, \\] \\(S\\) can be approximated by: \\[\\begin{eqnarray} &amp;\\hat{\\Gamma}_{0,T}&amp; \\quad \\mbox{if the $h(x_t;\\theta_0)$ are serially uncorrelated and} \\nonumber \\\\ &amp;\\hat{\\Gamma}_{0,T}&amp; + \\sum_{\\nu=1}^{q}[1-\\nu/(q+1)](\\hat{\\Gamma}_{\\nu,T}+\\hat{\\Gamma}_{\\nu,T}&#39;) \\quad \\mbox{otherwise.} \\tag{5.1} \\end{eqnarray}\\] Asymptotic distribution of \\(\\hat\\theta_T\\) We have: \\[ \\sqrt{T}(\\hat\\theta_T - \\theta_0) \\overset{\\mathcal{L}}{\\rightarrow} \\mathcal{N}(0,V), \\] where \\(V = (DS^{-1}D&#39;)^{-1}\\). \\(V\\) can be approximated by \\((\\hat{D}_T\\hat{S}_T^{-1}\\hat{D}_T&#39;)^{-1}\\), where \\(\\hat{S}_T\\) is given by Eq. (5.1) and \\[ \\hat{D}&#39;_T := \\left.\\frac{\\partial g(\\underline{x_T};\\theta)}{\\partial \\theta&#39;}\\right|_{\\theta = \\hat\\theta_T}. \\] 5.1.2 Example: Estimation of the Stochastic Discount Factor (s.d.f.) Under the no-arbitrage assumption, there exists a random variable \\(\\mathcal{M}_{t,t+1}\\) such that \\[ \\mathbb{E}_t(\\mathcal{M}_{t,t+1}R_{t+1})=1 \\] for any (gross) asset return \\(R_t\\). In the following, \\(R_t\\) denotes a \\(n_r\\)-dimensional vector of gross returns. We consider the following specification of the s.d.f.: \\[\\begin{equation} \\mathcal{M}_{t,t+1} = 1 - \\textbf{b}_M&#39;(F_{t+1} - \\mathbb{E}_t(F_{t+1})), \\tag{5.2} \\end{equation}\\] where \\(F_t\\) is a vector of factors. Eq. (5.2) then reads: \\[ \\mathbb{E}_t([1 - \\textbf{b}_M&#39;(F_{t+1} - \\mathbb{E}_t(F_{t+1}))]R_{t+1})=1. \\] Assume that the date-\\(t\\) information set is \\(\\mathcal{I}_t=\\{\\textbf{z}_t,\\mathcal{I}_{t-1}\\}\\), where \\(\\textbf{z}_t\\) is a vector of variables observed on date \\(t\\). (We then have \\(\\mathbb{E}_t(\\bullet) \\equiv \\mathbb{E}(\\bullet|\\mathcal{I}_t)\\).) We can use \\(\\textbf{z}_t\\) as an instrument. Indeed, we have: \\[\\begin{eqnarray} &amp;&amp;\\mathbb{E}(z_{i,t} [\\textbf{b}_M&#39;\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1]) \\nonumber \\\\ &amp;=&amp;\\mathbb{E}(\\mathbb{E}_t\\{z_{i,t} [\\textbf{b}_M&#39;\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1]\\})\\nonumber\\\\ &amp;=&amp;\\mathbb{E}(z_{i,t} \\underbrace{\\mathbb{E}_t\\{\\textbf{b}_M&#39;\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1\\}}_{=0})=0.\\tag{5.3} \\end{eqnarray}\\] We have then converted a conditional moment condition into a unconditional one (which we need to implement the theory above). However, at that stage, we cannot still not directly use the GMM formulas because of the conditional expectation \\(\\mathbb{E}_t(F_{t+1})\\) that appears in \\(\\mathbb{E}(z_{i,t} [\\textbf{b}_M&#39;\\{F_{t+1} - \\mathbb{E}_t(F_{t+1})\\}R_{t+1}-R_{t+1}+1])=0\\). To go further, let us assume that: \\[ \\mathbb{E}_t(F_{t+1}) = \\textbf{b}_F \\textbf{z}_t. \\] We can then easily estimate matrix \\(\\textbf{b}_F\\) (of dimension \\(n_F \\times n_z\\)) by OLS. Note here that these OLS can be seen as a special GMM case. Indeed, as was done in Eq. (5.3), we can show that, for the \\(j^{th}\\) component of \\(F_t\\), we have: \\[ \\mathbb{E}( [F_{j,t+1} - \\textbf{b}_{F,j} \\textbf{z}_t]\\textbf{z}_{t})=0, \\] where \\(\\textbf{b}_{F,j}\\) denotes the \\(j^{th}\\) row of \\(\\textbf{b}_{F}\\). This yields the OLS formula. At that stage, we count on the following moment restrictions to estimate \\(\\textbf{b}_M\\): \\[ \\mathbb{E}(z_{i,t} [\\textbf{b}_M&#39;\\{F_{t+1} - \\textbf{b}_F \\textbf{z}_t\\}R_{t+1}-R_{t+1}+1])=0. \\] Specifically, the number of restrictions is \\(n_R \\times n_z\\). Let us implement this approach in the U.S. context, using data extracted from the FRED database. In factor \\(F_t\\), we use the changes in the VIX and in the personal consumption expenditures. The returns (\\(R_t\\)) are based on the Wilshire 5000 Price Index (a stock price index) and on the ICE BofA BBB US Corporate Index Total Return Index (a bond return index). library(fredr) fredr_set_key(&quot;df65e14c054697a52b4511e77fcfa1f3&quot;) start_date &lt;- as.Date(&quot;1990-01-01&quot;); end_date &lt;- as.Date(&quot;2022-01-01&quot;) f &lt;- function(ticker){ fredr(series_id = ticker, observation_start = start_date,observation_end = end_date, frequency = &quot;m&quot;,aggregation_method = &quot;avg&quot;) } vix &lt;- f(&quot;VIXCLS&quot;) # VIX pce &lt;- f(&quot;PCE&quot;) # Personal consumption expenditures sto &lt;- f(&quot;WILL5000PRFC&quot;) # Wilshire 5000 Full Cap Price Index bdr &lt;- f(&quot;BAMLCC0A4BBBTRIV&quot;) # ICE BofA BBB US Corporate Index Total Return Index T &lt;- dim(vix)[1] dvix &lt;- c(vix$value[3:T]/vix$value[2:(T-1)]) # change in VIX t+1 dpce &lt;- c(pce$value[3:T]/pce$value[2:(T-1)]) # change in PCE t+1 dsto &lt;- c(sto$value[3:T]/sto$value[2:(T-1)]) # return t+1 dbdr &lt;- c(bdr$value[3:T]/bdr$value[2:(T-1)]) # return t+1 dvix_1 &lt;- c(vix$value[2:(T-1)]/vix$value[1:(T-2)]) # change in VIX t dpce_1 &lt;- c(pce$value[2:(T-1)]/pce$value[1:(T-2)]) # change in PCE t dsto_1 &lt;- c(sto$value[2:(T-1)]/sto$value[1:(T-2)]) # return t dbdr_1 &lt;- c(bdr$value[2:(T-1)]/bdr$value[1:(T-2)]) # return t Define the matrices containing the \\(F_{t+1}\\), \\(\\textbf{z}_t\\), and \\(R_{t+1}\\) vectors: F_tp1 &lt;- cbind(dvix,dpce) Z &lt;- cbind(1,dvix_1,dpce_1,dsto_1,dbdr_1) b_F &lt;- t(solve(t(Z) %*% Z) %*% t(Z) %*% F_tp1) F_innov &lt;- F_tp1 - Z %*% t(b_F) R_tp1 &lt;- cbind(dsto,dbdr) n_F &lt;- dim(F_tp1)[2]; n_R &lt;- dim(R_tp1)[2]; n_z &lt;- dim(Z)[2] Function f_aux compute the \\(h(x_t;\\theta)\\) and the \\(g(\\underline{x_T};\\theta)\\); function f2beMin is the function to be minimized. f_aux &lt;- function(theta){ b_M &lt;- matrix(theta[1:n_F],ncol=1) R_aux &lt;- matrix(F_innov %*% b_M,T-2,n_R) * R_tp1 - R_tp1 + 1 H &lt;- (R_aux %x% matrix(1,1,n_z)) * (matrix(1,1,n_R) %x% Z) g &lt;- matrix(apply(H,2,mean),ncol=1) return(list(g=g,H=H)) } f2beMin &lt;- function(theta,W){# function to be minimized res &lt;- f_aux(theta) return(t(res$g) %*% W %*% res$g) } Now, let’s minimize this function. We consider 5 iterations (where \\(W\\) is updated). theta &lt;- c(rep(0,n_F)) # inital value for(i in 1:5){# recursion on W res &lt;- f_aux(theta) W &lt;- solve(1/T * t(res$H) %*% res$H) res.optim &lt;- optim(theta,f2beMin,W=W, method=&quot;BFGS&quot;, # could be &quot;Nelder-Mead&quot; control=list(trace=FALSE,maxit=200),hessian=TRUE) theta &lt;- res.optim$par } Finally, let’s compute the standard deviation of the parameter estimates. eps &lt;- .0001 g0 &lt;- f_aux(theta)$g D &lt;- NULL for(i in 1:length(theta)){ theta.i &lt;- theta theta.i[i] &lt;- theta.i[i] + eps gi &lt;- f_aux(theta.i)$g D &lt;- cbind(D,(gi-g0)/eps) } V &lt;- 1/T * solve(t(D) %*% W %*% D) std.dev &lt;- sqrt(diag(V));t.stud &lt;- theta/std.dev cbind(theta,std.dev,t.stud) ## theta std.dev t.stud ## [1,] -0.6774355 0.3182614 -2.1285502 ## [2,] 4.3612746 13.1163581 0.3325065 The Hansen statistic can be used to test the model. If the model is correct, we have: \\[ T g(\\underline{x_T};\\theta)&#39;\\, S^{-1} \\, g(\\underline{x_T};\\theta) \\sim \\,i.i.d.\\,\\chi^2(J - K), \\] where \\(J\\) is the number of moment contraints (\\(n_z \\times n_r\\) here) and \\(K\\) is the number of estimated parameters (\\(=n_F\\) here). g &lt;- f_aux(theta)$g Hanse_stat &lt;- T * t(g) %*% W %*% g pvalue &lt;- pchisq(q = Hanse_stat,df = n_R*n_z - n_F) 5.2 Maximum Likelihood Estimation Intuition behind the Maximum Likelihood Estimation: Estimator = the value of \\(\\theta\\) that is such that the probability of having observed \\(\\bv{y}\\) is the highest possible. Assume that the time periods between the arrivals of two customers in a shop, denoted by \\(y_i\\), are i.i.d. and follow an exponential distribution, i.e. \\(y_i \\sim \\mathcal{E}(\\lambda)\\). You have observed these arrivals for some time, thereby constituting a sample \\(\\{y_1,\\dots,y_n\\}\\). You want to estimate \\(\\lambda\\) (i.e. in that case, the vector of parameters is simply \\(\\theta = \\lambda\\)). The density of \\(Y\\) is \\(f(y;\\lambda) = \\dfrac{1}{\\lambda}\\exp(-y/\\lambda)\\). Fig. 5.1 represents that density functions for different values of \\(\\lambda\\). Your 200 observations are reported at the bottom of Fig. 5.1 (red). You build the histogram and report it on the same chart. Figure 5.1: The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations What is your estimate of \\(\\lambda\\)? Now, assume that you have only four observations: \\(y_1=1.1\\), \\(y_2=2.2\\), \\(y_3=0.7\\) and \\(y_4=5.0\\). What was the probability of observing, for a small \\(\\varepsilon\\), \\(1.1-\\varepsilon \\le Y_1 &lt; 1.1+\\varepsilon\\), \\(2.2-\\varepsilon \\le Y_2 &lt; 2.2+\\varepsilon\\), \\(0.7-\\varepsilon \\le Y_3 &lt; 0.7+\\varepsilon\\) and \\(5.0-\\varepsilon \\le Y_4 &lt; 5.0+\\varepsilon\\)? Because the \\(y_i\\)s are i.i.d., this probability is \\(\\prod_{i=1}^4(2\\varepsilon f(y_i,\\lambda))\\). The next plot shows the probability (divided by \\(16\\varepsilon^4\\)) as a function of \\(\\lambda\\). Figure 5.2: Proba. that \\(y_i-\\varepsilon \\le Y_i &lt; y_i+\\varepsilon\\), \\(i \\in \\{1,2,3,4\\}\\). The vertical red line indicates the maximum of the function. Back to the example with 200 observations: Figure 5.3: Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function. 5.2.1 Notations \\(f(y;\\boldsymbol\\theta)\\) denotes the probability density function (p.d.f.) of a random variable \\(Y\\) which depends on a set of parameters \\(\\boldsymbol\\theta\\). Density of \\(n\\) independent and identically distributed (i.i.d.) observations of \\(Y\\): \\[ f(y_1,\\dots,y_n;\\boldsymbol\\theta) = \\prod_{i=1}^n f(y_i;\\boldsymbol\\theta). \\] \\(\\bv{y}\\) denotes the vector of observations; \\(\\bv{y} = \\{y_1,\\dots,y_n\\}\\). Definition 5.2 (Likelihood function) \\(\\mathcal{L}: \\boldsymbol\\theta \\rightarrow \\mathcal{L}(\\boldsymbol\\theta;\\bv{y})=f(y_1,\\dots,y_n;\\boldsymbol\\theta)\\) is the likelihood function. We often work with \\(\\log \\mathcal{L}\\), the log-likelihood function. Example 5.1 (Gaussian distribution) If \\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), then \\[ \\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y}) = - \\frac{1}{2}\\sum_{i=1}^n\\left( \\log \\sigma^2 + \\log 2\\pi + \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right). \\] Definition 5.3 (Score) The score \\(S(y;\\boldsymbol\\theta)\\) is given by \\(\\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\). If \\(y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) (Example 5.1), then \\[ \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} = \\left[\\begin{array}{c} \\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\mu}\\\\ \\dfrac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\sigma^2} \\end{array}\\right] = \\left[\\begin{array}{c} \\dfrac{y-\\mu}{\\sigma^2}\\\\ \\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right) \\end{array}\\right]. \\] Proposition 5.1 (Score expectation) The expectation of the score is zero. Proof. We have: \\[\\begin{eqnarray*} \\mathbb{E}\\left(\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\right) &amp;=&amp; \\int \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta) dy \\\\ &amp;=&amp; \\int \\frac{\\partial f(y;\\boldsymbol\\theta)/\\partial \\boldsymbol\\theta}{f(y;\\boldsymbol\\theta)} f(y;\\boldsymbol\\theta) dy = \\frac{\\partial}{\\partial \\boldsymbol\\theta} \\int f(y;\\boldsymbol\\theta) dy = \\partial 1 /\\partial \\boldsymbol\\theta = 0, \\end{eqnarray*}\\] which gives the result. Definition 5.4 (Fisher information matrix) The information matrix is (minus) the the expectation of the second derivatives of the log-likelihood function: \\[ \\mathcal{I}_Y(\\boldsymbol\\theta) = - \\mathbb{E} \\left( \\frac{\\partial^2 \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;} \\right). \\] Proposition 5.2 We have \\(\\mathcal{I}_Y(\\boldsymbol\\theta) = \\mathbb{E} \\left[ \\left( \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} \\right) \\left( \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} \\right)&#39; \\right] = \\mathbb{V}ar[S(Y;\\boldsymbol\\theta)]\\). Proof. We have \\(\\frac{\\partial^2 \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;} = \\frac{\\partial^2 f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;}\\frac{1}{f(Y;\\boldsymbol\\theta)} - \\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta}\\frac{\\partial \\log f(Y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta&#39;}\\). The expectation of the first right-hand side term is \\(\\partial^2 1 /(\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;) = \\bv{0}\\), which gives the result. Example 5.2 If \\(y_i \\sim\\,i.i.d.\\, \\mathcal{N}(\\mu,\\sigma^2)\\), let \\(\\boldsymbol\\theta = [\\mu,\\sigma^2]&#39;\\) then \\[ \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} = \\left[\\frac{y-\\mu}{\\sigma^2} \\quad \\frac{1}{2\\sigma^2}\\left(\\frac{(y-\\mu)^2}{\\sigma^2}-1\\right) \\right]&#39; \\] and \\[ \\mathcal{I}_Y(\\boldsymbol\\theta) = \\mathbb{E}\\left( \\frac{1}{\\sigma^4} \\left[ \\begin{array}{cc} \\sigma^2&amp;y-\\mu\\\\ y-\\mu &amp; \\frac{(y-\\mu)^2}{\\sigma^2}-\\frac{1}{2} \\end{array}\\right] \\right)= \\left[ \\begin{array}{cc} 1/\\sigma^2&amp;0\\\\ 0 &amp; 1/(2\\sigma^4) \\end{array}\\right]. \\] Proposition 5.3 (Additive property of the Info. mat.) The information matrix resulting from two independent experiments is the sum of the information matrices: \\[ \\mathcal{I}_{X,Y}(\\boldsymbol\\theta) = \\mathcal{I}_X(\\boldsymbol\\theta) + \\mathcal{I}_Y(\\boldsymbol\\theta). \\] :::{.proof} Immediately obtained from the definition (see Def. @ref{def:Fisher}). ::: Theorem 5.1 (Fr\\'echet-Darmois-Cram\\'er-Rao bound) Consider an unbiased estimator of \\(\\boldsymbol\\theta\\) denoted by \\(\\hat{\\boldsymbol\\theta}(Y)\\). The variance of the random variable \\(\\boldsymbol\\omega&#39;\\hat{\\boldsymbol\\theta}\\) (which is a linear combination of the components of \\(\\hat{\\boldsymbol\\theta}\\)) is larger than: \\[ (\\boldsymbol\\omega&#39;\\boldsymbol\\omega)^2/(\\boldsymbol\\omega&#39; \\mathcal{I}_Y(\\boldsymbol\\theta) \\boldsymbol\\omega). \\] Proof. The Cauchy-Schwarz inequality implies that \\(\\sqrt{\\mathbb{V}ar(\\boldsymbol\\omega&#39;\\hat{\\boldsymbol\\theta}(Y))\\mathbb{V}ar(\\boldsymbol\\omega&#39;S(Y;\\boldsymbol\\theta))} \\ge |\\boldsymbol\\omega&#39;\\mathbb{C}ov[\\hat{\\boldsymbol\\theta}(Y),S(Y;\\boldsymbol\\theta)]\\boldsymbol\\omega |\\). Now, \\(\\mathbb{C}ov[\\hat{\\boldsymbol\\theta}(Y),S(Y;\\boldsymbol\\theta)] = \\int_y \\hat{\\boldsymbol\\theta}(y) \\frac{\\partial \\log f(y;\\boldsymbol\\theta)}{\\partial \\boldsymbol\\theta} f(y;\\boldsymbol\\theta)dy = \\frac{\\partial}{\\partial \\boldsymbol\\theta}\\int_y \\hat{\\boldsymbol\\theta}(y) f(y;\\boldsymbol\\theta)dy = \\bv{I}\\) because \\(\\hat{\\boldsymbol\\theta}\\) is unbiased. Therefore \\(\\mathbb{V}ar(\\boldsymbol\\omega&#39;\\hat{\\boldsymbol\\theta}(Y)) \\ge \\mathbb{V}ar(\\boldsymbol\\omega&#39;S(Y;\\boldsymbol\\theta))^{-1} (\\boldsymbol\\omega&#39;\\boldsymbol\\omega)^2\\). Prop. 5.2 leads to the result. Definition 5.5 The vector of parameters \\(\\boldsymbol\\theta\\) is identifiable if, for any other vector \\(\\boldsymbol\\theta^*\\): \\[ \\boldsymbol\\theta^* \\ne \\boldsymbol\\theta \\Rightarrow \\mathcal{L}(\\boldsymbol\\theta^*;\\bv{y}) \\ne \\mathcal{L}(\\boldsymbol\\theta;\\bv{y}). \\] Definition 5.6 (Maximum Likelihood Estimator (MLE)) The maximum likelihood estimator (MLE) is the vector \\(\\boldsymbol\\theta\\) that maximizes the likelihood function. Formally: \\[\\begin{equation} \\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\mathcal{L}(\\boldsymbol\\theta;\\bv{y}) = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y}).\\tag{5.4} \\end{equation}\\] Definition 5.7 (Likelihood equation) Necessary condition for maximizing the likelihood function: \\[\\begin{equation} \\dfrac{\\partial \\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y})}{\\partial \\boldsymbol\\theta} = \\bv{0}. \\end{equation}\\] Hypothesis 5.1 (Regularity assumptions) We have: \\(\\boldsymbol\\theta \\in \\Theta\\) where \\(\\Theta\\) is compact. \\(\\boldsymbol\\theta_0\\) is identified. The log-likelihood function is continuous in \\(\\boldsymbol\\theta\\). \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\) exists. The log-likelihood function is such that \\((1/n)\\log\\mathcal{L}(\\boldsymbol\\theta;\\bv{y})\\) converges almost surely to \\(\\mathbb{E}_{\\boldsymbol\\theta_0}(\\log f(Y;\\boldsymbol\\theta))\\), uniformly in \\(\\boldsymbol\\theta \\in \\Theta\\). The log-likelihood function is twice continuously differentiable in an open neighborood of \\(\\boldsymbol\\theta_0\\). The matrix \\(\\bv{I}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\theta;\\bv{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;}\\right) \\quad \\mbox{(Fisher Information matrix)}\\) exists and is nonsingular. Proposition 5.4 (Properties of MLE) Under regularity conditions (Assumptions 5.1), the MLE is: Consistent: \\(\\mbox{plim}\\quad \\boldsymbol\\theta_{MLE} = \\theta_0\\) (\\(\\theta_0\\) is the true vector of parameters). Asymptotically normal: \\(\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}(\\boldsymbol\\theta_0,\\bv{I}(\\boldsymbol\\theta_0)^{-1})\\), where \\[ \\bv{I}(\\boldsymbol\\theta_0) = - \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\theta;\\bv{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;}\\right) = n \\mathcal{I}_Y(\\boldsymbol\\theta_0). \\quad \\mbox{(Fisher Info. matrix)} \\] Asymptotically efficient: \\(\\boldsymbol\\theta_{MLE}\\) is asymptotically efficient and achieves the Fr'echet-Darmois-Cram'er-Rao lower bound for consistent estimators. Invariant: The MLE of \\(g(\\boldsymbol\\theta_0)\\) is \\(g(\\boldsymbol\\theta_{MLE})\\) if \\(g\\) is a continuous and continuously differentiable function. Proof. See Online additional material. Note that (b) also writes: \\[\\begin{equation} \\sqrt{n}(\\boldsymbol\\theta_{MLE} - \\boldsymbol\\theta_{0}) \\overset{d}{\\rightarrow} \\mathcal{N}(0,\\mathcal{I}_Y(\\boldsymbol\\theta_0)^{-1}). \\tag{5.5} \\end{equation}\\] The asymptotic covariance matrix of the MLE is: \\[ [\\bv{I}(\\boldsymbol\\theta_0)]^{-1} = \\left[- \\mathbb{E}_0 \\left( \\frac{\\partial^2 \\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y})}{\\partial \\boldsymbol\\theta \\partial \\boldsymbol\\theta&#39;}\\right) \\right]^{-1}. \\] A direct (analytical) evaluation of this expectation is often out of reach. It can however be estimated by, either: \\[\\begin{eqnarray} \\hat{\\bv{I}}_1^{-1} &amp;=&amp; \\left( - \\frac{\\partial^2 \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};\\bv{y})}{\\partial {\\boldsymbol\\theta} \\partial {\\boldsymbol\\theta}&#39;}\\right)^{-1}, \\tag{5.6}\\\\ \\hat{\\bv{I}}_2^{-1} &amp;=&amp; \\left( \\sum_{i=1}^n \\frac{\\partial \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};y_i)}{\\partial {\\boldsymbol\\theta}} \\frac{\\partial \\log \\mathcal{L}({\\boldsymbol\\theta_{MLE}};y_i)}{\\partial {\\boldsymbol\\theta&#39;}} \\right)^{-1}. \\tag{5.7} \\end{eqnarray}\\] 5.2.2 To sum up – MLE in practice A parametric model (depending on the vector of parameters \\(\\boldsymbol\\theta\\) whose “true” value is \\(\\boldsymbol\\theta_0\\)) is specified. i.i.d. sources of randomness are identified. The density associated to one observation \\(y_i\\) is computed analytically (as a function of \\(\\boldsymbol\\theta\\)): \\(f(y;\\boldsymbol\\theta)\\). The log-likelihood is \\(\\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y}) = \\sum_i \\log f(y_i;\\boldsymbol\\theta)\\). The MLE estimator results from the optimization problem (this is Eq. (5.4)): \\[\\begin{equation} \\boldsymbol\\theta_{MLE} = \\arg \\max_{\\boldsymbol\\theta} \\log \\mathcal{L}(\\boldsymbol\\theta;\\bv{y}). \\end{equation}\\] We have: \\(\\boldsymbol\\theta_{MLE} \\sim \\mathcal{N}(\\theta_0,\\bv{I}(\\boldsymbol\\theta_0)^{-1})\\), where \\(\\bv{I}(\\boldsymbol\\theta_0)^{-1}\\) is estimated by means of Eq. (5.6) or Eq. (5.7). Most of the time, this computation is numerical. 5.2.3 Example: MLE estimation of a mixture of Gaussian distribution Consider the returns of the SMI index. Let’s assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. \\(f(x;\\boldsymbol\\theta)\\), with \\(\\boldsymbol\\theta = [\\mu_1,\\sigma_1,\\mu_2,\\sigma_2,p]&#39;\\), is given by: \\[ p \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}\\exp\\left(-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2}\\right) + (1-p)\\frac{1}{\\sqrt{2\\pi\\sigma_2^2}}\\exp\\left(-\\frac{(x - \\mu_2)^2}{2\\sigma_2^2}\\right). \\] (See p.d.f. of mixtures of Gaussian dist.) The maximum likelihood estimate is \\(\\boldsymbol\\theta_{MLE}=[0.30,1.40,-1.45,3.61,0.87]&#39;\\). The first two entries of the diagonal of \\(\\hat{\\bv{I}}_1^{-1}\\) are \\(0.00528\\) and \\(0.00526\\). They are the estimates of \\(\\mathbb{V}ar(\\mu_{1,MLE})\\) and of \\(\\mathbb{V}ar(\\sigma_{1,MLE})\\), respectively. 95% confidence intervals for \\(\\mu_1\\) and \\(\\sigma_1\\) are, respectively: \\[ 0.30 \\pm 1.96\\underbrace{\\sqrt{0.00528}}_{=0.0726} \\quad \\mbox{ and } \\quad 1.40 \\pm 1.96\\underbrace{\\sqrt{0.00526}.}_{=0.0725} \\] smi &lt;- read.csv(&quot;https://raw.githubusercontent.com/jrenne/Data4courses/master/SMI/SMI.csv&quot;, dec = &quot;.&quot;,header = TRUE, na.strings = &quot;null&quot;) smi$Date &lt;- as.Date(smi$Date,&quot;%m/%d/%y&quot;) T &lt;- dim(smi)[1] h &lt;- 5 # holding period (one week) smi$r &lt;- c(rep(NaN,h), 100*c(log(smi$Close[(1+h):T]/smi$Close[1:(T-h)]))) indic.dates &lt;- seq(1,T,by=5) # weekly returns smi &lt;- smi[indic.dates,] smi &lt;- smi[complete.cases(smi),] par(mfrow=c(1,1)) plot(smi$Date,smi$r,type=&quot;l&quot;,xlab=&quot;&quot;,ylab=&quot;in percent&quot;) abline(h=0,col=&quot;blue&quot;) abline(h=mean(smi$r,na.rm = TRUE)+2*sd(smi$r,na.rm = TRUE),lty=3,col=&quot;blue&quot;) abline(h=mean(smi$r,na.rm = TRUE)-2*sd(smi$r,na.rm = TRUE),lty=3,col=&quot;blue&quot;) Figure 5.4: Time series of SMI weekly returns (source: Yahoo Finance). f &lt;- function(theta,y){ # Likelihood function mu.1 &lt;- theta[1]; mu.2 &lt;- theta[2] sigma.1 &lt;- theta[3]; sigma.2 &lt;- theta[4] p &lt;- exp(theta[5])/(1+exp(theta[5])) res &lt;- p*1/sqrt(2*pi*sigma.1^2)*exp(-(y-mu.1)^2/(2*sigma.1^2)) + (1-p)*1/sqrt(2*pi*sigma.2^2)*exp(-(y-mu.2)^2/(2*sigma.2^2)) return(res) } log.f &lt;- function(theta,y){ #log-Likelihood function return(-sum(log(f(theta,y)))) } res.optim &lt;- optim(c(0,0,0.5,1.5,.5), log.f, y=smi$r, method=&quot;BFGS&quot;, # could be &quot;Nelder-Mead&quot; control=list(trace=FALSE,maxit=100),hessian=TRUE) theta &lt;- res.optim$par theta ## [1] 0.3012379 -1.3167476 1.7715072 4.8197596 1.9454889 Now, let us compute estimates of the covariance matrix of the MLE: # Hessian approach: J &lt;- res.optim$hessian I.1 &lt;- solve(J) # Outer-product of gradient approach: log.f.0 &lt;- log(f(theta,smi$r)) epsilon &lt;- .00000001 d.log.f &lt;- NULL for(i in 1:length(theta)){ theta.i &lt;- theta theta.i[i] &lt;- theta.i[i] + epsilon log.f.i &lt;- log(f(theta.i,smi$r)) d.log.f &lt;- cbind(d.log.f, (log.f.i - log.f.0)/epsilon) } V &lt;- t(d.log.f) %*% d.log.f I.2 &lt;- solve(t(d.log.f) %*% d.log.f) # Misspecification-robust approach (sandwich formula): I.3 &lt;- solve(J) %*% V %*% solve(J) cbind(diag(I.1),diag(I.2),diag(I.3)) ## [,1] [,2] [,3] ## [1,] 0.003683422 0.003199481 0.00586160 ## [2,] 0.226892824 0.194283391 0.38653389 ## [3,] 0.005764271 0.002769579 0.01712255 ## [4,] 0.194081311 0.047466419 0.83130838 ## [5,] 0.092114437 0.040366005 0.31347858 According to the first (respectively third) type of estimate for the covariance matrix, a 95% confidence interval for \\(\\mu_1\\) is [0.182, 0.42] (resp. [0.151, 0.451]). x &lt;- seq(-5,5,by=.01) plot(x,f(theta,x),type=&quot;l&quot;,lwd=2,xlab=&quot;returns, in percent&quot;,ylab=&quot;&quot;, ylim=c(0,1.4*max(f(theta,x)))) lines(density(smi$r),type=&quot;l&quot;,lwd=2,lty=3) lines(x,dnorm(x,mean=mean(smi$r),sd = sd(smi$r)),col=&quot;red&quot;,lty=2,lwd=2) rug(smi$r,col=&quot;blue&quot;) legend(&quot;topleft&quot;, c(&quot;Kernel estimate (non-parametric)&quot;,&quot;Estimated mixture of Gaussian distr. (MLE, parametric)&quot;,&quot;Normal distribution&quot;), lty=c(3,1,2), # gives the legend appropriate symbols (lines) lwd=c(2), # line width col=c(&quot;black&quot;,&quot;black&quot;,&quot;red&quot;), # gives the legend lines the correct color and width pt.bg=c(1), pt.cex = c(1), bg=&quot;white&quot;, seg.len = 4 ) Figure 5.5: Comparison of different estimates of the distribution of returns. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

# Estimation Methods

## Generalized Method of Moments

### Framework

We denote by $x_t$ a $p \times 1$ vector of (stationary) variables observed at date $t$; by $\theta$ an $a \times 1$ vector of parameters, and by $h(x_t;\theta)$ a continuous $r \times 1$ vector-valued function.

We denote by $\theta_0$ the true value of $\theta$ and we assume that $\theta_0$ satisfies:
$$
\mathbb{E}[h(x_t;\theta_0)] = 0.
$$

We denote by $\underline{x_t}$ the information contained in the current and past observations of $x_t$, that is: $\underline{x_t} = \{x_t,x_{t-1},\dots,x_1\}$. We denote by $g(\underline{x_T};\theta)$ the sample average of $h(x_t;\theta)$, i.e.:
$$
g(\underline{x_T};\theta) = \frac{1}{T} \sum_{t=1}^{T} h(x_t;\theta).
$$

Intuition behind GMM: Choose $\theta$ so as to make the sample moment as close as possible to 0.

A GMM estimator of $\theta_0$ is given by:
$$
\hat{\theta}_T = \mbox{argmin}_{\theta} \quad g(\underline{x_T};\theta)'\, W_T \, g(\underline{x_T};\theta),
$$
where $W_T$ is a positive definite matrix (that may depend on $\underline{x_T}$).

If $a = r$, $\hat{\theta}_T$ is such that:
$$
g(\underline{x_T};\hat{\theta}_T) = 0.
$$
Under regularity and identification conditions:
$$
\hat{\theta}_{T} \overset{P}{\rightarrow} \theta_0,
$$
i.e. $\forall \varepsilon>0$, $\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\theta}_{T} - \theta_0|>\varepsilon) = 0$.

**Optimal weighting matrix**. The GMM estimator achieving the minimum asymptotic variance is obtained when $W_T$ is the inverse of the matrix $S$ defined by:
$$
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
$$
where $\Gamma_\nu := \mathbb{E}[h(x_t;\theta_0) h(x_{t-\nu};\theta_0)']$.

For $\nu \ge 0$, let us define $\hat{\Gamma}_{\nu,T}$ by:
$$
\hat{\Gamma}_{\nu,T} = \frac{1}{T} \sum_{t=\nu + 1}^{T} h(x_t;\hat{\theta}_T)h(x_{t-\nu};\hat{\theta}_T)',
$$
$S$ can be approximated by:
\begin{eqnarray}
&\hat{\Gamma}_{0,T}& \quad \mbox{if the $h(x_t;\theta_0)$ are serially uncorrelated and} \nonumber \\
&\hat{\Gamma}_{0,T}& + \sum_{\nu=1}^{q}[1-\nu/(q+1)](\hat{\Gamma}_{\nu,T}+\hat{\Gamma}_{\nu,T}') \quad \mbox{otherwise.}	(\#eq:Shat)
\end{eqnarray}

**Asymptotic distribution of $\hat\theta_T$**

We have:
$$
\sqrt{T}(\hat\theta_T - \theta_0) \overset{\mathcal{L}}{\rightarrow} \mathcal{N}(0,V),
$$
where $V = (DS^{-1}D')^{-1}$.

$V$ can be approximated by $(\hat{D}_T\hat{S}_T^{-1}\hat{D}_T')^{-1}$,
where $\hat{S}_T$ is given by Eq. \@ref(eq:Shat) and
$$
\hat{D}'_T := \left.\frac{\partial g(\underline{x_T};\theta)}{\partial \theta'}\right|_{\theta = \hat\theta_T}.
$$


# Estimation Methods


Context and Objective: 

* You observe a sample $\bv{y}=\{y_1,\dots,y_n\}$.
* You know that these data have been generated by a model parameterized by $\theta_0 \in \mathbb{R}^K$.


## Generalized Method of Moments (GMM)

### Framework

We denote by $x_t$ a $p \times 1$ vector of (stationary) variables observed at date $t$; by $\theta$ an $a \times 1$ vector of parameters, and by $h(x_t;\theta)$ a continuous $r \times 1$ vector-valued function.

We denote by $\theta_0$ the true value of $\theta$ and we assume that $\theta_0$ satisfies:
$$
\mathbb{E}[h(x_t;\theta_0)] = 0.
$$

We denote by $\underline{x_t}$ the information contained in the current and past observations of $x_t$, that is: $\underline{x_t} = \{x_t,x_{t-1},\dots,x_1\}$. We denote by $g(\underline{x_T};\theta)$ the sample average of $h(x_t;\theta)$, i.e.:
$$
g(\underline{x_T};\theta) = \frac{1}{T} \sum_{t=1}^{T} h(x_t;\theta).
$$

Intuition behind GMM: Choose $\theta$ so as to make the sample moment as close as possible to 0.

:::{.definition #GMM}
A GMM estimator of $\theta_0$ is given by:
$$
\hat{\theta}_T = \mbox{argmin}_{\theta} \quad g(\underline{x_T};\theta)'\, W_T \, g(\underline{x_T};\theta),
$$
where $W_T$ is a positive definite matrix (that may depend on $\underline{x_T}$).
:::

If $a = r$ (the dimension of $\theta$ is the same as that of $h(x_t;\theta)$, or $g(\underline{x_T};\theta)$), $\hat{\theta}_T$ is such that:
$$
g(\underline{x_T};\hat{\theta}_T) = 0.
$$
Under regularity and identification conditions:
$$
\hat{\theta}_{T} \overset{p}{\rightarrow} \theta_0,
$$
i.e. $\forall \varepsilon>0$, $\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\theta}_{T} - \theta_0|>\varepsilon) = 0$.

**Optimal weighting matrix**. The GMM estimator achieving the minimum asymptotic variance is obtained when $W_T$ is the inverse of the matrix $S$ defined by:
$$
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
$$
where $\Gamma_\nu := \mathbb{E}[h(x_t;\theta_0) h(x_{t-\nu};\theta_0)']$.

For $\nu \ge 0$, let us define $\hat{\Gamma}_{\nu,T}$ by:
$$
\hat{\Gamma}_{\nu,T} = \frac{1}{T} \sum_{t=\nu + 1}^{T} h(x_t;\hat{\theta}_T)h(x_{t-\nu};\hat{\theta}_T)',
$$
$S$ can be approximated by:
\begin{eqnarray}
&\hat{\Gamma}_{0,T}& \quad \mbox{if the $h(x_t;\theta_0)$ are serially uncorrelated and} \nonumber \\
&\hat{\Gamma}_{0,T}& + \sum_{\nu=1}^{q}[1-\nu/(q+1)](\hat{\Gamma}_{\nu,T}+\hat{\Gamma}_{\nu,T}') \quad \mbox{otherwise.}	(\#eq:Shat)
\end{eqnarray}

**Asymptotic distribution of $\hat\theta_T$**

We have:
$$
\sqrt{T}(\hat\theta_T - \theta_0) \overset{\mathcal{L}}{\rightarrow} \mathcal{N}(0,V),
$$
where $V = (DS^{-1}D')^{-1}$.

$V$ can be approximated by $(\hat{D}_T\hat{S}_T^{-1}\hat{D}_T')^{-1}$,
where $\hat{S}_T$ is given by Eq. \@ref(eq:Shat) and
$$
\hat{D}'_T := \left.\frac{\partial g(\underline{x_T};\theta)}{\partial \theta'}\right|_{\theta = \hat\theta_T}.
$$

### Example: Estimation of the Stochastic Discount Factor (s.d.f.)

Under the no-arbitrage assumption, there exists a random variable $\mathcal{M}_{t,t+1}$ such that
$$
\mathbb{E}_t(\mathcal{M}_{t,t+1}R_{t+1})=1
$$
for any (gross) asset return $R_t$. 

We consider the following specification of the s.d.f.:
\begin{equation}
\mathcal{M}_{t,t+1} = 1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1})), (\#eq:sdf)
\end{equation}
where $F_t$ is a vector of factors. Eq. \@ref(eq:sdf) then reads:
$$
\mathbb{E}_t([1 - \textbf{b}_M'(F_{t+1} - \mathbb{E}_t(F_{t+1}))]R_{t+1})=1.
$$

Assume that the date-$t$ information set is $\mathcal{I}_t=\{\textbf{z}_t,\mathcal{I}_{t-1}\}$, where $\textbf{z}_t$ is a vector of variables observed on date $t$. (We then have $\mathbb{E}_t(\bullet) \equiv \mathbb{E}(\bullet|\mathcal{I}_t)$.)

We can use $\textbf{z}_t$ as an instrument. Indeed, we have:
\begin{eqnarray}
&&\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]) \nonumber \\
&=&\mathbb{E}(\mathbb{E}_t\{z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1]\})\nonumber\\
&=&\mathbb{E}(z_{i,t} \underbrace{\mathbb{E}_t\{\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1\}}_{=0})=0.(\#eq:momF)
\end{eqnarray}
We have then converted a conditional moment condition into a unconditional one (which we need to implement the theory above). However, at that stage, we cannot still not directly use the GMM formulas because of the conditional expectation $\mathbb{E}_t(F_{t+1})$ that appears in $\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \mathbb{E}_t(F_{t+1})\}R_{t+1}-R_{t+1}+1])=0$.

To go further, let us assume that:
$$
\mathbb{E}_t(F_{t+1}) = \textbf{b}_F \textbf{z}_t.
$$
We can then easily estimate matrix $\textbf{b}_F$ (of dimension $n_F \times n_z$) by OLS. Note here that these OLS can be seen as a special GMM case. Indeed, as was done in Eq. \@ref(eq:momF), we can show that, for the $j^{th}$ component of $F_t$, we have:
$$
\mathbb{E}( [F_{j,t+1} - \textbf{b}_{F,j} \textbf{z}_t]\textbf{z}_{t})=0,
$$
where $\textbf{b}_{F,j}$ denotes the $j^{th}$ row of $\textbf{b}_{F}$. This yields the OLS formula.

At that stage, we count on the following moment restrictions to estimate $\textbf{b}_M$:
$$
\mathbb{E}(z_{i,t} [\textbf{b}_M'\{F_{t+1} - \textbf{b}_F \textbf{z}_t\}R_{t+1}-R_{t+1}+1])=0.
$$
Specifically, the number of restrictions is $n_R \times n_z$. Let us implement this approach in the U.S. context, using data extracted from the [FRED database](https://fred.stlouisfed.org). In factor $F_t$, we use the changes in the VIX and in the personal consumption expenditures. The returns ($R_t$) are based on the Wilshire 5000 Price Index (a stock price index) and on the ICE BofA BBB US Corporate Index Total Return Index (a bond return index).


```{r gmm1}
library(fredr)
fredr_set_key("df65e14c054697a52b4511e77fcfa1f3")
start_date <- as.Date("1990-01-01"); end_date <- as.Date("2022-01-01")
f <- function(ticker){
  fredr(series_id = ticker,
        observation_start = start_date,observation_end = end_date,
        frequency = "m",aggregation_method = "avg")
}
vix <- f("VIXCLS") # VIX
pce <- f("PCE") # Personal consumption expenditures
sto <- f("WILL5000PRFC") # Wilshire 5000 Full Cap Price Index
bdr <- f("BAMLCC0A4BBBTRIV") # ICE BofA BBB US Corporate Index Total Return Index
T <- dim(vix)[1]
dvix <- c(vix$value[3:T]/vix$value[2:(T-1)]) # change in VIX t+1
dpce <- c(pce$value[3:T]/pce$value[2:(T-1)]) # change in PCE t+1
dsto <- c(sto$value[3:T]/sto$value[2:(T-1)]) # return t+1
dbdr <- c(bdr$value[3:T]/bdr$value[2:(T-1)]) # return t+1
dvix_1 <- c(vix$value[2:(T-1)]/vix$value[1:(T-2)]) # change in VIX t
dpce_1 <- c(pce$value[2:(T-1)]/pce$value[1:(T-2)]) # change in PCE t
dsto_1 <- c(sto$value[2:(T-1)]/sto$value[1:(T-2)]) # return t
dbdr_1 <- c(bdr$value[2:(T-1)]/bdr$value[1:(T-2)]) # return t
```
  
Define the matrices containing the $F_{t+1}$, $\textbf{z}_t$, and $R_{t+1}$ vectors: 

```{r gmm2}
F_tp1 <- cbind(dvix,dpce)
Z     <- cbind(1,dvix_1,dpce_1,dsto_1,dbdr_1)
b_F <- t(solve(t(Z) %*% Z) %*% t(Z) %*% F_tp1)
F_innov <- F_tp1 - Z %*% t(b_F)
R_tp1 <- cbind(dsto,dbdr)
n_F <- dim(F_tp1)[2]; n_R <- dim(R_tp1)[2]; n_z <- dim(Z)[2]
```

Function `f_aux` compute the $h(x_t;\theta)$ and the $g(\underline{x_T};\theta)$; function `f2beMin` is the function to be minimized.

```{r gmm3}
f_aux <- function(theta){
  b_M <- matrix(theta[1:n_F],ncol=1)
  R_aux <- matrix(F_innov %*% b_M,T-2,n_R) * R_tp1 - R_tp1 + 1
  H <- (R_aux %x% matrix(1,1,n_z)) * (matrix(1,1,n_R) %x% Z)
  g <- matrix(apply(H,2,mean),ncol=1)
  return(list(g=g,H=H))
}
f2beMin <- function(theta,W){# function to be minimized
  res <- f_aux(theta)
  return(t(res$g) %*% W %*% res$g)
}
```

Now, let's minimize this function. We consider 5 iterations (where $W$ is updated).

```{r gmm4}
theta <- c(rep(0,n_F)) # inital value
for(i in 1:5){# recursion on W
  res <- f_aux(theta)
  W <- solve(1/T * t(res$H) %*% res$H)
  res.optim <- optim(theta,f2beMin,W=W,
                     method="BFGS", # could be "Nelder-Mead"
                     control=list(trace=FALSE,maxit=200),hessian=TRUE)
  theta <- res.optim$par
}
```

Finally, let's compute the standard deviation of the parameter estimates.

```{r gmm5}
eps <- .0001
g0 <- f_aux(theta)$g
D <- NULL
for(i in 1:length(theta)){
  theta.i <- theta
  theta.i[i] <- theta.i[i] + eps
  gi <- f_aux(theta.i)$g
  D <- cbind(D,(gi-g0)/eps)
}
V <- 1/T * solve(t(D) %*% W %*% D)
std.dev <- sqrt(diag(V));t.stud <- theta/std.dev
cbind(theta,std.dev,t.stud)
```



## Maximum Likelihood Estimation

Intuition behind the Maximum Likelihood Estimation: Estimator = the value of $\theta$ that is such that the probability of having observed $\bv{y}$ is the highest possible.

Assume that the time periods between the arrivals of two customers in a shop, denoted by $y_i$, are i.i.d. and follow an exponential distribution, i.e. $y_i \sim \mathcal{E}(\lambda)$.

You have observed these arrivals for some time, thereby constituting a sample $\{y_1,\dots,y_n\}$. You want to estimate $\lambda$ (i.e. in that case, the vector of parameters is simply $\theta = \lambda$).

The density of $Y$ is $f(y;\lambda) = \dfrac{1}{\lambda}\exp(-y/\lambda)$. Fig. \@ref(fig:MLE1) represents that density functions for different values of $\lambda$.

Your 200 observations are reported at the bottom of Fig. \@ref(fig:MLE1) (red).
You build the histogram and report it on the same chart.

```{r MLE1, echo=FALSE, warning=FALSE, fig.cap="The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations", fig.asp = .6, out.width = "90%", fig.align = 'center'}
y <- rexp(200,rate = 1/3)
y.save <- y
hist(y,xlim=c(0,12),ylim=c(0,1),
     freq = FALSE,breaks = 20,xlab="",ylab="",main="",col="white")
x <- seq(0,12,by=.01)
lambda = 1
par(new=TRUE)
plot(x,1/lambda * exp(-x/lambda),type="l",lwd=2,
     ylim=c(0,1),
     xlab="Time period between two arrivals",
     ylab="Density")
rug(y,col="red")
lambda = 3
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="blue")
lambda = 5
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="red")
lambda = 7
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="green")

legend("topright",
       c(expression(paste(lambda," = 1",sep="")),
         expression(paste(lambda," = 3",sep="")),
         expression(paste(lambda," = 5",sep="")),
         expression(paste(lambda," = 7",sep=""))),
       lty=c(1,1,1,1), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2,2,2), # line width
       col=c("black","blue","red","green"),
       seg.len=3)
```

What is your estimate of $\lambda$?

Now, assume that you have only four observations: $y_1=1.1$, $y_2=2.2$, $y_3=0.7$ and $y_4=5.0$.
What was the probability of observing, for a small $\varepsilon$,

*	$1.1-\varepsilon \le Y_1 < 1.1+\varepsilon$, 
*	$2.2-\varepsilon \le Y_2 < 2.2+\varepsilon$, 
*	$0.7-\varepsilon \le Y_3 < 0.7+\varepsilon$ and
*	$5.0-\varepsilon \le Y_4 < 5.0+\varepsilon$?

Because the $y_i$s are i.i.d., this probability is $\prod_{i=1}^4(2\varepsilon f(y_i,\lambda))$.
The next plot shows the probability (divided by $16\varepsilon^4$) as a function of $\lambda$.

```{r MLE2, echo=FALSE, warning=FALSE, fig.cap="Proba. that $y_i-\\varepsilon \\le Y_i < y_i+\\varepsilon$, $i \\in \\{1,2,3,4\\}$. The vertical red line indicates the maximum of the function.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
log.f <- function(y,lambda){
  return(sum(log(1/lambda*exp(-y/lambda))))
}
y <- c(1.1,2.2,0.7,5)
all.log.f <- NULL
all.lambda <- seq(.1,10,by=.1)
for(lambda in all.lambda){
  all.log.f <- c(all.log.f,log.f(y,lambda))
}
par(plt=c(.15,.95,.2,.85),mfrow=c(1,2))
plot(all.lambda,exp(all.log.f),type="l",lwd=2,xlab=expression(lambda),
     ylab="Probability",main="(a) Probability")
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
plot(all.lambda,all.log.f,type="l",lwd=2,ylim=c(-20,-5),
     xlab=expression(lambda),ylab="Probability",main="(b) log(Probability)")
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
```


Back to the example with 200 observations:

```{r MLE3, echo=FALSE, warning=FALSE, fig.cap="Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
y <- y.save
all.log.f <- NULL
all.lambda <- seq(.1,10,by=.1)
for(lambda in all.lambda){
  all.log.f <- c(all.log.f,log.f(y,lambda))
}
par(mfrow=c(1,1),plt=c(.15,.95,.2,.85))
plot(all.lambda,all.log.f,type="l",lwd=2,xlab=expression(lambda),
     ylab="log(Probability)",ylim=c(-500,-400))
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
```



### Notations

$f(y;\boldsymbol\theta)$ denotes the probability density function (p.d.f.) of a random variable $Y$ which depends on a set of parameters $\boldsymbol\theta$.

Density of $n$ independent and identically distributed (i.i.d.) observations of $Y$:
$$
f(y_1,\dots,y_n;\boldsymbol\theta) = \prod_{i=1}^n f(y_i;\boldsymbol\theta).
$$
$\bv{y}$ denotes the vector of observations; $\bv{y} = \{y_1,\dots,y_n\}$.

***
:::{.definition #likelihood name="Likelihood function"}
$\mathcal{L}: \boldsymbol\theta \rightarrow  \mathcal{L}(\boldsymbol\theta;\bv{y})=f(y_1,\dots,y_n;\boldsymbol\theta)$ is the **likelihood function**.
:::
***

We often work with $\log \mathcal{L}$, the **log-likelihood function**.

:::{.example #normal name="Gaussian distribution"}
If $y_i \sim \mathcal{N}(\mu,\sigma^2)$, then
$$
\log \mathcal{L}(\boldsymbol\theta;\bv{y}) = - \frac{1}{2}\sum_{i=1}^n\left( \log \sigma^2 + \log 2\pi + \frac{(y_i-\mu)^2}{\sigma^2} \right).
$$
:::


***
:::{.definition #score name="Score"}
The score $S(y;\boldsymbol\theta)$ is given by $\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta}$.
:::
***

If $y_i \sim \mathcal{N}(\mu,\sigma^2)$ (Example \@ref(exm:normal)), then
$$
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} =
\left[\begin{array}{c}
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \mu}\\
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \sigma^2}
\end{array}\right] =
\left[\begin{array}{c}
\dfrac{y-\mu}{\sigma^2}\\
\frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right)
\end{array}\right].
$$

***
:::{.proposition #score name="Score expectation"}
The expectation of the score is zero.
:::
***

:::{.proof}
We have:
\begin{eqnarray*}
\mathbb{E}\left(\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\right) &=&
\int \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta) dy \\
&=& \int \frac{\partial f(y;\boldsymbol\theta)/\partial \boldsymbol\theta}{f(y;\boldsymbol\theta)} f(y;\boldsymbol\theta) dy =
\frac{\partial}{\partial \boldsymbol\theta} \int f(y;\boldsymbol\theta) dy =
\partial 1 /\partial \boldsymbol\theta = 0,
\end{eqnarray*}
which gives the result.
:::

:::{.definition #Fisher name="Fisher information matrix"}
The **information matrix** is (minus) the the expectation of the second derivatives of the log-likelihood function:
$$
\mathcal{I}_Y(\boldsymbol\theta) = - \mathbb{E} \left( \frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right).
$$
:::

***
:::{.proposition #Fisher}
We have $\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E} \left[ \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)
\left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)' \right] = \mathbb{V}ar[S(Y;\boldsymbol\theta)]$.
:::
***

:::{.proof}
We have $\frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = \frac{\partial^2 f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\frac{1}{f(Y;\boldsymbol\theta)} - \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta'}$. The expectation of the first right-hand side term is $\partial^2 1 /(\partial \boldsymbol\theta \partial \boldsymbol\theta') = \bv{0}$, which gives the result.
:::

:::{.example}
If $y_i \sim\,i.i.d.\, \mathcal{N}(\mu,\sigma^2)$, let $\boldsymbol\theta = [\mu,\sigma^2]'$ then
$$
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} = \left[\frac{y-\mu}{\sigma^2} \quad \frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right) \right]'
$$
and
$$
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E}\left( \frac{1}{\sigma^4}
\left[
\begin{array}{cc}
\sigma^2&y-\mu\\
y-\mu & \frac{(y-\mu)^2}{\sigma^2}-\frac{1}{2}
\end{array}\right]
\right)=
\left[
\begin{array}{cc}
1/\sigma^2&0\\
0 & 1/(2\sigma^4)
\end{array}\right].
$$
:::


***
:::{.proposition #additiv name="Additive property of the Info. mat."}
The information matrix resulting from two independent experiments is the sum of the information matrices:
$$
\mathcal{I}_{X,Y}(\boldsymbol\theta) = \mathcal{I}_X(\boldsymbol\theta) + \mathcal{I}_Y(\boldsymbol\theta).
$$
:::
***
:::{.proof} Immediately obtained from the definition (see Def. \@ref{def:Fisher}).
:::

***
:::{.theorem #FDCR name="Fr\\'echet-Darmois-Cram\\'er-Rao bound"}
Consider an unbiased estimator of $\boldsymbol\theta$ denoted by $\hat{\boldsymbol\theta}(Y)$.

The variance of the random variable $\boldsymbol\omega'\hat{\boldsymbol\theta}$ (which is a linear combination of the components of $\hat{\boldsymbol\theta}$) is larger than:
$$
(\boldsymbol\omega'\boldsymbol\omega)^2/(\boldsymbol\omega' \mathcal{I}_Y(\boldsymbol\theta) \boldsymbol\omega).
$$
:::
***

:::{.proof}
The Cauchy-Schwarz inequality implies that $\sqrt{\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y))\mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))} \ge |\boldsymbol\omega'\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)]\boldsymbol\omega |$. Now, $\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)] = \int_y \hat{\boldsymbol\theta}(y) \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta)dy =
\frac{\partial}{\partial \boldsymbol\theta}\int_y \hat{\boldsymbol\theta}(y) f(y;\boldsymbol\theta)dy = \bv{I}$ because $\hat{\boldsymbol\theta}$ is unbiased. 

Therefore $\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y)) \ge \mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))^{-1} (\boldsymbol\omega'\boldsymbol\omega)^2$. Prop. \@ref(prp:Fisher) leads to the result.
:::

:::{.definition #identif}
The vector of parameters $\boldsymbol\theta$ is identifiable if, for any other vector $\boldsymbol\theta^*$:
$$
\boldsymbol\theta^* \ne \boldsymbol\theta \Rightarrow \mathcal{L}(\boldsymbol\theta^*;\bv{y}) \ne \mathcal{L}(\boldsymbol\theta;\bv{y}).
$$
:::

:::{.definition #MLEest name="Maximum Likelihood Estimator (MLE)"}
The maximum likelihood estimator (MLE) is the vector $\boldsymbol\theta$ that maximizes the likelihood function. Formally:
\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\bv{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\bv{y}).(\#eq:MLEestimator)
\end{equation}
:::

:::{.definition #likFunction name="Likelihood equation"}
Necessary condition for maximizing the likelihood function:
\begin{equation}
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta} = \bv{0}.
\end{equation}
:::

:::{.hypothesis #MLEregularity name="Regularity assumptions"}

We have:

i. $\boldsymbol\theta \in \Theta$ where $\Theta$ is compact.

ii. $\boldsymbol\theta_0$ is identified.

iii. The log-likelihood function is continuous in $\boldsymbol\theta$.

iv. $\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))$ exists.

v. The log-likelihood function is such that $(1/n)\log\mathcal{L}(\boldsymbol\theta;\bv{y})$ converges almost surely to $\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))$, uniformly in $\boldsymbol\theta \in \Theta$.

vi. The log-likelihood function is twice continuously differentiable in an open neighborood of $\boldsymbol\theta_0$.

vii. The matrix $\bv{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \quad \mbox{(Fisher Information matrix)}$ exists and is nonsingular.
:::

***
:::{.proposition #MLEproperties name="Properties of MLE"}
Under regularity conditions (Assumptions \@ref(hyp:MLEregularity)), the MLE is:

a. **Consistent**: $\mbox{plim}\quad  \boldsymbol\theta_{MLE} = \theta_0$ ($\theta_0$ is the true vector of parameters).

b. **Asymptotically normal**: $\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\bv{I}(\boldsymbol\theta_0)^{-1})$, where
$$
\bv{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0). \quad \mbox{(Fisher Info. matrix)}
$$

c. **Asymptotically efficient**: $\boldsymbol\theta_{MLE}$ is asymptotically efficient and achieves the Fr\'echet-Darmois-Cram\'er-Rao lower bound for consistent estimators.

d. **Invariant**: The MLE of $g(\boldsymbol\theta_0)$ is $g(\boldsymbol\theta_{MLE})$ if $g$ is a continuous and continuously differentiable function.
:::
***

:::{.proof}
See [Online additional material](https://www.dropbox.com/s/8xelbghtvnd43yh/Proofs_MLE.pdf?dl=0).
:::


Note that (b) also writes:
\begin{equation}
\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0)^{-1}). (\#eq:normMLE)
\end{equation}


The asymptotic covariance matrix of the MLE is:
$$
[\bv{I}(\boldsymbol\theta_0)]^{-1} = \left[- \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \right]^{-1}.
$$
A direct (analytical) evaluation of this expectation is often out of reach.

It can however be estimated by, either:
\begin{eqnarray}
\hat{\bv{I}}_1^{-1} &=&  \left( - \frac{\partial^2 \log \mathcal{L}({\boldsymbol\theta_{MLE}};\bv{y})}{\partial {\boldsymbol\theta} \partial {\boldsymbol\theta}'}\right)^{-1}, (\#eq:III1)\\
\hat{\bv{I}}_2^{-1} &=&  \left( \sum_{i=1}^n \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta}} \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta'}} \right)^{-1}.  (\#eq:I2)
\end{eqnarray}

### To sum up -- MLE in practice

* A parametric model (depending on the vector of parameters $\boldsymbol\theta$ whose "true" value is $\boldsymbol\theta_0$) is specified.

* i.i.d. sources of randomness are identified.

* The density associated to one observation $y_i$ is computed analytically (as a function of $\boldsymbol\theta$): $f(y;\boldsymbol\theta)$.

* The log-likelihood is $\log \mathcal{L}(\boldsymbol\theta;\bv{y}) = \sum_i \log f(y_i;\boldsymbol\theta)$.

* The MLE estimator results from the optimization problem (this is Eq. \@ref(eq:MLEestimator)):
\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\bv{y}).
\end{equation}

* We have: $\boldsymbol\theta_{MLE} \sim \mathcal{N}(\theta_0,\bv{I}(\boldsymbol\theta_0)^{-1})$, where $\bv{I}(\boldsymbol\theta_0)^{-1}$ is estimated by means of Eq. \@ref(eq:III1) or Eq. \@ref(eq:I2). Most of the time, this computation is numerical.


### Example: MLE estimation of a mixture of Gaussian distribution

Consider the returns of the SMI index. Let's assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. $f(x;\boldsymbol\theta)$, with $\boldsymbol\theta = [\mu_1,\sigma_1,\mu_2,\sigma_2,p]'$, is given by:
$$
p \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) + (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right).
$$
(See [p.d.f. of mixtures of Gaussian dist.](https://jrenne.shinyapps.io/density/))

The maximum likelihood estimate is $\boldsymbol\theta_{MLE}=[0.30,1.40,-1.45,3.61,0.87]'$.

The first two entries of the diagonal of $\hat{\bv{I}}_1^{-1}$ are $0.00528$ and $0.00526$. They are the estimates of $\mathbb{V}ar(\mu_{1,MLE})$ and of $\mathbb{V}ar(\sigma_{1,MLE})$, respectively.

95\% confidence intervals for $\mu_1$ and $\sigma_1$ are, respectively:
$$
0.30 \pm 1.96\underbrace{\sqrt{0.00528}}_{=0.0726} \quad \mbox{ and } \quad 1.40 \pm 1.96\underbrace{\sqrt{0.00526}.}_{=0.0725}
$$

```{r smiData, echo=TRUE, warning=FALSE, fig.cap="Time series of SMI weekly returns (source: Yahoo Finance).", fig.asp = .6, out.width = "90%", fig.align = 'center'}
smi <- read.csv("https://raw.githubusercontent.com/jrenne/Data4courses/master/SMI/SMI.csv",
                dec = ".",header = TRUE, na.strings = "null")
smi$Date <- as.Date(smi$Date,"%m/%d/%y")
T <- dim(smi)[1]
h <- 5 # holding period (one week)
smi$r <- c(rep(NaN,h),
           100*c(log(smi$Close[(1+h):T]/smi$Close[1:(T-h)])))
indic.dates <- seq(1,T,by=5)  # weekly returns
smi <- smi[indic.dates,]
smi <- smi[complete.cases(smi),]
par(mfrow=c(1,1))
plot(smi$Date,smi$r,type="l",xlab="",ylab="in percent")
abline(h=0,col="blue")
abline(h=mean(smi$r,na.rm = TRUE)+2*sd(smi$r,na.rm = TRUE),lty=3,col="blue")
abline(h=mean(smi$r,na.rm = TRUE)-2*sd(smi$r,na.rm = TRUE),lty=3,col="blue")
```

```{r smi_mle}
f <- function(theta,y){ # Likelihood function
  mu.1 <- theta[1]; mu.2 <- theta[2]
  sigma.1 <- theta[3]; sigma.2 <- theta[4]
  p <- exp(theta[5])/(1+exp(theta[5]))
  res <- p*1/sqrt(2*pi*sigma.1^2)*exp(-(y-mu.1)^2/(2*sigma.1^2)) + 
    (1-p)*1/sqrt(2*pi*sigma.2^2)*exp(-(y-mu.2)^2/(2*sigma.2^2))
  return(res)
}
log.f <- function(theta,y){ #log-Likelihood function
  return(-sum(log(f(theta,y))))
}
res.optim <- optim(c(0,0,0.5,1.5,.5),
                   log.f,
                   y=smi$r,
                   method="BFGS", # could be "Nelder-Mead"
                   control=list(trace=FALSE,maxit=100),hessian=TRUE)
theta <- res.optim$par
theta
```
Now, let us compute estimates of the covariance matrix of the MLE:

```{r smiCovar}
# Hessian approach:
J <- res.optim$hessian
I.1 <- solve(J)
# Outer-product of gradient approach:
log.f.0 <- log(f(theta,smi$r))
epsilon <- .00000001
d.log.f <- NULL
for(i in 1:length(theta)){
  theta.i <- theta
  theta.i[i] <- theta.i[i] + epsilon
  log.f.i <- log(f(theta.i,smi$r))
  d.log.f <- cbind(d.log.f,
                   (log.f.i - log.f.0)/epsilon)
}
V <- t(d.log.f) %*% d.log.f
I.2 <- solve(t(d.log.f) %*% d.log.f)
# Misspecification-robust approach (sandwich formula):
I.3 <- solve(J) %*% V %*% solve(J)
cbind(diag(I.1),diag(I.2),diag(I.3))
```
According to the first (respectively third) type of estimate for the covariance matrix, a 95\% confidence interval for $\mu_1$ is [`r c(round(theta[1]+qnorm(.025)*sqrt(I.1[1,1]),3),round(theta[1]+qnorm(.975)*sqrt(I.1[1,1]),3))`] (resp. [`r c(round(theta[1]+qnorm(.025)*sqrt(I.3[1,1]),3),round(theta[1]+qnorm(.975)*sqrt(I.3[1,1]),3))`]).

```{r smidistri, echo=TRUE, warning=FALSE, fig.cap="Comparison of different estimates of the distribution of returns.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
x <- seq(-5,5,by=.01)
plot(x,f(theta,x),type="l",lwd=2,xlab="returns, in percent",ylab="",
     ylim=c(0,1.4*max(f(theta,x))))
lines(density(smi$r),type="l",lwd=2,lty=3)
lines(x,dnorm(x,mean=mean(smi$r),sd = sd(smi$r)),col="red",lty=2,lwd=2)
rug(smi$r,col="blue")

legend("topleft",
       c("Kernel estimate (non-parametric)","Estimated mixture of Gaussian distr. (MLE, parametric)","Normal distribution"),
       lty=c(3,1,2), # gives the legend appropriate symbols (lines)
       lwd=c(2), # line width
       col=c("black","black","red"), # gives the legend lines the correct color and width
       pt.bg=c(1),
       pt.cex = c(1),
       bg="white",
       seg.len = 4
)
```

<!-- \begin{figure} -->
<!-- \caption{Density of 5-day returns on SMI index} -->
<!-- \includegraphics[width=.9\linewidth]{../../figures/Figure_kernel_smi.pdf} -->
<!-- \label{fig:illuskernel_smi} -->
<!-- \begin{tiny} -->
<!-- Gaussian kernel, $h=0.5$ (in percent). -->

<!-- The data spans the period from 2 June 2006 to 23 February 2016 at the daily frequency. -->

<!-- Left-hand plot: the blue lines indicates $\mu \pm 2 \sigma$, where $\mu$ is the sample mean of the returns and $\sigma$ is their sample standard deviation. Right-hand plot: the red dotted line is the density $\mathcal{N}(\mu,\sigma^2)$ -->
<!-- \end{tiny} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


<!-- \begin{frame}{} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[MLE estim. of a mixture of Gaussian distri. (cont'd)] -->
<!-- \begin{figure} -->
<!-- \caption{Estimated density (vs kernel-based estimate)} -->
<!-- \includegraphics[width=1\linewidth]{../../figures/Figure_kernel_smiMLE.pdf} -->
<!-- \label{fig:MLE1} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->



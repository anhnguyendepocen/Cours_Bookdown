# Estimation Methods


Context and Objective: 

* You observe a sample $\bv{y}=\{y_1,\dots,y_n\}$.
* You know that these data have been generated by a model parameterized by $\theta_0 \in \mathbb{R}^K$.


## Generalized Method of Moments

### Framework

We denote by $x_t$ a $p \times 1$ vector of (stationary) variables observed at date $t$; by $\theta$ an $a \times 1$ vector of parameters, and by $h(x_t;\theta)$ a continuous $r \times 1$ vector-valued function.

We denote by $\theta_0$ the true value of $\theta$ and we assume that $\theta_0$ satisfies:
$$
\mathbb{E}[h(x_t;\theta_0)] = 0.
$$

We denote by $\underline{x_t}$ the information contained in the current and past observations of $x_t$, that is: $\underline{x_t} = \{x_t,x_{t-1},\dots,x_1\}$. We denote by $g(\underline{x_T};\theta)$ the sample average of $h(x_t;\theta)$, i.e.:
$$
g(\underline{x_T};\theta) = \frac{1}{T} \sum_{t=1}^{T} h(x_t;\theta).
$$

Intuition behind GMM: Choose $\theta$ so as to make the sample moment as close as possible to 0.

A GMM estimator of $\theta_0$ is given by:
$$
\hat{\theta}_T = \mbox{argmin}_{\theta} \quad g(\underline{x_T};\theta)'\, W_T \, g(\underline{x_T};\theta),
$$
where $W_T$ is a positive definite matrix (that may depend on $\underline{x_T}$).

If $a = r$, $\hat{\theta}_T$ is such that:
$$
g(\underline{x_T};\hat{\theta}_T) = 0.
$$
Under regularity and identification conditions:
$$
\hat{\theta}_{T} \overset{P}{\rightarrow} \theta_0,
$$
i.e. $\forall \varepsilon>0$, $\lim_{n \rightarrow \infty} \mathbb{P}(|\hat{\theta}_{T} - \theta_0|>\varepsilon) = 0$.

**Optimal weighting matrix**. The GMM estimator achieving the minimum asymptotic variance is obtained when $W_T$ is the inverse of the matrix $S$ defined by:
$$
S := \sum_{\nu = -\infty}^{\infty} \Gamma_\nu,
$$
where $\Gamma_\nu := \mathbb{E}[h(x_t;\theta_0) h(x_{t-\nu};\theta_0)']$.

For $\nu \ge 0$, let us define $\hat{\Gamma}_{\nu,T}$ by:
$$
\hat{\Gamma}_{\nu,T} = \frac{1}{T} \sum_{t=\nu + 1}^{T} h(x_t;\hat{\theta}_T)h(x_{t-\nu};\hat{\theta}_T)',
$$
$S$ can be approximated by:
\begin{eqnarray}
&\hat{\Gamma}_{0,T}& \quad \mbox{if the $h(x_t;\theta_0)$ are serially uncorrelated and} \nonumber \\
&\hat{\Gamma}_{0,T}& + \sum_{\nu=1}^{q}[1-\nu/(q+1)](\hat{\Gamma}_{\nu,T}+\hat{\Gamma}_{\nu,T}') \quad \mbox{otherwise.}	(\#eq:Shat)
\end{eqnarray}

**Asymptotic distribution of $\hat\theta_T$**

We have:
$$
\sqrt{T}(\hat\theta_T - \theta_0) \overset{\mathcal{L}}{\rightarrow} \mathcal{N}(0,V),
$$
where $V = (DS^{-1}D')^{-1}$.

$V$ can be approximated by $(\hat{D}_T\hat{S}_T^{-1}\hat{D}_T')^{-1}$,
where $\hat{S}_T$ is given by Eq. \@ref(eq:Shat) and
$$
\hat{D}'_T := \left.\frac{\partial g(\underline{x_T};\theta)}{\partial \theta'}\right|_{\theta = \hat\theta_T}.
$$

## Maximum Likelihood Estimation

Intuition behind the Maximum Likelihood Estimation: Estimator = the value of $\theta$ that is such that the probability of having observed $\bv{y}$ is the highest possible.

Assume that the time periods between the arrivals of two customers in a shop, denoted by $y_i$, are i.i.d. and follow an exponential distribution, i.e. $y_i \sim \mathcal{E}(\lambda)$.

You have observed these arrivals for some time, thereby constituting a sample $\{y_1,\dots,y_n\}$. You want to estimate $\lambda$ (i.e. in that case, the vector of parameters is simply $\theta = \lambda$).

The density of $Y$ is $f(y;\lambda) = \dfrac{1}{\lambda}\exp(-y/\lambda)$. Fig. \@ref(fig:MLE1) represents that density functions for different values of $\lambda$.

Your 200 observations are reported at the bottom of Fig. \@ref(fig:MLE1) (red).
You build the histogram and report it on the same chart.

```{r MLE1, echo=FALSE, warning=FALSE, fig.cap="The red ticks, at the bottom, indicate observations (there are 200 of them). The historgram is based on these 200 observations", fig.asp = .6, out.width = "90%", fig.align = 'center'}
y <- rexp(200,rate = 1/3)
y.save <- y
hist(y,xlim=c(0,12),ylim=c(0,1),
     freq = FALSE,breaks = 20,xlab="",ylab="",main="",col="white")
x <- seq(0,12,by=.01)
lambda = 1
par(new=TRUE)
plot(x,1/lambda * exp(-x/lambda),type="l",lwd=2,
     ylim=c(0,1),
     xlab="Time period between two arrivals",
     ylab="Density")
rug(y,col="red")
lambda = 3
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="blue")
lambda = 5
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="red")
lambda = 7
lines(x,1/lambda * exp(-x/lambda),type="l",lwd=2,col="green")

legend("topright",
       c(expression(paste(lambda," = 1",sep="")),
         expression(paste(lambda," = 3",sep="")),
         expression(paste(lambda," = 5",sep="")),
         expression(paste(lambda," = 7",sep=""))),
       lty=c(1,1,1,1), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2,2,2), # line width
       col=c("black","blue","red","green"),
       seg.len=3)
```

What is your estimate of $\lambda$?

Now, assume that you have only four observations: $y_1=1.1$, $y_2=2.2$, $y_3=0.7$ and $y_4=5.0$.
What was the probability of observing, for a small $\varepsilon$,

*	$1.1-\varepsilon \le Y_1 < 1.1+\varepsilon$, 
*	$2.2-\varepsilon \le Y_2 < 2.2+\varepsilon$, 
*	$0.7-\varepsilon \le Y_3 < 0.7+\varepsilon$ and
*	$5.0-\varepsilon \le Y_4 < 5.0+\varepsilon$?

Because the $y_i$s are i.i.d., this probability is $\prod_{i=1}^4(2\varepsilon f(y_i,\lambda))$.
The next plot shows the probability (divided by $16\varepsilon^4$) as a function of $\lambda$.

```{r MLE2, echo=FALSE, warning=FALSE, fig.cap="Proba. that $y_i-\\varepsilon \\le Y_i < y_i+\\varepsilon$, $i \\in \\{1,2,3,4\\}$. The vertical red line indicates the maximum of the function.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
log.f <- function(y,lambda){
  return(sum(log(1/lambda*exp(-y/lambda))))
}
y <- c(1.1,2.2,0.7,5)
all.log.f <- NULL
all.lambda <- seq(.1,10,by=.1)
for(lambda in all.lambda){
  all.log.f <- c(all.log.f,log.f(y,lambda))
}
par(plt=c(.15,.95,.2,.85),mfrow=c(1,2))
plot(all.lambda,exp(all.log.f),type="l",lwd=2,xlab=expression(lambda),
     ylab="Probability",main="(a) Probability")
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
plot(all.lambda,all.log.f,type="l",lwd=2,ylim=c(-20,-5),
     xlab=expression(lambda),ylab="Probability",main="(b) log(Probability)")
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
```


Back to the example with 200 observations:

```{r MLE3, echo=FALSE, warning=FALSE, fig.cap="Log-likelihood function associated with the 200 i.i.d. observations. The vertical red line indicates the maximum of the function.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
y <- y.save
all.log.f <- NULL
all.lambda <- seq(.1,10,by=.1)
for(lambda in all.lambda){
  all.log.f <- c(all.log.f,log.f(y,lambda))
}
par(mfrow=c(1,1),plt=c(.15,.95,.2,.85))
plot(all.lambda,all.log.f,type="l",lwd=2,xlab=expression(lambda),
     ylab="log(Probability)",ylim=c(-500,-400))
abline(v=all.lambda[which(all.log.f==max(all.log.f))],col="red")
```



### Notations

$f(y;\boldsymbol\theta)$ denotes the probability density function (p.d.f.) of a random variable $Y$ which depends on a set of parameters $\boldsymbol\theta$.

Density of $n$ independent and identically distributed (i.i.d.) observations of $Y$:
$$
f(y_1,\dots,y_n;\boldsymbol\theta) = \prod_{i=1}^n f(y_i;\boldsymbol\theta).
$$
$\bv{y}$ denotes the vector of observations; $\bv{y} = \{y_1,\dots,y_n\}$.

***
:::{.definition #likelihood name="Likelihood function"}
$\mathcal{L}: \boldsymbol\theta \rightarrow  \mathcal{L}(\boldsymbol\theta;\bv{y})=f(y_1,\dots,y_n;\boldsymbol\theta)$ is the **likelihood function**.
:::
***

We often work with $\log \mathcal{L}$, the **log-likelihood function**.

:::{.example #normal name="Gaussian distribution"}
If $y_i \sim \mathcal{N}(\mu,\sigma^2)$, then
$$
\log \mathcal{L}(\boldsymbol\theta;\bv{y}) = - \frac{1}{2}\sum_{i=1}^n\left( \log \sigma^2 + \log 2\pi + \frac{(y_i-\mu)^2}{\sigma^2} \right).
$$
:::


***
:::{.definition #score name="Score"}
The score $S(y;\boldsymbol\theta)$ is given by $\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta}$.
:::
***

If $y_i \sim \mathcal{N}(\mu,\sigma^2)$ (Example \@ref(exm:normal)), then
$$
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} =
\left[\begin{array}{c}
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \mu}\\
\dfrac{\partial \log f(y;\boldsymbol\theta)}{\partial \sigma^2}
\end{array}\right] =
\left[\begin{array}{c}
\dfrac{y-\mu}{\sigma^2}\\
\frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right)
\end{array}\right].
$$

***
:::{.proposition #score name="Score expectation"}
The expectation of the score is zero.
:::
***

:::{.proof}
We have:
\begin{eqnarray*}
\mathbb{E}\left(\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\right) &=&
\int \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta) dy \\
&=& \int \frac{\partial f(y;\boldsymbol\theta)/\partial \boldsymbol\theta}{f(y;\boldsymbol\theta)} f(y;\boldsymbol\theta) dy =
\frac{\partial}{\partial \boldsymbol\theta} \int f(y;\boldsymbol\theta) dy =
\partial 1 /\partial \boldsymbol\theta = 0,
\end{eqnarray*}
which gives the result.
:::

:::{.definition #Fisher name="Fisher information matrix"}
The **information matrix** is (minus) the the expectation of the second derivatives of the log-likelihood function:
$$
\mathcal{I}_Y(\boldsymbol\theta) = - \mathbb{E} \left( \frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} \right).
$$
:::

***
:::{.proposition #Fisher}
We have $\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E} \left[ \left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)
\left( \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta} \right)' \right] = \mathbb{V}ar[S(Y;\boldsymbol\theta)]$.
:::
***

:::{.proof}
We have $\frac{\partial^2 \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = \frac{\partial^2 f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\frac{1}{f(Y;\boldsymbol\theta)} - \frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta}\frac{\partial \log f(Y;\boldsymbol\theta)}{\partial \boldsymbol\theta'}$. The expectation of the first right-hand side term is $\partial^2 1 /(\partial \boldsymbol\theta \partial \boldsymbol\theta') = \bv{0}$, which gives the result.
:::

:::{.example}
If $y_i \sim\,i.i.d.\, \mathcal{N}(\mu,\sigma^2)$, let $\boldsymbol\theta = [\mu,\sigma^2]'$ then
$$
\frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} = \left[\frac{y-\mu}{\sigma^2} \quad \frac{1}{2\sigma^2}\left(\frac{(y-\mu)^2}{\sigma^2}-1\right) \right]'
$$
and
$$
\mathcal{I}_Y(\boldsymbol\theta) = \mathbb{E}\left( \frac{1}{\sigma^4}
\left[
\begin{array}{cc}
\sigma^2&y-\mu\\
y-\mu & \frac{(y-\mu)^2}{\sigma^2}-\frac{1}{2}
\end{array}\right]
\right)=
\left[
\begin{array}{cc}
1/\sigma^2&0\\
0 & 1/(2\sigma^4)
\end{array}\right].
$$
:::


***
:::{.proposition #additiv name="Additive property of the Info. mat."}
The information matrix resulting from two independent experiments is the sum of the information matrices:
$$
\mathcal{I}_{X,Y}(\boldsymbol\theta) = \mathcal{I}_X(\boldsymbol\theta) + \mathcal{I}_Y(\boldsymbol\theta).
$$
:::
***
:::{.proof} Immediately obtained from the definition (see Def. \@ref{def:Fisher}).
:::

***
:::{.theorem #FDCR name="Fr\\'echet-Darmois-Cram\\'er-Rao bound"}
Consider an unbiased estimator of $\boldsymbol\theta$ denoted by $\hat{\boldsymbol\theta}(Y)$.

The variance of the random variable $\boldsymbol\omega'\hat{\boldsymbol\theta}$ (which is a linear combination of the components of $\hat{\boldsymbol\theta}$) is larger than:
$$
(\boldsymbol\omega'\boldsymbol\omega)^2/(\boldsymbol\omega' \mathcal{I}_Y(\boldsymbol\theta) \boldsymbol\omega).
$$
:::
***

:::{.proof}
The Cauchy-Schwarz inequality implies that $\sqrt{\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y))\mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))} \ge |\boldsymbol\omega'\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)]\boldsymbol\omega |$. Now, $\mathbb{C}ov[\hat{\boldsymbol\theta}(Y),S(Y;\boldsymbol\theta)] = \int_y \hat{\boldsymbol\theta}(y) \frac{\partial \log f(y;\boldsymbol\theta)}{\partial \boldsymbol\theta} f(y;\boldsymbol\theta)dy =
\frac{\partial}{\partial \boldsymbol\theta}\int_y \hat{\boldsymbol\theta}(y) f(y;\boldsymbol\theta)dy = \bv{I}$ because $\hat{\boldsymbol\theta}$ is unbiased. 

Therefore $\mathbb{V}ar(\boldsymbol\omega'\hat{\boldsymbol\theta}(Y)) \ge \mathbb{V}ar(\boldsymbol\omega'S(Y;\boldsymbol\theta))^{-1} (\boldsymbol\omega'\boldsymbol\omega)^2$. Prop. \@ref(prp:Fisher) leads to the result.
:::

:::{.definition #identif}
The vector of parameters $\boldsymbol\theta$ is identifiable if, for any other vector $\boldsymbol\theta^*$:
$$
\boldsymbol\theta^* \ne \boldsymbol\theta \Rightarrow \mathcal{L}(\boldsymbol\theta^*;\bv{y}) \ne \mathcal{L}(\boldsymbol\theta;\bv{y}).
$$
:::

:::{.definition #MLEest name="Maximum Likelihood Estimator (MLE)"}
The maximum likelihood estimator (MLE) is the vector $\boldsymbol\theta$ that maximizes the likelihood function. Formally:
\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \mathcal{L}(\boldsymbol\theta;\bv{y})  = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\bv{y}).(\#eq:MLEestimator)
\end{equation}
:::

:::{.definition #likFunction name="Likelihood equation"}
Necessary condition for maximizing the likelihood function:
\begin{equation}
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta} = \bv{0}.
\end{equation}
:::

:::{.hypothesis #MLEregularity name="Regularity assumptions"}

We have:

i. $\boldsymbol\theta \in \Theta$ where $\Theta$ is compact.

ii. $\boldsymbol\theta_0$ is identified.

iii. The log-likelihood function is continuous in $\boldsymbol\theta$.

iv. $\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))$ exists.

v. The log-likelihood function is such that $(1/n)\log\mathcal{L}(\boldsymbol\theta;\bv{y})$ converges almost surely to $\mathbb{E}_{\boldsymbol\theta_0}(\log f(Y;\boldsymbol\theta))$, uniformly in $\boldsymbol\theta \in \Theta$.

vi. The log-likelihood function is twice continuously differentiable in an open neighborood of $\boldsymbol\theta_0$.

vii. The matrix $\bv{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \quad \mbox{(Fisher Information matrix)}$ exists and is nonsingular.
:::

***
:::{.proposition #MLEproperties name="Properties of MLE"}
Under regularity conditions (Assumptions \@ref(hyp:MLEregularity)), the MLE is:

a. **Consistent**: $\mbox{plim}\quad  \boldsymbol\theta_{MLE} = \theta_0$ ($\theta_0$ is the true vector of parameters).

b. **Asymptotically normal**: $\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\bv{I}(\boldsymbol\theta_0)^{-1})$, where
$$
\bv{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0). \quad \mbox{(Fisher Info. matrix)}
$$

c. **Asymptotically efficient**: $\boldsymbol\theta_{MLE}$ is asymptotically efficient and achieves the Fr\'echet-Darmois-Cram\'er-Rao lower bound for consistent estimators.

d. **Invariant**: The MLE of $g(\boldsymbol\theta_0)$ is $g(\boldsymbol\theta_{MLE})$ if $g$ is a continuous and continuously differentiable function.
:::
***

:::{.proof}
See [Online additional material](https://www.dropbox.com/s/8xelbghtvnd43yh/Proofs_MLE.pdf?dl=0).
:::


Note that (b) also writes:
\begin{equation}
\sqrt{n}(\boldsymbol\theta_{MLE} - \boldsymbol\theta_{0}) \overset{d}{\rightarrow} \mathcal{N}(0,\mathcal{I}_Y(\boldsymbol\theta_0)^{-1}). (\#eq:normMLE)
\end{equation}


The asymptotic covariance matrix of the MLE is:
$$
[\bv{I}(\boldsymbol\theta_0)]^{-1} = \left[- \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) \right]^{-1}.
$$
A direct (analytical) evaluation of this expectation is often out of reach.

It can however be estimated by, either:
\begin{eqnarray}
\hat{\bv{I}}_1^{-1} &=&  \left( - \frac{\partial^2 \log \mathcal{L}({\boldsymbol\theta_{MLE}};\bv{y})}{\partial {\boldsymbol\theta} \partial {\boldsymbol\theta}'}\right)^{-1}, (\#eq:III1)\\
\hat{\bv{I}}_2^{-1} &=&  \left( \sum_{i=1}^n \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta}} \frac{\partial \log \mathcal{L}({\boldsymbol\theta_{MLE}};y_i)}{\partial {\boldsymbol\theta'}} \right)^{-1}.  (\#eq:I2)
\end{eqnarray}

### To sum up -- MLE in practice

* A parametric model (depending on the vector of parameters $\boldsymbol\theta$ whose "true" value is $\boldsymbol\theta_0$) is specified.

* i.i.d. sources of randomness are identified.

* The density associated to one observation $y_i$ is computed analytically (as a function of $\boldsymbol\theta$): $f(y;\boldsymbol\theta)$.

* The log-likelihood is $\log \mathcal{L}(\boldsymbol\theta;\bv{y}) = \sum_i \log f(y_i;\boldsymbol\theta)$.

* The MLE estimator results from the optimization problem (this is Eq. \@ref(eq:MLEestimator)):
\begin{equation}
\boldsymbol\theta_{MLE} = \arg \max_{\boldsymbol\theta} \log \mathcal{L}(\boldsymbol\theta;\bv{y}).
\end{equation}

* We have: $\boldsymbol\theta_{MLE} \sim \mathcal{N}(\theta_0,\bv{I}(\boldsymbol\theta_0)^{-1})$, where $\bv{I}(\boldsymbol\theta_0)^{-1}$ is estimated by means of Eq. \@ref(eq:III1) or Eq. \@ref(eq:I2). Most of the time, this computation is numerical.


### Example: MLE estimation of a mixture of Gaussian distribution

Consider the returns of the SMI index. Let's assume that these returns are independently drawn from a mixture of Gaussian distributions. The p.d.f. $f(x;\boldsymbol\theta)$, with $\boldsymbol\theta = [\mu_1,\sigma_1,\mu_2,\sigma_2,p]'$, is given by:
$$
p \frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) + (1-p)\frac{1}{\sqrt{2\pi\sigma_2^2}}\exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right).
$$
(See [p.d.f. of mixtures of Gaussian dist.](https://jrenne.shinyapps.io/density/))

The maximum likelihood estimate is $\boldsymbol\theta_{MLE}=[0.30,1.40,-1.45,3.61,0.87]'$.

The first two entries of the diagonal of $\hat{\bv{I}}_1^{-1}$ are $0.00528$ and $0.00526$. They are the estimates of $\mathbb{V}ar(\mu_{1,MLE})$ and of $\mathbb{V}ar(\sigma_{1,MLE})$, respectively.

95\% confidence intervals for $\mu_1$ and $\sigma_1$ are, respectively:
$$
0.30 \pm 1.96\underbrace{\sqrt{0.00528}}_{=0.0726} \quad \mbox{ and } \quad 1.40 \pm 1.96\underbrace{\sqrt{0.00526}.}_{=0.0725}
$$

\begin{figure}
\caption{Density of 5-day returns on SMI index}
\includegraphics[width=.9\linewidth]{../../figures/Figure_kernel_smi.pdf}
\label{fig:illuskernel_smi}
\begin{tiny}
Gaussian kernel, $h=0.5$ (in percent).

The data spans the period from 2 June 2006 to 23 February 2016 at the daily frequency.

Left-hand plot: the blue lines indicates $\mu \pm 2 \sigma$, where $\mu$ is the sample mean of the returns and $\sigma$ is their sample standard deviation. Right-hand plot: the red dotted line is the density $\mathcal{N}(\mu,\sigma^2)$
\end{tiny}
\end{figure}
\end{exmpl}
\end{scriptsize}
\end{frame}


\begin{frame}{}
\begin{scriptsize}
\addtocounter{exmpl}{-1}
\begin{exmpl}[MLE estim. of a mixture of Gaussian distri. (cont'd)]
\begin{figure}
\caption{Estimated density (vs kernel-based estimate)}
\includegraphics[width=1\linewidth]{../../figures/Figure_kernel_smiMLE.pdf}
\label{fig:MLE1}
\end{figure}
\end{exmpl}
\end{scriptsize}
\end{frame}



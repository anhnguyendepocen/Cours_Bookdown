# Microeconometrics

## Binary-choice models

In many instances, the variables to be explained ($y_i$s) have only two possible values ($0$ and $1$, say).

Assume we suspect some variable $\bv{x}_i$ ($K \times 1$) to be able to account for the probability that $y_i=1$.

The model reads:
\begin{equation}\label{eq:binaryBenroulli}
y_i | \bv{X} \sim \mathcal{B}(g(\bv{x}_i;\boldsymbol\theta)),
\end{equation}
where $g(\bv{x}_i;\boldsymbol\theta)$ is the parameter of the Bernoulli distribution. In other words, conditionally on $\bv{X}$:
$$
y_i = \left\{
\begin{array}{cl}
1 & \mbox{ with probability } g(\bv{x}_i;\boldsymbol\theta)\\
0 & \mbox{ with probability } 1-g(\bv{x}_i;\boldsymbol\theta),
\end{array}
\right.
$$
where $\boldsymbol\theta$ is a vector of parameters to be estimated.

The objective is to estimate the vector of population parameters $\boldsymbol\theta$.

Binary-choice models can be used to account for...

* any binary decisions (e.g. in referendums, being owner or renter, living in the city or in the countryside, in/out of the labour force,...),
* contamination (disease or default),
* success/failure (exams).

A possibility is to run a linear regression (a situation called **Linear Probability Model, LPM**):
$$
y_i = \boldsymbol\theta'\bv{x}_i + \varepsilon_i.
$$

Such a regression could be consistent with the *conditional-mean-zero assumption* (Hypothesis \@ref(hyp:exogeneity)) and with the *assumption of non-correlated residuals* (Hypothesis \@ref(hyp:noncorrelResid)), but more difficultly with the **homoskedasticity assumption** (Hypothesis \@ref(hyp:homoskedasticity)). Moreover, the $\varepsilon_i$s cannot be Gaussian (because $y_i \in \{0,1\}$). Therefore, using a linear regression to study the relationship between $\bv{x}_i$ and $y_i$ can be consistent but it is inefficient.


```{r LPM, echo=FALSE, fig.cap="Fitting a binary variable with a linear model (Linear Probability Model, LPM). The model is $\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)$, where $\\Phi$ is the c.d.f. of the normal distribution and where $x_i \\sim \\,i.i.d.\\,\\mathcal{N}(0,1)$.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
beta0 <- 0.5;beta1 <- 2
nb.sim <- 100
xx <- rnorm(nb.sim)
P.vec <- pnorm(beta0 + beta1*xx)
U <- runif(nb.sim)
yy <- (U<P.vec)
eqY <- lm(yy~xx)
par(mfrow=c(1,1));par(plt=c(.2,.95,.25,.85))
plot(xx,yy,pch=19,xlab="x",ylab="y",xlim=c(-5,3))
abline(eqY,col="blue",lwd=2)
legend("topleft",
       c("Observations","Linear regression line (LPM)"),
       lty=c(NaN,1), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2), # line width
       col=c("black","blue"),
       pch=c(19,NaN),seg.len=2)
```




Table: (\#tab:foo) This table provides examples of function $g$, s.t. $\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol	heta) = g(\boldsymbol\theta'\bv{x}_i)$. The "linear" case is given for comparison, but note that it does not satisfy $g(\boldsymbol\theta'\bv{x}_i)$ for any value of $\boldsymbol\theta'\bv{x}_i$.

| Model   | Function $g$ | Derivative                       |
|---------|---------------------------------------------------------------------------------------------------|----------------------------------|
| Probit  | $\Phi$                                                                                            | $\phi$                           |
| Logit   | $\dfrac{\exp(x)}{1+\exp(x)}$                                                                      | $\dfrac{\exp(x)}{(1+\exp(x))^2}$ |
| log-log | $1 - \exp(-\exp(x))$                                                                              | $\exp(-\exp(x))\exp(x)$          |
| linear  | $x$                                                                                               | 1                                |

Two prominent models to tackle this situation. In both models, we have:
$$
\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta)=g(\boldsymbol\theta'\bv{x}_i).
$$

In the **Probit model**, we have
\begin{equation}
g(z) = \Phi(z),(\#eq:probit)
\end{equation}
where $\Phi$ is the c.d.f. of the normal distribution.

And for the **logit model**:
\begin{equation}
g(z) = \frac{1}{1+\exp(-z)}.(\#eq:logit)
\end{equation}

Figure \@ref(fig:ProbLogit) displays the functions $g$ appearing in Table \@ref(tab:foo).

```{r ProbLogit, echo=FALSE, fig.cap="Probit, Logit, and Log-log functions.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
x <- seq(-5,5,by=.01)
plot(x,pnorm(x),type="l",lwd=2,col="black",xlab="x",ylab="")
lines(x,1/(1+exp(-x)),col="blue",lwd=2)
lines(x,1- exp(-exp(x)),col="red",lwd=2)
legend("topleft",
       c("Probit","Logit","Log-log"),
       lty=c(1,1,1), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2,2), # line width
       col=c("black","blue","red"),
       seg.len=2)
```


```{r LPM2, echo=FALSE, fig.cap="The model is $\\mathbb{P}(y_i=1|x_i)=\\Phi(0.5+2x_i)$, where $\\Phi$ is the c.d.f. of the normal distribution and where $x_i \\sim \\,i.i.d.\\,\\mathcal{N}(0,1)$. Crosses give the model-implied probabilities of having $y_i=1$.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
P.vec <- pnorm(beta0 + beta1*xx)
par(mfrow=c(1,1));par(plt=c(.2,.95,.25,.85))
plot(xx,yy,pch=19,xlab="x",ylab="y",xlim=c(-5,3))
abline(eqY,col="blue",lwd=2)
points(xx,P.vec,pch=3,col="red",lwd=2)
legend("topleft",
       c("Observations","fitted linear regressions",expression(paste(P,"(",y[i],"=",1,"|",x[i],")",sep=""))),
       lty=c(NaN,1,NaN), # gives the legend appropriate symbols (lines)       
       lwd=c(2,2,2), # line width
       col=c("black","blue","red"),
       pch=c(19,NaN,3),
       seg.len=2)
```

### Interpretation in terms of latent variable, and utility-based models

The probit model has an interpretation in terms of **latent variables**. In this model, we indeed have:
$$
\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta) = \Phi(\boldsymbol\theta'\bv{x}_i) = \mathbb{P}(-\varepsilon_{i}<\boldsymbol\theta'\bv{x}_i),
$$
where $\varepsilon_{i} \sim \mathcal{N}(0,1)$. That is:
$$
\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta) = \mathbb{P}(0< y_i^*),
$$
where $y_i^* = \boldsymbol\theta'\bv{x}_i + \varepsilon_i$, with $\varepsilon_{i} \sim \mathcal{N}(0,1)$. Variable $y_i^*$ can be interpreted as a (latent) variable that determines $y_i$ since $y_i = \mathbb{I}_{\{y_i^*>0\}}$.

```{r Latent, echo=FALSE, fig.cap="Distribution of $y_i^*$ conditional on $\\bv{x}_i$.", fig.asp = .6, out.width = "90%", fig.align = 'center'}
par(plt=c(.12,.95,.25,.85))
mu <- 1
x <- seq(-4,4,by=.01)
plot(x,dnorm(x-mu),type="l",lwd=2,
     xlab=expression(paste("Possible values of ",y[i],"*",sep="")),
     ylab="")
abline(v=mu,lty=2,col="grey")
abline(v=0)
abline(h=0)
text(mu,.02,expression(paste(theta,"'",x_i,sep="")),pos=4)
x.0 <- seq(-4,0,by=.01)
polygon(c(x.0,x.0[length(x.0):1]),c(dnorm(x.0-mu),0*x.0),col="grey")
abline(v=0)
arrows(x0=-2, y0=.3, x1 = -1, y1 = .01, length = 0.05, angle = 20,
       code = 2)
text(-2,.3,expression(paste(P,"(",y[i],"=0|",x[i],";",theta,")",sep="")),pos=2)
arrows(x0=2.5, y0=.3, x1 = 1.5, y1 = .2, length = 0.05, angle = 20,
       code = 2)
text(2.5,.3,expression(paste(P,"(",y[i],"=1|",x[i],";",theta,")",sep="")),pos=4)
```


**Random Utility Models (RUM)** are based on such a view of probit models. Assume that agent ($i$) chooses $y_i=1$ if the utility associated with this choice ($U_{i,1}$) is higher than the one associated with $y_i=0$ ($U_{i,0}$). Assume further that the utility of agent $i$, if she chooses outcome $j$ ($\in \{0,1\}$), is given by
$$
U_{i,j} = V_{i,j} + \varepsilon_{i,j},
$$
where $V_{i,j}$ is the deterministic component of the utility associated with choice  and $\varepsilon_{i,j}$ is a random (agent-specific) component.

Moreover, posit $V_{i,j} = \boldsymbol\theta_j'\bv{x}_i$. We then have:
\begin{eqnarray}
\mathbb{P}(y_i = 1|\bv{x}_i;\boldsymbol\theta) &=& \mathbb{P}(\boldsymbol\theta_1'\bv{x}_i+\varepsilon_{i,1}>\boldsymbol\theta_0'\bv{x}_i+\varepsilon_{i,0}) \nonumber\\
&=& F(\boldsymbol\theta_1'\bv{x}_i-\boldsymbol\theta_0'\bv{x}_i) = F([\boldsymbol\theta_1-\boldsymbol\theta_0]'\bv{x}_i),(\#eq:utility)
\end{eqnarray}
where $F$ is the c.d.f. of $\varepsilon_{i,0}-\varepsilon_{i,1}$.

Note that only the difference $\boldsymbol\theta_1-\boldsymbol\theta_0$ is identifiable (as opposed to $\boldsymbol\theta_1$ AND $\boldsymbol\theta_0$). Indeed, replacing $U$ with $aU$ ($a>0$) gives the same model $\Leftrightarrow$ scaling issue, solved by fixing the variance of $\varepsilon_{i,0}-\varepsilon_{i,1}$.

Other types of structural models --based on the comparison of marginal costs and benefits-- give rise to the existence of latent variable and to probit models. An example is @Nakosteen_Zimmer_1980. The main ingredients of their approach are as follows:

* Wage that can be earned at the present location: $y_p^* = \boldsymbol\theta_p'\bv{x}_p + \varepsilon_p$.

* Migration cost: $C^*= \boldsymbol\theta_c'\bv{x}_c + \varepsilon_c$.

* Wage earned elsewhere: $y_m^* = \boldsymbol\theta_m'\bv{x}_m + \varepsilon_m$.

In this context, agents decision to migrate if $y_m^* >  y_p^* + C^*$, i.e. if
$$
y^* = y_m^* -  y_p^* - C^* =  \boldsymbol\theta'\bv{x} + \underbrace{\varepsilon}_{=\varepsilon_m - \varepsilon_c - \varepsilon_p}>0,
$$
where $\bv{x}$ is the union of the $\bv{x}_i$s, for $i \in \{p,m,c\}$.


### Alternative-Varying Regressors {#Avregressors}

In some cases, the regressors may depend on the considered alternative ($0$ or $1$). For instance:

* When modeling the decision to participate in the labour force (or not), the wage depends on the alternative. It cannot be included among the regressors given it is not observed if the considered agent has decided not to work.

* In the context of the choice of transportation mode, the *time cost* depends on the considered transportation mode.

In terms of utility, we then have:
$$
V_{i,j} = {\theta^{(u)}_{j}}'\bv{u}_{i,j} + {\theta^{(v)}_{j}}'\bv{v}_{i},
$$
where the $\bv{u}_{i,j}$s are regressors associated with agent $i$, but taking different values for the different choices ($j=0$ or $j=1$).

In that case, Eq. \@ref(eq:utility) becomes:
\begin{equation}
\mathbb{P}(y_i = 1|\bv{x}_i;\boldsymbol\theta)  = F\left({\theta^{(u)}_{1}}'\bv{u}_{i,1}-{\theta^{(u)}_{0}}'\bv{u}_{i,0}+[\boldsymbol\theta_1^{(v)}-\boldsymbol\theta_0^{(v)}]'\bv{v}_i\right),(\#eq:utility2)
\end{equation}
and, if $\theta^{(u)}_{1}=\theta^{(u)}_{0}=\theta^{(u)}$ --as is customary-- we get:
\begin{equation}
\mathbb{P}(y_i = 1|\bv{x}_i;\boldsymbol\theta)  = F\left({\theta^{(u)}_{1}}'(\bv{u}_{i,1}-\bv{u}_{i,0})+[\boldsymbol\theta_1^{(v)}-\boldsymbol\theta_0^{(v)}]'\bv{v}_i\right).(\#eq:utility3)
\end{equation}

The fishing-mode dataset used in @Cameron_Trivedi_2005 (Chapter 14 and 15) contains alternative-specific variables. Specifically, for each individual, the price and catch rate depend on the fishing model. In the table reported below, lines `price` and `catch` correspond to the prices and catch rates associated with the chosen alternative.

```{r fishing1, warning=FALSE, message=FALSE}
library(Ecdat)
data(Fishing)
stargazer::stargazer(Fishing,type="text")
```

### Estimation

These models can be estimated by Maximum Likelihood approaches (see Section \@ref(secMLE)).

To simplify the exposition, we consider the $\bv{x}_i$ to be deterministic. Also, we assume that the r.v. are independent across entities $i$. How to write the likelihood here? It can be seen that:
\begin{eqnarray}
f(y_i|\bv{x}_i;\boldsymbol\theta) &=& y_i g(\boldsymbol\theta'\bv{x}_i) + (1-y_i) (1-g(\boldsymbol\theta'\bv{x}_i)) \nonumber \\
&=&  g(\boldsymbol\theta'\bv{x}_i)^{y_i}(1-g(\boldsymbol\theta'\bv{x}_i))^{1-y_i}.
\end{eqnarray}

Therefore, if the observations $(\bv{x}_i,y_i)$ are independent across entities $i$, then:
$$
\log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X}) = \sum_{i=1}^{n}y_i \log[g(\boldsymbol\theta'\bv{x}_i)] + (1-y_i)\log[1-g(\boldsymbol\theta'\bv{x}_i)].
$$

The likelihood equation reads (FOC of the optimization program, see Def. \@ref(def:likFunction)):
$$
\dfrac{\partial \log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X})}{\partial \boldsymbol\theta} = \bv{0},
$$
that is:
$$
\sum_{i=1}^{n} y_i \bv{x}_i\frac{g'(\boldsymbol\theta'\bv{x}_i)}{g(\boldsymbol\theta'\bv{x}_i)} - (1-y_i) \bv{x}_i \frac{g'(\boldsymbol\theta'\bv{x}_i)}{1-g(\boldsymbol\theta'\bv{x}_i)} = \bv{0}.
$$

This is a nonlinear equation that generally has to be numerically solved. Under regularity conditions (Hypotheses \@ref(hyp:MLEregularity)), we have (Prop. \@ref(prp:MLEproperties)):
$$
\boldsymbol\theta_{MLE} \sim \mathcal{N}(\boldsymbol\theta_0,\bv{I}(\boldsymbol\theta_0)^{-1}),
$$
where
$$
\bv{I}(\boldsymbol\theta_0) = - \mathbb{E}_0 \left( \frac{\partial^2 \log \mathcal{L}(\theta;\bv{y},\bv{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right) = n \mathcal{I}_Y(\boldsymbol\theta_0).
$$

For finite samples, we can e.g. approximate $\bv{I}(\boldsymbol\theta_0)^{-1}$ by (Eq. \@ref(eq:III1)):
$$
\bv{I}(\boldsymbol\theta_0)^{-1} \approx -\left(\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta_{MLE};\bv{y},\bv{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'}\right)^{-1}.
$$


In the Probit case (see Table \@ref(tab:foo)), it can be shown that we have:
\begin{eqnarray*}
&&\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = - \sum_{i=1}^{n} g'(\boldsymbol\theta'\bv{x}_i) [\bv{x}_i \bv{x}_i'] \times \\
&&\left[y_i \frac{g'(\boldsymbol\theta'\bv{x}_i) + \boldsymbol\theta'\bv{x}_ig(\boldsymbol\theta'\bv{x}_i)}{g(\boldsymbol\theta'\bv{x}_i)^2} + (1-y_i) \frac{g'(\boldsymbol\theta'\bv{x}_i) - \boldsymbol\theta'\bv{x}_i (1 - g(\boldsymbol\theta'\bv{x}_i))}{(1-g(\boldsymbol\theta'\bv{x}_i))^2}\right].
\end{eqnarray*}

In the Logit case (see Table \@ref(tab:foo)), it can be shown that we have:
$$
\frac{\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X})}{\partial \boldsymbol\theta \partial \boldsymbol\theta'} = - \sum_{i=1}^{n} g'(\boldsymbol\theta'\bv{x}_i) \bv{x}_i\bv{x}_i',
$$
where $g'(x)=\dfrac{\exp(-x)}{(1 + \exp(-x))^2}$.

Since $g'(x)>0$, it can be checked that $-\partial^2 \log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X})/\partial \boldsymbol\theta \partial \boldsymbol\theta'$ is positive definite.

### Marginal effectss

How to measure marginal effects, i.e. the effect on the probability that $y_i=1$ of a marginal increase of $x_{i,k}$? This  object is given by:
$$
\frac{\partial \mathbb{P}(y_i=1|\bv{x_i};\boldsymbol\theta)}{\partial x_{i,k}} = \underbrace{g'(\boldsymbol\theta'\bv{x}_i)}_{>0}\theta_k,
$$
which is of the same sign as $\theta_k$.

It can be estimated by $g'(\boldsymbol\theta_{MLE}'\bv{x}_i)\theta_{MLE,k}$. It is important to see that the marginal effect depends on $\bv{x}_i$: increases by 1 unit of $x_{i,k}$ (entity $i$) and of $x_{j,k}$ (entity $j$) do not necessarily have the same effects on $\mathbb{P}(y_i=1|\bv{x_i};\boldsymbol\theta)$ and on $\mathbb{P}(y_j=1|\bv{x_j};\boldsymbol\theta)$, respectively.


To address this issue, one can compute some measures of "average" marginal effect. There are two main solutions. For each explanatory variable $k$:

i. Denoting by $\hat{\bv{x}}$ the sample average of the $\bv{x}_i$s, compute $g'(\boldsymbol\theta_{MLE}'\hat{\bv{x}})\theta_{MLE,k}$.

ii. Compute the average (across $i$) of $g'(\boldsymbol\theta_{MLE}'\bv{x}_i)\theta_{MLE,k}$.


### Goodness of fit

There is no obvious version of "$R^2$" for binary-choice models. Existing measures are called **pseudo-$R^2$} measures**.

Denoting by $\log \mathcal{L}_0(\bv{y})$ the (maximum) log-likelihood that would be obtained for a model containing only a constant term (i.e. with $\bv{x}_i = 1$ for all $i$), the **McFadden's pseudo-$R^2$** is given by:
$$
R^2_{MF} = 1 - \frac{\log \mathcal{L}(\boldsymbol\theta;\bv{y})}{\log \mathcal{L}_0(\bv{y})}.
$$
Intuitively, $R^2_{MF}=0$ if the explanatory variables do not allow to predict the decision ($y$).

### Example: Credit data

This example makes use of the `credit` data of package `AEC`. The objective is to model the default probabilities of lenders.

```{r Probitlending}
library(AEC)
credit$Default <- 0
credit$Default[credit$loan_status == "Charged Off"] <- 1
credit$Default[credit$loan_status == "Does not meet the credit policy. Status:Charged Off"] <- 1
credit$amt2income <- credit$loan_amnt/credit$annual_inc
plot(as.factor(credit$Default)~log(credit$annual_inc),
     ylevels=2:1,ylab="Default status",xlab="log(annual income)")
```

We consider three specifications. The first one, with no explanatory variables, is trivial. It will just be used to compute the pseudo-$R^2$.

```{r Probitlending2}
eq0 <- glm(Default ~ 1,data=credit,family=binomial(link="probit"))
eq1 <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs + log(annual_inc)+ I(log(annual_inc)^2),
           data=credit,family=binomial(link="probit"))
eq2 <- glm(Default ~ grade + log(loan_amnt) + amt2income + delinq_2yrs + log(annual_inc)+ I(log(annual_inc)^2),
           data=credit,family=binomial(link="probit"))
logL0 <- logLik(eq0)
logL1 <- logLik(eq1)
logL2 <- logLik(eq2)
1 - logL1/logL0 # pseudo R2
1 - logL2/logL0 # pseudo R2
stargazer::stargazer(eq0,eq1,eq2,type="text")
```

Let us now compute marginal effects.

```{r Probitlending3}
mean(dnorm(predict(eq2)),na.rm=TRUE)*eq2$coefficients
```
There is, however, an issue for the `annual_inc` variable. Indeed, the previous computation does not realize that this variable appears twice among the explanatory variables. To address this, one can proceed as follows.

```{r Probitlending4}
new_credit <- credit
new_credit$annual_inc <- 1.01 * new_credit$annual_inc # increase of income by 1%
bas_predict_eq2  <- predict(eq2, newdata = credit, type = "response")
# This is equivalent to pnorm(predict(eq2, newdata = credit))
new_predict_eq2  <- predict(eq2, newdata = new_credit, type = "response")
mean(new_predict_eq2 - bas_predict_eq2)
```
This average effect is pretty low. To compare, let us compute the average effect associated with a unit increase in the number of delinquencies:

```{r Probitlending5}
new_credit <- credit
new_credit$delinq_2yrs <- credit$delinq_2yrs + 1
new_predict_eq2  <- predict(eq2, newdata = new_credit, type = "response")
mean(new_predict_eq2 - bas_predict_eq2)
```

We can employ a likelihood ratio test (see Def. \@ref(def:LR)) to see if the two variables associated with annual income are jointly statistically significant (in the context of `eq1`):
```{r Probitlending6}
eq1restr <- glm(Default ~ log(loan_amnt) + amt2income + delinq_2yrs,
                data=credit,family=binomial(link="probit"))
LRstat <- 2*(logL1 - logLik(eq1restr))
pvalue <- 1 - c(pchisq(LRstat,df=2))
```

The computation gives a p-value of `r round(pvalue,4)`.



<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \begin{exmpl}[Labor market participation] -->
<!-- \input{../../../../../Google Drive/Teaching/UNIL/Rcode/tables/outfile_SwissLabor.txt} -->

<!-- \begin{center} -->
<!-- \begin{tiny} -->
<!-- Source: \href{http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-1255(199605)11:3\%3C321::AID-JAE391\%3E3.0.CO;2-K/abstract}{Gerfin (1996)}. -->
<!-- \end{tiny} -->
<!-- \end{center} -->
<!-- Marginal effects: -->

<!-- income: $-0.22$, age: $0.69$, education: $0.006$, youngkids: $-0.24$, oldkids: $-0.05$, foreignyes: $0.24$, age2: $-0.10$. -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->

<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[Labor market participation (cont'd)] -->
<!-- 		\begin{figure} -->
<!-- 		\caption{Labor market participation vs. age and education} -->
<!-- 			\includegraphics[width=1\linewidth]{../../../../../Google Drive/Teaching/UNIL/Figures/Figure_Probit4.pdf} -->

<!-- 		\begin{tiny} -->
<!-- 		\end{tiny} -->
<!-- 		\end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \addtocounter{exmpl}{-1} -->
<!-- \begin{exmpl}[Leverage Bubbles] -->

<!-- \begin{figure} -->
<!-- 		\caption{Logit estimations} -->
<!-- 			\includegraphics[width=.75\linewidth]{../figures/Figure_Table_JordaSchularickTaylor.pdf} -->

<!-- \begin{center} -->
<!-- \begin{tiny} -->
<!-- Source: \href{https://www.sciencedirect.com/science/article/abs/pii/S0304393215000987}{J\'orda, Schularick and Taylor (2015)}/ \href{https://www.frbsf.org/economic-research/files/wp2015-10.pdf}{Working paper version}/ \href{http://www.macrohistory.net/data/}{Dataset}. -->
<!-- \end{tiny} -->
<!-- \end{center} -->
<!-- \end{figure} -->
<!-- \end{exmpl} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->




### Replicating Table 14.2 of Cameron and Trivedi (2005)

The following lines of codes replicate Table 14.2 of @Cameron_Trivedi_2005.

```{r fishing2}
data.reduced <- subset(Fishing,mode %in% c("charter","pier"))
data.reduced$lnrelp <- log(data.reduced$pcharter/data.reduced$ppier)
data.reduced$y <- 1*(data.reduced$mode=="charter")
# check first line of Table 14.1:
price.charter.y0 <- mean(data.reduced$pcharter[data.reduced$y==0])
price.charter.y1 <- mean(data.reduced$pcharter[data.reduced$y==1])
price.charter    <- mean(data.reduced$pcharter)
# Run probit regression:
reg.probit <- glm(y ~ lnrelp,
                  data=data.reduced,
                  family=binomial(link="probit"))
# Run Logit regression:
reg.logit <- glm(y ~ lnrelp,
                 data=data.reduced,
                 family=binomial(link="logit"))
# Run OLS regression:
reg.OLS <- lm(y ~ lnrelp,
              data=data.reduced)
# Replicates Table 14.2 of Cameron and Trivedi:
stargazer::stargazer(reg.logit, reg.probit, reg.OLS,
                     type="text")
```


### Predictions

How to define predicted outcomes? As is the case for $y_i$, predicted outcomes $\hat{y}_i$ need to be valued in $\{0,1\}$. A natural choice consists in considering that $\hat{y}_i=1$ if $\mathbb{P}(y_i=1|\bv{x}_i;\boldsymbol\theta) > 0.5$, i.e., in taking a cutoff of $c=0.5$.However, we may have some models where all predicted probabilities are small, but some less than others. In this context, a model-implied probability of 10\% (say) could characterize a "high-risk" entity. However, using a cutoff of 50\% would not identify this level of riskiness.

The **receiver operating characteristics (ROC)** curve consitutes a more general approach. It works as follows:

For each potential cutoff $c \in [0,1]$, compute (and plot):

*	The fraction of $y = 1$ values correctly classified (*True Positive Rate*) against
* The fraction of $y = 0$ values incorrectly specified (*False Positive Rate*).

Such a curve mechanically starts at (0,0) [situation when $c=1$] and terminates at (1,1) [situation when $c=0$].

In the case of no predictive ability (worst situation), the ROC curve is a straight line between (0,0) and (1,1).


```{r fishing3, message=FALSE, warning=FALSE}
library(pROC)
predict_model <- predict.glm(reg.probit,type = "response")
roc(data.reduced$y, predict_model, percent=T,
    boot.n=1000, ci.alpha=0.9, stratified=T, plot=TRUE, grid=TRUE,
    show.thres=TRUE, legacy.axes = TRUE, reuse.auc = TRUE,
    print.auc = TRUE, print.thres.col = "blue", ci=TRUE,
    ci.type="bars", print.thres.cex = 0.7, col = 'red',
    main = paste("ROC curve using","(N = ",nrow(data.reduced),")") )
```

## Multiple Choice Models

We will now consider cases where the number of possible outcomes (or alternatives) is larger than two. Let us denote by $J$ this number. We have $y_j \in \{1,\dots,J\}$. We also consider variables $\bv{x}_i$ ($K \times 1$) that are suspected to account for the probabilities that $y_i=j$, $j \in \{1,\dots,J\}$.

We will assume that the $y_i$s are assumed to be independently distributed, with: 
$$
y_i = \left\{
\begin{array}{cl}
1 & \mbox{ with probability } g_1(\bv{x}_i;\boldsymbol\theta)\\
\vdots \\
J & \mbox{ with probability } g_J(\bv{x}_i;\boldsymbol\theta),
\end{array}
\right.
$$
where $\boldsymbol\theta$ is a vector of parameters to be estimated.

Of course, for all entities ($i$), we must have:
$$
\sum_{j=1}^J g_j(\bv{x}_i;\boldsymbol\theta)=1.
$$

Our objective is to estimate the vector of population parameters $\boldsymbol\theta$ given functional forms for the $g_j$s.

Multiple-choice models can for instance be used to account for

*Opinions: strongly opposed / opposed / neutral / support (ranked choices, see Slide\,\ref{slide:ordered}),
* Occupational field: lawyer / farmer / engineer / doctor / ...,
* Alternative shopping areas,
* Transportation types.

In a few cases, the values associated with the choices will themselves be meaningful, for example, number of accidents per day: $y = 0, 1,2, \dots$ (count data). In most cases, the values are meaningless.

### Ordered case

Sometimes, there exist a natural order for the different alternatives. This is typically the case of opinion surveys, where respondents have to choose between 

A first situation is the one 

\begin{frame}{The Ordered Case}\label{slide:ordered}
\begin{scriptsize}
\begin{itemize}
\item The {\color{blue} ordered case} is a direct extension of the binary case (Section\,\ref{section:Binary}).
\item In the {\color{blue} ordered multinomial case}, the dependent variable can take $J$ different values that feature a natural ordering.
\item Examples: income buckets, opinions ($-2$, $-1$, $0$, $+1$, $+2$, say), (credit) ratings.
%	\item We have:
%	$$
%	y_{i} \in \{1,\dots,J\}.
%	$$
\item A convenient approach consists in considering a latent variable $y_i^*$ as in Slide\,\ref{slide:latentfactor}:
$$
y_{i}^* = \boldsymbol\theta'\bv{x}_i + \varepsilon_i,
$$
and to assume that $\exists\, \alpha_j$, $j \in \{1,\dots,J-1\}$, s.t. $\forall (i,j) \in \mathbb{N}^+ \times \{1,\dots,J\}$:
\begin{eqnarray*}
\mathbb{P}(y_i = j | \bv{x}_i) &=& \mathbb{P}(\alpha_{j-1} <y^*_i < \alpha_{j} |\bv{x}_i) \\
&=& \mathbb{P}(\alpha_{j-1} - \boldsymbol\theta'\bv{x}_i  <\varepsilon_i < \alpha_{j} - \boldsymbol\theta'\bv{x}_i) \\
&=& F(\alpha_{j} - \boldsymbol\theta'\bv{x}_i) - F(\alpha_{j-1} - \boldsymbol\theta'\bv{x}_i),
\end{eqnarray*}
where $F$ is the c.d.f. of $\varepsilon_i$, $\alpha_0 = - \infty$ and $\alpha_J = + \infty$.
\end{itemize}
\end{scriptsize}
\end{frame}



\begin{frame}{The Ordered Case}
\begin{scriptsize}
\begin{itemize}
\item If one component of $\bv{x}_i$ ($\forall i$) is 1 (intercept), then one of the $\alpha_j$ ($j\in\{1,\dots,J-1\}$) is not identified. (One can then arbitrarily set $\alpha_1=0$, which is done in the binary logit/probit cases.)
\item The log-likelihood is given by Eq.\,(\ref{eq:multipleLogLik}).
\item We have:
$$
\mathbb{P}(y_i \le j | \bv{x}_i) = F(\alpha_{j} - \boldsymbol\theta'\bv{x}_i) \Rightarrow \frac{\partial \mathbb{P}(y_i \le j | \bv{x}_i)}{\bv{x}_i} =- \underbrace{F'(\alpha_{j} - \boldsymbol\theta'\bv{x}_i)}_{>0}\boldsymbol\theta.
$$
Hence the sign of $\theta_k$ indicates whether $\mathbb{P}(y_i \le j | \bv{x}_i)$ increases or decreases w.r.t. $x_{i,k}$ (the $k^{th}$ component of $\bv{x}_i$).
\item By contrast:
$$
\frac{\partial \mathbb{P}(y_i = j | \bv{x}_i)}{\bv{x}_i} = \underbrace{\left(-F'(\alpha_{j} + \boldsymbol\theta'\bv{x}_i)+F'(\alpha_{j-1} + \boldsymbol\theta'\bv{x}_i)\right)}_{A}\boldsymbol\theta,
$$
therefore the signs of the components of $\boldsymbol\theta$ are not necessarily those of the marginal effects. (For the sign of $A$ is a priori unknown.)
\end{itemize}
\end{scriptsize}
\end{frame}



<!-- \begin{specialframe} -->
<!-- \begin{tiny} -->
<!-- \begin{table}[!htbp] \centering  -->
<!--   \caption{Lending Club dataset (see Example\,\ref{exmpl:lendingclub})}  -->
<!--   \label{}  -->
<!-- \begin{tabular}{@{\extracolsep{5pt}}lccc}  -->
<!-- \\[-1.8ex]\hline  -->
<!-- \hline \\[-1.8ex]  -->
<!--  & \multicolumn{3}{c}{\textit{Dependent variable:}} \\  -->
<!-- \cline{2-4}  -->
<!-- \\[-1.8ex] & \multicolumn{3}{c}{grade.ordered} \\  -->
<!-- \\[-1.8ex] & (1) & (2) & (3)\\  -->
<!-- \hline \\[-1.8ex]  -->
<!--  y\textgreater =B & 1.042$^{***}$ & 1.032$^{***}$ & 1.037$^{***}$ \\  -->
<!--   & (0.040) & (0.038) & (0.031) \\  -->
<!--   y\textgreater =C & $-$0.228$^{***}$ & $-$0.237$^{***}$ & $-$0.227$^{***}$ \\  -->
<!--   & (0.039) & (0.037) & (0.029) \\  -->
<!--   y\textgreater =D & $-$1.497$^{***}$ & $-$1.507$^{***}$ & $-$1.490$^{***}$ \\  -->
<!--   & (0.040) & (0.039) & (0.031) \\  -->
<!--   y\textgreater =E & $-$3.155$^{***}$ & $-$3.164$^{***}$ & $-$3.139$^{***}$ \\  -->
<!--   & (0.047) & (0.046) & (0.039) \\  -->
<!--   loan\_amnt & $-$7.963$^{***}$ & $-$7.950$^{***}$ &  \\  -->
<!--   & (1.727) & (1.727) &  \\  -->
<!--   income.2.loan & 0.013$^{***}$ & 0.013$^{***}$ &  \\  -->
<!--   & (0.002) & (0.002) &  \\  -->
<!--   emp\_length\_high10y & $-$0.023 &  &  \\  -->
<!--   & (0.027) &  &  \\  -->
<!--   home\_ownership=OWN & 0.215$^{***}$ & 0.215$^{***}$ & 0.211$^{***}$ \\  -->
<!--   & (0.041) & (0.041) & (0.041) \\  -->
<!--   home\_ownership=RENT & 0.402$^{***}$ & 0.406$^{***}$ & 0.409$^{***}$ \\  -->
<!--   & (0.029) & (0.028) & (0.028) \\  -->
<!--   annual\_inc & $-$2.093$^{***}$ & $-$2.099$^{***}$ & $-$1.816$^{***}$ \\  -->
<!--   & (0.259) & (0.259) & (0.218) \\  -->
<!--   term= 60 months & 1.432$^{***}$ & 1.431$^{***}$ & 1.267$^{***}$ \\  -->
<!--   & (0.033) & (0.033) & (0.029) \\  -->
<!--  \hline \\[-1.8ex]  -->
<!-- Observations & 19,596 & 19,596 & 19,596 \\  -->
<!-- R$^{2}$ & 0.116 & 0.116 & 0.108 \\  -->
<!-- $\chi^{2}$ & 2,307.324$^{***}$ (df = 7) & 2,306.576$^{***}$ (df = 6) & 2,125.180$^{***}$ (df = 4) \\  -->
<!-- \hline  -->
<!-- \hline \\[-1.8ex]  -->
<!-- \textit{Note:}  & \multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\  -->
<!-- \end{tabular}  -->
<!-- \end{table} -->
<!-- \end{tiny} -->
<!-- \end{specialframe} -->





<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- %\addtocounter{exmpl}{-1} -->
<!-- \begin{exerc}[Political information] -->
<!-- Using the data available \href{https://vincentarelbundock.github.io/Rdatasets/csv/pscl/politicalInformation.csv}{here} and documented \href{https://vincentarelbundock.github.io/Rdatasets/doc/pscl/politicalInformation.html}{here}, propose a model for the political information level. -->
<!-- \end{exerc} -->

<!-- \vspace{.5cm} -->

<!-- \begin{exerc}[Wine quality] -->
<!-- Using the data available \href{https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv}{here} and documented \href{https://archive.ics.uci.edu/ml/datasets/Wine+Quality}{here}, propose a model for wine quality. -->
<!-- \end{exerc} -->

<!-- \end{scriptsize} -->
<!-- \end{frame} -->







<!-- \subsection{General multinomial model} -->


<!-- \begin{frame}{The Different Types of Regressors}\label{slide:typeregressors} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item Regressors may be {\color{blue}alternative-specific} or {\color{blue}alternative invariant} (see also Slide\,\ref{slide:Avregressors}). -->
<!-- 	\item Fishing data: prices and catch rates are alternative-specific, income is alternative-invariant. -->
<!-- 	 \item Notations: as in Slide\,\ref{slide:Avregressors}, that is ... -->
<!-- 	 \begin{itemize} -->
<!-- 	 	\item $\bv{u}_{i,j}$: vector of variables associated with agent $i$ and alternative $j$ (alternative-specific regressors). -->

<!-- 		\vspace{.1cm} -->
<!-- 		Example: Travel time per type of transportation (transportation choice), wage per type of work, cost per type of car. -->
<!-- 	 	\item $\bv{v}_{i}$: vector of variables associated with agent $i$ but alternative-invariant. -->

<!-- 		\vspace{.1cm} -->
<!-- 		Example: age or gender of agent $i$,  -->
<!-- 	 \end{itemize} -->
<!-- 	 \item We may e.g. organize $\bv{x}_i$ as follows: -->
<!-- 	 \begin{equation}\label{eq:x_organiz} -->
<!-- 	 \bv{x}_i = [\bv{u}_{i,1}',\dots,\bv{u}_{i,J}',\bv{v}_{i}']'. -->
<!-- 	 \end{equation} -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->

<!-- \begin{frame}{``General'' Multinomial Logit} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item General formulation of a multinomial logit: -->
<!-- 	\begin{equation}\label{eq:GeneralMNL} -->
<!-- 	g_j(\bv{x}_i;\boldsymbol\theta) = \frac{\exp(\theta_j'\bv{x}_i)}{\sum_{k=1}^J \exp(\theta_k'\bv{x}_i)}. -->
<!-- 	\end{equation} -->
<!-- 	(natural extension of the binary logit model, see Table\,\ref{table:differentspecif}.) -->
<!-- 	\item Note: By construction, $g_j(\bv{x}_i;\boldsymbol\theta) \in [0,1]$ and $\sum_{j}g_j(\bv{x}_i;\boldsymbol\theta)=1$. -->
<!-- 	\item If $\bv{x}_i$ organized as in Eq.\,(\ref{eq:x_organiz}), then, with obvious notations, $\theta_j$ is of the form: -->
<!-- 	\begin{equation}\label{eq:theta_organiz} -->
<!-- 	 \theta_j = [{\theta^{(u)}_{1,j}}',\dots,{\theta^{(u)}_{J,j}}',{\theta_j^{(v)}}']', -->
<!-- 	 \end{equation} -->
<!-- 	 (and $\boldsymbol\theta=[\theta_1',\dots,\theta_J']'$). -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->

<!-- \begin{frame}{Classical specifications}\label{slide:differentLogitspecif} -->
<!-- \begin{scriptsize} -->

<!-- \begin{itemize} -->
<!-- 	\item {\color{blue}Conditional logit (CL)} with alternative-varying regressors: -->
<!-- 	\begin{equation}\label{eq:theta_organizCL} -->
<!-- 	 \theta_j = [\bv{0}',\dots,\bv{0}',\underbrace{\boldsymbol\beta'}_{\mbox{j$^{th}$ position}},\bv{0}',\dots]', -->
<!-- 	 \end{equation} -->
<!-- 	 i.e. we have $\boldsymbol\beta=\theta^{(u)}_{1,1}=\dots=\theta^{(u)}_{J,J}$ and $\theta^{(u)}_{i,j}=0$ for $i \ne j$. -->
<!-- 	 \item {\color{blue}Multinomial logit (MNL)} with alternative-invariant regressors: -->
<!-- 	\begin{equation}\label{eq:theta_organizML} -->
<!-- 	 \theta_j = \left[\bv{0}',\dots,\bv{0}',{\theta_j^{(v)}}'\right]', -->
<!-- 	 \end{equation} -->
<!-- 	 (can embed intercepts.) -->
<!-- 	 \item {\color{blue}Mixed logit}: -->
<!-- 	\begin{equation}\label{eq:theta_organizCL} -->
<!-- 	 \theta_j = \left[\bv{0}',\dots,\bv{0}',\boldsymbol\beta',\bv{0}',\dots,\bv{0}',{\theta_j^{(v)}}'\right]'. -->
<!-- 	 \end{equation} -->
<!-- 	 \item Remark: The labelling ``CL'' and ``MNL'' -- used in the literature -- are relatively \textit{ad hoc} (see 15.4.1 in \href{http://cameron.econ.ucdavis.edu/mmabook/mma.html}{Cameron and Trivedi}). -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


<!-- % ================================= -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- % \setlength\extrarowheight{-2pt}  -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- % ================================= -->


<!-- \begin{specialframe} -->
<!-- \begin{tiny} -->
<!-- % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu -->
<!-- % Date and time: Fri, Dec 27, 2019 - 18:04:51 -->
<!-- \begin{table}[!htbp] \centering  -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- \setlength\extrarowheight{-1.5pt}  -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- % ================================= -->
<!--   \caption{Fishing Mode Multinomial Choice (replicates Table\,15.2 of \href{http://cameron.econ.ucdavis.edu/mmabook/mma.html}{Cameron and Trivedi})} -->
<!--   \label{}  -->
<!-- \begin{tabular}{@{\extracolsep{5pt}}lcccc}  -->
<!-- \\[-1.8ex]\hline  -->
<!-- \hline \\[-1.8ex]  -->
<!--  & \multicolumn{4}{c}{\textit{Dependent variable:}} \\  -->
<!-- \cline{2-5}  -->
<!-- \\[-1.8ex] & \multicolumn{4}{c}{mode} \\  -->
<!-- \\[-1.8ex] & (1) & (2) & (3) & (4)\\  -->
<!-- \hline \\[-1.8ex]  -->
<!--  boat:(intercept) & 0.871$^{***}$ &  & 0.739$^{***}$ & 0.527$^{**}$ \\  -->
<!--   & (0.114) &  & (0.197) & (0.223) \\  -->
<!--   & & & & \\  -->
<!--  charter:(intercept) & 1.499$^{***}$ &  & 1.341$^{***}$ & 1.694$^{***}$ \\  -->
<!--   & (0.133) &  & (0.195) & (0.224) \\  -->
<!--   & & & & \\  -->
<!--  pier:(intercept) & 0.307$^{***}$ &  & 0.814$^{***}$ & 0.778$^{***}$ \\  -->
<!--   & (0.115) &  & (0.229) & (0.220) \\  -->
<!--   & & & & \\  -->
<!--  price & $-$0.025$^{***}$ & $-$0.020$^{***}$ &  & $-$0.025$^{***}$ \\  -->
<!--   & (0.002) & (0.001) &  & (0.002) \\  -->
<!--   & & & & \\  -->
<!--  catch & 0.377$^{***}$ & 0.953$^{***}$ &  & 0.358$^{***}$ \\  -->
<!--   & (0.110) & (0.089) &  & (0.110) \\  -->
<!--   & & & & \\  -->
<!--  boat:income &  &  & 0.0001$^{**}$ & 0.0001$^{*}$ \\  -->
<!--   &  &  & (0.00004) & (0.0001) \\  -->
<!--   & & & & \\  -->
<!--  charter:income &  &  & $-$0.00003 & $-$0.00003 \\  -->
<!--   &  &  & (0.00004) & (0.0001) \\  -->
<!--   & & & & \\  -->
<!--  pier:income &  &  & $-$0.0001$^{***}$ & $-$0.0001$^{**}$ \\  -->
<!--   &  &  & (0.0001) & (0.0001) \\  -->
<!--   & & & & \\  -->
<!-- \hline \\[-1.8ex]  -->
<!-- Observations & 1,182 & 1,182 & 1,182 & 1,182 \\  -->
<!-- R$^{2}$ & 0.178 & 0.014 & 0.189 &  \\  -->
<!-- Log Likelihood & $-$1,230.784 & $-$1,311.980 & $-$1,477.151 & $-$1,215.138 \\  -->
<!-- LR Test & 533.878$^{***}$ (df = 5) &  & 41.145$^{***}$ (df = 6) & 565.171$^{***}$ (df = 8) \\  -->
<!-- \hline  -->
<!-- \hline \\[-1.8ex]  -->
<!-- \textit{Note:}  & \multicolumn{4}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\  -->
<!-- & \multicolumn{4}{l}{Data description: see Slide\,\ref{slide:fishingdata}.} -->
<!-- \end{tabular}  -->
<!-- \end{table} -->
<!-- \end{tiny} -->
<!-- \end{specialframe} -->


<!-- \subsection{ML estimation} -->

<!-- \begin{frame}{Maximum Likelihood Estimation (MLE)} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item Consider the general model described in Slide\,\ref{slide:IntroMNL}. -->
<!-- 	\item It can be noted that: -->
<!-- 	$$ -->
<!-- 	f(y_i|\bv{x}_i;\boldsymbol\theta) = \prod_{j=1}^J g_j(\bv{x}_i;\boldsymbol\theta)^{\mathds{1}_{\{y_i=j\}}}, -->
<!-- 	$$ -->
<!-- 	or that -->
<!-- 	$$ -->
<!-- 	\log f(y_i|\bv{x}_i;\boldsymbol\theta) = \sum_{j=1}^J \mathds{1}_{\{y_i=j\}} \log \left(g_j(\bv{x}_i;\boldsymbol\theta)\right). -->
<!-- 	$$ -->
<!-- 	\item The log-likelihood function is therefore given by: -->
<!-- 	\begin{equation}\label{eq:multipleLogLik} -->
<!-- 	\log \mathcal{L}(\boldsymbol\theta;\bv{y},\bv{X}) = \sum_{i=1}^n  \sum_{j=1}^J \mathds{1}_{\{y_i=j\}} \log \left(g_j(\bv{x}_i;\boldsymbol\theta)\right). -->
<!-- 	\end{equation} -->
<!-- 	\item Numerical maximization. -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->




<!-- \begin{frame}{Marginal Effects} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item Let's use the notation $p_{i,j} \equiv \mathbb{P}(y_i=j|\bv{x}_i;\boldsymbol\theta)$. -->
<!-- 	\item According to the general logit formulation of Eq.\,(\ref{eq:GeneralMNL}), we have: -->
<!-- 	\begin{eqnarray*} -->
<!-- 	\frac{\partial p_{i,j}}{\partial x_{i,s}} &=& \frac{\theta_{j,s}\exp(\theta_j'\bv{x}_i)\sum_{k=1}^J \exp(\theta_k'\bv{x}_i)}{(\sum_{k=1}^J \exp(\theta_k'\bv{x}_i))^2} \\ -->
<!-- 	&& - \frac{\exp(\theta_j'\bv{x}_i)\sum_{k=1}^J \theta_{k,s} \exp(\theta_k'\bv{x}_i)}{(\sum_{k=1}^J \exp(\theta_k'\bv{x}_i))^2}\\ -->
<!-- 	&=& \theta_{j,s} p_{i,j} - \sum_{k=1}^J \theta_{k,s} p_{i,j}p_{i,k}\\ -->
<!-- 	&=&  p_{i,j} \times \Big(\theta_{j,s} - \underbrace{\sum_{k=1}^J \theta_{k,s} p_{i,k}}_{=\overline{\boldsymbol{\theta}}^{(i)}_{s}}\Big), -->
<!-- 	\end{eqnarray*} -->
<!-- 	where $\overline{\boldsymbol\theta}^{(i)}_{s}$ does not depend on $j$. -->
<!-- 	\item[$\Rightarrow$] The sign of the marginal effect is not necessarily that of $\theta_{j,s}$. -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


<!-- \subsection{Utility models} -->

<!-- \begin{frame}{Relationship with Utility Models}\label{slide:UMmulti} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item More general models can be defined. Notably by resorting to {\color{blue} Additive Random Utility Models, ARUM} (see Slide\,\ref{slide:ARUM_Binary}). -->
<!-- 	\item Let's drop the $i$ subscript for simplicity. -->
<!-- 	\item Utility derived form choosing $j$: $U_j = V_j + \varepsilon_j$. -->
<!-- 	\item We have (with obvious notations): -->
<!-- 	\begin{eqnarray*} -->
<!-- 	\mathbb{P}(y=j) &=& \mathbb{P}(U_j>U_k,\,\forall k \ne j)\\ -->
<!-- 	\mathbb{P}(y=j) &=& \mathbb{P}(U_k-U_j<0,\,\forall k \ne j)\\ -->
<!-- 	\mathbb{P}(y=j) &=& \mathbb{P}(\underbrace{\varepsilon_k-\varepsilon_j}_{=:\tilde\varepsilon_{k,j}}<\underbrace{V_j - V_k}_{=:-\tilde{V}_{k,j}},\,\forall k \ne j). -->
<!-- 	\end{eqnarray*} -->
<!-- 	\item The last expression is an $(J-1)$-variate integral. It has, in general, no analytical solution.  -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->



<!-- \begin{frame}{Utility Models and the Multinomial Logit} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item Gumbel distribution ($\mathcal{W}$), support: $\mathbb{R}$ (see Theorem\,\ref{thm:Extreme}): -->
<!-- 	$$ -->
<!-- 	F(u) = \exp(-\exp(-u)), \qquad f(u)=\exp(-u-\exp(u)). -->
<!-- 	$$ -->
<!-- 	\item If $X\sim\mathcal{W}$, then $\mathbb{E}(X)=0.577$ (Euler constant, see box below) and $\mathbb{V}ar(X)=\pi^2/6$. -->
<!-- 	\end{itemize} -->
<!-- \begin{prop}[Utility Model with a Gumbel distribution]\label{prop:Weibull} -->
<!-- In the context of the utility model described in Slide\,\ref{slide:UMmulti}, if $\varepsilon_j \sim \,i.i.d.\,\mathcal{W}$, then  -->
<!-- 	$$ -->
<!-- 	\mathbb{P}(y=j) = \frac{\exp(V_j)}{\sum_{k=1}^J \exp(V_k)}. -->
<!-- 	$$ -->
<!-- \end{prop} -->
<!-- \begin{tiny} -->
<!-- {\color{blue} Proof:} Slide\,\ref{slide:proofWeibull}. -->
<!-- \end{tiny} -->

<!-- \vspace{.2cm} -->

<!-- \begin{block}{Euler constant} -->
<!-- $$ -->
<!-- \gamma = \lim_{n\rightarrow \infty} \left(- \ln(n) + \sum_{k=1}^n \frac{1}{k}\right) -->
<!-- $$ -->
<!-- \end{block} -->

<!-- \end{scriptsize} -->
<!-- \end{frame} -->



<!-- \begin{frame}\label{slide:proofWeibull} -->
<!-- \begin{tiny} {\color{blue} Proof. of Prop.\,\ref{prop:Weibull}} We have: -->
<!-- \begin{eqnarray*} -->
<!-- \mathbb{P}(y=j) &=& \mathbb{P}(\forall\,k \ne j,\,U_k < U_j) =  \mathbb{P}(\forall\,k \ne j,\,\varepsilon_k < V_j - V_k + \varepsilon_j) \\ -->
<!--  &=& \int \prod_{k \ne j} F(V_j - V_k + \varepsilon) f(\varepsilon)d\varepsilon. -->
<!-- \end{eqnarray*} -->
<!-- After computation, it comes that -->
<!-- $$ -->
<!-- \prod_{k \ne j} F(V_j - V_k + \varepsilon) f(\varepsilon) = \exp\left[-\varepsilon-\exp(-\varepsilon+\lambda_j)\right], -->
<!-- $$ -->
<!-- where $\lambda_j = \log\left(1 + \frac{\sum_{k \ne j} \exp(V_k)}{\exp(V_j)}\right)$. We then have: -->
<!-- \begin{eqnarray*} -->
<!-- \mathbb{P}(y=j) &=& \int  \exp\left[-\varepsilon-\exp(-\varepsilon+\lambda_j)\right] d\varepsilon\\ -->
<!-- &=& \int  \exp\left[-t - \lambda_j-\exp(-t)\right] d\varepsilon = \exp(- \lambda_j).\qed -->
<!-- \end{eqnarray*} -->
<!-- \end{tiny} -->
<!-- \end{frame} -->




<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->

<!-- \begin{block}{Gumbel distribution} -->
<!-- 		\begin{figure} -->
<!-- 		\caption{C.d.f. of Gumbel distribution} -->
<!-- 			\includegraphics[width=.8\linewidth]{../figures/Figure_Weibull.pdf} -->

<!-- 			\begin{tiny} -->
<!-- 			\begin{center} -->
<!-- 			See Theorem\,\ref{thm:Extreme}. The C.d.f. is: $x \rightarrow \exp(-\exp(x))$. -->
<!-- 			\end{center} -->
<!-- 			\end{tiny} -->
<!-- 		\end{figure} -->
<!-- \end{block} -->

<!-- \end{scriptsize} -->
<!-- \end{frame} -->





<!-- \begin{frame}{Remarks on identification} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item We have: -->
<!-- 	\begin{eqnarray*} -->
<!-- 	\mathbb{P}(y=j) &=& \frac{\exp(V_j)}{\sum_{k=1}^J \exp(V_k)}\\ -->
<!-- 	&=& \frac{\exp(V^*_j)}{1 + \sum_{k=2}^J \exp(V^*_k)}, -->
<!-- 	\end{eqnarray*} -->
<!-- 	where $V^*_j = V_j - V_1$. -->
<!-- 	\item[$\Rightarrow$] We can always assume that $V_{1}=0$. -->
<!-- 	\item Case where $V_{i,j} = \theta_j'\bv{x}_i = \boldsymbol\beta'\bv{u}_{i,j}+{\theta_j^{(v)}}'\bv{v}_i$ (see Eqs.\,\ref{eq:x_organiz} and \ref{eq:theta_organizCL}): we can assume that  -->
<!-- 	$$ -->
<!-- 	\begin{array}{clll} -->
<!-- 	(A) & \bv{u}_{i,1}&=&0,\\ -->
<!-- 	(B) & \theta_1^{(v)} &=& 0. -->
<!-- 	\end{array} -->
<!-- 	$$ -->
<!-- 	If (A) does not hold, we can replace $\bv{u}_{i,j}$ by $\bv{u}_{i,j}-\bv{u}_{i,1}$. -->
<!-- 	\item If $J=2$ and $j \in \{0,1\}$ (shift by one unit), we have $\mathbb{P}(y=1|\bv{x})=\dfrac{\exp(\boldsymbol\theta'\bv{x})}{1+\exp(\boldsymbol\theta'\bv{x})}$, this is the logit model (Table\,\ref{table:differentspecif}). -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->



<!-- \subsection{Limitations} -->

<!-- \begin{frame}{Limitation of Logit Models}\label{slide:limitLogit} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item In a Logit model: -->
<!-- 	\begin{equation}\label{eq:condiProba} -->
<!-- 	\mathbb{P}(y=j|y \in \{k,j\}) = \frac{\exp(\theta_j'\bv{x})}{\exp(\theta_j'\bv{x}) + \exp(\theta_k'\bv{x})}. -->
<!-- 	\end{equation} -->
<!-- 	\item This conditional proba does not depend on other alternatives (i.e. does not depend on $\theta_m$, $m \ne j,k$). -->
<!-- 	\item In particular, if $\bv{x} = [\bv{u}_1',\dots,\bv{u}_J',\bv{v}']'$ (see Slide\,\ref{slide:typeregressors}), then changes in $\bv{u}_m$ ($m \ne j,\,k$) have no impact on (\ref{eq:condiProba}). -->
<!-- 	\item[$\Rightarrow$] A Multinomial Logit  = series of pairwise comparisons that are unaffected by the characteristics of alternatives. -->
<!-- 	\item Important properties of multinomial logit model is {\color{blue}independence from irrelevant alternatives (IIA)}. -->

<!-- 	IIA property: for any individual, the ratio of probabilities of choosing two alternatives is independent of the availability or attributes of any other alternatives. -->
<!-- 	\item In this model, the choice of commuting by car or (red) bus is independent of whether commuting using a blue bus is an option. -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \begin{remark}[Red-Blue Buses]\label{remark:redblue} -->
<!-- \begin{itemize} -->
<!-- 	\item Assume one has a logit modelling the ``car ($y=1$) vs red bus ($y=2$)'' choice. -->

<!-- 	\item If a blue bus ($y=3$) is exactly as a red bus, except for the color, then one would expect to have: -->
<!-- 	$$ -->
<!-- 	\mathbb{P}(y=3|y \in \{2,3\}) = 0.5, -->
<!-- 	$$ -->
<!-- 	i.e. $\theta_2 = \theta_3$. -->
<!-- 		%\item Introducing a blue bus should not affect $\mathbb{P}(y=1)$, and we would expect to have an increase of $\mathbb{P}(y=1|y \in \{1,2\})$ by $100\%$. This cannot be obtained with this parameterization. ADDRESSED BY OTHER MODELS? -->
<!-- 	\item Assume we had $V_1=V_2$. We expect to have $V_2=V_3$ (hence $p_2=p_3$). The model will then  imply $p_1=p_2=p_3=0.33$. -->

<!-- 	It would seem more reasonable to have $p_1 = p_2 + p_3 = 0.5$ and $p_2=p_3=0.25$. -->
<!-- \end{itemize} -->
<!-- \end{remark} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->




<!-- \subsection{Sequential models} -->


<!-- \begin{frame}{Sequential Models} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item Naturally, other possible (\textit{ad hoc}) modelling strategy, e.g. sequential modelling: -->
<!-- 	\begin{eqnarray*} -->
<!-- 	\mathbb{P}(y=1|\bv{x}) &=& F(\theta_1'\bv{x}) \\ -->
<!-- 	\mathbb{P}(y=2|\bv{x}) &=& (1-F(\theta_1'\bv{x})) \times F(\theta_2'\bv{x}) \\ -->
<!-- 	\mathbb{P}(y=3|\bv{x}) &=& (1-F(\theta_1'\bv{x})) \times (1 - F(\theta_2'\bv{x})). -->
<!-- 	\end{eqnarray*} -->
<!-- 	\item Easily checked that $\sum_j \mathbb{P}(y=j|\bv{x})=1$. -->
<!-- 	\item Example: car purchase; 1st choice = brand, 2nd choice = color. -->
<!-- 	\item Likelihood function: -->
<!-- 	\begin{eqnarray*} -->
<!-- 		\mathcal{L}(\theta_1,\theta_2;\bv{y},\bv{X}) &=& \prod_{j \in \{1,2,3\}} \left( \prod_{i\,s.t.\,y_i=j} \mathbb{P}(y_i=j|\bv{x}_i)\right). -->
<!-- 	\end{eqnarray*} -->
<!-- 	\item The maximization of the log-likelihood can be performed sequentially (w.r.t. to $\theta_1$ first, and next w.r.t. $\theta_2$). -->
<!-- 	\item Weaknesses: results may depend on the chosen sequence + no clear mapping with utility framework. -->
<!-- 	\item Alternative: nested logit model (Slide\,\ref{slide:nested}) where utilities of alternatives 2 and 3 are affected by correlated shocks. -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->



<!-- \subsection{Nested logits} -->


<!-- \begin{frame}{Nested Logits}\label{slide:nested} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item Nested Logits are natural extensions of logit models when choices feature a nesting structure. -->
<!-- 	\item Important assumption: $\exists$ different alternatives that individuals choose sequentially. -->

<!-- 	\item Clustering of choices in ``nests'', defined by the specification. -->

<!-- 	\item Idea of unobserved agent-specific and nest-specific variable. -->

<!-- %	\item Choice of selecting $j_1 \in \{1,\dots,J_1\}$ and then $j_2 \in \{1,\dots,J_2\}$ is given by: -->
<!-- %	$$ -->
<!-- %	\mathbb{P}(y_1 = j_1,y_2=j_2) = \underbrace{\mathbb{P}(y_2=j_2|y_1 = j_1)}_{\mbox{2$^{nd}$ logit}}\underbrace{\mathbb{P}(y_1 = j_1)}_{\mbox{1$^{st}$ logit}}. -->
<!-- %	$$ -->
<!-- 	\item Common vocabulary: $J$ ``limbs''. For each limb $j$, $K_j$ ``branches''. -->
<!-- 	\item Utility interpretation: -->
<!-- 	\begin{center} -->
<!-- 	\{Limb $j$ (choice = $y_1$) and branch $k$ (choice = $y_2$) \} -->

<!-- 	$\rightarrow$ -->

<!-- 	Utility $U_{j,k} = V_{j,k} + \varepsilon_{j,k}$ -->
<!-- 	\end{center} -->

<!-- 	and: -->
<!-- 	$$ -->
<!-- 	\mathbb{P}[(y_1,y_2) = (j,k)|\bv{x}] = \mathbb{P}(U_{j,k}>U_{l,m},\,(l,m) \ne (j,k)|\bv{x}). -->
<!-- 	$$ -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->





<!-- \begin{frame}{Nested Logits} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item[(i)] Specification of the deterministic part of utility: -->
<!-- 	$$ -->
<!-- 	V_{j,k} = \bv{u}_j'\boldsymbol\alpha + \bv{v}_{j,k}'\boldsymbol\beta_j, -->
<!-- 	$$ -->
<!-- 	where $\boldsymbol\alpha$ is common to all nests and $\boldsymbol\beta_j$'s are nest-specific. -->
<!-- 	\item[(ii)] Disturbances $\boldsymbol\varepsilon$ follows the Generalized Extreme Value (GEV) distribution (see Def.\,\ref{defn:GEVdistri}). -->

<!-- 	\item Under (i) and (ii): -->
<!-- 	\begin{eqnarray}\label{eq:Nested} -->
<!-- 	\mathbb{P}[(y_1,y_2) = (j,k)|\bv{x}] &=& \underbrace{\frac{\exp(\bv{u}_j'\boldsymbol\alpha + \rho_j I_j)}{\sum_{m=1}^J \exp(\bv{u}_m'\boldsymbol\alpha + \rho_m I_m)}}_{= \mathbb{P}[y_1 = j|\bv{x}]} \times \nonumber\\ -->
<!-- 	&& \underbrace{\frac{\exp(\bv{v}_{j,k}'\boldsymbol\beta_j/\rho_j)}{\sum_{l=1}^{K_j} \exp(\bv{v}_{j,l}'\boldsymbol\beta_j/\rho_j)}}_{= \mathbb{P}[y_2 = k|y_1=j,\bv{x}]}, -->
<!-- 	\end{eqnarray} -->
<!-- 	where $I_j$'s are {\color{blue}inclusive value} or {\color{blue}log sum}, given by: -->
<!-- 	$$ -->
<!-- 	I_j = \log \left( \sum_{l=1}^{K_j} \exp(\bv{v}_{j,l}'\boldsymbol\beta_j/\rho_j)\right). -->
<!-- 	$$ -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->



<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \begin{defn}[Generalized Extreme Value (GEV) distribution, \href{http://onlinepubs.trb.org/Onlinepubs/trr/1978/673/673-012.pdf}{McFadden (1978)}]\label{defn:GEVdistri} -->
<!-- 	The vector of disturbances $\boldsymbol\varepsilon=[\varepsilon_{1,1},\dots,\varepsilon_{1,K_1},\dots,\varepsilon_{J,1},\dots,\varepsilon_{J,K_J}]'$ follows the Generalized Extreme Value (GEV) distribution if its c.d.f. is: -->
<!-- 	$$ -->
<!-- 	F(\boldsymbol\varepsilon,\boldsymbol\rho) = \exp(-G(e^{-\varepsilon_{1,1}},\dots,e^{-\varepsilon_{J,K_J}};\boldsymbol\rho)) -->
<!-- 	$$ -->
<!-- 	with -->
<!-- 	\begin{eqnarray*} -->
<!-- 	G(\bv{Y};\boldsymbol\rho) &\equiv&  G(Y_{1,1},\dots,Y_{1,K_1},\dots,Y_{J,1},\dots,Y_{J,K_J};\boldsymbol\rho) \\ -->
<!-- 	&=& \sum_{j=1}^J\left(\sum_{k=1}^{K_j} Y_{jk}^{1/\rho_j} -->
<!-- 	\right)^{\rho_j} -->
<!-- 	\end{eqnarray*} -->

<!-- \end{defn} -->
<!-- \begin{remark}[]\label{remarks:GEV} -->
<!-- 	\begin{itemize} -->
<!-- 	\item[(a)] It can be shown that $\rho_j = \sqrt{1 - \mathbb{C}or(\varepsilon_{j,k},\varepsilon_{j,l})}$, for $k \ne l$. -->
<!-- 	\item[(b)] $\rho_j=1 \Rightarrow$, $\varepsilon_{j,k}$ and $\varepsilon_{j,l}$ are uncorrelated ($\Rightarrow$ Multinomial logit). -->
<!-- 	\item[(c)] When $J=1$:  -->
<!-- 	$$ -->
<!-- 	F([\varepsilon_1,\dots,\varepsilon_K]',\rho) = \exp\left(-\left(\sum_{k=1}^{K} \exp(-\varepsilon_k/\rho)\right)^{\rho}\right). -->
<!-- 	$$ -->
<!-- \end{itemize} -->
<!-- \end{remark} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->




<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->

<!-- 		\begin{figure} -->
<!-- 		\caption{Simulations of bivariate GEV (Def.\,\ref{defn:GEVdistri})} -->
<!-- 			\includegraphics[width=1\linewidth]{../figures/Figure_GEV.pdf} -->
<!-- 		\end{figure} -->

<!-- 		\begin{center} -->
<!-- 		\begin{tiny} -->
<!-- 		Note: Gumbel simulation method based on \href{http://www.caee.utexas.edu/prof/bhat/ABSTRACTS/Supp_material.pdf}{Bhat}. -->
<!-- 		\end{tiny} -->
<!-- 		\end{center} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->




<!-- \begin{frame}{Nested Logits} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item It can be shown that: -->
<!-- 		\begin{eqnarray*} -->
<!-- 	I_j = \mathbb{E}(\max_k(U_{j,k})) &=& \mathbb{E}(\max_k(V_{j,k} + \varepsilon_{j,k})), -->
<!-- 	\end{eqnarray*} -->
<!-- 	\item[$\Rightarrow$] Inclusive value = measure of the relative attractiveness of a nest.	 -->

<!-- 	\vspace{.4cm} -->
<!-- 	 \item Correlation across $\varepsilon_{j,k}$ (for a given $j$) $\Rightarrow$ existence of an (unobserved) ``common error component'' for the alternatives of a same nest = increased similarity between sets of nested alternatives. -->
<!-- 	 \item[$\Rightarrow$] Leads to a higher sensitivity (cross-elasticity) between alternatives. -->

<!-- 	 \item If the common component is reduced to zero (i.e. $\rho_i=1$), the model reduces to the multinomial logit model with no covariance of error terms among the alternatives. -->

<!-- %	\vspace{.4cm} -->
<!-- %	\item Page R: \href{https://cran.r-project.org/web/packages/mlogit/vignettes/e2nlogit.html}{tuto} -->
<!-- %	\item Applications: TravelMode ou bien Heating systems (voir dans biblio mLogitExercises.pdf). -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->


<!-- \begin{frame} -->
<!-- \begin{scriptsize} -->
<!-- \begin{remark}[Red-Blue Buses (extends Remark\,\ref{remark:redblue})] -->
<!-- \begin{itemize} -->
<!-- 	\item Nested logits can solve the Red-Blue problem described in Remark\,\ref{remark:redblue}. -->
<!-- 	\item Assume you have estimated a model specifying $U_{1} = V_{1} + \varepsilon_{1}$ (car choice) and $U_{2} = V_{2} + \varepsilon_{2}$ (red bus choice). -->
<!-- 	\item You can then assume that the blue-bus utility is of the form $U_{3} = V_{2} + \varepsilon_{3}$ where $\varepsilon_{3}$ is perfectly correlated to $\varepsilon_{2}$. -->
<!-- 	\item This is done by redefining the set of choices as follows: -->
<!-- 	\begin{eqnarray*} -->
<!-- 	j=1 &\Leftrightarrow& (j'=1,k=1) \\ -->
<!-- 	j=2 &\Leftrightarrow& (j'=2,k=1) \\ -->
<!-- 	j=3 &\Leftrightarrow& (j'=2,k=2), -->
<!-- 	\end{eqnarray*} -->
<!-- 	and by setting $\rho_2 \rightarrow 0$. -->
<!-- \end{itemize} -->
<!-- \end{remark} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->



<!-- \begin{frame}{Nested Logits} -->
<!-- \begin{scriptsize} -->
<!-- \begin{itemize} -->
<!-- 	\item IIA (see Slide\,\ref{slide:limitLogit}) holds within a nest, but not when considering alternatives in different nests. -->
<!-- 	\item Indeed (using Eq.\,\ref{eq:Nested}): -->
<!-- 	$$ -->
<!-- 	\frac{\mathbb{P}[y_1=j,y_2=k_A|\bv{x}] }{\mathbb{P}[y_1=j,y_2=k_B|\bv{x}]} = \frac{\exp(\bv{v}_{j,k_A}'\boldsymbol\beta_j/\rho_j)}{\exp(\bv{v}_{j,k_B}'\boldsymbol\beta_j/\rho_j)}, -->
<!-- 	$$ -->
<!-- 	i.e. we have IIA in nest $j$. -->

<!-- 	By contrast: -->
<!-- 	\begin{eqnarray*} -->
<!-- 	\frac{\mathbb{P}[y_1=j_A,y_2=k_A|\bv{x}] }{\mathbb{P}[y_1=j_B,y_2=k_B|\bv{x}]} &=& \frac{\exp(\bv{u}_{j_A}'\boldsymbol\alpha + \rho_{j_A} I_{j_A})\exp(\bv{v}_{{j_A},{k_A}}'\boldsymbol\beta_{j_A}/\rho_{j_A})}{\exp(\bv{u}_{j_B}'\boldsymbol\alpha + \rho_{j_B} I_{j_B})\exp(\bv{v}_{{j_B},{k_B}}'\boldsymbol\beta_{j_B}/\rho_{j_B})}\times\\ -->
<!-- 	&& \frac{\sum_{l=1}^{K_{j_B}} \exp(\bv{v}_{{j_B},l}'\boldsymbol\beta_{j_B}/\rho_{j_B})}{\sum_{l=1}^{K_{j_A}} \exp(\bv{v}_{{j_A},l}'\boldsymbol\beta_{J_A}/\rho_{j_A})}, -->
<!-- 	\end{eqnarray*} -->
<!-- 	which depends on the expected utilities of all alternatives in nest $j_A$ and $j_B$. So the IIA does not hold. -->
<!-- \end{itemize} -->
<!-- \end{scriptsize} -->
<!-- \end{frame} -->




<!-- \begin{frame} -->
<!-- \begin{tiny} -->

<!-- % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu -->
<!-- % Date and time: Sat, Dec 28, 2019 - 07:18:00 -->
<!-- \begin{table}[!htbp] \centering  -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- \setlength\extrarowheight{-1.8pt}  -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- % ================================= -->
<!--   \caption{Travel mode (\href{https://www.sciencedirect.com/science/article/pii/S0191261500000357}{Hensher and Greene, 2002})}  -->
<!--   \label{}  -->
<!-- \begin{tabular}{@{\extracolsep{5pt}}lcc}  -->
<!-- \\[-1.8ex]\hline  -->
<!-- \hline \\[-1.8ex]  -->
<!--  & \multicolumn{2}{c}{\textit{Dependent variable:}} \\  -->
<!-- \cline{2-3}  -->
<!-- \\[-1.8ex] & \multicolumn{2}{c}{choice} \\  -->
<!-- \\[-1.8ex] & (1) & (2)\\  -->
<!-- \hline \\[-1.8ex]  -->
<!--  train:(intercept) & $-$0.633 & 0.006 \\  -->
<!--   & (0.494) & (0.636) \\  -->
<!--   & & \\  -->
<!--  bus:(intercept) & $-$1.432$^{**}$ & $-$0.774 \\  -->
<!--   & (0.712) & (0.809) \\  -->
<!--   & & \\  -->
<!--  car:(intercept) & $-$6.507$^{***}$ & $-$6.154$^{***}$ \\  -->
<!--   & (1.148) & (1.178) \\  -->
<!--   & & \\  -->
<!--  gcost & $-$0.014$^{***}$ & $-$0.020$^{***}$ \\  -->
<!--   & (0.005) & (0.006) \\  -->
<!--   & & \\  -->
<!--  wait & $-$0.111$^{***}$ & $-$0.106$^{***}$ \\  -->
<!--   & (0.020) & (0.021) \\  -->
<!--   & & \\  -->
<!--  incomeother & 0.447$^{***}$ & 0.426$^{***}$ \\  -->
<!--   & (0.111) & (0.113) \\  -->
<!--   & & \\  -->
<!--  iv & 1.293$^{***}$ &  \\  -->
<!--   & (0.343) &  \\  -->
<!--   & & \\  -->
<!--  iv:public &  & 0.969$^{***}$ \\  -->
<!--   &  & (0.305) \\  -->
<!--   & & \\  -->
<!--  iv:other &  & 1.724$^{***}$ \\  -->
<!--   &  & (0.519) \\  -->
<!--   & & \\  -->
<!-- \hline \\[-1.8ex]  -->
<!-- Observations & 210 & 210 \\  -->
<!-- R$^{2}$ & 0.330 & 0.336 \\  -->
<!-- Log Likelihood & $-$190.178 & $-$188.433 \\  -->
<!-- LR Test & 187.162$^{***}$ (df = 7) & 190.652$^{***}$ (df = 8) \\  -->
<!-- \hline  -->
<!-- \hline \\[-1.8ex]  -->
<!-- \textit{Note:}  & \multicolumn{2}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\  -->
<!-- & \multicolumn{2}{l}{\href{https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/ModeChoice.html}{Dataset description}.} -->
<!-- \end{tabular}  -->
<!-- \end{table}  -->
<!-- \end{tiny} -->
<!-- \end{frame} -->



<!-- \begin{frame} -->
<!-- \begin{tiny} -->

<!-- % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu -->
<!-- % Date and time: Sun, Dec 29, 2019 - 07:10:45 -->
<!-- \begin{table}[!htbp] \centering  -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- \setlength\extrarowheight{-1.8pt}  -->
<!-- % ================================= -->
<!-- % ================================= -->
<!-- % ================================= -->

<!--   \caption{Heating/cooling mode}  -->
<!--   \label{}  -->
<!-- \begin{tabular}{@{\extracolsep{5pt}}lccc}  -->
<!-- \\[-1.8ex]\hline  -->
<!-- \hline \\[-1.8ex]  -->
<!--  & \multicolumn{3}{c}{\textit{Dependent variable:}} \\  -->
<!-- \cline{2-4}  -->
<!-- \\[-1.8ex] & \multicolumn{3}{c}{depvar} \\  -->
<!-- \\[-1.8ex] & (1) & (2) & (3)\\  -->
<!-- \hline \\[-1.8ex]  -->
<!-- % ecc:(intercept) & 0.055 & 2.171 & 2.180 \\  -->
<!-- %  & (3.163) & (3.402) & (3.390) \\  -->
<!-- %  & & & \\  -->
<!-- % er:(intercept) & $-$3.265$^{**}$ & $-$2.455$^{**}$ & $-$2.453$^{**}$ \\  -->
<!-- %  & (1.428) & (1.071) & (1.064) \\  -->
<!-- %  & & & \\  -->
<!-- % erc:(intercept) & $-$1.165 & 1.756 & 1.765 \\  -->
<!-- %  & (3.289) & (3.548) & (3.541) \\  -->
<!-- %  & & & \\  -->
<!-- % gc:(intercept) & 0.068 & $-$0.208 & $-$0.200 \\  -->
<!-- %  & (1.166) & (0.469) & (0.436) \\  -->
<!-- %  & & & \\  -->
<!-- % gcc:(intercept) & 0.705 & 2.234 & 2.240 \\  -->
<!-- %  & (3.176) & (3.384) & (3.367) \\  -->
<!-- %  & & & \\  -->
<!-- % hpc:(intercept) & $-$2.156 & 1.273 & 1.278 \\  -->
<!-- %  & (3.249) & (3.618) & (3.601) \\  -->
<!-- %  & & & \\  -->
<!--  occa & $-$1.159$^{*}$ & $-$0.966 & $-$0.967 \\  -->
<!--   & (0.700) & (0.708) & (0.708) \\  -->
<!--   & & & \\  -->
<!--  icca & $-$0.048 & $-$0.051 & $-$0.051 \\  -->
<!--   & (0.067) & (0.081) & (0.079) \\  -->
<!--   & & & \\  -->
<!--  och & $-$2.096$^{***}$ & $-$0.869$^{*}$ & $-$0.870$^{*}$ \\  -->
<!--   & (0.448) & (0.445) & (0.445) \\  -->
<!--   & & & \\  -->
<!--  ich & $-$0.354$^{***}$ & $-$0.205$^{**}$ & $-$0.205$^{**}$ \\  -->
<!--   & (0.064) & (0.091) & (0.091) \\  -->
<!--   & & & \\  -->
<!--  iv:cooling &  & 0.334$^{*}$ &  \\  -->
<!--   &  & (0.172) &  \\  -->
<!--   & & & \\  -->
<!--  iv:noncool &  & 0.329 &  \\  -->
<!--   &  & (0.212) &  \\  -->
<!--   & & & \\  -->
<!--  iv &  &  & 0.334$^{*}$ \\  -->
<!--   &  &  & (0.171) \\  -->
<!--   & & & \\  -->
<!-- \hline \\[-1.8ex]  -->
<!-- Observations & 250 & 250 & 250 \\  -->
<!-- R$^{2}$ & 0.144 & 0.165 & 0.165 \\  -->
<!-- Log Likelihood & $-$192.877 & $-$188.035 & $-$188.035 \\  -->
<!-- LR Test & 64.669$^{***}$ (df = 10) & 74.354$^{***}$ (df = 12) & 74.353$^{***}$ (df = 11) \\  -->
<!-- \hline  -->
<!-- \hline \\[-1.8ex]  -->
<!-- \textit{Note:}  & \multicolumn{3}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\  -->
<!-- & \multicolumn{3}{l}{\href{https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/HC.html}{Dataset description}.} -->
<!-- \end{tabular}  -->
<!-- \end{table} -->
<!-- \end{tiny} -->
<!-- \end{frame} -->


<!-- %\begin{frame}{Nested Logits} -->
<!-- %\begin{scriptsize} -->
<!-- %\begin{itemize} -->
<!-- %	\item Very good tuto on mLogit \href{http://www2.uaem.mx/r-mirror/web/packages/mlogit/vignettes/mlogit.pdf}{Croissant} -->
<!-- %	\item Page R: \href{https://cran.r-project.org/web/packages/mlogit/vignettes/e2nlogit.html}{tuto} -->
<!-- %	\item \href{http://onlinepubs.trb.org/Onlinepubs/trr/1993/1413/1413-011.pdf}{Pas mal:} The common error component, ee, for the nested alter- natives represents a covariance relationship that describes an increased similarity between pairs of nested alternatives and leads to a higher sensitivity (cross-elasticity) between alter- natives. If this common component, ee, is reduced to zero, the model reduces to the multinomial logit model with no covariance of error terms among the alternatives. -->
<!-- % -->
<!-- %	\item We are back to a standard MNL model when $\rho_i=1$. -->
<!-- %\end{itemize} -->
<!-- %\end{scriptsize} -->
<!-- %\end{frame} -->







